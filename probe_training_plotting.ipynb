{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6180d7-9c28-4589-b3b5-2aa691a5eebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannes/miniforge3/envs/maze-goal/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import jax\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "from einops import einsum, rearrange, reduce\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from flax.training import orbax_utils\n",
    "\n",
    "import orbax.checkpoint\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "from maze_dataset.plotting import MazePlot\n",
    "from maze_dataset.tokenization.token_utils import strings_to_coords\n",
    "\n",
    "from dataset import CustomMazeDataset\n",
    "from dataset import NumpyLoader\n",
    "\n",
    "from model import TransformerLM, TransformerConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc55bc5-a3fd-4443-bc89-cad58ee6070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config details\n",
    "checkpoint_path = \"data/2023-10-31_16-24-46\"\n",
    "base_path = \"data\"\n",
    "save = True\n",
    "\n",
    "np_seed = 0\n",
    "jnp_seed = 0\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "n_train_steps = 10000000\n",
    "\n",
    "save_every_n_steps = 1000\n",
    "keep_n_checkpoints = 100\n",
    "\n",
    "n_worker = 8\n",
    "\n",
    "# n_eval = 1024\n",
    "emb_dim: int = 256\n",
    "num_heads: int = 16\n",
    "num_layers: int = 12\n",
    "qkv_dim: int = 256  # 512\n",
    "mlp_dim: int = 1024  # 2048\n",
    "max_len = 256\n",
    "\n",
    "grid_n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77bdb3e3-461f-4947-85d9-287fdaf98ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading step 953000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    params = state['params']\n",
    "    opt_state = state['opt_state']\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, batch)\n",
    "    updates, opt_state = tx.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    step = state['step'] + 1\n",
    "\n",
    "    return {'params': params, 'opt_state': opt_state, 'loss': loss, 'step': step}\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    params = state['params']\n",
    "    loss = loss_fn(params, batch)\n",
    "    return loss\n",
    "\n",
    "dataset = CustomMazeDataset(include_maze=False)\n",
    "train_loader = NumpyLoader(dataset, batch_size=batch_size, num_workers=n_worker)\n",
    "\n",
    "losses = []\n",
    "eval_losses = []\n",
    "\n",
    "key = random.PRNGKey(jnp_seed)\n",
    "rng, key = random.split(key)\n",
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    output_vocab_size=dataset.vocab_size,\n",
    "    max_len=max_len,\n",
    "    emb_dim=emb_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    qkv_dim=qkv_dim,\n",
    "    mlp_dim=mlp_dim\n",
    ")\n",
    "\n",
    "model = TransformerLM(config=config)\n",
    "\n",
    "\n",
    "def loss_fn(params, batch):\n",
    "    preds, act = model.apply(params, batch['data'])\n",
    "    preds = preds[:, 0:-1]\n",
    "    targets = batch['data'][:, 1:]\n",
    "    idx = jnp.arange(targets.shape[1])[None, :]\n",
    "    mask = jnp.where((idx < batch['end_index'][:, None]) & (idx >= batch['start_index'][:, None]), 1., 0.)\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=preds,\n",
    "        labels=targets\n",
    "    ) * mask\n",
    "\n",
    "    loss = loss.sum() / mask.sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "tx = optax.adamw(lr)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "params = model.init(rng, batch['data'])\n",
    "\n",
    "apply_fn = jax.jit(model.apply)\n",
    "\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "state = {'params': params, 'opt_state': opt_state, 'loss': 0., 'step': 0}\n",
    "\n",
    "# checkpoint management / loading model\n",
    "\n",
    "if save and not checkpoint_path:\n",
    "    # make new run dir ect\n",
    "\n",
    "    # Get the current date and time\n",
    "    current_datetime = datetime.datetime.now()\n",
    "\n",
    "    # Create a directory name with the date and unique ID\n",
    "    checkpoint_dir_name = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Create the full path for the checkpoint directory\n",
    "    checkpoint_path = os.path.join(base_path, checkpoint_dir_name)\n",
    "\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        # Create the directory\n",
    "        os.makedirs(checkpoint_path)\n",
    "        print(f\"Checkpoint directory created: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Checkpoint directory already exists: {checkpoint_path}\")\n",
    "\n",
    "if checkpoint_path:\n",
    "    orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "    options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=keep_n_checkpoints)\n",
    "    checkpoint_manager = orbax.checkpoint.CheckpointManager(checkpoint_path, orbax_checkpointer, options)\n",
    "\n",
    "    dummy_dict = {\n",
    "        'state': state,\n",
    "        'loss': np.array([0.])}\n",
    "\n",
    "\n",
    "    step = checkpoint_manager.latest_step()\n",
    "\n",
    "    if step:\n",
    "        print(f'loading step {step}')\n",
    "        load_dict = checkpoint_manager.restore(step, items=dummy_dict)\n",
    "        state = load_dict['state']\n",
    "        losses = list(load_dict['loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1347783-d28e-4513-aaec-826305d0d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_map = {v: k for k, v in dataset.tokenizer.tokenizer_map.items()}\n",
    "vocab_map = dataset.tokenizer.tokenizer_map.get\n",
    "\n",
    "from dataset import find_from_right\n",
    "\n",
    "def ints_to_coords(arr):\n",
    "    # Map the integers in the list back to their corresponding tokens\n",
    "    tok_list = [reverse_map.get(i) for i in list(np.array(arr))]\n",
    "    coords = strings_to_coords(tok_list[:find_from_right(tok_list,'<PATH_END>')])\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28d622e-54be-4d98-9bf9-601eedce6880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[[-2.2925186 , -9.66244   , -9.907211  , ..., -0.9442662 ,\n",
       "          -1.1290259 , -2.6165648 ],\n",
       "         [ 3.3481827 , -9.749592  , -9.7007675 , ..., -0.5582606 ,\n",
       "           1.7104595 , -1.4670912 ],\n",
       "         [-0.9140706 , -8.227923  , -8.327029  , ..., -1.2923465 ,\n",
       "           0.9014423 , -1.0806873 ],\n",
       "         ...,\n",
       "         [-8.051864  , -7.8536158 , -7.8760014 , ...,  1.8560418 ,\n",
       "           2.8588028 , -0.48173887],\n",
       "         [-8.009118  , -7.828022  , -7.8512406 , ...,  1.8994629 ,\n",
       "           3.0053287 , -0.59309745],\n",
       "         [-7.932233  , -7.7624016 , -7.785763  , ...,  1.929503  ,\n",
       "           3.1037676 , -0.81382155]],\n",
       " \n",
       "        [[-2.2925186 , -9.66244   , -9.907211  , ..., -0.9442662 ,\n",
       "          -1.1290259 , -2.6165648 ],\n",
       "         [ 3.876052  , -7.4630747 , -7.8600135 , ..., -1.0477525 ,\n",
       "           1.7660744 , -1.800968  ],\n",
       "         [-0.05687809, -8.116321  , -8.211412  , ..., -1.2615001 ,\n",
       "           0.5373314 , -1.1881708 ],\n",
       "         ...,\n",
       "         [-6.707423  , -7.0495586 , -6.9266872 , ...,  2.150945  ,\n",
       "           6.340853  , -2.3013222 ],\n",
       "         [-6.843214  , -7.165188  , -7.0468397 , ...,  2.4630477 ,\n",
       "           6.6255655 , -2.973101  ],\n",
       "         [-6.8686476 , -7.172136  , -7.0613313 , ...,  2.6577191 ,\n",
       "           6.745262  , -3.632564  ]]], dtype=float32),\n",
       " {'stream': [Array([[[ 1.3325796 , -0.5016933 , -0.09933711, ...,  1.5221072 ,\n",
       "             1.3538977 ,  1.7753223 ],\n",
       "           [-0.79666257,  0.5566262 ,  1.2326028 , ...,  0.86264217,\n",
       "             3.3314385 ,  0.4937576 ],\n",
       "           [ 2.2194684 ,  1.6104724 ,  1.5513132 , ...,  1.966788  ,\n",
       "             0.43343967, -0.05298948],\n",
       "           ...,\n",
       "           [ 0.67091405,  3.4854045 , -1.45937   , ...,  0.03017557,\n",
       "            -1.6127996 ,  0.20981634],\n",
       "           [ 0.7894166 ,  3.12006   , -0.64075595, ...,  0.03017384,\n",
       "            -1.6128011 ,  0.20981503],\n",
       "           [ 1.6541035 ,  2.2450275 ,  0.02758557, ...,  0.03017205,\n",
       "            -1.6128026 ,  0.20981371]],\n",
       "   \n",
       "          [[ 1.3325796 , -0.5016933 , -0.09933711, ...,  1.5221072 ,\n",
       "             1.3538977 ,  1.7753223 ],\n",
       "           [ 0.48769453,  1.457659  , -0.2803282 , ...,  1.0975735 ,\n",
       "             1.2966716 ,  2.49616   ],\n",
       "           [ 2.2194684 ,  1.6104724 ,  1.5513132 , ...,  1.966788  ,\n",
       "             0.43343967, -0.05298948],\n",
       "           ...,\n",
       "           [ 0.67091405,  3.4854045 , -1.45937   , ...,  0.03017557,\n",
       "            -1.6127996 ,  0.20981634],\n",
       "           [ 0.7894166 ,  3.12006   , -0.64075595, ...,  0.03017384,\n",
       "            -1.6128011 ,  0.20981503],\n",
       "           [ 1.6541035 ,  2.2450275 ,  0.02758557, ...,  0.03017205,\n",
       "            -1.6128026 ,  0.20981371]]], dtype=float32),\n",
       "   Array([[[-2.453247  , -0.1387428 , -3.286148  , ..., -1.0081451 ,\n",
       "             2.7948828 ,  5.393422  ],\n",
       "           [-2.7021627 ,  1.8331051 ,  0.3621983 , ...,  0.991195  ,\n",
       "             3.0331864 ,  1.4075478 ],\n",
       "           [ 2.9163468 ,  1.8055836 ,  9.244377  , ...,  0.80409646,\n",
       "            -0.7021782 , -0.85172576],\n",
       "           ...,\n",
       "           [-0.36837405,  6.3280764 , -4.751853  , ..., -3.175531  ,\n",
       "             0.9150038 , -0.8956369 ],\n",
       "           [-0.43079585,  6.3869133 , -4.1220074 , ..., -3.2099729 ,\n",
       "             1.1593695 , -1.0191321 ],\n",
       "           [ 0.16705233,  5.9624443 , -3.7210908 , ..., -3.2276316 ,\n",
       "             1.3719866 , -1.160393  ]],\n",
       "   \n",
       "          [[-2.453247  , -0.1387428 , -3.286148  , ..., -1.0081451 ,\n",
       "             2.7948828 ,  5.393422  ],\n",
       "           [-1.1101466 ,  3.2565415 , -1.2397027 , ...,  1.0786246 ,\n",
       "             0.64467776,  2.752471  ],\n",
       "           [ 3.065878  ,  1.6921463 ,  9.420925  , ...,  1.0221936 ,\n",
       "            -0.75058806, -0.9486592 ],\n",
       "           ...,\n",
       "           [-0.87463945,  7.1316056 , -4.310452  , ..., -1.634763  ,\n",
       "            -0.12092996, -1.3913298 ],\n",
       "           [-0.9352022 ,  7.175373  , -3.708645  , ..., -1.6990432 ,\n",
       "             0.15534115, -1.5218172 ],\n",
       "           [-0.3377905 ,  6.7682357 , -3.3043694 , ..., -1.7387516 ,\n",
       "             0.4295929 , -1.6823514 ]]], dtype=float32),\n",
       "   Array([[[-11.70278   ,  -2.1227448 , -12.311556  , ...,   0.5477322 ,\n",
       "             -8.475763  ,   3.8468657 ],\n",
       "           [-12.702683  ,  -6.312969  ,   0.3215686 , ...,   2.1891525 ,\n",
       "              2.7329814 ,   2.303036  ],\n",
       "           [  1.3737028 ,  -5.745293  ,   6.5277033 , ...,   3.420209  ,\n",
       "             -3.0864956 ,   1.509305  ],\n",
       "           ...,\n",
       "           [ -1.1855996 ,   4.7307954 ,  -3.4547393 , ...,  -3.2465158 ,\n",
       "             -2.4535413 ,  -0.97885984],\n",
       "           [ -1.3896744 ,   4.8015494 ,  -2.7236953 , ...,  -3.2660747 ,\n",
       "             -2.2639232 ,  -1.1110947 ],\n",
       "           [ -0.9874778 ,   4.422864  ,  -2.210266  , ...,  -3.2589197 ,\n",
       "             -2.1014643 ,  -1.274134  ]],\n",
       "   \n",
       "          [[-11.70278   ,  -2.1227448 , -12.311556  , ...,   0.5477322 ,\n",
       "             -8.475763  ,   3.8468657 ],\n",
       "           [ -3.744677  ,   0.16039515,  -9.722192  , ...,  -2.3231635 ,\n",
       "             -4.7497725 ,   2.7134826 ],\n",
       "           [  0.53423786,  -5.9337273 ,   5.790366  , ...,   4.1549015 ,\n",
       "             -3.4351983 ,   1.2078347 ],\n",
       "           ...,\n",
       "           [ -2.6605353 ,   5.2879553 ,  -3.2710094 , ...,  -1.6401036 ,\n",
       "             -3.4300964 ,  -1.7121282 ],\n",
       "           [ -2.88723   ,   5.4415455 ,  -2.5333147 , ...,  -1.6048627 ,\n",
       "             -3.185965  ,  -1.8719456 ],\n",
       "           [ -2.5255418 ,   5.2105446 ,  -2.0596912 , ...,  -1.5436807 ,\n",
       "             -2.999279  ,  -2.0416474 ]]], dtype=float32),\n",
       "   Array([[[-11.53933  ,  -2.7648695, -15.384839 , ...,   2.621955 ,\n",
       "            -11.826974 ,   3.5100093],\n",
       "           [-13.722782 ,  -7.443744 ,  -0.9668173, ...,   2.3801441,\n",
       "              0.767187 ,   2.9831269],\n",
       "           [  1.7470437,  -7.463793 ,   7.5588136, ...,   3.1141315,\n",
       "             -5.567969 ,   1.7649615],\n",
       "           ...,\n",
       "           [ -0.9283687,   5.065361 ,  -0.401389 , ...,  -4.833981 ,\n",
       "             -2.469912 ,   2.1850595],\n",
       "           [ -1.2019718,   5.099136 ,   0.3627069, ...,  -4.917643 ,\n",
       "             -2.3360057,   2.199182 ],\n",
       "           [ -0.8509782,   4.722488 ,   0.8775493, ...,  -5.0346193,\n",
       "             -2.2322943,   2.2064898]],\n",
       "   \n",
       "          [[-11.53933  ,  -2.7648695, -15.384839 , ...,   2.621955 ,\n",
       "            -11.826974 ,   3.5100093],\n",
       "           [ -4.880074 ,  -1.7508323, -13.164964 , ...,  -4.7545714,\n",
       "             -5.398012 ,   0.4691714],\n",
       "           [  1.5770695,  -7.5223403,   6.532727 , ...,   4.5201945,\n",
       "             -5.607695 ,   2.3432362],\n",
       "           ...,\n",
       "           [ -0.8944948,   3.4563289,  -3.0214815, ...,  -2.2093613,\n",
       "             -6.190199 ,   1.4642882],\n",
       "           [ -1.0746229,   3.5699532,  -2.2963138, ...,  -2.2849329,\n",
       "             -6.0526514,   1.4305911],\n",
       "           [ -0.646839 ,   3.3302166,  -1.8722018, ...,  -2.3539913,\n",
       "             -5.979043 ,   1.4144588]]], dtype=float32),\n",
       "   Array([[[-14.990758  ,  -0.40442657, -13.572094  , ...,   2.6881785 ,\n",
       "            -15.215783  ,   6.0593586 ],\n",
       "           [-15.648909  ,  -7.314696  ,  -0.33464706, ...,   0.50399566,\n",
       "             -1.8158984 ,   3.3158295 ],\n",
       "           [  2.2491803 ,  -5.8602905 ,   6.203656  , ...,   2.0131123 ,\n",
       "             -7.20631   ,   4.2975397 ],\n",
       "           ...,\n",
       "           [ -2.607112  ,   2.975916  ,   0.15180534, ...,  -6.507215  ,\n",
       "             -0.7918741 ,  -0.18976063],\n",
       "           [ -2.721853  ,   3.2051282 ,   0.9310776 , ...,  -6.891544  ,\n",
       "             -0.46966863,  -0.20168504],\n",
       "           [ -2.087488  ,   3.135302  ,   1.4908394 , ...,  -7.4526134 ,\n",
       "             -0.02390885,  -0.27731943]],\n",
       "   \n",
       "          [[-14.990758  ,  -0.40442657, -13.572094  , ...,   2.6881785 ,\n",
       "            -15.215783  ,   6.0593586 ],\n",
       "           [ -6.1391063 ,   0.80666286, -14.828203  , ...,  -5.7368183 ,\n",
       "             -8.242169  ,   0.48378566],\n",
       "           [  1.6439223 ,  -5.7220454 ,   4.841557  , ...,   4.2651935 ,\n",
       "             -8.077252  ,   5.215588  ],\n",
       "           ...,\n",
       "           [  3.433176  ,   4.100037  ,  -3.4163697 , ...,  -3.1839325 ,\n",
       "             -2.2079158 ,  -1.2356409 ],\n",
       "           [  3.4684024 ,   4.509262  ,  -2.4988756 , ...,  -3.6419852 ,\n",
       "             -1.7473363 ,  -1.4785764 ],\n",
       "           [  4.291455  ,   4.594142  ,  -1.8023188 , ...,  -4.113101  ,\n",
       "             -1.2942171 ,  -1.611279  ]]], dtype=float32),\n",
       "   Array([[[-1.9293283e+01,  5.6334704e-02, -1.3646656e+01, ...,\n",
       "             1.7318509e+00, -1.6092163e+01,  9.8127680e+00],\n",
       "           [-1.9247566e+01, -5.1154361e+00, -4.9760535e-01, ...,\n",
       "            -1.8356380e-01, -3.0674036e+00,  5.2672048e+00],\n",
       "           [ 2.0403135e-01, -4.7769265e+00,  6.3203807e+00, ...,\n",
       "             2.6094577e+00, -8.5658493e+00,  5.6332574e+00],\n",
       "           ...,\n",
       "           [-8.0461473e+00, -3.0431867e-01,  8.2833725e-01, ...,\n",
       "            -8.6637335e+00,  1.2053478e-01, -3.9132094e-01],\n",
       "           [-8.2859344e+00, -1.0940993e-01,  1.5193059e+00, ...,\n",
       "            -9.1226740e+00,  4.3362081e-01, -3.6492801e-01],\n",
       "           [-7.7629914e+00, -2.5840199e-01,  1.9591832e+00, ...,\n",
       "            -9.7655210e+00,  9.6176600e-01, -4.4675136e-01]],\n",
       "   \n",
       "          [[-1.9293283e+01,  5.6334704e-02, -1.3646656e+01, ...,\n",
       "             1.7318509e+00, -1.6092163e+01,  9.8127680e+00],\n",
       "           [-9.5739365e+00,  1.7378349e+00, -1.3748678e+01, ...,\n",
       "            -4.8119044e+00, -9.7197027e+00,  3.2555923e+00],\n",
       "           [ 1.8002492e-01, -5.5667338e+00,  5.7524462e+00, ...,\n",
       "             5.1030946e+00, -1.0216638e+01,  6.2741790e+00],\n",
       "           ...,\n",
       "           [ 1.7324023e+00,  5.3168640e+00, -8.1960213e-01, ...,\n",
       "            -4.0759177e+00,  2.5831742e+00, -5.2340674e+00],\n",
       "           [ 1.6096156e+00,  5.5520306e+00,  5.5236816e-03, ...,\n",
       "            -4.5481257e+00,  2.8605266e+00, -5.4285235e+00],\n",
       "           [ 2.2440517e+00,  5.4385643e+00,  5.8042908e-01, ...,\n",
       "            -5.0525842e+00,  3.1777163e+00, -5.4936109e+00]]], dtype=float32),\n",
       "   Array([[[-18.578907  ,  -2.8709524 , -13.300779  , ...,   3.659339  ,\n",
       "            -17.724134  ,  12.548632  ],\n",
       "           [-17.914452  ,  -8.709064  ,  -1.0568582 , ...,  -0.09270447,\n",
       "             -3.9220335 ,   5.608254  ],\n",
       "           [  0.78573066,  -7.140859  ,   6.013736  , ...,   3.473327  ,\n",
       "             -9.662419  ,   5.299576  ],\n",
       "           ...,\n",
       "           [ -5.8139324 ,  -1.6422752 ,   1.3221537 , ..., -10.5179615 ,\n",
       "             -0.93037575,   2.712977  ],\n",
       "           [ -5.9386463 ,  -1.3457537 ,   1.9700844 , ..., -10.924979  ,\n",
       "             -0.68291736,   2.7679079 ],\n",
       "           [ -5.2452507 ,  -1.4443458 ,   2.3510787 , ..., -11.541441  ,\n",
       "             -0.25312185,   2.6063833 ]],\n",
       "   \n",
       "          [[-18.578907  ,  -2.8709524 , -13.300779  , ...,   3.659339  ,\n",
       "            -17.724134  ,  12.548632  ],\n",
       "           [-10.776177  ,  -1.1238936 , -13.244445  , ...,  -4.4402924 ,\n",
       "            -10.288102  ,   3.7308898 ],\n",
       "           [  0.39691758,  -7.9130673 ,   7.337101  , ...,   6.339695  ,\n",
       "            -12.31967   ,   6.252912  ],\n",
       "           ...,\n",
       "           [  3.4454803 ,   7.197814  ,   0.29822627, ...,   0.58659005,\n",
       "              5.244989  , -15.436311  ],\n",
       "           [  3.2889988 ,   7.4224644 ,   1.4530845 , ...,  -0.1818862 ,\n",
       "              5.1264815 , -15.000137  ],\n",
       "           [  3.9712596 ,   7.3906937 ,   2.2196264 , ...,  -1.0862284 ,\n",
       "              5.162406  , -14.5696745 ]]], dtype=float32),\n",
       "   Array([[[-19.512949  ,  -1.9050598 , -13.888476  , ...,   7.0759373 ,\n",
       "            -15.926492  ,  12.072531  ],\n",
       "           [-17.272388  ,  -8.268752  ,  -2.2821987 , ...,   1.4397236 ,\n",
       "             -1.841965  ,   4.770874  ],\n",
       "           [  0.70618945,  -5.789556  ,   5.3038735 , ...,   2.6115265 ,\n",
       "             -9.036009  ,   3.2191052 ],\n",
       "           ...,\n",
       "           [-10.306797  ,  -4.746706  ,  -1.682319  , ..., -10.005951  ,\n",
       "             -2.5407686 ,   1.2686784 ],\n",
       "           [-10.3306265 ,  -4.307048  ,  -0.95861703, ..., -10.664651  ,\n",
       "             -2.14585   ,   1.4681903 ],\n",
       "           [ -9.396536  ,  -4.1851697 ,  -0.62850213, ..., -11.472813  ,\n",
       "             -1.5956794 ,   1.2388825 ]],\n",
       "   \n",
       "          [[-19.512949  ,  -1.9050598 , -13.888476  , ...,   7.0759373 ,\n",
       "            -15.926492  ,  12.072531  ],\n",
       "           [-10.565611  ,  -0.30315343, -12.962971  , ...,  -3.1366374 ,\n",
       "            -10.068646  ,   3.6382284 ],\n",
       "           [ -0.6557786 ,  -6.6322427 ,   7.8597136 , ...,   6.101632  ,\n",
       "            -12.834698  ,   5.2663703 ],\n",
       "           ...,\n",
       "           [  3.5965374 ,   6.327136  ,   4.08675   , ...,   2.9190388 ,\n",
       "              2.53771   , -16.206257  ],\n",
       "           [  3.5774448 ,   6.574911  ,   5.153021  , ...,   2.2256296 ,\n",
       "              2.6385882 , -15.832769  ],\n",
       "           [  4.4362636 ,   6.5932417 ,   5.7166386 , ...,   1.4056778 ,\n",
       "              2.7292433 , -15.690734  ]]], dtype=float32),\n",
       "   Array([[[-13.9723835 ,  -2.6162164 , -14.919819  , ...,   3.7347314 ,\n",
       "            -11.82464   ,  11.356466  ],\n",
       "           [-13.088089  ,  -9.721491  ,  -4.3195615 , ...,   0.43544292,\n",
       "             -0.14710423,   3.255274  ],\n",
       "           [  3.8136497 ,  -6.0258937 ,   3.6630015 , ...,   0.6037836 ,\n",
       "             -9.494951  ,   3.0067663 ],\n",
       "           ...,\n",
       "           [ -9.54888   ,  -6.543648  ,   2.5894644 , ..., -13.601583  ,\n",
       "             -4.7034154 ,   2.6665306 ],\n",
       "           [ -9.491269  ,  -6.1525593 ,   3.3639455 , ..., -14.127112  ,\n",
       "             -4.319825  ,   2.7164361 ],\n",
       "           [ -8.433911  ,  -5.979373  ,   3.743961  , ..., -14.68681   ,\n",
       "             -3.7447479 ,   2.494739  ]],\n",
       "   \n",
       "          [[-13.9723835 ,  -2.6162164 , -14.919819  , ...,   3.7347314 ,\n",
       "            -11.82464   ,  11.356466  ],\n",
       "           [ -7.810594  ,  -3.1160917 , -13.745932  , ...,  -2.5855036 ,\n",
       "             -5.0022774 ,   1.8759329 ],\n",
       "           [  2.2603312 ,  -7.4032965 ,   6.9480076 , ...,   4.372444  ,\n",
       "            -11.837257  ,   4.5390778 ],\n",
       "           ...,\n",
       "           [  2.4502625 ,   9.425264  ,   2.5720735 , ...,  -1.607624  ,\n",
       "             -0.54547334, -15.038408  ],\n",
       "           [  2.6898036 ,   9.647372  ,   4.1844015 , ...,  -2.8574982 ,\n",
       "             -0.07503963, -14.560803  ],\n",
       "           [  3.868434  ,   9.892952  ,   5.2390285 , ...,  -4.0271635 ,\n",
       "              0.32735252, -14.300717  ]]], dtype=float32),\n",
       "   Array([[[-24.623003  ,  -3.136791  ,  -8.041325  , ...,  -2.852195  ,\n",
       "            -12.232169  ,   6.138184  ],\n",
       "           [-17.372425  , -11.706124  ,  -0.14740892, ...,  -2.4399705 ,\n",
       "             -1.0836732 ,   0.57951534],\n",
       "           [ -0.12137118,  -6.3986473 ,   8.492903  , ...,  -2.7300308 ,\n",
       "             -9.7775755 ,   0.8806499 ],\n",
       "           ...,\n",
       "           [-13.68588   ,  -9.145719  ,   3.5930583 , ..., -19.906485  ,\n",
       "             -3.9249203 ,  -1.8500416 ],\n",
       "           [-13.460345  ,  -8.72025   ,   4.4411407 , ..., -20.28618   ,\n",
       "             -3.5968413 ,  -1.5848083 ],\n",
       "           [-12.229879  ,  -8.424573  ,   5.0012674 , ..., -20.604433  ,\n",
       "             -3.1340747 ,  -1.5481236 ]],\n",
       "   \n",
       "          [[-24.623003  ,  -3.136791  ,  -8.041325  , ...,  -2.852195  ,\n",
       "            -12.232169  ,   6.138184  ],\n",
       "           [-11.611134  ,  -3.3140278 ,  -8.79083   , ...,  -9.848045  ,\n",
       "             -4.9748454 ,  -0.49851143],\n",
       "           [ -2.7753527 ,  -7.02864   ,  12.343106  , ...,   1.4031011 ,\n",
       "            -12.571289  ,   3.4887743 ],\n",
       "           ...,\n",
       "           [  3.0964482 ,  10.226548  ,   8.082338  , ...,  -3.7435417 ,\n",
       "             -3.8947983 , -15.086732  ],\n",
       "           [  3.0964487 ,  10.332237  ,   9.876916  , ...,  -5.1018825 ,\n",
       "             -3.9087648 , -14.734668  ],\n",
       "           [  4.1233625 ,  10.3701315 ,  11.020812  , ...,  -6.3515534 ,\n",
       "             -3.8087878 , -14.342532  ]]], dtype=float32),\n",
       "   Array([[[-23.237404  ,  -3.0659397 ,  -7.128496  , ...,  -1.8691204 ,\n",
       "            -15.421621  ,   4.1032934 ],\n",
       "           [-22.142279  , -10.325992  ,  -1.7708509 , ...,  -1.6630752 ,\n",
       "             -6.026594  ,  -2.827336  ],\n",
       "           [ -2.7023659 ,  -5.646553  ,   8.276561  , ...,   0.78293735,\n",
       "            -12.590221  ,  -1.2484027 ],\n",
       "           ...,\n",
       "           [-15.926372  , -10.53193   ,   9.109278  , ..., -19.852066  ,\n",
       "             -9.583408  ,  -0.79972124],\n",
       "           [-15.650555  , -10.247338  ,   9.778015  , ..., -20.217144  ,\n",
       "             -9.205018  ,  -0.4778819 ],\n",
       "           [-14.246775  ,  -9.998225  ,  10.13826   , ..., -20.578203  ,\n",
       "             -8.742945  ,  -0.3487172 ]],\n",
       "   \n",
       "          [[-23.237404  ,  -3.0659397 ,  -7.128496  , ...,  -1.8691204 ,\n",
       "            -15.421621  ,   4.1032934 ],\n",
       "           [-14.340098  ,  -4.1482773 ,  -9.032999  , ...,  -9.706671  ,\n",
       "             -9.991018  ,  -3.3568006 ],\n",
       "           [ -5.5511537 ,  -7.246362  ,  12.653345  , ...,   4.743398  ,\n",
       "            -15.96733   ,   1.5470582 ],\n",
       "           ...,\n",
       "           [  3.382535  ,   8.015343  ,  18.163774  , ...,   0.7375617 ,\n",
       "             -5.941289  , -14.897969  ],\n",
       "           [  3.825714  ,   8.25718   ,  19.973076  , ...,  -0.3870244 ,\n",
       "             -5.916423  , -14.586029  ],\n",
       "           [  5.168822  ,   8.421674  ,  21.265371  , ...,  -1.5151453 ,\n",
       "             -5.820386  , -14.064586  ]]], dtype=float32),\n",
       "   Array([[[-28.701603  ,   4.959987  , -12.11343   , ...,  -1.0090585 ,\n",
       "            -18.59909   ,  11.51474   ],\n",
       "           [-25.990017  ,  -7.393317  ,  -7.475201  , ...,  -5.696846  ,\n",
       "             -8.279701  ,   2.7387652 ],\n",
       "           [ -1.8124132 ,  -5.4144273 ,   2.8059087 , ...,  -2.3874218 ,\n",
       "            -11.533215  ,   0.56054366],\n",
       "           ...,\n",
       "           [-20.153412  ,  -5.052498  ,   6.3903284 , ..., -33.009064  ,\n",
       "            -17.294283  ,  -1.0869603 ],\n",
       "           [-19.297974  ,  -4.986985  ,   6.7290754 , ..., -33.2529    ,\n",
       "            -16.71865   ,  -0.77248716],\n",
       "           [-17.27298   ,  -4.825075  ,   6.68225   , ..., -33.524403  ,\n",
       "            -16.097023  ,  -0.7094979 ]],\n",
       "   \n",
       "          [[-28.701603  ,   4.959987  , -12.11343   , ...,  -1.0090585 ,\n",
       "            -18.59909   ,  11.51474   ],\n",
       "           [-17.758465  ,   1.5991726 , -11.897639  , ..., -10.201769  ,\n",
       "            -13.185772  ,  -1.2795088 ],\n",
       "           [ -3.6210992 ,  -6.268371  ,   6.661134  , ...,   1.4910259 ,\n",
       "            -15.346537  ,   2.5256414 ],\n",
       "           ...,\n",
       "           [ 18.471783  ,  12.176207  ,   5.172415  , ..., -10.07124   ,\n",
       "            -11.936243  , -19.256004  ],\n",
       "           [ 18.512249  ,  12.31075   ,   7.768048  , ..., -10.880675  ,\n",
       "            -12.512986  , -18.503105  ],\n",
       "           [ 19.541058  ,  12.817066  ,   9.888107  , ..., -12.257524  ,\n",
       "            -13.266621  , -17.725435  ]]], dtype=float32),\n",
       "   Array([[[-4.62909012e+01, -3.04087639e-01, -4.98684931e+00, ...,\n",
       "             6.77192211e-02, -5.72663021e+00,  1.80859814e+01],\n",
       "           [-3.27923126e+01, -5.32354021e+00, -2.15057640e+01, ...,\n",
       "            -1.71792965e+01,  1.40309906e+00,  4.52440834e+00],\n",
       "           [-1.03017654e+01, -1.90470123e+01, -6.47559941e-01, ...,\n",
       "            -1.01943645e+01, -3.11879253e+00,  1.14637547e+01],\n",
       "           ...,\n",
       "           [-1.61149475e+02, -4.11782341e+01,  8.73091354e+01, ...,\n",
       "             2.53240128e+01, -1.42299576e+01,  1.35762924e+02],\n",
       "           [-1.58146347e+02, -4.31114616e+01,  8.55874634e+01, ...,\n",
       "             2.18714676e+01, -1.50168362e+01,  1.29022324e+02],\n",
       "           [-1.53766006e+02, -4.53688126e+01,  8.32599335e+01, ...,\n",
       "             1.83480606e+01, -1.55236816e+01,  1.22494286e+02]],\n",
       "   \n",
       "          [[-4.62909012e+01, -3.04087639e-01, -4.98684931e+00, ...,\n",
       "             6.77192211e-02, -5.72663021e+00,  1.80859814e+01],\n",
       "           [-2.34327888e+01, -1.59915209e-01, -1.07151508e+01, ...,\n",
       "            -1.30379190e+01, -1.07345457e+01, -1.03744411e+01],\n",
       "           [-1.16236572e+01, -1.63018532e+01,  6.38716221e+00, ...,\n",
       "            -3.28957629e+00, -9.64718246e+00,  1.26007700e+01],\n",
       "           ...,\n",
       "           [ 1.26606750e+01, -5.22258644e+01,  1.52328930e+01, ...,\n",
       "             4.65902405e+01, -6.56330185e+01,  4.77015228e+01],\n",
       "           [ 2.34048615e+01, -5.60423279e+01,  1.20401649e+01, ...,\n",
       "             4.17060356e+01, -7.58907089e+01,  4.26045609e+01],\n",
       "           [ 3.27320404e+01, -5.74957542e+01,  9.73699093e+00, ...,\n",
       "             3.54272232e+01, -8.36103973e+01,  3.70946350e+01]]],      dtype=float32)]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(state[\"params\"], batch[\"data\"][:2], intervention=lambda x, layer: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c167e-37e2-4895-94ce-e7c16ed6e73e",
   "metadata": {},
   "source": [
    "# Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89edc510-175a-4751-8f87-a5a19d3c00f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951000\n",
      "0.17161499\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHFCAYAAAAaD0bAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB7pUlEQVR4nO3dd1hTZxsG8DtsRMCB4AAREQUEF6iAe+Eetf3EXatWbbXOarVu6+6QatVqrVLbitg6q1ZF60DBwbIO3ANEEGU7mDnfH5RICOtgQoDcv+vKdZmT97znyQHJk3dKBEEQQERERKRBtNQdABEREVFZYwJEREREGocJEBEREWkcJkBERESkcZgAERERkcZhAkREREQahwkQERERaRwmQERERKRxmAARERGRxmECRKQEPj4+kEgkkEgkOHPmjMLrgiCgUaNGkEgk6Ny5c5nHVxKBgYFYsmQJkpKS1B1Kifj5+aFp06YwNDSERCJBeHi4ukNSqadPn2LJkiUqe5+5v8OPHj0Sfe6jR48gkUjg4+Oj9LiIVIUJEJESGRsb4+eff1Y4fvbsWdy/fx/GxsZqiKpkAgMDsXTp0gqRAD1//hyjRo2Cra0tjh07hqCgIDRu3FjdYanU06dPsXTpUpUlQH379kVQUBDq1Kkj+tw6deogKCgIffv2VUFkRKrBBIhIiby8vLB3716kpKTIHf/555/h7u6O+vXrqyky5Xvz5o3arn3nzh1kZmZi5MiR6NSpE9zc3FClSpV3qvP169dKiq703rx5A2Vtzyj2/dSqVQtubm7Q19cXfS19fX24ubmhVq1aos8lUhcmQERKNGzYMACAr6+v7FhycjL27t2LsWPHFnjO0qVL0bZtW9SoUQMmJiZo1aoVfv75Z7kPwrxdbPkfebvUBEHApk2b0KJFCxgaGqJ69er44IMP8ODBgyLjXrJkCWbPng0AsLGxUejOa9CgAfr164d9+/ahZcuWMDAwwNKlSwEAGzduRMeOHWFubg4jIyM4Oztj7dq1yMzMlLtG586d4eTkhCtXrqBDhw6oUqUKGjZsiNWrV0MqlcrKSaVSLF++HE2aNIGhoSGqVauGZs2a4fvvvwcAjBkzBu3btweQk3DmvweHDh2Cu7s7qlSpAmNjY/To0QNBQUEK71cikSA0NBQffPABqlevDltbW7n3evjwYbRs2RKGhoZwcHDA4cOHZT8LBwcHGBkZoU2bNggODla4n8HBwRgwYABq1KgBAwMDtGzZEnv27JErk/szPXHiBMaOHYtatWqhSpUqSE9PV6jvzJkzaN26NQDgo48+kv18lixZIrsnVatWxbVr1+Dp6QljY2N069YNAODv74+BAwfC0tISBgYGaNSoESZOnIgXL14UGE/eLrCS/swK6gLLvcc3btzAsGHDYGpqCgsLC4wdOxbJycly105KSsK4ceNQo0YNVK1aFX379sWDBw/k3iORsumoOwCiysTExAQffPABtm/fjokTJwLISYa0tLTg5eUFb29vhXMePXqEiRMnylqHLl68iM8++wzR0dFYtGgRgLfdE3kFBQVh5syZaNq0qezYxIkT4ePjg6lTp2LNmjVISEjAsmXL4OHhgatXr8LCwqLAuMePH4+EhARs2LAB+/btk3WDODo6ysqEhoYiIiICCxYsgI2NDYyMjAAA9+/fx/Dhw2FjYwM9PT1cvXoVK1aswK1bt7B9+3a568TGxmLEiBGYNWsWFi9ejP3792PevHmoW7cuRo8eDQBYu3YtlixZggULFqBjx47IzMzErVu3ZF1zCxcuRJs2bTB58mSsXLkSXbp0gYmJCQBg165dGDFiBDw9PeHr64v09HSsXbsWnTt3xqlTp2SJU67Bgwdj6NChmDRpEl69eiU7fvXqVcybNw/z58+Hqakpli5disGDB2PevHk4deoUVq5cCYlEgi+++AL9+vXDw4cPYWhoCAA4ffo0evXqhbZt2+LHH3+Eqakpdu/eDS8vL7x+/RpjxoyRi2Hs2LHo27cvfv31V7x69Qq6uroKP59WrVphx44d+Oijj7BgwQJZV5OlpaWsTEZGBgYMGICJEydi7ty5yMrKkv183N3dMX78eJiamuLRo0f47rvv0L59e1y7dq3A64n9mRXl/fffh5eXF8aNG4dr165h3rx5ACD73ZBKpejfvz+Cg4OxZMkStGrVCkFBQejVq1exdRO9E4GI3tmOHTsEAMKVK1eE06dPCwCE69evC4IgCK1btxbGjBkjCIIgNG3aVOjUqVOh9WRnZwuZmZnCsmXLhJo1awpSqbTAcrdu3RJq1qwpdOnSRUhPTxcEQRCCgoIEAMK3334rVzYqKkowNDQU5syZU+R7+PrrrwUAwsOHDxVes7a2FrS1tYXbt28XWUdu/Dt37hS0tbWFhIQE2WudOnUSAAiXLl2SO8fR0VHo2bOn7Hm/fv2EFi1aFHmd3Hv8xx9/yF27bt26grOzs5CdnS07npqaKpibmwseHh6yY4sXLxYACIsWLSrwvRoaGgpPnjyRHQsPDxcACHXq1BFevXolO37gwAEBgHDo0CHZMXt7e6Fly5ZCZmamXL39+vUT6tSpI4st93dm9OjRRb7XXFeuXBEACDt27FB47cMPPxQACNu3by+yDqlUKmRmZgqPHz8WAAgHDx6UvZYbT96ff0l/Zg8fPlSILfcer127Vu7cTz/9VDAwMJD9bh85ckQAIGzevFmu3KpVqwQAwuLFi4t8T0SlxS4wIiXr1KkTbG1tsX37dly7dg1XrlwptPsLAP755x90794dpqam0NbWhq6uLhYtWoT4+HjExcUplI+NjUWvXr1Qp04d7N+/H3p6egCAw4cPQyKRYOTIkcjKypI9ateujebNmxc4O02MZs2aFTjQOCwsDAMGDEDNmjVl8Y8ePRrZ2dm4c+eOXNnatWujTZs2CvU+fvxY9rxNmza4evUqPv30Uxw/flxhPFVhbt++jadPn2LUqFHQ0nr7p61q1ap4//33cfHiRYVxMe+//36BdbVo0QL16tWTPXdwcACQ0yWUd6xR7vHc+O/du4dbt25hxIgRACD3c+jTpw9iYmJw+/btEsVQGgXVFRcXh0mTJsHKygo6OjrQ1dWFtbU1ACAiIqLYOkvyMyvKgAEDFM5NS0uT/W6fPXsWADBkyBC5crndyUSqwi4wIiWTSCT46KOPsH79eqSlpaFx48bo0KFDgWUvX74MT09PdO7cGT/99BMsLS2hp6eHAwcOYMWKFQoDjVNTU9GnTx9kZmbi77//hqmpqey1Z8+eQRCEQru5GjZs+E7vq6DZQZGRkejQoQOaNGmC77//Hg0aNICBgQEuX76MyZMnK8Rfs2ZNhTr09fXlys2bNw9GRkb47bff8OOPP0JbWxsdO3bEmjVr4OrqWmh88fHxhcZZt25dSKVSJCYmyiUwhc14qlGjhtzz3CSzsONpaWkAcn4GAPD555/j888/L7Du/GNvSjPrqiBVqlSRdQXmkkql8PT0xNOnT7Fw4UI4OzvDyMgIUqkUbm5uJRrIXpKfmZjzcwdZ554fHx8PHR0dhXtb2O8xkbIwASJSgTFjxmDRokX48ccfsWLFikLL7d69G7q6ujh8+DAMDAxkxw8cOKBQNjMzE++//z7u37+PgIAAufEfAGBmZgaJRIKAgIACZ/KUZnZPXhKJROHYgQMH8OrVK+zbt0/WqgDgnaZq6+joYObMmZg5cyaSkpJw8uRJfPnll+jZsyeioqIKne2V+0EbExOj8NrTp0+hpaWF6tWrF/ue3oWZmRmAnCRu8ODBBZZp0qSJSmIoqJ7r16/j6tWr8PHxwYcffig7fu/ePaVcUxlq1qyJrKwsJCQkyCVBsbGxaoyKNAG7wIhUoF69epg9ezb69+8v98GTn0QigY6ODrS1tWXH3rx5g19//VWh7Lhx43DmzBns27cPzZo1U3i9X79+EAQB0dHRcHV1VXg4OzsXGXP+b+Ylkfuhmze5EgQBP/30U4nrKEq1atXwwQcfYPLkyUhISChykb4mTZqgXr162LVrl9wMulevXmHv3r2ymWGq1KRJE9jZ2eHq1asF/gxcXV1LvRaUsn4+ALBly5ZSxaAKnTp1ApCzsGVeu3fvVkc4pEHYAkSkIqtXry62TN++ffHdd99h+PDhmDBhAuLj4/HNN98ofGB9/fXX+PXXX/HZZ5/ByMgIFy9elL1mYmICR0dHtGvXDhMmTMBHH32E4OBgdOzYEUZGRoiJicH58+fh7OyMTz75pNBYchOk77//Hh9++CF0dXXRpEmTIj+we/ToAT09PQwbNgxz5sxBWloaNm/ejMTExGLfe2H69+8PJycnuLq6olatWnj8+DG8vb1hbW0NOzu7Qs/T0tLC2rVrMWLECPTr1w8TJ05Eeno6vv76ayQlJZXo56EMW7ZsQe/evdGzZ0+MGTMG9erVQ0JCAiIiIhAaGoo//vijVPXa2trC0NAQv//+OxwcHFC1alXUrVsXdevWLfQce3t72NraYu7cuRAEATVq1MBff/0Ff3//0r49pevVqxfatWuHWbNmISUlBS4uLggKCsLOnTsBQG48F5Ey8TeLSI26du0qGyzdv39/zJ8/Hx988AHmzp0rV+7GjRsAgA0bNsDd3V3u8emnn8rKbdmyBT/88APOnTuHoUOHom/fvli0aBFevXqlMJA1v86dO2PevHn466+/0L59e7Ru3RohISFFnmNvb4+9e/ciMTERgwcPxmeffYYWLVpg/fr1pbwjQJcuXXDu3DlMmjQJPXr0wIIFC9CtWzecPXu22Cnbw4cPx4EDBxAfHw8vLy989NFHMDExwenTpxWmwKtKly5dcPnyZVSrVg3Tp09H9+7d8cknn+DkyZPo3r17qeutUqUKtm/fjvj4eHh6eqJ169bYunVrkefo6urir7/+QuPGjTFx4kQMGzYMcXFxOHnyZKnjUDYtLS389ddfGDp0KFavXo2BAwciICAAv/32G4CcVkAiVZAIgpKWHSUiIlKS3DWdLly4AA8PD3WHQ5UQEyAiIlIrX19fREdHw9nZGVpaWrh48SK+/vprtGzZUjZNnkjZOAaIiIjUytjYGLt378by5cvx6tUr1KlTB2PGjMHy5cvVHRpVYmwBIiIiIo2j9kHQmzZtgo2NDQwMDODi4oKAgIBCy545c6bAzSBv3bolV27v3r1wdHSEvr4+HB0dsX//flW/DSIiIqpA1JoA+fn5Yfr06Zg/fz7CwsLQoUMH9O7dG5GRkUWed/v2bcTExMgeeafGBgUFwcvLC6NGjcLVq1cxatQoDBkyBJcuXVL12yEiIqIKQq1dYG3btkWrVq2wefNm2TEHBwcMGjQIq1atUih/5swZdOnSBYmJiYVOjfTy8kJKSgr+/vtv2bFevXqhevXq8PX1Vfp7ICIioopHbYOgMzIyEBISorDeiaenJwIDA4s8t2XLlkhLS4OjoyMWLFiALl26yF4LCgrCjBkz5Mr37NkT3t7ehdaXnp6O9PR02XOpVIqEhATUrFlT6UvlExERkWoIgoDU1FTUrVu32EU01ZYAvXjxAtnZ2Qob3llYWBS6B0ydOnWwdetWuLi4ID09Hb/++iu6deuGM2fOoGPHjgBy9o8RUycArFq1CkuXLn3Hd0RERETlQVRUlMJ+ifmpfRp8/hYWQRAKbXVp0qSJ3EaC7u7uiIqKwjfffCNLgMTWCeRsXDhz5kzZ8+TkZNSvXx9RUVEKuysTERFR+ZSSkgIrK6sS7bmntgTIzMwM2traCi0zcXFxCi04RXFzc5MtmQ4AtWvXFl2nvr5+gTtlm5iYMAEiIiKqYEoyfEVts8D09PTg4uKisCmfv7+/qGXPw8LCUKdOHdlzd3d3hTpPnDjBpdSJiIhIRq1dYDNnzsSoUaPg6uoKd3d3bN26FZGRkZg0aRKAnK6p6Oho2a7A3t7eaNCgAZo2bYqMjAz89ttv2Lt3L/bu3Surc9q0aejYsSPWrFmDgQMH4uDBgzh58iTOnz+vlvdIRERE5Y9aEyAvLy/Ex8dj2bJliImJgZOTE44ePQpra2sAQExMjNyaQBkZGfj8888RHR0NQ0NDNG3aFEeOHEGfPn1kZTw8PLB7924sWLAACxcuhK2tLfz8/NC2bdsyf39ERERUPnErjAKkpKTA1NQUycnJHANERERUQYj5/Fb7VhhEREREZY0JEBEREWkcJkBERESkcZgAERERkcZhAkREREQahwkQERERaRwmQERERKRxmAARERGRxmECVMbSMrMhlXLtSSIiInViAlSGXqZnwWnxcQzYyH3JiIiI1IkJUBm69CAeWVIB16NT1B0KERGRRmMCRERERBqHCRARERFpHCZAavIqPUvdIRAREWksJkBqcvlRgrpDICIi0lhMgMqQRPL234evxqgvECIiIg3HBEhN9oY+UXcIREREGosJEBEREWkcJkBlKCubK0ATERGVB0yAyhB3wCAiIiofmAARERGRxmECVKbYBERERFQeMAEqQwLzHyIionKBCVAZYv5DRERUPjABIiIiIo3DBIiIiIg0DhMgIiIi0jhMgMoQB0ETERGVD0yAypDAYdBERETlAhMgIiIi0jhMgMoQu8CIiIjKByZAZYj5DxERUfnABKgMCWwCIiIiKheYABEREZHGYQJEREREGocJUBliDxgREVH5wASoDHEdICIiovJB7QnQpk2bYGNjAwMDA7i4uCAgIKBE5124cAE6Ojpo0aKF3HEfHx9IJBKFR1pamgqiF4ctQEREROWDWhMgPz8/TJ8+HfPnz0dYWBg6dOiA3r17IzIyssjzkpOTMXr0aHTr1q3A101MTBATEyP3MDAwUMVbICIiogpIrQnQd999h3HjxmH8+PFwcHCAt7c3rKyssHnz5iLPmzhxIoYPHw53d/cCX5dIJKhdu7bcg4iIiCiX2hKgjIwMhISEwNPTU+64p6cnAgMDCz1vx44duH//PhYvXlxomZcvX8La2hqWlpbo168fwsLCiowlPT0dKSkpcg9VYBcYERFR+aC2BOjFixfIzs6GhYWF3HELCwvExsYWeM7du3cxd+5c/P7779DR0SmwjL29PXx8fHDo0CH4+vrCwMAA7dq1w927dwuNZdWqVTA1NZU9rKysSv/GiIiIqNxT+yBoiUQi91wQBIVjAJCdnY3hw4dj6dKlaNy4caH1ubm5YeTIkWjevDk6dOiAPXv2oHHjxtiwYUOh58ybNw/JycmyR1RUVOnfEBEREZV7BTejlAEzMzNoa2srtPbExcUptAoBQGpqKoKDgxEWFoYpU6YAAKRSKQRBgI6ODk6cOIGuXbsqnKelpYXWrVsX2QKkr68PfX39d3xHxWMPGBERUfmgthYgPT09uLi4wN/fX+64v78/PDw8FMqbmJjg2rVrCA8Plz0mTZqEJk2aIDw8HG3bti3wOoIgIDw8HHXq1FHJ+xCDe4ERERGVD2prAQKAmTNnYtSoUXB1dYW7uzu2bt2KyMhITJo0CUBO11R0dDR27twJLS0tODk5yZ1vbm4OAwMDueNLly6Fm5sb7OzskJKSgvXr1yM8PBwbN24s0/dWEKY/RERE5YNaEyAvLy/Ex8dj2bJliImJgZOTE44ePQpra2sAQExMTLFrAuWXlJSECRMmIDY2FqampmjZsiXOnTuHNm3aqOItiMMMiIiIqFyQCOyXUZCSkgJTU1MkJyfDxMREafXuuRKFOXv/lT1/tLqv0uomIiLSdGI+v9U+C4yIiIiorDEBKkNSNrYRERGVC0yAiIiISOOIToCOHTuG8+fPy55v3LgRLVq0wPDhw5GYmKjU4Cobtv8QERGVD6IToNmzZ8v2yrp27RpmzZqFPn364MGDB5g5c6bSA6xM2ANGRERUPoieBv/w4UM4OjoCAPbu3Yt+/fph5cqVCA0NRZ8+fZQeIBEREZGyiW4B0tPTw+vXrwEAJ0+elO3mXqNGDZXtok5ERESkTKJbgNq3b4+ZM2eiXbt2uHz5Mvz8/AAAd+7cgaWlpdIDrEwEjgIiIiIqF0S3AP3www/Q0dHBn3/+ic2bN6NevXoAgL///hu9evVSeoCVydOkN+oOgYiIiFCKFqD69evj8OHDCsfXrVunlIAqs+pV9NQdAhEREaEULUChoaG4du2a7PnBgwcxaNAgfPnll8jIyFBqcJWNoZ62ukMgIiIilCIBmjhxIu7cuQMAePDgAYYOHYoqVargjz/+wJw5c5QeYGUigUTdIRARERFKkQDduXMHLVq0AAD88ccf6NixI3bt2gUfHx/s3btX2fFVKhLmP0REROWC6ARIEARIpVIAOdPgc9f+sbKywosXL5QbHREREZEKiE6AXF1dsXz5cvz66684e/Ys+vbtCyBngUQLCwulB0hERESkbKITIG9vb4SGhmLKlCmYP38+GjVqBAD4888/4eHhofQAKxP2gBEREZUPoqfBN2vWTG4WWK6vv/4a2tqc5VQUjgEiIiIqH0QnQLlCQkIQEREBiUQCBwcHtGrVSplxVUqcBUZERFQ+iE6A4uLi4OXlhbNnz6JatWoQBAHJycno0qULdu/ejVq1aqkizkopPSsb+jpsNSMiIiproscAffbZZ0hNTcWNGzeQkJCAxMREXL9+HSkpKZg6daoqYqy0xvkEqzsEIiIijSS6BejYsWM4efIkHBwcZMccHR2xceNG2c7wVDLn73HZACIiInUQ3QIklUqhq6urcFxXV1e2PhAVgkOAiIiIygXRCVDXrl0xbdo0PH36VHYsOjoaM2bMQLdu3ZQaXKUjqDsAIiIiAkqRAP3www9ITU1FgwYNYGtri0aNGsHGxgapqanYsGGDKmIkIiIiUirRY4CsrKwQGhoKf39/3Lp1C4IgwNHREd27d1dFfERERERKV+p1gHr06IEePXooMxYiIiKiMlGiBGj9+vUlrpBT4QsncBAQERFRuVCiBGjdunUlqkwikTABIiIionKvRAnQw4cPVR0HERERUZkRPQuMiIiIqKJjAkREREQahwlQGRI4BpqIiKhcYAJEREREGocJEBEREWkc0QlQgwYNsGzZMkRGRqoiHiIiIiKVE50AzZo1CwcPHkTDhg3Ro0cP7N69G+np6aqIrdKpX6OKukMgIiIilCIB+uyzzxASEoKQkBA4Ojpi6tSpqFOnDqZMmYLQ0FBVxFhpWFZnAkRERFQelHoMUPPmzfH9998jOjoaixcvxrZt29C6dWs0b94c27dvh1DCKU+bNm2CjY0NDAwM4OLigoCAgBKdd+HCBejo6KBFixYKr+3duxeOjo7Q19eHo6Mj9u/fL+atqQy3wiAiIiofSp0AZWZmYs+ePRgwYABmzZoFV1dXbNu2DUOGDMH8+fMxYsSIYuvw8/PD9OnTMX/+fISFhaFDhw7o3bt3seOLkpOTMXr0aHTr1k3htaCgIHh5eWHUqFG4evUqRo0ahSFDhuDSpUulfatKw2nwRERE5YNEKGlTzX9CQ0OxY8cO+Pr6QltbG6NGjcL48eNhb28vK3PlyhV07NgRb968KbKutm3bolWrVti8ebPsmIODAwYNGoRVq1YVet7QoUNhZ2cHbW1tHDhwAOHh4bLXvLy8kJKSgr///lt2rFevXqhevTp8fX1L9B5TUlJgamqK5ORkmJiYlOicknjxMh2uy0/KHXu0uq/S6iciItJkYj6/RbcAtW7dGnfv3sXmzZvx5MkTfPPNN3LJDwA4Ojpi6NChRdaTkZGBkJAQeHp6yh339PREYGBgoeft2LED9+/fx+LFiwt8PSgoSKHOnj17Fllneno6UlJS5B6qYFZVXyX1EhERkTgl2gw1rwcPHsDa2rrIMkZGRtixY0eRZV68eIHs7GxYWFjIHbewsEBsbGyB59y9exdz585FQEAAdHQKDj02NlZUnQCwatUqLF26tMh4iYiIqPIQnQDlJj/BwcGIiIiARCKBvb09XF1dSxWARCKRey4IgsIxAMjOzsbw4cOxdOlSNG7cWCl15po3bx5mzpwpe56SkgIrK6uShE9EREQVkOgE6MmTJxg2bBguXLiAatWqAQCSkpLg4eEBX1/fEicOZmZm0NbWVmiZiYuLU2jBAYDU1FQEBwcjLCwMU6ZMAQBIpVIIggAdHR2cOHECXbt2Re3atUtcZy59fX3o67N7ioiISFOIHgM0duxYZGZmIiIiAgkJCUhISEBERAQEQcC4ceNKXI+enh5cXFzg7+8vd9zf3x8eHh4K5U1MTHDt2jWEh4fLHpMmTUKTJk0QHh6Otm3bAgDc3d0V6jxx4kSBdRIREZFmEt0CFBAQgMDAQDRp0kR2rEmTJtiwYQPatWsnqq6ZM2di1KhRcHV1hbu7O7Zu3YrIyEhMmjQJQE7XVHR0NHbu3AktLS04OTnJnW9ubg4DAwO549OmTUPHjh2xZs0aDBw4EAcPHsTJkydx/vx5sW+ViIiIKinRCVD9+vWRmZmpcDwrKwv16tUTVZeXlxfi4+OxbNkyxMTEwMnJCUePHpWNM4qJiRG955iHhwd2796NBQsWYOHChbC1tYWfn5+shYiIiIhI9DpABw8exMqVK7Fx40a4uLhAIpEgODgYn332Gb744gsMGjRIRaGWHVWtAwQADeYekXvOdYCIiIiUQ8znt+gEqHr16nj9+jWysrJkU9Fz/21kZCRXNiEhQWTo5QMTICIioopHzOe36C4wb2/v0sZFREREVC6IToA+/PBDVcRBREREVGZEJ0BAzqKEBw4ckC2E6OjoiAEDBkBbW1vZ8REREREpnegE6N69e+jTpw+io6PRpEkTCIKAO3fuwMrKCkeOHIGtra0q4iQiIiJSGtELIU6dOhW2traIiopCaGgowsLCEBkZCRsbG0ydOlUVMRIREREplegWoLNnz+LixYuoUaOG7FjNmjWxevVq0QshEpCVLYWOtug8lIiIiN6B6E9efX19pKamKhx/+fIl9PT0lBIUERERkSqJToD69euHCRMm4NKlSxAEAYIg4OLFi5g0aRIGDBigihgrNVGLMBEREZFSiE6A1q9fD1tbW7i7u8PAwAAGBgZo164dGjVqhO+//14VMRIREREplagxQIIgIDk5Gb6+vnj69KlsF3hHR0c0atRIVTFWauLW4SYiIiJlEJ0A2dnZ4caNG7Czs2PSowQCO8GIiIjKnKguMC0tLdjZ2SE+Pl5V8RARERGpnOgxQGvXrsXs2bNx/fp1VcSjcdgFRkREVPZErwM0cuRIvH79Gs2bN4eenh4MDQ3lXq+oO8ATERGR5hCdAK1btw4SiUQVsRARERGVCdEJ0JgxY1QQhubKyJbCQJebyBIREZUl0WOAtLW1ERcXp3A8Pj6eu8GXQuA9DignIiIqa6ITIKGQUbvp6encCoOIiIgqhBJ3ga1fvx4AIJFIsG3bNlStWlX2WnZ2Ns6dOwd7e3vlR1jpcRoYERFRWStxArRu3ToAOS1AP/74o1x3l56eHho0aIAff/xR+RESERERKVmJE6CHDx8CALp06YJ9+/ahevXqKgtKs3BGHRERUVkTPQvs9OnTqoiDiIiIqMyIToCys7Ph4+ODU6dOIS4uDlKpVO71f/75R2nBaQaOASIiIiprohOgadOmwcfHB3379oWTkxMXRSQiIqIKR3QCtHv3buzZswd9+vRRRTwa51REHHo51VF3GERERBpF9DpAenp6aNSokSpi0Uh/hDxRdwhEREQaR3QCNGvWLHz//feFLohIREREVN6J7gI7f/48Tp8+jb///htNmzaFrq6u3Ov79u1TWnBEREREqiA6AapWrRree+89VcRCREREVCZEJ0A7duxQRRwaQ19HC+lZ0uILEhERkcqIHgMEAFlZWTh58iS2bNmC1NRUAMDTp0/x8uVLpQZXGVmYGKg7BCIiIo0nOgF6/PgxnJ2dMXDgQEyePBnPnz8HAKxduxaff/650gOsbN5vZanuEIiIiDSe6ARo2rRpcHV1RWJiIgwNDWXH33vvPZw6dUqpwVVG5ib66g6BiIhI45VqFtiFCxegp6cnd9za2hrR0dFKC4yIiIhIVUS3AEmlUmRnZyscf/LkCYyNjZUSVGXW0MxI3SEQERFpPNEJUI8ePeDt7S17LpFI8PLlSyxevJjbY5SAZY0q6g6BiIhI44lOgNatW4ezZ8/C0dERaWlpGD58OBo0aIDo6GisWbNGdACbNm2CjY0NDAwM4OLigoCAgELLnj9/Hu3atUPNmjVhaGgIe3t7rFu3Tq6Mj48PJBKJwiMtLU10bKpQr5ph8YWIiIhIpUSPAapbty7Cw8Oxe/duhISEQCqVYty4cRgxYoTcoOiS8PPzw/Tp07Fp0ya0a9cOW7ZsQe/evXHz5k3Ur19fobyRkRGmTJmCZs2awcjICOfPn8fEiRNhZGSECRMmyMqZmJjg9u3bcucaGHD6OREREeWQCGrc1Ktt27Zo1aoVNm/eLDvm4OCAQYMGYdWqVSWqY/DgwTAyMsKvv/4KIKcFaPr06UhKSip1XCkpKTA1NUVycjJMTExKXU9hGsw9Ivf80eq+Sr8GERGRphHz+V2qhRBzmZiY4MGDB6U6NyMjAyEhIfD09JQ77unpicDAwBLVERYWhsDAQHTq1Enu+MuXL2FtbQ1LS0v069cPYWFhRdaTnp6OlJQUuQcRERFVXu+UAL1L49GLFy+QnZ0NCwsLueMWFhaIjY0t8lxLS0vo6+vD1dUVkydPxvjx42Wv2dvbw8fHB4cOHYKvry8MDAzQrl073L17t9D6Vq1aBVNTU9nDysqq1O+LiIiIyj/RY4CUTSKRyD0XBEHhWH4BAQF4+fIlLl68iLlz56JRo0YYNmwYAMDNzQ1ubm6ysu3atUOrVq2wYcMGrF+/vsD65s2bh5kzZ8qep6SkMAkiIiKqxN4pARo5cmSpx8iYmZlBW1tbobUnLi5OoVUoPxsbGwCAs7Mznj17hiVLlsgSoPy0tLTQunXrIluA9PX1oa/PFZqJiIg0xTt1gW3evBlmZmalGnCsp6cHFxcX+Pv7yx339/eHh4dHiesRBAHp6elFvh4eHo46deqIjpGIiIgqJ9EJ0Jo1a+Dn5yd7PmTIENSsWRP16tXD1atXRdU1c+ZMbNu2Ddu3b0dERARmzJiByMhITJo0CUBO19To0aNl5Tdu3Ii//voLd+/exd27d7Fjxw588803GDlypKzM0qVLcfz4cTx48ADh4eEYN24cwsPDZXUSERERie4C27JlC3777TcAOa01/v7++Pvvv7Fnzx7Mnj0bJ06cKHFdXl5eiI+Px7JlyxATEwMnJyccPXoU1tbWAICYmBhERkbKykulUsybNw8PHz6Ejo4ObG1tsXr1akycOFFWJikpCRMmTEBsbCxMTU3RsmVLnDt3Dm3atBH7VomIiKiSEr0OkKGhIe7cuQMrKytMmzYNaWlp2LJlC+7cuYO2bdsiMTFRVbGWGa4DREREVPGodB2g6tWrIyoqCgBw7NgxdO/eHUDOWJuCNkklIiIiKm9Ed4ENHjwYw4cPh52dHeLj49G7d28AQHh4OBo1aqT0AImIiIiUTXQCtG7dOjRo0ABRUVFYu3YtqlatCiBnvM6nn36q9ACJiIiIlE10AqSrq4vPP/9c4fj06dOVEQ8RERGRyokeA/TLL7/gyJG3g3jnzJmDatWqwcPDA48fP1ZqcERERESqIDoBWrlyJQwNDQEAQUFB+OGHH7B27VqYmZlhxowZSg+QiIiISNlEd4FFRUXJBjsfOHAAH3zwASZMmIB27dqhc+fOyo6PiIiISOlEtwBVrVoV8fHxAIATJ07IpsEbGBjgzZs3yo2OiIiISAVEtwD16NED48ePR8uWLXHnzh307ZuziN+NGzfQoEEDZcdHREREpHSiW4A2btwId3d3PH/+HHv37kXNmjUBACEhIYXuyE7yahrpqTsEIiIijSa6BahatWr44YcfFI4vXbpUKQFpAolE3REQERFpNtEJEJCz4ejPP/+MiIgISCQSODg4YNy4cTA1NVV2fJWSFjMgIiIitRLdBRYcHAxbW1usW7cOCQkJePHiBdatWwdbW1uEhoaqIsZKhwkQERGReoluAZoxYwYGDBiAn376CTo6OadnZWVh/PjxmD59Os6dO6f0ICsb5j9ERETqJToBCg4Olkt+AEBHRwdz5syBq6urUoOrrNgCREREpF6iu8BMTEwQGRmpcDwqKgrGxsZKCaqyY/5DRESkXqITIC8vL4wbNw5+fn6IiorCkydPsHv3bowfP57T4EuICRAREZF6ie4C++abbyCRSDB69GhkZWUByNkh/pNPPsHq1auVHmBlxC4wIiIi9RKVAGVnZyMoKAiLFy/GqlWrcP/+fQiCgEaNGqFKlSqqirHSYQJERESkXqK6wLS1tdGzZ08kJyejSpUqcHZ2RrNmzZj8iDTa3VrdIRAREWk00WOAnJ2d8eDBA1XEojFqcCsMIiIitRKdAK1YsQKff/45Dh8+jJiYGKSkpMg9qHjtGpmpOwQiIiKNJnoQdK9evQAAAwYMgCTPWBZBECCRSJCdna286CqpKnra6g6BiIhIo4lOgE6fPq2KODSKBBwETUREpE6iE6BOnTqpIg6NYqAruueRiIiIlEj0J/GOHTvwxx9/KBz/448/8MsvvyglqMpOwmnwREREaiU6AVq9ejXMzBQH8Zqbm2PlypVKCYqIiIhIlUQnQI8fP4aNjY3CcWtr6wL3CCMiIiIqb0QnQObm5vj3338Vjl+9ehU1a9ZUSlBEREREqiQ6ARo6dCimTp2K06dPIzs7G9nZ2fjnn38wbdo0DB06VBUxEhERESmV6Flgy5cvx+PHj9GtWzfo6OScLpVKMXr0aI4BIiIiogpBdAKkp6cHPz8/LF++HOHh4TA0NISzszOsrbm/FREREVUMohOgXHZ2drCzs1NmLERERERlgivyERERkcZhAkREREQahwkQERERaRwmQERERKRxSpUAJSUl4cSJE/jtt9+wc+dOuYdYmzZtgo2NDQwMDODi4oKAgIBCy54/fx7t2rVDzZo1YWhoCHt7e6xbt06h3N69e+Ho6Ah9fX04Ojpi//79ouMiIiKiykv0LLC//voLI0aMwKtXr2BsbCy3sadEIsHo0aNLXJefnx+mT5+OTZs2oV27dtiyZQt69+6Nmzdvon79+grljYyMMGXKFDRr1gxGRkY4f/48Jk6cCCMjI0yYMAEAEBQUBC8vL3z11Vd47733sH//fgwZMgTnz59H27Ztxb5dIiIiqoQkgiAIYk5o3Lgx+vTpg5UrV6JKlSrvdPG2bduiVatW2Lx5s+yYg4MDBg0ahFWrVpWojsGDB8PIyAi//vorAMDLywspKSn4+++/ZWV69eqF6tWrw9fXt0R1pqSkwNTUFMnJyTAxMRHxjkquwdwjsn8/Wt1XJdcgIiLSJGI+v0V3gUVHR2Pq1KnvnPxkZGQgJCQEnp6ecsc9PT0RGBhYojrCwsIQGBiITp06yY4FBQUp1NmzZ88i60xPT0dKSorcg4iIiCov0QlQz549ERwc/M4XfvHiBbKzs2FhYSF33MLCArGxsUWea2lpCX19fbi6umLy5MkYP3687LXY2FjRda5atQqmpqayh5WVVSneEREREVUUoscA9e3bF7Nnz8bNmzfh7OwMXV1dudcHDBggqr68Y4gAQBAEhWP5BQQE4OXLl7h48SLmzp2LRo0aYdiwYaWuc968eZg5c6bseUpKCpMgIiKiSkx0AvTxxx8DAJYtW6bwmkQiQXZ2donqMTMzg7a2tkLLTFxcnEILTn42NjYAAGdnZzx79gxLliyRJUC1a9cWXae+vj709fVLFDcRERFVfKK7wKRSaaGPkiY/QM6mqi4uLvD395c77u/vDw8PjxLXIwgC0tPTZc/d3d0V6jxx4oSoOomIiKhyK/VmqACQlpYGAwODUp8/c+ZMjBo1Cq6urnB3d8fWrVsRGRmJSZMmAcjpmoqOjpatL7Rx40bUr18f9vb2AHLWBfrmm2/w2WefyeqcNm0aOnbsiDVr1mDgwIE4ePAgTp48ifPnz7/DOyUiIqLKRHQClJ2djZUrV+LHH3/Es2fPcOfOHTRs2BALFy5EgwYNMG7cuBLX5eXlhfj4eCxbtgwxMTFwcnLC0aNHYW1tDQCIiYlBZGSkrLxUKsW8efPw8OFD6OjowNbWFqtXr8bEiRNlZTw8PLB7924sWLAACxcuhK2tLfz8/LgGEBEREcmIXgdo2bJl+OWXX7Bs2TJ8/PHHuH79Oho2bIg9e/Zg3bp1CAoKUlWsZYbrABEREVU8Kl0HaOfOndi6dStGjBgBbW1t2fFmzZrh1q1b4qMlIiIiKmOlWgixUaNGCselUikyMzOVEhQRERGRKolOgJo2bVrghqV//PEHWrZsqZSgiIiIiFRJ9CDoxYsXY9SoUYiOjoZUKsW+fftw+/Zt7Ny5E4cPH1ZFjERERERKJboFqH///vDz88PRo0chkUiwaNEiRERE4K+//kKPHj1UEWOll5ElVXcIREREGkV0C1BUVBR69uyJnj17Krx28eJFuLm5KSUwTVLMzh9ERESkZKJbgHr06IH4+HiF4xcuXECvXr2UEhQRERGRKolOgDp06ABPT0+kpqbKjp07dw59+vTB4sWLlRqcpmADEBERUdkSnQBt3boVNjY26Nu3L9LS0nD69Gn07dsXy5Ytw4wZM1QRIxEREZFSiU6AJBIJfH19YWBggG7dumHAgAFYtWoVpk2bpor4NIKEg4CIiIjKVIkGQf/7778KxxYvXoxhw4Zh5MiR6Nixo6xMs2bNlBshERERkZKVKAFq0aIFJBIJ8m4blvt8y5Yt2Lp1KwRBgEQiQXZ2tsqCJSIiIlKGEiVADx8+VHUcRERERGWmRAmQtbW1quMgIiIiKjOiF0IEgPv378Pb2xsRERGQSCRwcHDAtGnTYGtrq+z4iIiIiJRO9Cyw48ePw9HREZcvX0azZs3g5OSES5cuoWnTpvD391dFjERERERKJboFaO7cuZgxYwZWr16tcPyLL77gfmBERERU7oluAYqIiMC4ceMUjo8dOxY3b95USlCa5ubTFHWHQEREpFFEJ0C1atVCeHi4wvHw8HCYm5srIyaNczLimbpDICIi0iiiu8A+/vhjTJgwAQ8ePICHhwckEgnOnz+PNWvWYNasWaqIsdKLSnyt7hCIiIg0iugEaOHChTA2Nsa3336LefPmAQDq1q2LJUuWYOrUqUoPUBPkWV+SiIiIyoDoBEgikWDGjBmYMWOGbEd4Y2NjpQemSQRmQERERGVK9Bigrl27IikpCUBO4pOb/KSkpKBr165KDU5TMP0hIiIqW6IToDNnziAjI0PheFpaGgICApQSlKaJf6l4P4mIiEh1StwFlndH+Js3byI2Nlb2PDs7G8eOHUO9evWUG52GOH/vhbpDICIi0iglToByd4SXSCQFdnUZGhpiw4YNSg2OiIiISBVKnAA9fPgQgiCgYcOGuHz5MmrVqiV7TU9PD+bm5tDW1lZJkERERETKVOIEKHdHeKlUqrJgiIiIiMqC6EHQeZmYmODBgwfKioWIiIioTLxTAsT1a4iIiKgieqcEiIiIiKgiEpUAZWZm4qOPPpJ1e40cORImJiYqCUzTnLzJDVGJiIjKiqgESFdXF/v375c937x5M8zMzJQelCYavzNY3SEQERFpDNFdYO+99x4OHDigglCIiIiIyobozVAbNWqEr776CoGBgXBxcYGRkZHc69wRnoiIiMo70QnQtm3bUK1aNYSEhCAkJETuNYlEwgSIiIiIyj3RXWAPHz4s9FGaNYE2bdoEGxsbGBgYwMXFpcgNVfft24cePXqgVq1aMDExgbu7O44fPy5XxsfHR7ZlR95HWlqa6NiIiIiocir1NPiMjAzcvn0bWVlZpb64n58fpk+fjvnz5yMsLAwdOnRA7969ERkZWWD5c+fOoUePHjh69ChCQkLQpUsX9O/fH2FhYXLlTExMEBMTI/cwMDAodZxERERUuYhOgF6/fo1x48ahSpUqaNq0qSxZmTp1KlavXi2qru+++w7jxo3D+PHj4eDgAG9vb1hZWWHz5s0Flvf29sacOXPQunVr2NnZYeXKlbCzs8Nff/0lV04ikaB27dpyDyIiIqJcohOgefPm4erVqzhz5oxcq0r37t3h5+dX4noyMjIQEhICT09PueOenp4IDAwsUR1SqRSpqamoUaOG3PGXL1/C2toalpaW6Nevn0ILUXnFlbWJiIjKhugE6MCBA/jhhx/Qvn17SCQS2XFHR0fcv3+/xPW8ePEC2dnZsLCwkDtuYWGB2NjYEtXx7bff4tWrVxgyZIjsmL29PXx8fHDo0CH4+vrCwMAA7dq1w927dwutJz09HSkpKXIPdXjw4pVarktERKRpRCdAz58/h7m5ucLxV69eySVEJZX/HEEQSlSPr68vlixZAj8/P7l43NzcMHLkSDRv3hwdOnTAnj170LhxY2zYsKHQulatWgVTU1PZw8rKSvT7UIb1p94maelZ2UhNy1RLHERERJWd6ASodevWOHLkiOx5brLy008/wd3dvcT1mJmZQVtbW6G1Jy4uTqFVKD8/Pz+MGzcOe/bsQffu3Yssq6WlhdatWxfZAjRv3jwkJyfLHlFRUSV+H8oUmfBa9m+3lafgvOQEUpgEERERKZ3odYBWrVqFXr164ebNm8jKysL333+PGzduICgoCGfPni1xPXp6enBxcYG/vz/ee+892XF/f38MHDiw0PN8fX0xduxY+Pr6om/fvsVeRxAEhIeHw9nZudAy+vr60NfXL3HsqhIWmST7d+LrnMTnenQyPGy53QgREZEyiW4B8vDwwIULF/D69WvY2trixIkTsLCwQFBQEFxcXETVNXPmTGzbtg3bt29HREQEZsyYgcjISEyaNAlATsvM6NGjZeV9fX0xevRofPvtt3Bzc0NsbCxiY2ORnJwsK7N06VIcP34cDx48QHh4OMaNG4fw8HBZnURERESiW4AAwNnZGb/88ss7X9zLywvx8fFYtmwZYmJi4OTkhKNHj8La2hoAEBMTI7cm0JYtW5CVlYXJkydj8uTJsuMffvghfHx8AABJSUmYMGECYmNjYWpqipYtW+LcuXNo06bNO8erFpwYRkREpHQSoRRzr7Ozs7F//35ERERAIpHAwcEBAwcOhI5OqfKpciclJQWmpqZITk6GiYmJSq7RYO6RAo/f+qoXDHS1Za/vGt8WHo3YBUZEJFZWthSpaVmobqSn7lCojIj5/BadsVy/fh0DBw5EbGwsmjRpAgC4c+cOatWqhUOHDhU51oaKt+Gfu5jd017dYRARVXgDfriAmzEpOPN5ZzQwMyr+BNIooscAjR8/Hk2bNsWTJ08QGhqK0NBQREVFoVmzZpgwYYIqYtQoh/+NwZPEt7PB2ANGRFQ6N2Ny1nQ7ej1GzZFQeSS6Bejq1asIDg5G9erVZceqV6+OFStWoHXr1koNThM9jn+N9mtOqzsMIiKiSk10C1CTJk3w7NkzheNxcXFo1KiRUoIieWmZ2Rj18yVsOVvylbaJiIiocKIToJUrV2Lq1Kn4888/8eTJEzx58gR//vknpk+fjjVr1qh9O4nKaF9oNALuvsCqv2+pOxQiIqJKQXQXWL9+/QAAQ4YMka0CnTuRrH///rLnEokE2dnZyopTo73J5H0kIiJSJtEJ0OnTHJ9CREREFZvoBKhTp06qiIOIiIiozIgeA0TKMbhVvRKVEwRAouJYiIgqMwn/ilIBmACpiZFe5Vg1m4iIqCJiAqQmApc4JCIiUhsmQOVc/kTp2xO31RQJEalatpRfjIjKSqkSoKysLJw8eRJbtmxBamoqAODp06d4+fKlUoOrzHS1S37rJXm6rzf8c09uqwwiqhyOXouB46Jj8L+puNAsESmf6ATo8ePHcHZ2xsCBAzF58mQ8f/4cALB27Vp8/vnnSg+wsprevXGpz03LlCoxEiIqDz79PRTpWVJ8vDNY3aEQaQTRCdC0adPg6uqKxMREGBoayo6/9957OHXqlFKDq8xMDXVLVO7us4Ja1dhMTkRE9C5ET0U6f/48Lly4AD09Pbnj1tbWiI6OVlpglGPZ4Zswq6pXfEEiIiqQhLPgqQCiW4CkUmmBW1w8efIExsbGSgmK5L14maHuEIiIiCoV0QlQjx494O3tLXsukUjw8uVLLF68GH369FFmbFQIgT1gRERE70R0F9i6devQpUsXODo6Ii0tDcOHD8fdu3dhZmYGX19fVcRIREREpFSiE6C6desiPDwcvr6+CA0NhVQqxbhx4zBixAi5QdGkOmwAIiIiejel2o/B0NAQY8eOxdixY5UdD5UAu8CIiIjejegE6NChQwUel0gkMDAwQKNGjWBjY/POgVHhuI0GEWm6J4mvUdNIH4Z62uoOhSoo0QnQoEGDIJFIIORrhsg9JpFI0L59exw4cADVq1dXWqCkKPdnIOEcTyISKTNbisiE17CtVVXdoYh2OzYVPb3PobaJAS5+2a3Y8vwLSQURPQvM398frVu3hr+/P5KTk5GcnAx/f3+0adMGhw8fxrlz5xAfH89VoVVMEASM/PkSRmy7pJCMElHl4XclEn9fi1F6vWN9rqDbt2dx6OpTpdetDIIgFPq3zf9mLAAgNiWtLEOiSkZ0C9C0adOwdetWeHh4yI5169YNBgYGmDBhAm7cuAFvb2+OD1IhQQASXmXgwr14ADn/rllVv1R1fXX4Ju4/f4ntH7aGlpZqvyf5XYlEllTAiLbWKr0OUWURlfAaX+y9BgB4tLqvUusOuPsCAPBL4CMMaF5XqXW/q8xsKfp8H4CGtYywZZSrusPRGNeeJGPj6Xv4orc9bMyM1B2OyolOgO7fvw8TExOF4yYmJnjw4AEAwM7ODi9evHj36KhQeTeNfpcusJ/PPwQAXHmUgLYNa75rWIV6k5Et+0Pe17kOqlXh6tak2a5HJ+PQ1af4rGsjGBsUvDVO8pvMMo6qfAh+lIi7cS9xN44bbJel/j+cBwDcjUvFqVmd1RtMGRDdBebi4oLZs2fLNkEFgOfPn2POnDlo3bo1AODu3buwtLRUXpQkR5p//JUS6syWKjY1H7seixVHbkJawGtiZUrfbuDKzVyJgH4bzmPruQdY/fctdYdCJOdx/Gt1h1AmRLcA/fzzzxg4cCAsLS1hZWUFiUSCyMhINGzYEAcPHgQAvHz5EgsXLlR6sJRDEORngqlqDPSk30IAAM6W1cpdEzlReZKVLYWWRFKqbuTbsakqiIhULfhRAl6mZ6FzE3N1h6J0mjKqVHQC1KRJE0REROD48eO4c+cOBEGAvb09evToAS2tnAalQYMGKTtOyudWzNs/mhJltAEVUcXz1PR3r5+okkrPykb7NadhWd0Q+z9th9S0TEQmvEbTuqbvXDcneJZfH/wYBAC4/GU3mJsYqDkaKo1SLYQokUjQq1cv9OrVS9nxUAn023AepoZ5xgzwjySR2lx7koznqemyLwpdvz2L56np2DW+LTwamb1T3Ur5clNC2VIBH/lcgZ15VSzs51hm1y2IshM/VSaScanpTIAqqFIlQK9evcLZs2cRGRmJjAz5ncqnTp2qlMCoaHkHR0okQMjjREz+PRSL+juij3OdYs+VS6Cg+j+0nKlPlUG2VMD3p+7CzaZGoclNbiJ04uazAsu8Ss/Ci5dvW1XLSyvPpQfxOHfnOc7deV5mCdC1J8lYcfQm5vV2QHOramVyTSqepiytIjoBCgsLQ58+ffD69Wu8evUKNWrUwIsXL1ClShWYm5szAVKDxFcZeH9zIADg099Di5wu+2vQIyw8eAML+jpgfIeGcq+lpmXC70oUejvXQb1qb/d1U/bf5/LyB78ySMvMhoEuV8ItK3tDnmD9qbtYj9JPS++w9jQSXr394ljUl4+y+L+Se4lMJUx2EOuDHwORniXF4M2BuL+yT5lfX1NdeZQAvytRmNfbvlRLqAiCgOcv02FuXLFbvkTPApsxYwb69++PhIQEGBoa4uLFi3j8+DFcXFzwzTffqCJGKkaf7wPknp+/+wLrT90tcPbWwoM3AADLj0TIHZdIgMUHb2D5kQgM/OGC6oIFW4OU5c6zVNgvPIY5f15Vdyga43HCK9m/BUHAv0+SkJ5V+KzGuJQ0fOd/BxN2BuPB85wp3XmTn+LkTYBU/a38wj3lLV2y4MA19PI+h7TM7CLL5d673FmoUQmvkS0VRH3pWn/qrlpbLPInqVKpgJM3n+FZOV6k8X8/BuHPkCdY8tfNUp2/7PBNtFlxCn+GPAGQ87sp5ve6vBCdAIWHh2PWrFnQ1taGtrY20tPTYWVlhbVr1+LLL79URYyV1rA29ZVSz6sM+T8yI3++hO/87+CIiNVjJQDO/bcwWt7m+dLadSkSQffj39bPVh+l23T6HgBgT/ATNUeimXwvR2HADxcw8udLBb5+82kK2qw8hfWn7uLEzWf4yOcKUtIKWNeniP8bBbUOJb3OwAebA7HrUqTCa1KpgJfpWXiTUXTikZcAIC41DVvPPSjxOcX57WIkbsWmwv/msxKfczA8Gh3WnsZU3zBR1/rO/w5O344rUdnMbNUvwbEvLBrjdwajw5rTKr/Wu4qMf1Xg8eLSyR0XHgEAVh2NwNOkN5i15ypafeWPfaEV62+R6ARIV1dXtvCehYUFIiNz/hOamprK/k0lM7S1lUrrf5L4RuQZyvkWFfI4AV/uv4ZhP11USn3l3bOUNHy5/xpuxaaoOxQqQ1/uz1nYM2/jw8v0LNm/Lz9KkCv/OP41mi05oVhRCf/b5V7nh3/uIfhxouz6uRYdvI6GXx6F0+LjcFh0DGuPlXx9ofwzPVPSMgtNor7cfw2LDl7/LyYB6VmFJ1ti/qJsPnMfAER9ccsVm5yOyw8TMMMvvNAvcKv+jkDjBX8Xu+zA7D+uik7C8jrzXzKWkS1F0mvVtIoIgqCU9dne9Ztp/KsMeKz+B/vCogEAS0vZoqQuohOgli1bIjg4GADQpUsXLFq0CL///jumT58OZ2dnpQdYmam6VURA4XvpKMaivHEIUQliEy/Ve5aShtHbL+NURMm/keaKSniNVUcjEJNc8PuatjsMuy5Fopd3QIGvqwp7Esves5SiW0cjS7GAXP5EKa+8//cysqW4GpWE1LSsAsvuDHos93zTmfsYs+NysR+UBS2C2mzJCTgvOa5w/HlqOnZdisTOoMe4+CAeNvOOosmCY0W+70cvXiHkcaLsefzLdOwLfaLQPfauvVhDtgRhf1g0Fh+6UeDrW84+gCAA35y4XWgdrzOy8EfIExy6+hQLD1zHiG0XsTek6FaN/K10ed9Gi2X+Bd7f4hT0dzv5TSa6fnMGa4/dwvubA9F93VlkFdCidT06GWfvPJc7dvlhAo78q5hYFvanPe/lQx4nov+G87hSxO9p3hgLcz06udyteSU6AVq5ciXq1MmZZfTVV1+hZs2a+OSTTxAXF4etW7cqPcDKTNXd1muP3caYHVdKVFYiebd4Ltx7Ae+Tdwr9Y6vucT+LDl7HuTvPMe6XYNHnDvvpIrace4DxhZx786l6Wn7UfU9L4sztOEz6NUQp3arlwZtixrTsCY5S2rWkUgF3n73dCmLKrlAM3HgBfnmu8WvQI7lWp/zO3H6OSw+L/uAKj0oqsKstq4D/y3k/zIdufdvC+0vQo0Lr7/zNGby/OVCWJA3dehEz91yF/cJjcuXyvo/8X8iypQIu3Hsh+4DN//rTpLdfTu4U8CF75vbbhKCo/zd5X/v14mNcuBePWX8ojrEr6otl/pb34sZB5ZeSlon2a06jwdwjWPrXDdnf1N8uPsaDF6+w6cx9hEYm4cHzV3j0XxfW+bsvMGjjBdyKTUG/Defx4fbLePjibffWkC1BmLwrFJvP3JeLJ/c2Po5/hcP/Frwp7gc/BuJadDL+99+6R6WRkpaJfhvOo6f3OeW0XCmJqARIEATUqlULbm5uAIBatWrh6NGjSElJQWhoKJo3by46gE2bNsHGxgYGBgZwcXFBQEDh36L37duHHj16oFatWjAxMYG7uzuOH1f8lrJ37144OjpCX18fjo6O2L9/v+i4ykJZ/BqcvfMc3/nfwb24ojPvd22MGrHtErxP3sVf/z6VW6W6wGu9w8Uu3HuB8b8EF9oaU5h3Wcwx9w/aDTUlOhXZmB1XcOxGLL46XLGaxvM7/O9TzPnzaoHfovPyCXxUqvrv5dvzKjY5DVN3h2HyrlDZsZMRiuNcFh68gfn7r8kGoxYkSyrFrkuRuPQgHlceJWCd/50Sj4VZeTQCt2JTcDA8ulQDjfN+2J2/9wJXo5IK3d8rOqng/9PPU9Px+6XHGLHtkmy2a/5YfvhvPBwA3I17idS0TLkygXnGIxb1l7ewv01SqYCNp+/hcgHJZP5zrkYlyT0XAETEpKDP9wE4fSvnZ5i35WZf6BOM2XEZ9+JScfFBPPwuR8nuxY4Lj9Dwy6MIi0wssiVp5M+XEB6VJPclLTJBsVVuzbFbcomn1n/Bd/r6DKbsUuz2EwShVF+0BEGQize3exPIacksL0RNgxcEAXZ2drhx4wbs7Oze+eJ+fn6YPn06Nm3ahHbt2mHLli3o3bs3bt68ifr1FQcInzt3Dj169MDKlStRrVo17NixA/3798elS5fQsmVLAEBQUBC8vLzw1Vdf4b333sP+/fsxZMgQnD9/Hm3btn3nmJWprMYFrz91F+tP3RU1bfeb42+bicXEGZXwGvWqGxZfsJRGbMsZcJqZLcUvY9uU+Lzy851DeSrSe4pNLr8zYoqT+CqjwA8HZer+3Vm5526rTpX43IPhT3EwvOBv7wCw4kgEbuVrFaleRX4dsLCoRBRk67kHssHRB8KisWpwswLLFfYhOd0vXPbv/GOWSuo7/9u4819LWP5EsTDBjxLRqXGtIstkZkvhezkSHrZmaGReFZvO3MOx67EFlj0QHo2v//ubGLygO16nv21FeZr0Bg51cjYIz9vVl6vVMn/Zh/5HPlfw84euGL8zGGvfb4YPXCwxc09OC1NuK1U3e8WtNUb9fBldCzien9hxn0X9bb8alYSBG0s3I3j8L8G4/SwVPh+1QQ0jPbkEqPXykwj6shuq6pdqGUKlEtUCpKWlBTs7O8THxxdfuAS+++47jBs3DuPHj4eDgwO8vb1hZWWFzZs3F1je29tbtumqnZ0dVq5cCTs7O/z1119yZXr06IF58+bB3t4e8+bNQ7du3eDt7a2UmJXJqd67L5WvTHn/huX9RlXY+KCMLCkOhkcjLvXth1tJvi0oo+umPE8xLSsVabGyihOposLG3FQU+ZMfAArTn+fvv15sPadvP5cNfs7vn1vix9YVZciWt90tqWlZconF8sM38c2JO0We/5HPlUK75XJb0n4JfIRFB2+g+3dnEZeahrXHbuPfJ8kFnvOd/9vruS4/iY5fv53hNe6XYNnfo7E+ikMO8rd4jPslGIIAzP7zX9jMO6pQ/tQtxZa+l+lZOHRVMcndHxaN1IJmFgK4+ywVWdlSLC7kZ5arsL8jBSU/MclvkFHEsg8AMNMvHKduxeFJ4ht0/+4sWn3lL/d6anqWrCVM3USPAVq7di1mz56N69eL/w9TlIyMDISEhMDT01PuuKenJwIDA0tUh1QqRWpqKmrUqCE7FhQUpFBnz549S1xnWdIuxcaJqlKabqnNZ+5j2u5w9F1/Xu54Qf+f1D0NXpW5QkX+cKfiFTfuR5OcKGRa+6M8g6AfvSh4anVpHc7X7bjt/MMSnVfcjKS83WKFje/LVVzLStuVpxCV8LrIQcCqsPH0/UJnrC0/EgH31f/gl3yD4/MKfpwo61YsCfdV/2DAD+eLLJM7I6won/mGYX+Y+qfMi26DGjlyJF6/fo3mzZtDT08Phoby3R0JCcWPFAeAFy9eIDs7GxYWFnLHLSwsEBtbcDNkft9++y1evXqFIUOGyI7FxsaKrjM9PR3p6W/HiKSkaMZYj/zTV8UuZHXyvxlVecfXFJYMlDYBSXiVgYcvXsLFukbxhTVMhUq8KlSw8k7cKNnfI013//lLJL3OFPWBqi6CIMjNlCqs5UeMDmvVs+7P6dvPC32tJGMfQyOTRF2voBbF0pjhdxXvtbRUSl2lJToBUnZXUv7uFUEQipySncvX1xdLlizBwYMHYW4u3zcqts5Vq1Zh6dKlIqKuHJosyDsLo/D7o+z1bcS0BnmsPoW0TCl2jm2DjsX06RelAn/+Fq5SvqnypxxNWinXun17tvhC5cSk30JKNT2dlKukn/eqIjoB+vDDD5VyYTMzM2hrayu0zMTFxSm04OTn5+eHcePG4Y8//kD37t3lXqtdu7boOufNm4eZM2fKnqekpMDKSrWLFKpDUavDFvU76Hs5qtDBj/kJQslbexJeZWDCzmD8z9USXq0LXhU7LTOnv/nM7efvlACRehU3M7AsXY9Oxgy/cMzrY4+u9kX/rQHKV+ykHMdvKHfMEpVO25WncHl+9+ILqojoMUAAcP/+fSxYsADDhg1DXFzOYKZjx47hxo2CF6AqiJ6eHlxcXODvLz9Ayt/fHx4eHoWe5+vrizFjxmDXrl3o21dxVpO7u7tCnSdOnCiyTn19fZiYmMg9KiOHRccKfU0dOfg6/zsIfpyIL/aKnx0iuhm2Ag0YLil+MJfORz5XcDfuJcb6iF8TioiUJ+4dlidRBtEJ0NmzZ+Hs7IxLly5h3759ePkyZ1riv//+i8WLF4uqa+bMmdi2bRu2b9+OiIgIzJgxA5GRkZg0aRKAnJaZ0aNHy8r7+vpi9OjR+Pbbb+Hm5obY2FjExsYiOflt/+20adNw4sQJrFmzBrdu3cKaNWtw8uRJTJ8+XexbLRODW9ZTdwgAlLfGjZgP5aIWcKPKpTzln4XNmikMe0qIKifRCdDcuXOxfPly+Pv7Q09PT3a8S5cuCAoSt1Kkl5cXvL29sWzZMrRo0QLnzp3D0aNHYW1tDQCIiYmR219sy5YtyMrKwuTJk1GnTh3ZY9q0abIyHh4e2L17N3bs2IFmzZrBx8cHfn5+5W4NoFwuDaqrOwQAwIID4mb1ZUuFQpOd4j4vlP1hmC0VEBGTUuQKo0VdMmeg9TvMXFHTB2R5SiqUrbg9psrSswq8hhERFU70GKBr165h165dCsdr1apVqvWBPv30U3z66acFvubj4yP3/MyZMyWq84MPPsAHH3wgOhZ18HK1KtEaHOXJ89R0eK47i8TXit+kk99kwrKMc7oFB67D93IkpnZthJmeTUSfn7tOxfkvusCyehVlh6cy5TEB+vdJEl6mZ8HD1uyd6pniG4Yj/8bg4rxuqG1qoJTY7jxLxfIjEbJxZUSk2US3AFWrVg0xMYrLwYeFhaFevfLRnVOR6GiXahhWmcvtrkrLzEbrFScLTH6AnKXbtxezTkdBA64D778ocGO/XNsvPMS3hWxi6Hs5p5Vw/T/3CnwdKFmycD363afCaroBP1zA8J8uIS7fQpVic7XcLSd2X4kspmTJjdl+GefuFD5lmIg0i+hP3+HDh+OLL75AbGwsJBIJpFIpLly4gM8//1xuvA5VLk6Lj8NrS1CB+8vkdzNG/Hii4T9dgt2Cv7Et4EGhZTYUkeAo0+uMLHhtCSo0lqxsKSbvCi020VO18jwIWt2DGwvylF1ZRJSH6ARoxYoVqF+/PurVq4eXL1/C0dERHTt2hIeHBxYsWKCKGKmcuPQwAbsvK2+36/wEIWf1UpXULSJZ+P1iJC49TCg0luM3nuHIvzFYpuYNPpXZBfY4/hWGbAmSW6L+dmwqZv9xFVElSHqLU5G27SAizSB6DJCuri5+//13LFu2DGFhYZBKpWjZsqVSNkel8q+wTROLk3fGV8DdF9DVliCiFC1FpVXY529B3W7FbX3wqpSz19Iys/Hvk2S0ql+tRF2fp2/FYcM/d/HN/5qjYa2q2HHhIbKlAsZ3aFiq6xdlhl84QiOTcPlhAh6t7gtBENDT+xyAnBmCR6d1KPL8u89SEaGkFWLzUvU2FNN2h6GxhTEmd2mk0usQUfkjOgE6e/YsOnXqBFtbW9ja2qoiJo3zgYslztyOw4uX4raiUIcwkcumA8DKoxGyHaUB4PM/rhZZfm/IE7zvYlnszs/hUUloYVVN4bggCDj8bwyepaTJkoW8CdCt2BR8c/w2ZvZogl2XH+c5Dwi4+7z4LUHyjGG6F5eK1BImRNN2h+H4jWf4qF0DzOjRGCYGukWW/+i/jRWn7Q6H30Q32d5G77eyRHUjvaJOFS3/717eheJuPys+semx7lyRr5e2/WfL2QfQ0ZJgdk/7UtZQtNxd1JkAEWke0QlQjx49ULt2bQwfPhwjR46Ek5OTKuLSKN/8rzkEQShwZ+CKrvf3AaJbemb9cRUNaxnhvU1F7yk0aOMFtKpfTe7Yp7+HIDLhNa5H51yzg10tNKltLPcB3Ms7AMDbXaFzfX/qbrELLIZHJSE0z87Uk34LLebdAPP2/YsHz1/h0sOcffJ2XHiEHRce4dZXvWCgqy0rt/rvW7Aw0cdH7Wzkzk98nYHMrLfvIHd36YKSikcvXiH+VQZcrMVNxcvfRZh3/ytlLJJZXA/Yl/uvwVhfB/P6OCi8tvH0fYUE6HlqOkIjE9HN3rzEEwkkkvI5c46I1EP0GKCnT59izpw5CAgIQLNmzdCsWTOsXbsWT56of2fXikyd+6GoUmm7uY6VcAPK/Bv5Hb0WK0t+gJzkoaRKsrr0oI0XsPvK23FQ8S8LHuybkpYpW5fI93KULPnJ60ni27E1t2NT8ePZ+8XuYA28TUgK+jDv/M0ZvL85UPSO3HnrCnks382pjF/NovKOqITX2HUpElvOPSh0f6asbKlc12Pv7wMw8dcQ+AQ+evfgiEgjiU6AzMzMMGXKFFy4cAH379+Hl5cXdu7ciQYNGqBr166qiJE00Jazhc8GE2Po1ot4kvhaZYNwC1oO4M6zVDRbcgJjf7lS4nqKWhX7SeIbJORJ5Dy9zyEm+Q3yphWB915g0MYLcjH8fS0Guy5F4v7zorsSAfkEKP9u3pnZyr135+48R89159BsyXH8evGxrEWrKH3WB6Dp4uNI/K978sV/iefJiJLv6VSaPC49KxtXnySV4kwiKu9Ed4HlZWNjg7lz56J58+ZYuHAhzp6tOLsBk+Zov+Y07Gsbl8m1DoZH4/J/rT1nbj9XWA+nMPfiim596vLNGdm/k15nwn3VP3KvD992Se75ubvP8dvFt2voPFqtuG+eGLsvRyLoQTy++V9z6JZm7ao8Gdbo7Zdl/1544DpOzeqUp5iAglKVO89ykriWX/ljYIu6JbrkP7eeYcWRCHw3pAWaW1Ur1ZYWE38NEb/vHBFVCKVehe/ChQv49NNPUadOHQwfPhxNmzbF4cOHlRmbxhnRtuAd0endlfZD7NeLj4svlMe03eH4/dLbxOOrIqf1v/2gP3f3hezfPb47iwZzj4i6bn55k5/8XmdkKbSIFddCNnffNRwMf4qOa0/j9K04PEtJK7T7T6wzt98uThiTnFZsLLkDl4sz1icY95+/wsCNF0q8rYZUKmDJoRs4GB6Nhy9eycVGRJWL6BagL7/8Er6+vnj69Cm6d+8Ob29vDBo0CFWqVJwtBMqrRf0d5T48Sf0WitwjLb+/rhb+Yb3jwkOcuPkMz1PT0de5juz43WJmv5WGIAiQSCS4FZuCXt4BGNyqHta83wxxqenQ1ZYoLBJ48UHB29rEJKfJZqcBwPJBThjpZl3oNXNdfZKMe3Ev8fXxWwrlvsqznlKHtadFvi/gWUoaLEwMZO+xIB/vDCmynsD7L7Do4A10alwLPoGP4FP0+HsiqgQkgsjBER4eHhgxYgS8vLxgZia/3094eDhatGihzPjUIiUlBaampkhOToaJiUmZXvtdv/kTFWRq10ZIfpOJX4Letmg51DFRylpMD1f1UZjBePiz9niWkoZxvwS/c/0lMb+PA7ZfeIjBrephevfG0NXW4v8logrgXbvn8xPz+S06AcovOTkZv//+O7Zt24arV68iO7t87OD8LpgAEb0bvwlu8Np6UW3XX+fVHDP8il5viojUT50JUKnHAP3zzz8YOXIk6tSpgw0bNqBPnz4IDi6bb3uVWeBczqSjii+2hIO/VYXJDxEVR9QYoCdPnsDHxwfbt2/Hq1evMGTIEGRmZmLv3r1wdHRUVYwapW41QzQ0M8IDkeu4EJUn16OT1R0CEVGRStwC1KdPHzg6OuLmzZvYsGEDnj59ig0bNqgyNo2Vd1owUUX0U8BDdYdARFSkErcAnThxAlOnTsUnn3zCjU9VrLKuCk1ERFRelLgFKCAgAKmpqXB1dUXbtm3xww8/4PlzrpFBREREFU+JEyB3d3f89NNPiImJwcSJE7F7927Uq1cPUqkU/v7+SE3laqnKlH+TTyIiIlIe0bPAqlSpgrFjx+L8+fO4du0aZs2ahdWrV8Pc3BwDBgxQRYwaafuY1uoOgYiIqNIq9TR4AGjSpIlsJ3hfX19lxUQAqlXRQ00jPXWHQUREVCm9UwKUS1tbG4MGDcKhQ4eUUR39Z/kgJ3WHQEREVCkpJQEi1eBkMCIiItVgAlSOvdsmJURERFQYJkBERESkcZgAlWNsACIiIlINJkBERESkcZgAlWM2ZkbqDoGIiKhSErUbPJUthzom2DLKBbVNDGBrXhVOi4+rOyQiIqJKgS1A5VzPprXR3KoaqurroFfT2uoOh4iIqFJgAlSBbBzRSt0hEBERVQpMgCoQbS0JWjeoru4wiIiIKjwmQBXM0gHcHoOIiOhdMQGqYBzrmuD+yj5o16imukMhIiKqsJgAVUDaWhJsGuGi7jCIiIgqLCZAFZSpoa66QyAiIqqwmAARERGRxlF7ArRp0ybY2NjAwMAALi4uCAgIKLRsTEwMhg8fjiZNmkBLSwvTp09XKOPj4wOJRKLwSEtLU+G7ICIioopErQmQn58fpk+fjvnz5yMsLAwdOnRA7969ERkZWWD59PR01KpVC/Pnz0fz5s0LrdfExAQxMTFyDwMDA1W9DbXp2LgWquhpqzsMIiKiCketW2F89913GDduHMaPHw8A8Pb2xvHjx7F582asWrVKoXyDBg3w/fffAwC2b99eaL0SiQS1a1f+VZN/+ag1sqUCbsWmQhCA/j+cV3dIREREFYLaWoAyMjIQEhICT09PueOenp4IDAx8p7pfvnwJa2trWFpaol+/fggLC3un+soriUQCHW0tONUzhbOlKX4d10bdIRFVag1rFb5B8e4Jbjj8Wfti6whf1EOZIRXowOR2Kr8G0bsa195GrddXWwL04sULZGdnw8LCQu64hYUFYmNjS12vvb09fHx8cOjQIfj6+sLAwADt2rXD3bt3Cz0nPT0dKSkpco+KqINdLfw02lXdYRBVWn9P64C5ve0Vjrs1rAG3hjXhVM8UVxd5YqRbfdlrtYz15cpWq6Kn0hgnd7FFC6tqxZZbOqBpgce72ZsXe+6JGR1FxeTpaAFPR4viC+ZjW0TCWRrfDWkOD9uKuYbalfnd1R2CnJ1jS/+F29urBcIX9cDCfo5KjEg8tQ+Clkgkcs8FQVA4JoabmxtGjhyJ5s2bo0OHDtizZw8aN26MDRs2FHrOqlWrYGpqKntYWVmV+vrq1sPRAneW91Z3GESVkr6ONnoWsCnx+60sZf82raKLOb1ykiRdbQmuzO+OMR4N5MqbGBQ/+mB2zyZFvr7r47a49VUvPFrdFx+4vL1+b6c6ABS/XY92t5Z7/qFHAyzp74hBLeri6w+aAQC++V9zbPvQFV9/0AzO9Uzlyp/+vDN6O9XGjyNbobGFsex4g5pVsKCvA7rZm2P5oIJXqh/axgrmJm8TwTY2NQosd+urXrg4rxtm92yCzSNa4dSszkXeg8L0alobXZrUAgD8Oq4N7q7ojVtf9cLgVpbY9bGb7LX88v+cCqKrXfrPp8OftVdIwP6Z1alE59Yy1od7w7fnjmhbH8EL1JMUmVXVR8fGtXB/ZR/R5zaxMMaglvVU/kWgJNQ2BsjMzAza2toKrT1xcXEKrULvQktLC61bty6yBWjevHmYOXOm7HlKSkqFToL0dNSe1xJVWjZmRvh+aAtM2x1eaBkTA12EL+oBfZ2cSQqL+jnCq7WVLHH4e3pHrPO/g7HtbJD8JhObztxDwN0X+epQ/PPczd4cp27FyeIw0M2pf1x7G/wZ8gQAUFU/57xBLerh5/MPAQB/TWkPx7om2Bn0GADQsn41AMCYdm+TpP+5Wsn9+3+uVvC9HIl5+67Jrrd5pOICrAKA8R0aYnyHhsjKliLofjy0tCT4so89ahjp4UniG9jWqorTt57LzvF0tMDlhwkKdRnoaqO2qTYmd2mk8Jqxvg5S07MUjue3e4Ib3BrWhCAISHqdiepGOR+0unnmi3z9v+b48cx9bPvv/uRaMqApPB0tEHg/Ht0czOFQxwS62lqw/fKorEwPRwscvZbzudXcqhr+fZIEQSg4lker+6LB3CMAgFWDneFUzxR5v9/3da6DhrWq4sHKPrgZk4L0LClORTzDpjP3ZfVfjUqC+X+tiDp5kq8V7zkXeb2S+KKXPcyN9WGgq43Ju0IVXv9uSHPM3HNV4bhVDUMAOYvytmtUExfuxRd5nQtzu6Ld6n/+i7v8bOektgRIT08PLi4u8Pf3x3vvvSc77u/vj4EDByrtOoIgIDw8HM7Oir8sufT19aGvr1/o60REeQ1sUQ+R8a/xrf8dAIot2YB8V5eWlgQOdUxkz+tVM8Q3/3s7kzUuNU0hAfqfqxUWHrwhd2ykmzXuP3+JzGwB5sZvZ7Y61DHBhI4Noa0lQQOznG4jXZ23MTlb5rTmmFXVw4uXGdg6qmRd5V6uVrAzrwr7PLEXRUdbCxtHtJI7ZlurKgCgap6E7kOPBtDV1kK7RjVhW6sq1hy7rdDilGt+HwdsO/8Aeya6Y9HBGzh75zkc6pggIiZnqMLPH7pi3C/BAHK6Vtz+ayWRSCSy5Cc/s6r6WNDPUZYAfdzBBiPa5rSQeTQyg0cjswLPW/tBM/Ryqi1LgAY2r4u9k9wxZEsQGplXxfEbz5D8JlPuHN+P3XDv+UsMa5PTLWpi8HYR297OOa2JWloSOP33/l2sq8O5nimqGuigmWU17L4ciX7N6wIA3BrWVPg9yVWtiuLiuDO6N0Zrm+oY/tMlADldl57rzsnu1aCW9WRlJ+9SrFNHWwt25lVxN+6l3PH1Q1vK/v3r2LZ4k5kNI30dWfI1qZMtAKCFlSna2tSU+znk/b1VN7XOAps5cyZGjRoFV1dXuLu7Y+vWrYiMjMSkSZMA5LTMREdHY+fOnbJzwsPDAeQMdH7+/DnCw8Ohp6cHR8ecvsSlS5fCzc0NdnZ2SElJwfr16xEeHo6NGzeW+ftTp77OdXDkWozcsUtfdsPNpyn4yOeKmqIiqjwmdbaVJUCdC+lSKSltrbfJypX53aElyWkN+Z+LJf74r2UHyPmgPDWrMwRBkDsHAL7s4yD3vImFMQa3rCc3BunC3K54nZ5daGKQn5aWBK4NCu6uylVY60d+kzrZIiwyEQNb1IOuthY+zNPdVNC4qlwfd2yI8R1sIJFI8MvYNohLSUN1Iz3Yzf8bAOTia1LbuLBqitS6QQ1Z4liQs7M7IzQyEQOb14NWnvtuoKsNHW0t7Ps0Z9D55z3TsM7/Lnwvv13Kxd22JtzzdHst6u+IK48S0Kp+dfR1rlPg9XrnOT7xv2QCAD7u0BA1jPTQzvZtgpab1OZ2j+36uC2G/3QJo92tMa27HaISXsvKWlY3xMmZHREWmYQB/yVV+R2Y3A6DNl6QPd8/uR3uPkvFq/RsjPz5EqZ0aQSrGlVkr2tpSWD0X6vjZ10bISwyCbM8G0NXW74nYtf4tkh4nYH6NaugvFBrAuTl5YX4+HgsW7YMMTExcHJywtGjR2FtnZOJx8TEKKwJ1LLl28wzJCQEu3btgrW1NR49egQASEpKwoQJExAbGwtTU1O0bNkS586dQ5s2mjVD6ofhLXFknnwCZGFigOpV9GCgq4Wq+jqY2NEWK45GqClCoopNV1sL91f2wZvMbFm3U2l1d7BAEwtjtLKuJpewrBzsjJFu1hj43weSvo7Wf4lP8eNQJBIJvvNqIXdMX0db1i2nLHVMS/aN3tRQF7snuJfqGnlb2MxNcq53bYkn0rOkMDXUxYzujRGbkgb7UiZAxY07ta5pBOuabxOkL3rZI+DucwxuVU+unLmxAVYNdsaEjg1Ro5AxLnVMDRG8oHQzAfV0tGQtSbn2f9oOh64+xcjcFixbM1xf2lP2O6mv+zYR0ZJI0MjcGI3MFe9T2MIeePEyHXYWxnCxro6wyER0squFqvo6aFm/OgAgYlkvGBax9twsz8LHrRXWqqZOEkEoaf6uOVJSUmBqaork5GSYmJSs6bc86rD2H0QlvMGnnW3hblsTHexyvqWmZWZDW0sCXW0tRCe9kfXN5rfmfWd8sfdaWYZMVK49Wt1XLdf9+vgt3I97hU0jWsm1QKjTpQfx+CngIZYObIp61QzVHU6pDNx4ARFPUxC6qMc7J7Hl2Tr/OzDU05Z1TRVHKhWQkS2VjTGrSMR8fjMBKkBlSYAys6V4k5kt1+dckPN3X2Dkz5cUjj9a3RdxKWk4fiMWXx2OQEa2VFWhElUI6kqASDWkUgGZUqnSW8VIfcR8flfelJegq62l0A9bkPZ2ZmhVvxpCI5Nkx3Kbds1NDDDKvQFGuTfAtSfJqGWsD7dVp1QVMhFRmdHSkkBfi8mPpuJ8aVJw66te+PZ/inutOVuaorapAR6t7ou/p3XAxI4NC62jehVdzM83KHNBXweFcga68r+CTSyMseb9wmfsERERKQMTIAIAfDXICYa62pjb2x4GutrFDgp0qGOCeX0UE5pc/jM74eOODbH/Uw/ZsTqmhvhxpAuq55mumX9Q5J5J7vBqXR9Hp3YAAOhoSXB/ZR9sHaW4/ggREVFpcQxQASrLGCCxsqWKU2uLM/HXYBy/8UzumKGuNiK+6iV7firiGUIjEzGrRxPZAM6UtExkZwuobqSHUxHPsPH0PfwwvBXq5hlM+SYjG5L/pgMDwOKD1/HLfwu5jXSrj/dbWaJl/eoY9fMlhbUx+jWrg8P/ys+CU6Z1Xs0xw09xgTCq3DgGiKh8E/P5zRYgkhGb/AA5S+d/+7/msMmzhkZji6pyZbo5WGB2T3u52SsmBrqytUi6OVhg36ft5JIfADDU05abhTCnlz2mdrPDsekdsHyQs2xqZvV8002/H9oCPwxvhS5NakGngPdU0B5IuSubFmbt+83knrta18BnXeVXqzUuwfYG7RqV/32IVg9mFyQRVX5MgOidGBvo4n0XSwxq8XY9jPwrwSqLkb4OZvZoDPva8ln9gr4O8LCtiY3DW+HR6r4Y+F8s28e0xs1lveTKTutmJ7cA2+CW9fBxBxt8+78WCtfLu/P3kNZWuPVVL+wc2wY7xrSGVY0qmNrNTvZ6/RpVEDSvm0Idvh+74fbyXvBytcKPI1vh9/Fusm7BqV0b4dKXiufIvWc9bdxb0RvLBzlhene7Issqy9B864zklX+TzRZW1RSm1rbNs89T7h5TAEq98WFh+zYNb1tfboPNDnaqXWdkx0etVVo/EZUtzgIjpTDSf9tSY1m9bFf6NDcxwK6P3RSOSyQS6OXZDmDHR63RpYn8Tte25lUxuUsjpKZl5j8dXw10wohtb5cHMNDVRsfGbz+MdbVzFpR8mZ4lt0khkJME1DDSg1vDGpBIJFiTJxFoWb867izvLduzbWCLujgY/hT/c7FEH+c6spW6h7Wxwsr3nCGRSDDSLWeRs8B78bj8SHEPJX0dLdxe3htuK08hNiUNR6a2R9O6ppBKBcz+81/sDc1ZTfjR6r64+ywVXx2JwPTudjhzKw43Y1JwMiJnf6mCdnie2aMxvvtvxeNtH7pCKhXwR8gTuFpXR9v/3vePZ+/Lyu8c1wZ3Yl/Csa4JtCTAoatPkZaZjbHtGqCtTQ3M8AvH2g+aISUtCzsDH8n2tgKA+yv7QALg099DcexG7H8/t5yYzt55jg+3XwYA9G9eFyv/2wvpTUY2Hie8QhMLYzyOf40N/9zD+A426P19AADgz0nu+ODHIIX3JcbZ2Z3lFsIjooqPCRApxUg3a5y98xxd7c2LL1zG5vdxwK3YVHSye5u86OtoIT1LKms1MDbQxcmZHaGrrYW7z14iSypFu0ZmWDqgaZHdY0endsDf12Mwws0a+nk2od3xUWtU0Sv8v1feDWvXvN8M77eyRBubGjDQ1cbZ2Z1xKzYVno4WioPRC+mlXPVft9XFL7tBEATZeTnL1MtP87WzMJYlOq3+60Z8lpIGc2P9Age/j3SzliVAOloSVKuqr7BZZQc7M9k4LH0dbdneU8DbpEoiydnvyH/m292vOzWuhQNh0ZjuF46vP2gm64ZdOdgZltUN5Tbo7JQn+cybcBrqactaBRuYGeHbITkzGK/M746Y5DdoZlkN3w1pjsfxr/H9qYI3Ra6ip42gud0waNMFPHzxSuF1Jj9ElQ8HQRdAUwdBa5Lk15l4mvxGboNKZbgXl4psaen3JCpObiuIoa425vRqgrY2NRESmYgRbeoXukJw/Mt0jN8ZjCGuVgrL6Bem6zdn8ODFK9SrZogLc7tihl84JIDC1gq5El5lwPdyJAa3qoc6puJXBU7Pyi7RYnRPEl8jNDIJfZ3rlGrMWsN5RyAVgK2jXNCktjE6fX0GAGBf2xjHpneUvW8AmN7dDt4ncxImDn4mqhi4EvQ7YgJE5dnz1HSYVdUrdqmCdxGV8Bpbzz3A+A42lar1Iy4lDXeevUS7RjUhkUjQYtkJJL3OxMRODTGvtwN6fx8g22X84ao++Pn8QzjVM5XtME5E5RsToHfEBIhIM8Qkv8HZ288xqGU9GOhq43ZsKib+GowZPRrLBtMTUcXBBOgdMQEiIiKqeLgOEBEREVERmAARERGRxmECRERERBqHCRARERFpHCZAREREpHGYABEREZHGYQJEREREGocJEBEREWkcJkBERESkcZgAERERkcZhAkREREQahwkQERERaRwmQERERKRxmAARERGRxtFRdwDlkSAIAICUlBQ1R0JEREQllfu5nfs5XhQmQAVITU0FAFhZWak5EiIiIhIrNTUVpqamRZaRCCVJkzSMVCrF06dPYWxsDIlEotS6U1JSYGVlhaioKJiYmCi1bioa77168L6rD++9+vDeq4cgCEhNTUXdunWhpVX0KB+2ABVAS0sLlpaWKr2GiYkJ/1OoCe+9evC+qw/vvfrw3pe94lp+cnEQNBEREWkcJkBERESkcZgAlTF9fX0sXrwY+vr66g5F4/Deqwfvu/rw3qsP7335x0HQREREpHHYAkREREQahwkQERERaRwmQERERKRxmAARERGRxmECVIY2bdoEGxsbGBgYwMXFBQEBAeoOqdxYtWoVWrduDWNjY5ibm2PQoEG4ffu2XBlBELBkyRLUrVsXhoaG6Ny5M27cuCFXJj09HZ999hnMzMxgZGSEAQMG4MmTJ3JlEhMTMWrUKJiamsLU1BSjRo1CUlKSXJnIyEj0798fRkZGMDMzw9SpU5GRkSFX5tq1a+jUqRMMDQ1Rr149LFu2rET7z5R3q1atgkQiwfTp02XHeO9VJzo6GiNHjkTNmjVRpUoVtGjRAiEhIbLXee9VIysrCwsWLICNjQ0MDQ3RsGFDLFu2DFKpVFaG976SE6hM7N69W9DV1RV++ukn4ebNm8K0adMEIyMj4fHjx+oOrVzo2bOnsGPHDuH69etCeHi40LdvX6F+/frCy5cvZWVWr14tGBsbC3v37hWuXbsmeHl5CXXq1BFSUlJkZSZNmiTUq1dP8Pf3F0JDQ4UuXboIzZs3F7KysmRlevXqJTg5OQmBgYFCYGCg4OTkJPTr10/2elZWluDk5CR06dJFCA0NFfz9/YW6desKU6ZMkZVJTk4WLCwshKFDhwrXrl0T9u7dKxgbGwvffPONiu+Ual2+fFlo0KCB0KxZM2HatGmy47z3qpGQkCBYW1sLY8aMES5duiQ8fPhQOHnypHDv3j1ZGd571Vi+fLlQs2ZN4fDhw8LDhw+FP/74Q6hatarg7e0tK8N7X7kxASojbdq0ESZNmiR3zN7eXpg7d66aIirf4uLiBADC2bNnBUEQBKlUKtSuXVtYvXq1rExaWppgamoq/Pjjj4IgCEJSUpKgq6sr7N69W1YmOjpa0NLSEo4dOyYIgiDcvHlTACBcvHhRViYoKEgAINy6dUsQBEE4evSooKWlJURHR8vK+Pr6Cvr6+kJycrIgCIKwadMmwdTUVEhLS5OVWbVqlVC3bl1BKpUq+3aUidTUVMHOzk7w9/cXOnXqJEuAeO9V54svvhDat29f6Ou896rTt29fYezYsXLHBg8eLIwcOVIQBN57TcAusDKQkZGBkJAQeHp6yh339PREYGCgmqIq35KTkwEANWrUAAA8fPgQsbGxcvdQX18fnTp1kt3DkJAQZGZmypWpW7cunJycZGWCgoJgamqKtm3bysq4ubnB1NRUroyTkxPq1q0rK9OzZ0+kp6fLuiaCgoLQqVMnuUXOevbsiadPn+LRo0fKvBVlZvLkyejbty+6d+8ud5z3XnUOHToEV1dX/O9//4O5uTlatmyJn376SfY6773qtG/fHqdOncKdO3cAAFevXsX58+fRp08fALz3moAJUBl48eIFsrOzYWFhIXfcwsICsbGxaoqq/BIEATNnzkT79u3h5OQEALL7VNQ9jI2NhZ6eHqpXr15kGXNzc4Vrmpuby5XJf53q1atDT0+vyDK5zyviz3T37t0IDQ3FqlWrFF7jvVedBw8eYPPmzbCzs8Px48cxadIkTJ06FTt37gTAe69KX3zxBYYNGwZ7e3vo6uqiZcuWmD59OoYNGwaA914TcDf4MiSRSOSeC4KgcIyAKVOm4N9//8X58+cVXivNPcxfpqDyyigj/DcYsaL9TKOiojBt2jScOHECBgYGhZbjvVc+qVQKV1dXrFy5EgDQsmVL3LhxA5s3b8bo0aNl5Xjvlc/Pzw+//fYbdu3ahaZNmyI8PBzTp09H3bp18eGHH8rK8d5XXmwBKgNmZmbQ1tZWyNLj4uIUMnpN99lnn+HQoUM4ffo0LC0tZcdr164NQPGbTt57WLt2bWRkZCAxMbHIMs+ePVO47vPnz+XK5L9OYmIiMjMziywTFxcHQPEbY3kXEhKCuLg4uLi4QEdHBzo6Ojh79izWr18PHR2dQr9l8t6/uzp16sDR0VHumIODAyIjIwHw916VZs+ejblz52Lo0KFwdnbGqFGjMGPGDFkrKO995ccEqAzo6enBxcUF/v7+csf9/f3h4eGhpqjKF0EQMGXKFOzbtw///PMPbGxs5F63sbFB7dq15e5hRkYGzp49K7uHLi4u0NXVlSsTExOD69evy8q4u7sjOTkZly9flpW5dOkSkpOT5cpcv34dMTExsjInTpyAvr4+XFxcZGXOnTsnN031xIkTqFu3Lho0aKCku1I2unXrhmvXriE8PFz2cHV1xYgRIxAeHo6GDRvy3qtIu3btFJZ7uHPnDqytrQHw916VXr9+DS0t+Y9AbW1t2TR43nsNUIYDrjVa7jT4n3/+Wbh586Ywffp0wcjISHj06JG6QysXPvnkE8HU1FQ4c+aMEBMTI3u8fv1aVmb16tWCqampsG/fPuHatWvCsGHDCpySamlpKZw8eVIIDQ0VunbtWuCU1GbNmglBQUFCUFCQ4OzsXOCU1G7dugmhoaHCyZMnBUtLS7kpqUlJSYKFhYUwbNgw4dq1a8K+ffsEExOTSjMlNe8sMEHgvVeVy5cvCzo6OsKKFSuEu3fvCr///rtQpUoV4bfffpOV4b1XjQ8//FCoV6+ebBr8vn37BDMzM2HOnDmyMrz3lRsToDK0ceNGwdraWtDT0xNatWolm+JNggCgwMeOHTtkZaRSqbB48WKhdu3agr6+vtCxY0fh2rVrcvW8efNGmDJlilCjRg3B0NBQ6NevnxAZGSlXJj4+XhgxYoRgbGwsGBsbCyNGjBASExPlyjx+/Fjo27evYGhoKNSoUUOYMmWK3PRTQRCEf//9V+jQoYOgr68v1K5dW1iyZEmlmY6aPwHivVedv/76S3BychL09fUFe3t7YevWrXKv896rRkpKijBt2jShfv36goGBgdCwYUNh/vz5Qnp6uqwM733lJhEELiNJREREmoVjgIiIiEjjMAEiIiIijcMEiIiIiDQOEyAiIiLSOEyAiIiISOMwASIiIiKNwwSIiIiINA4TICIqNzp37ozp06eXuPyjR48gkUgQHh6uspiIqHLiQohEJFpxu09/+OGH8PHxEV1vQkICdHV1YWxsXKLy2dnZeP78OczMzKCjoyP6esrw6NEj2NjYICwsDC1atFBLDEQknnr+YhBRhZZ300Y/Pz8sWrRIblNPQ0NDufKZmZnQ1dUttt4aNWqIikNbW1u2azcRkRjsAiMi0WrXri17mJqaQiKRyJ6npaWhWrVq2LNnDzp37gwDAwP89ttviI+Px7Bhw2BpaYkqVarA2dkZvr6+cvXm7wJr0KABVq5cibFjx8LY2Bj169fH1q1bZa/n7wI7c+YMJBIJTp06BVdXV1SpUgUeHh4KO64vX74c5ubmMDY2xvjx4zF37twiW28SExMxYsQI1KpVC4aGhrCzs8OOHTsA5OwaDgAtW7aERCJB586dZeft2LEDDg4OMDAwgL29PTZt2qQQ++7du+Hh4QEDAwM0bdoUZ86cKdF1iejdMAEiIpX44osvMHXqVERERKBnz55IS0uDi4sLDh8+jOvXr2PChAkYNWoULl26VGQ93377LVxdXREWFoZPP/0Un3zyCW7dulXkOfPnz8e3336L4OBg6OjoYOzYsbLXfv/9d6xYsQJr1qxBSEgI6tevj82bNxdZ38KFC3Hz5k38/fffiIiIwObNm2FmZgYAuHz5MgDg5MmTiImJwb59+wAAP/30E+bPn48VK1YgIiICK1euxMKFC/HLL7/I1T179mzMmjULYWFh8PDwwIABAxAfH1/sdYnoHal3L1Yiquh27NghmJqayp4/fPhQACB4e3sXe26fPn2EWbNmyZ7n34Xe2tpaGDlypOy5VCoVzM3Nhc2bN8tdKywsTBAEQTh9+rQAQDh58qTsnCNHjggAhDdv3giCIAht27YVJk+eLBdHu3bthObNmxcaZ//+/YWPPvqowNfyx5DLyspK2LVrl9yxr776SnB3d5c7b/Xq1bLXMzMzBUtLS2HNmjXFXpeI3g1bgIhIJVxdXeWeZ2dnY8WKFWjWrBlq1qyJqlWr4sSJE4iMjCyynmbNmsn+ndvVFhcXV+Jz6tSpAwCyc27fvo02bdrIlc//PL9PPvkEu3fvRosWLTBnzhwEBgYWWf758+eIiorCuHHjULVqVdlj+fLluH//vlxZd3d32b91dHTg6uqKiIiIUl2XiEqOCRARqYSRkZHc82+//Rbr1q3DnDlz8M8//yA8PBw9e/ZERkZGkfXkHzwtkUgglUpLfE7ujLW85+SfxSYUMxm2d+/eePz4MaZPn46nT5+iW7du+Pzzzwstn3utn376CeHh4bLH9evXcfHixSKvlTc+sdclopJjAkREZSIgIAADBw7EyJEj0bx5czRs2BB3794t8ziaNGkiG7eTKzg4uNjzatWqhTFjxuC3336Dt7e3bDC2np4egJwWrlwWFhaoV68eHjx4gEaNGsk9cgdN58qbEGVlZSEkJAT29vbFXpeI3g2nwRNRmWjUqBH27t2LwMBAVK9eHd999x1iY2Ph4OBQpnF89tln+Pjjj+Hq6goPDw/4+fnh33//RcOGDQs9Z9GiRXBxcUHTpk2Rnp6Ow4cPy+I2NzeHoaEhjh07BktLSxgYGMDU1BRLlizB1KlTYWJigt69eyM9PR3BwcFITEzEzJkzZXVv3LgRdnZ2cHBwwLp165CYmCgbtF3UdYno3bAFiIjKxMKFC9GqVSv07NkTnTt3Ru3atTFo0KAyj2PEiBGYN28ePv/8c7Rq1QoPHz7EmDFjYGBgUOg5enp6mDdvHpo1a4aOHTtCW1sbu3fvBpAzbmf9+vXYsmUL6tati4EDBwIAxo8fj23btsHHxwfOzs7o1KkTfHx8FFqAVq9ejTVr1qB58+YICAjAwYMHZTO9irouEb0brgRNRBqvR48eqF27Nn799dcyuyZXkCZSL3aBEZFGef36NX788Uf07NkT2tra8PX1xcmTJ+Hv76/u0IioDDEBIiKNIpFIcPToUSxfvhzp6elo0qQJ9u7di+7du6s7NCIqQ+wCIyIiIo3DQdBERESkcZgAERERkcZhAkREREQahwkQERERaRwmQERERKRxmAARERGRxmECRERERBqHCRARERFpHCZAREREpHH+D+WqrcxR/FVuAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(losses))\n",
    "\n",
    "plt.plot(losses)\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "plt.title('Maze transformer training')\n",
    "plt.xlabel('Training steps')\n",
    "plt.ylabel('Average per-token cross-entropy loss')\n",
    "plt.savefig('maze_transformer_training.pdf')\n",
    "\n",
    "print(np.mean(losses[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c805f6d7-b305-4972-8190-2c31b62c95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def optimal_pred_loss_fn(batch):\n",
    "\n",
    "    path_len = batch['true_preds'].shape[1]\n",
    "    true_preds = batch['true_preds']\n",
    "    assert true_preds.shape == (batch_size, path_len, dataset.vocab_size)\n",
    "    \n",
    "    targets = batch['data']\n",
    "\n",
    "    pred_loss = 0\n",
    "    n = 0\n",
    "\n",
    "    last_step = np.zeros((1,dataset.vocab_size))\n",
    "    last_step[0,7] = 1.\n",
    "\n",
    "    for i, (sample, true_pred) in enumerate(zip(batch['data'],true_preds)):\n",
    "        target = sample[batch['start_index'][i]+1:batch['end_index'][i]+1]\n",
    "        pred = jnp.concatenate([true_pred[:1+batch['true_preds_end_index'][i]], last_step], axis=0)\n",
    "        #print(pred)\n",
    "        #print(target)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits = jnp.log(pred),\n",
    "        labels = target\n",
    "        )\n",
    "        pred_loss += loss.sum()\n",
    "        n+=target.shape[0]\n",
    "\n",
    "    return pred_loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb319da3-cf1b-4f06-bdde-104e133eac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start with, get an estimate of optimal loss, and of loss of our model, for the no loops case\n",
    "\n",
    "dataset = CustomMazeDataset(include_maze=False,no_loops=True)\n",
    "train_loader_iter = iter(NumpyLoader(dataset, batch_size=batch_size, num_workers=n_worker))\n",
    "\n",
    "model_loss = []\n",
    "optimal_loss = []\n",
    "\n",
    "for n in range(10):\n",
    "    batch = next(train_loader_iter)\n",
    "    loss = optimal_pred_loss_fn(batch)\n",
    "    optimal_loss.append(loss)\n",
    "    loss = loss_fn(state['params'],batch)\n",
    "    model_loss.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d88dcb01-35cb-4ae8-9b9c-6e282c77f437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "categories = ['Optimal Loss', 'Model Loss']\n",
    "values = [np.mean(optimal_loss), np.mean(model_loss)]  # Example values for optimal loss and model loss\n",
    "\n",
    "# Creating a bar plot\n",
    "plt.bar(categories, values)\n",
    "\n",
    "# Adding labels\n",
    "plt.ylabel('Average per-token cross-entropy loss')\n",
    "plt.title('Comparison of Maze Transformer to Optimal Policy')\n",
    "\n",
    "# Display the plot\n",
    "\n",
    "plt.savefig('optimal_vs_model.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c54d32-9af6-4209-b96b-b325b8f736e6",
   "metadata": {},
   "source": [
    "# Probe setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b9bfbd3-3328-4cf4-9c93-3ae6ff7124a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    # input should be batch_size x n_layers (one probe per layer) x emb_dim\n",
    "    #y = nn.Dense(features=128)(x)\n",
    "    #y = nn.relu(y)\n",
    "    #y = nn.Dense(features=64)(x)\n",
    "    #y = nn.relu(y)\n",
    "    # treat the layers as 1 D spatial dimension, the model dim as channel dim\n",
    "    bs = x.shape[0]\n",
    "    #print(x.shape)\n",
    "    #W_in = self.param('W', nn.initializers.xavier_uniform(), (num_layers+1, emb_dim, emb_dim))\n",
    "    W_out = self.param('W', nn.initializers.xavier_uniform(), (num_layers+1, emb_dim, 4))\n",
    "    b = self.param('b', nn.initializers.zeros, (num_layers+1, 4))\n",
    "    assert x.shape == (bs, num_layers+1, emb_dim)\n",
    "    x = nn.LayerNorm(reduction_axes=2, feature_axes=(1,2))(x)\n",
    "    #y = einsum(W_in, x, 'l m h, bs l m -> bs l h')\n",
    "    #y = nn.relu(y)\n",
    "    y = einsum(W_out, x, 'l m o, bs l m -> bs l o')\n",
    "    return y + b\n",
    "\n",
    "#def concat_acts(act, first_layer, last_layer):\n",
    "#  assert len(act['stream'])>=last_layer\n",
    "#  stream = act['stream'][first_layer:last_layer+1]\n",
    "#  #concat along model dim (i.e. keep batch and sequence positions separate)\n",
    "# acts = jnp.concatenate(stream,axis=-1)\n",
    "#  #print(acts.shape)\n",
    "#  #acts = acts[:,:,:]\n",
    "#  #print(acts.shape)\n",
    "#  return acts\n",
    "\n",
    "def prepare_acts(act):\n",
    "    assert act['stream'][0].shape[0] == batch_size, act['stream'][0].shape[2] == emb_dim\n",
    "    seq_len = act['stream'][0].shape[1]\n",
    "    # note: input has dimensions n_layer x batch_size x sequence_len x emb_dim\n",
    "    \n",
    "    acts  = rearrange(act['stream'], 'layer bs seq dim -> (bs seq) layer dim')\n",
    "\n",
    "    \n",
    "    # return a tensor in which all seq positions are in one big batch, then layer, then model dim\n",
    "    assert len(acts.shape)==3\n",
    "    assert acts.shape == (batch_size * seq_len, num_layers+1, emb_dim)\n",
    "\n",
    "    return acts, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47c940e2-255c-45fe-ae93-7c6e5fc0ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear probe training\n",
    "\n",
    "# Start a new experiment\n",
    "\n",
    "\n",
    "losses = []\n",
    "eval_losses = []\n",
    "\n",
    "#batch = next(iter(train_loader))\n",
    "\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "rng, key = random.split(key)\n",
    "\n",
    "\n",
    "#print(f'default loss: {jnp.log()}')\n",
    "\n",
    "pred, act = model.apply(state['params'], batch['data'])\n",
    "\n",
    "acts, seq_len = prepare_acts(act) # this is just to initialize the probe, so take first sequence position arbitrarily\n",
    "\n",
    "x = acts\n",
    "\n",
    "probe = LinearProbe()\n",
    "\n",
    "tx = optax.adamw(3e-4)\n",
    "\n",
    "params = probe.init(rng, x)\n",
    "\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "probe_state = {'params': params, 'opt_state': opt_state, 'loss': 0., 'step': 0}\n",
    "\n",
    "target_dict = {\n",
    "      11: 0,\n",
    "      27: 1,\n",
    "      29: 2,\n",
    "      35: 3\n",
    "  }\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(2,))\n",
    "def probe_loss_fn(params, batch, reduce_layers = 1):\n",
    "  pred, act = model.apply(state['params'], batch['data'])\n",
    "      \n",
    "  acts, seq_len = prepare_acts(act)\n",
    "  assert seq_len == batch['data'].shape[1]\n",
    "\n",
    "  targets = batch['data'][jnp.arange(0,pred.shape[0]),batch['end_index']-1]\n",
    "\n",
    "  #print(targets)\n",
    "\n",
    "  new_targets = targets.copy()\n",
    "\n",
    "  for k, v in target_dict.items():\n",
    "      new_targets = jnp.where(targets==k,v,new_targets)\n",
    "\n",
    "  #print(new_targets)\n",
    "  # expanding targets along seq len (result is shape \n",
    "  repeated_targets = jnp.repeat(new_targets, seq_len, axis = 0)\n",
    "  assert repeated_targets.shape == (seq_len * batch_size,)\n",
    "  repeated_targets = jnp.repeat(repeated_targets[:,None], num_layers+1, axis=1)\n",
    "  assert repeated_targets.shape == (seq_len * batch_size,num_layers+1)\n",
    "  \n",
    "\n",
    "  probe_pred = probe.apply(params, acts)\n",
    "\n",
    "  # probe preds are of shape (batch_size * seq_len, num_layer, 4)\n",
    "\n",
    "  assert repeated_targets.shape == probe_pred.shape[:-1]\n",
    "\n",
    "  assert len(repeated_targets.shape)==2\n",
    "\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "      logits = probe_pred,\n",
    "      labels = repeated_targets\n",
    "  )\n",
    "\n",
    "  # loss is of shape (batch_size * seq_len, num_layer). First, undo the reshaping.\n",
    "\n",
    "  loss = rearrange(loss, '(bs seq) layer -> bs seq layer', bs=batch_size, seq=seq_len, layer=num_layers+1)\n",
    "  \n",
    "  idx = jnp.arange(seq_len)[None, :]\n",
    "\n",
    "  mask = jnp.where((idx < batch['end_index'][:, None]) & (idx > batch['start_index'][:, None]), 1., 0.)\n",
    "\n",
    "  # need to broadcast mask over the last layer dimension\n",
    "  assert mask.shape == (batch_size,seq_len)\n",
    "  loss = loss * mask[:,:,None]\n",
    "\n",
    "  if reduce_layers:\n",
    "      # just add up losses over all the layers. that's fine.\n",
    "      loss = loss.sum() / mask.sum()\n",
    "  else:\n",
    "      loss = loss.sum(axis=(0,1))/mask.sum()\n",
    "\n",
    "  return loss\n",
    "\n",
    "@jax.jit\n",
    "def probe_train_step(probe_state,batch):\n",
    "  params = probe_state['params']\n",
    "  opt_state = probe_state['opt_state']\n",
    "  loss, grads = jax.value_and_grad(probe_loss_fn)(params,batch)\n",
    "  updates, opt_state = tx.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  step = probe_state['step'] + 1\n",
    "\n",
    "  return {'params': params, 'opt_state': opt_state, 'loss': loss, 'step': step}\n",
    "\n",
    "@jax.jit\n",
    "def probe_eval_step(state,batch):\n",
    "  params = state['params']\n",
    "  loss = probe_loss_fn(params,batch)\n",
    "  return loss\n",
    "\n",
    "probe_losses = []\n",
    "probe_eval_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c04f8eec-ecd7-4487-ba17-4a756bbbae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def optimal_loss_fn(batch):\n",
    "\n",
    "    path_len = batch['true_probs'].shape[1]\n",
    "    true_probs = batch['true_probs']\n",
    "    assert true_probs.shape == (batch_size, path_len, 4)\n",
    "    \n",
    "    targets = batch['data'][jnp.arange(0,batch['data'].shape[0]),batch['end_index']-1]\n",
    "    new_targets = targets.copy()\n",
    "    \n",
    "    for k, v in target_dict.items():\n",
    "      new_targets = jnp.where(targets==k,v,new_targets)\n",
    "\n",
    "    #print(new_targets)\n",
    "    # expanding targets along seq len (result is shape \n",
    "    repeated_targets = jnp.repeat(new_targets, path_len, axis = 0)\n",
    "    assert repeated_targets.shape == (path_len * batch_size,)\n",
    "\n",
    "    true_probs_reshaped = rearrange(true_probs, 'bs seq i -> (bs seq) i')\n",
    "    assert true_probs_reshaped.shape == (path_len * batch_size, 4)\n",
    "\n",
    "    log_probs = jnp.log(true_probs_reshaped)\n",
    "\n",
    "    #print(log_probs[:10])\n",
    "    #print(repeated_targets[:10])\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "          logits = log_probs,\n",
    "          labels = repeated_targets\n",
    "      )\n",
    "    #print(loss[:10])\n",
    "    \n",
    "    # loss is of shape (batch_size * seq_len). First, undo the reshaping.\n",
    "    \n",
    "    loss = rearrange(loss, '(bs seq) -> bs seq ', bs=batch_size, seq=path_len)\n",
    "    \n",
    "    idx = jnp.arange(path_len)[None, :]\n",
    "    \n",
    "    mask = jnp.where((idx <= batch['true_probs_end_index'][:, None]), 1., 0.)\n",
    "    \n",
    "    assert mask.shape == (batch_size,path_len)\n",
    "    loss = np.ma.array(loss, mask=1-mask)\n",
    "    #print(loss[:10])\n",
    "\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a27dd344-0edc-4ff3-a26f-929d6ac7c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "probe_options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=keep_n_checkpoints)\n",
    "probe_checkpoint_manager = orbax.checkpoint.CheckpointManager(os.path.join(base_path,'probe_layernorm_bias'), orbax_checkpointer, probe_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7b31e265-5753-4bcb-803e-d5f07eb79038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading step 30478\n",
      "12.799657\n"
     ]
    }
   ],
   "source": [
    "# PROBE LOADING\n",
    "\n",
    "\n",
    "dummy_dict = {\n",
    "            'probe_state': probe_state,\n",
    "            'probe_loss': np.zeros(1)}\n",
    "\n",
    "step = probe_checkpoint_manager.latest_step()\n",
    "print(f'loading step {step}')\n",
    "load_dict = probe_checkpoint_manager.restore(step, items=dummy_dict)\n",
    "probe_state = load_dict['probe_state']\n",
    "probe_losses = list(load_dict['probe_loss'])\n",
    "print(np.mean(probe_losses[-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec1257-a0ac-4662-9058-9e9fbd9c0403",
   "metadata": {},
   "source": [
    "# Probe training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7570854e-3825-480c-b00b-d8e9d4a1caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomMazeDataset(include_maze=False, no_loops=False)\n",
    "train_loader = NumpyLoader(dataset, batch_size=batch_size, num_workers=n_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2aaea6-bd4e-437f-8d9a-f207addc2a10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 30479\n",
      "loss: 13.317160606384277\n",
      "steps per second: 0.20494\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8870174884796143, layer: 11\n",
      "saving at step 30479\n",
      "----------\n",
      "\n",
      "\n",
      "step: 30480\n",
      "loss: 12.4995756149292\n",
      "steps per second: 0.27451\n",
      "step: 30481\n",
      "loss: 12.809378623962402\n",
      "steps per second: 0.55220\n",
      "step: 30482\n",
      "loss: 12.810978889465332\n",
      "steps per second: 0.56816\n",
      "step: 30483\n",
      "loss: 12.915701866149902\n",
      "steps per second: 0.53839\n",
      "step: 30484\n",
      "loss: 12.969505310058594\n",
      "steps per second: 0.55687\n",
      "step: 30485\n",
      "loss: 12.920815467834473\n",
      "steps per second: 0.54213\n",
      "step: 30486\n",
      "loss: 12.502585411071777\n",
      "steps per second: 0.56796\n",
      "step: 30487\n",
      "loss: 13.239951133728027\n",
      "steps per second: 0.53526\n",
      "step: 30488\n",
      "loss: 12.682209014892578\n",
      "steps per second: 0.53885\n",
      "step: 30489\n",
      "loss: 13.168342590332031\n",
      "steps per second: 0.54195\n",
      "step: 30490\n",
      "loss: 13.268899917602539\n",
      "steps per second: 0.54463\n",
      "step: 30491\n",
      "loss: 12.478841781616211\n",
      "steps per second: 0.52542\n",
      "step: 30492\n",
      "loss: 12.3281831741333\n",
      "steps per second: 0.53229\n",
      "step: 30493\n",
      "loss: 13.046053886413574\n",
      "steps per second: 0.58139\n",
      "step: 30494\n",
      "loss: 12.761725425720215\n",
      "steps per second: 0.53807\n",
      "step: 30495\n",
      "loss: 12.580958366394043\n",
      "steps per second: 0.53153\n",
      "step: 30496\n",
      "loss: 13.11741828918457\n",
      "steps per second: 0.54039\n",
      "step: 30497\n",
      "loss: 12.585529327392578\n",
      "steps per second: 0.55178\n",
      "step: 30498\n",
      "loss: 13.179287910461426\n",
      "steps per second: 0.56277\n",
      "step: 30499\n",
      "loss: 12.521368980407715\n",
      "steps per second: 0.54440\n",
      "step: 30500\n",
      "loss: 12.910385131835938\n",
      "steps per second: 0.53561\n",
      "step: 30501\n",
      "loss: 12.766538619995117\n",
      "steps per second: 0.52794\n",
      "step: 30502\n",
      "loss: 12.928037643432617\n",
      "steps per second: 0.56542\n",
      "step: 30503\n",
      "loss: 12.70790958404541\n",
      "steps per second: 0.56385\n",
      "step: 30504\n",
      "loss: 12.881819725036621\n",
      "steps per second: 0.54258\n",
      "step: 30505\n",
      "loss: 13.38232707977295\n",
      "steps per second: 0.54788\n",
      "step: 30506\n",
      "loss: 13.365983009338379\n",
      "steps per second: 0.50277\n",
      "step: 30507\n",
      "loss: 12.394474029541016\n",
      "steps per second: 0.54252\n",
      "step: 30508\n",
      "loss: 12.957321166992188\n",
      "steps per second: 0.48200\n",
      "step: 30509\n",
      "loss: 12.931253433227539\n",
      "steps per second: 0.43405\n",
      "step: 30510\n",
      "loss: 12.911656379699707\n",
      "steps per second: 0.43403\n",
      "step: 30511\n",
      "loss: 13.094532012939453\n",
      "steps per second: 0.50978\n",
      "step: 30512\n",
      "loss: 12.99975872039795\n",
      "steps per second: 0.53000\n",
      "step: 30513\n",
      "loss: 12.716851234436035\n",
      "steps per second: 0.50337\n",
      "step: 30514\n",
      "loss: 13.003417015075684\n",
      "steps per second: 0.46818\n",
      "step: 30515\n",
      "loss: 12.91506290435791\n",
      "steps per second: 0.46020\n",
      "step: 30516\n",
      "loss: 12.533567428588867\n",
      "steps per second: 0.57642\n",
      "step: 30517\n",
      "loss: 12.61023998260498\n",
      "steps per second: 0.60064\n",
      "step: 30518\n",
      "loss: 12.664144515991211\n",
      "steps per second: 0.57328\n",
      "step: 30519\n",
      "loss: 13.054727554321289\n",
      "steps per second: 0.54176\n",
      "step: 30520\n",
      "loss: 12.439871788024902\n",
      "steps per second: 0.53814\n",
      "step: 30521\n",
      "loss: 13.209712028503418\n",
      "steps per second: 0.50186\n",
      "step: 30522\n",
      "loss: 12.793041229248047\n",
      "steps per second: 0.53617\n",
      "step: 30523\n",
      "loss: 12.894018173217773\n",
      "steps per second: 0.52239\n",
      "step: 30524\n",
      "loss: 12.376986503601074\n",
      "steps per second: 0.53394\n",
      "step: 30525\n",
      "loss: 12.560823440551758\n",
      "steps per second: 0.51380\n",
      "step: 30526\n",
      "loss: 12.689082145690918\n",
      "steps per second: 0.54429\n",
      "step: 30527\n",
      "loss: 12.583770751953125\n",
      "steps per second: 0.52718\n",
      "step: 30528\n",
      "loss: 12.997308731079102\n",
      "steps per second: 0.52292\n",
      "step: 30529\n",
      "loss: 12.615036964416504\n",
      "steps per second: 0.55009\n",
      "step: 30530\n",
      "loss: 12.92390251159668\n",
      "steps per second: 0.51685\n",
      "step: 30531\n",
      "loss: 12.709468841552734\n",
      "steps per second: 0.51632\n",
      "step: 30532\n",
      "loss: 12.722745895385742\n",
      "steps per second: 0.53230\n",
      "step: 30533\n",
      "loss: 12.61755084991455\n",
      "steps per second: 0.50447\n",
      "step: 30534\n",
      "loss: 12.883993148803711\n",
      "steps per second: 0.50731\n",
      "step: 30535\n",
      "loss: 12.682846069335938\n",
      "steps per second: 0.53888\n",
      "step: 30536\n",
      "loss: 12.955850601196289\n",
      "steps per second: 0.56646\n",
      "step: 30537\n",
      "loss: 12.931529998779297\n",
      "steps per second: 0.50954\n",
      "step: 30538\n",
      "loss: 12.67731761932373\n",
      "steps per second: 0.53044\n",
      "step: 30539\n",
      "loss: 12.622530937194824\n",
      "steps per second: 0.49657\n",
      "step: 30540\n",
      "loss: 12.698331832885742\n",
      "steps per second: 0.54098\n",
      "step: 30541\n",
      "loss: 12.763496398925781\n",
      "steps per second: 0.57246\n",
      "step: 30542\n",
      "loss: 13.199332237243652\n",
      "steps per second: 0.53916\n",
      "step: 30543\n",
      "loss: 12.839884757995605\n",
      "steps per second: 0.52842\n",
      "step: 30544\n",
      "loss: 12.82880687713623\n",
      "steps per second: 0.56795\n",
      "step: 30545\n",
      "loss: 12.61970043182373\n",
      "steps per second: 0.57874\n",
      "step: 30546\n",
      "loss: 12.713399887084961\n",
      "steps per second: 0.56019\n",
      "step: 30547\n",
      "loss: 13.060932159423828\n",
      "steps per second: 0.52885\n",
      "step: 30548\n",
      "loss: 12.414806365966797\n",
      "steps per second: 0.47974\n",
      "step: 30549\n",
      "loss: 12.788897514343262\n",
      "steps per second: 0.55771\n",
      "step: 30550\n",
      "loss: 12.903310775756836\n",
      "steps per second: 0.52524\n",
      "step: 30551\n",
      "loss: 12.328218460083008\n",
      "steps per second: 0.53883\n",
      "step: 30552\n",
      "loss: 12.783390045166016\n",
      "steps per second: 0.52058\n",
      "step: 30553\n",
      "loss: 12.851459503173828\n",
      "steps per second: 0.60093\n",
      "step: 30554\n",
      "loss: 12.47585678100586\n",
      "steps per second: 0.54123\n",
      "step: 30555\n",
      "loss: 12.5960693359375\n",
      "steps per second: 0.50559\n",
      "step: 30556\n",
      "loss: 12.463648796081543\n",
      "steps per second: 0.52499\n",
      "step: 30557\n",
      "loss: 12.56743335723877\n",
      "steps per second: 0.53655\n",
      "step: 30558\n",
      "loss: 13.242589950561523\n",
      "steps per second: 0.52192\n",
      "step: 30559\n",
      "loss: 13.04407024383545\n",
      "steps per second: 0.52112\n",
      "step: 30560\n",
      "loss: 12.5457763671875\n",
      "steps per second: 0.56041\n",
      "step: 30561\n",
      "loss: 12.701101303100586\n",
      "steps per second: 0.54739\n",
      "step: 30562\n",
      "loss: 12.329682350158691\n",
      "steps per second: 0.53905\n",
      "step: 30563\n",
      "loss: 12.46810245513916\n",
      "steps per second: 0.56846\n",
      "step: 30564\n",
      "loss: 12.446290969848633\n",
      "steps per second: 0.56414\n",
      "step: 30565\n",
      "loss: 12.90825366973877\n",
      "steps per second: 0.56795\n",
      "step: 30566\n",
      "loss: 12.542520523071289\n",
      "steps per second: 0.53941\n",
      "step: 30567\n",
      "loss: 12.504165649414062\n",
      "steps per second: 0.53607\n",
      "step: 30568\n",
      "loss: 12.481362342834473\n",
      "steps per second: 0.50405\n",
      "step: 30569\n",
      "loss: 13.26511001586914\n",
      "steps per second: 0.53664\n",
      "step: 30570\n",
      "loss: 12.481749534606934\n",
      "steps per second: 0.53884\n",
      "step: 30571\n",
      "loss: 12.141091346740723\n",
      "steps per second: 0.52296\n",
      "step: 30572\n",
      "loss: 12.921629905700684\n",
      "steps per second: 0.53765\n",
      "step: 30573\n",
      "loss: 12.913853645324707\n",
      "steps per second: 0.53996\n",
      "step: 30574\n",
      "loss: 12.537571907043457\n",
      "steps per second: 0.52239\n",
      "step: 30575\n",
      "loss: 13.124593734741211\n",
      "steps per second: 0.55853\n",
      "step: 30576\n",
      "loss: 12.7432222366333\n",
      "steps per second: 0.52431\n",
      "step: 30577\n",
      "loss: 12.555375099182129\n",
      "steps per second: 0.52990\n",
      "step: 30578\n",
      "loss: 12.880553245544434\n",
      "steps per second: 0.53322\n",
      "step: 30579\n",
      "loss: 13.05047607421875\n",
      "steps per second: 0.53688\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.863604724407196, layer: 11\n",
      "saving at step 30579\n",
      "----------\n",
      "\n",
      "\n",
      "step: 30580\n",
      "loss: 13.019397735595703\n",
      "steps per second: 0.26500\n",
      "step: 30581\n",
      "loss: 12.638914108276367\n",
      "steps per second: 0.51546\n",
      "step: 30582\n",
      "loss: 12.973602294921875\n",
      "steps per second: 0.53271\n",
      "step: 30583\n",
      "loss: 12.916040420532227\n",
      "steps per second: 0.56362\n",
      "step: 30584\n",
      "loss: 13.14374828338623\n",
      "steps per second: 0.52369\n",
      "step: 30585\n",
      "loss: 13.096007347106934\n",
      "steps per second: 0.57072\n",
      "step: 30586\n",
      "loss: 12.823760032653809\n",
      "steps per second: 0.52985\n",
      "step: 30587\n",
      "loss: 13.087632179260254\n",
      "steps per second: 0.53403\n",
      "step: 30588\n",
      "loss: 12.61735725402832\n",
      "steps per second: 0.53807\n",
      "step: 30589\n",
      "loss: 13.520812034606934\n",
      "steps per second: 0.56261\n",
      "step: 30590\n",
      "loss: 12.6771821975708\n",
      "steps per second: 0.52896\n",
      "step: 30591\n",
      "loss: 13.271456718444824\n",
      "steps per second: 0.53349\n",
      "step: 30592\n",
      "loss: 12.18375301361084\n",
      "steps per second: 0.53593\n",
      "step: 30593\n",
      "loss: 12.822121620178223\n",
      "steps per second: 0.53870\n",
      "step: 30594\n",
      "loss: 13.229395866394043\n",
      "steps per second: 0.51254\n",
      "step: 30595\n",
      "loss: 12.620620727539062\n",
      "steps per second: 0.59900\n",
      "step: 30596\n",
      "loss: 12.923500061035156\n",
      "steps per second: 0.56200\n",
      "step: 30597\n",
      "loss: 12.53805160522461\n",
      "steps per second: 0.51321\n",
      "step: 30598\n",
      "loss: 12.773208618164062\n",
      "steps per second: 0.56363\n",
      "step: 30599\n",
      "loss: 12.78731918334961\n",
      "steps per second: 0.52209\n",
      "step: 30600\n",
      "loss: 12.835746765136719\n",
      "steps per second: 0.56862\n",
      "step: 30601\n",
      "loss: 13.101405143737793\n",
      "steps per second: 0.53669\n",
      "step: 30602\n",
      "loss: 12.670195579528809\n",
      "steps per second: 0.51693\n",
      "step: 30603\n",
      "loss: 12.436610221862793\n",
      "steps per second: 0.53386\n",
      "step: 30604\n",
      "loss: 13.105688095092773\n",
      "steps per second: 0.55611\n",
      "step: 30605\n",
      "loss: 12.669119834899902\n",
      "steps per second: 0.52678\n",
      "step: 30606\n",
      "loss: 12.703437805175781\n",
      "steps per second: 0.52997\n",
      "step: 30607\n",
      "loss: 12.701299667358398\n",
      "steps per second: 0.56251\n",
      "step: 30608\n",
      "loss: 12.635225296020508\n",
      "steps per second: 0.51031\n",
      "step: 30609\n",
      "loss: 12.455424308776855\n",
      "steps per second: 0.54085\n",
      "step: 30610\n",
      "loss: 13.119404792785645\n",
      "steps per second: 0.50605\n",
      "step: 30611\n",
      "loss: 12.6952486038208\n",
      "steps per second: 0.52974\n",
      "step: 30612\n",
      "loss: 13.020797729492188\n",
      "steps per second: 0.52649\n",
      "step: 30613\n",
      "loss: 13.207087516784668\n",
      "steps per second: 0.55586\n",
      "step: 30614\n",
      "loss: 12.736933708190918\n",
      "steps per second: 0.54341\n",
      "step: 30615\n",
      "loss: 12.590616226196289\n",
      "steps per second: 0.54581\n",
      "step: 30616\n",
      "loss: 12.790760040283203\n",
      "steps per second: 0.60152\n",
      "step: 30617\n",
      "loss: 13.22327709197998\n",
      "steps per second: 0.53887\n",
      "step: 30618\n",
      "loss: 13.172399520874023\n",
      "steps per second: 0.51649\n",
      "step: 30619\n",
      "loss: 13.0651273727417\n",
      "steps per second: 0.51245\n",
      "step: 30620\n",
      "loss: 13.225980758666992\n",
      "steps per second: 0.52508\n",
      "step: 30621\n",
      "loss: 13.458974838256836\n",
      "steps per second: 0.52360\n",
      "step: 30622\n",
      "loss: 12.915945053100586\n",
      "steps per second: 0.51177\n",
      "step: 30623\n",
      "loss: 12.659256935119629\n",
      "steps per second: 0.55904\n",
      "step: 30624\n",
      "loss: 13.200756072998047\n",
      "steps per second: 0.54618\n",
      "step: 30625\n",
      "loss: 12.47675895690918\n",
      "steps per second: 0.50128\n",
      "step: 30626\n",
      "loss: 13.480487823486328\n",
      "steps per second: 0.53789\n",
      "step: 30627\n",
      "loss: 12.704964637756348\n",
      "steps per second: 0.56758\n",
      "step: 30628\n",
      "loss: 13.035867691040039\n",
      "steps per second: 0.56550\n",
      "step: 30629\n",
      "loss: 13.487140655517578\n",
      "steps per second: 0.56007\n",
      "step: 30630\n",
      "loss: 12.52724838256836\n",
      "steps per second: 0.47856\n",
      "step: 30631\n",
      "loss: 12.776302337646484\n",
      "steps per second: 0.60280\n",
      "step: 30632\n",
      "loss: 12.182711601257324\n",
      "steps per second: 0.50714\n",
      "step: 30633\n",
      "loss: 12.900693893432617\n",
      "steps per second: 0.56931\n",
      "step: 30634\n",
      "loss: 12.898326873779297\n",
      "steps per second: 0.53187\n",
      "step: 30635\n",
      "loss: 13.159650802612305\n",
      "steps per second: 0.53724\n",
      "step: 30636\n",
      "loss: 12.80640697479248\n",
      "steps per second: 0.52873\n",
      "step: 30637\n",
      "loss: 12.7688627243042\n",
      "steps per second: 0.57333\n",
      "step: 30638\n",
      "loss: 12.862076759338379\n",
      "steps per second: 0.56328\n",
      "step: 30639\n",
      "loss: 12.617331504821777\n",
      "steps per second: 0.54623\n",
      "step: 30640\n",
      "loss: 12.812002182006836\n",
      "steps per second: 0.53238\n",
      "step: 30641\n",
      "loss: 12.689276695251465\n",
      "steps per second: 0.57187\n",
      "step: 30642\n",
      "loss: 12.634571075439453\n",
      "steps per second: 0.56274\n",
      "step: 30643\n",
      "loss: 13.454426765441895\n",
      "steps per second: 0.52288\n",
      "step: 30644\n",
      "loss: 13.425102233886719\n",
      "steps per second: 0.49532\n",
      "step: 30645\n",
      "loss: 13.168818473815918\n",
      "steps per second: 0.51315\n",
      "step: 30646\n",
      "loss: 12.433382034301758\n",
      "steps per second: 0.51372\n",
      "step: 30647\n",
      "loss: 12.746611595153809\n",
      "steps per second: 0.59948\n",
      "step: 30648\n",
      "loss: 12.554669380187988\n",
      "steps per second: 0.56167\n",
      "step: 30649\n",
      "loss: 12.859514236450195\n",
      "steps per second: 0.53420\n",
      "step: 30650\n",
      "loss: 12.95776653289795\n",
      "steps per second: 0.51656\n",
      "step: 30651\n",
      "loss: 12.516106605529785\n",
      "steps per second: 0.56392\n",
      "step: 30652\n",
      "loss: 12.523785591125488\n",
      "steps per second: 0.56730\n",
      "step: 30653\n",
      "loss: 13.001492500305176\n",
      "steps per second: 0.53197\n",
      "step: 30654\n",
      "loss: 12.59382152557373\n",
      "steps per second: 0.50448\n",
      "step: 30655\n",
      "loss: 12.547187805175781\n",
      "steps per second: 0.52774\n",
      "step: 30656\n",
      "loss: 12.885621070861816\n",
      "steps per second: 0.53927\n",
      "step: 30657\n",
      "loss: 13.052184104919434\n",
      "steps per second: 0.52096\n",
      "step: 30658\n",
      "loss: 12.846596717834473\n",
      "steps per second: 0.54554\n",
      "step: 30659\n",
      "loss: 12.9506196975708\n",
      "steps per second: 0.56325\n",
      "step: 30660\n",
      "loss: 12.746545791625977\n",
      "steps per second: 0.54628\n",
      "step: 30661\n",
      "loss: 12.789748191833496\n",
      "steps per second: 0.51412\n",
      "step: 30662\n",
      "loss: 12.676976203918457\n",
      "steps per second: 0.53195\n",
      "step: 30663\n",
      "loss: 12.67409610748291\n",
      "steps per second: 0.54910\n",
      "step: 30664\n",
      "loss: 12.381269454956055\n",
      "steps per second: 0.51350\n",
      "step: 30665\n",
      "loss: 13.215041160583496\n",
      "steps per second: 0.50425\n",
      "step: 30666\n",
      "loss: 12.87569522857666\n",
      "steps per second: 0.56046\n",
      "step: 30667\n",
      "loss: 13.129849433898926\n",
      "steps per second: 0.53841\n",
      "step: 30668\n",
      "loss: 12.888138771057129\n",
      "steps per second: 0.53439\n",
      "step: 30669\n",
      "loss: 12.633806228637695\n",
      "steps per second: 0.57178\n",
      "step: 30670\n",
      "loss: 12.617095947265625\n",
      "steps per second: 0.54447\n",
      "step: 30671\n",
      "loss: 13.116720199584961\n",
      "steps per second: 0.51557\n",
      "step: 30672\n",
      "loss: 13.218684196472168\n",
      "steps per second: 0.54205\n",
      "step: 30673\n",
      "loss: 12.82956600189209\n",
      "steps per second: 0.51585\n",
      "step: 30674\n",
      "loss: 13.098793983459473\n",
      "steps per second: 0.54737\n",
      "step: 30675\n",
      "loss: 13.082616806030273\n",
      "steps per second: 0.54674\n",
      "step: 30676\n",
      "loss: 12.34280776977539\n",
      "steps per second: 0.54776\n",
      "step: 30677\n",
      "loss: 12.947870254516602\n",
      "steps per second: 0.56198\n",
      "step: 30678\n",
      "loss: 13.065977096557617\n",
      "steps per second: 0.54686\n",
      "step: 30679\n",
      "loss: 12.925406455993652\n",
      "steps per second: 0.56521\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8423815965652466, layer: 11\n",
      "saving at step 30679\n",
      "----------\n",
      "\n",
      "\n",
      "step: 30680\n",
      "loss: 12.66202449798584\n",
      "steps per second: 0.27363\n",
      "step: 30681\n",
      "loss: 12.563323020935059\n",
      "steps per second: 0.53134\n",
      "step: 30682\n",
      "loss: 13.13177490234375\n",
      "steps per second: 0.54536\n",
      "step: 30683\n",
      "loss: 12.715588569641113\n",
      "steps per second: 0.52573\n",
      "step: 30684\n",
      "loss: 12.423327445983887\n",
      "steps per second: 0.54219\n",
      "step: 30685\n",
      "loss: 12.6766996383667\n",
      "steps per second: 0.53573\n",
      "step: 30686\n",
      "loss: 12.348647117614746\n",
      "steps per second: 0.54881\n",
      "step: 30687\n",
      "loss: 12.850569725036621\n",
      "steps per second: 0.51867\n",
      "step: 30688\n",
      "loss: 12.398527145385742\n",
      "steps per second: 0.51431\n",
      "step: 30689\n",
      "loss: 13.027036666870117\n",
      "steps per second: 0.54592\n",
      "step: 30690\n",
      "loss: 12.839455604553223\n",
      "steps per second: 0.54761\n",
      "step: 30691\n",
      "loss: 12.853184700012207\n",
      "steps per second: 0.54200\n",
      "step: 30692\n",
      "loss: 12.845913887023926\n",
      "steps per second: 0.54123\n",
      "step: 30693\n",
      "loss: 12.202224731445312\n",
      "steps per second: 0.52818\n",
      "step: 30694\n",
      "loss: 12.720731735229492\n",
      "steps per second: 0.54818\n",
      "step: 30695\n",
      "loss: 13.090721130371094\n",
      "steps per second: 0.60442\n",
      "step: 30696\n",
      "loss: 12.534754753112793\n",
      "steps per second: 0.54490\n",
      "step: 30697\n",
      "loss: 13.124183654785156\n",
      "steps per second: 0.56421\n",
      "step: 30698\n",
      "loss: 12.945444107055664\n",
      "steps per second: 0.48710\n",
      "step: 30699\n",
      "loss: 12.549833297729492\n",
      "steps per second: 0.54190\n",
      "step: 30700\n",
      "loss: 12.13701057434082\n",
      "steps per second: 0.56182\n",
      "step: 30701\n",
      "loss: 12.504083633422852\n",
      "steps per second: 0.52783\n",
      "step: 30702\n",
      "loss: 12.91631031036377\n",
      "steps per second: 0.53993\n",
      "step: 30703\n",
      "loss: 12.900270462036133\n",
      "steps per second: 0.54384\n",
      "step: 30704\n",
      "loss: 12.764768600463867\n",
      "steps per second: 0.53340\n",
      "step: 30705\n",
      "loss: 12.534802436828613\n",
      "steps per second: 0.60682\n",
      "step: 30706\n",
      "loss: 12.596295356750488\n",
      "steps per second: 0.57047\n",
      "step: 30707\n",
      "loss: 13.200874328613281\n",
      "steps per second: 0.53114\n",
      "step: 30708\n",
      "loss: 12.775801658630371\n",
      "steps per second: 0.56084\n",
      "step: 30709\n",
      "loss: 12.240144729614258\n",
      "steps per second: 0.49518\n",
      "step: 30710\n",
      "loss: 12.641973495483398\n",
      "steps per second: 0.51588\n",
      "step: 30711\n",
      "loss: 12.585780143737793\n",
      "steps per second: 0.54619\n",
      "step: 30712\n",
      "loss: 12.64307689666748\n",
      "steps per second: 0.53489\n",
      "step: 30713\n",
      "loss: 13.247965812683105\n",
      "steps per second: 0.52334\n",
      "step: 30714\n",
      "loss: 12.856119155883789\n",
      "steps per second: 0.49902\n",
      "step: 30715\n",
      "loss: 12.857436180114746\n",
      "steps per second: 0.53254\n",
      "step: 30716\n",
      "loss: 13.089316368103027\n",
      "steps per second: 0.53105\n",
      "step: 30717\n",
      "loss: 12.993749618530273\n",
      "steps per second: 0.53604\n",
      "step: 30718\n",
      "loss: 12.706713676452637\n",
      "steps per second: 0.52514\n",
      "step: 30719\n",
      "loss: 12.387125015258789\n",
      "steps per second: 0.54103\n",
      "step: 30720\n",
      "loss: 12.582183837890625\n",
      "steps per second: 0.56182\n",
      "step: 30721\n",
      "loss: 13.037370681762695\n",
      "steps per second: 0.52360\n",
      "step: 30722\n",
      "loss: 13.022801399230957\n",
      "steps per second: 0.51955\n",
      "step: 30723\n",
      "loss: 12.679211616516113\n",
      "steps per second: 0.53338\n",
      "step: 30724\n",
      "loss: 12.70761775970459\n",
      "steps per second: 0.57123\n",
      "step: 30725\n",
      "loss: 12.450983047485352\n",
      "steps per second: 0.56393\n",
      "step: 30726\n",
      "loss: 12.709126472473145\n",
      "steps per second: 0.53590\n",
      "step: 30727\n",
      "loss: 13.224467277526855\n",
      "steps per second: 0.51536\n",
      "step: 30728\n",
      "loss: 12.673388481140137\n",
      "steps per second: 0.53712\n",
      "step: 30729\n",
      "loss: 12.371028900146484\n",
      "steps per second: 0.53629\n",
      "step: 30730\n",
      "loss: 12.295332908630371\n",
      "steps per second: 0.52294\n",
      "step: 30731\n",
      "loss: 13.158879280090332\n",
      "steps per second: 0.52949\n",
      "step: 30732\n",
      "loss: 12.775331497192383\n",
      "steps per second: 0.57623\n",
      "step: 30733\n",
      "loss: 12.723222732543945\n",
      "steps per second: 0.49757\n",
      "step: 30734\n",
      "loss: 12.917896270751953\n",
      "steps per second: 0.53893\n",
      "step: 30735\n",
      "loss: 12.295263290405273\n",
      "steps per second: 0.53429\n",
      "step: 30736\n",
      "loss: 13.555008888244629\n",
      "steps per second: 0.51933\n",
      "step: 30737\n",
      "loss: 12.569860458374023\n",
      "steps per second: 0.56260\n",
      "step: 30738\n",
      "loss: 12.214548110961914\n",
      "steps per second: 0.53720\n",
      "step: 30739\n",
      "loss: 12.525983810424805\n",
      "steps per second: 0.59778\n",
      "step: 30740\n",
      "loss: 12.297317504882812\n",
      "steps per second: 0.55625\n",
      "step: 30741\n",
      "loss: 12.648187637329102\n",
      "steps per second: 0.55677\n",
      "step: 30742\n",
      "loss: 12.910921096801758\n",
      "steps per second: 0.59539\n",
      "step: 30743\n",
      "loss: 12.04780387878418\n",
      "steps per second: 0.52157\n",
      "step: 30744\n",
      "loss: 12.627434730529785\n",
      "steps per second: 0.54476\n",
      "step: 30745\n",
      "loss: 13.191827774047852\n",
      "steps per second: 0.51428\n",
      "step: 30746\n",
      "loss: 12.53062629699707\n",
      "steps per second: 0.52177\n",
      "step: 30747\n",
      "loss: 13.157609939575195\n",
      "steps per second: 0.55182\n",
      "step: 30748\n",
      "loss: 13.543769836425781\n",
      "steps per second: 0.59288\n",
      "step: 30749\n",
      "loss: 13.186479568481445\n",
      "steps per second: 0.46199\n",
      "step: 30750\n",
      "loss: 12.881464958190918\n",
      "steps per second: 0.52898\n",
      "step: 30751\n",
      "loss: 13.205220222473145\n",
      "steps per second: 0.53543\n",
      "step: 30752\n",
      "loss: 12.552491188049316\n",
      "steps per second: 0.50837\n",
      "step: 30753\n",
      "loss: 12.196027755737305\n",
      "steps per second: 0.50690\n",
      "step: 30754\n",
      "loss: 13.29716968536377\n",
      "steps per second: 0.55948\n",
      "step: 30755\n",
      "loss: 12.558026313781738\n",
      "steps per second: 0.54237\n",
      "step: 30756\n",
      "loss: 13.466793060302734\n",
      "steps per second: 0.54423\n",
      "step: 30757\n",
      "loss: 13.688297271728516\n",
      "steps per second: 0.54286\n",
      "step: 30758\n",
      "loss: 12.689242362976074\n",
      "steps per second: 0.51094\n",
      "step: 30759\n",
      "loss: 12.579065322875977\n",
      "steps per second: 0.52444\n",
      "step: 30760\n",
      "loss: 12.486442565917969\n",
      "steps per second: 0.54676\n",
      "step: 30761\n",
      "loss: 12.79372501373291\n",
      "steps per second: 0.56248\n",
      "step: 30762\n",
      "loss: 12.937633514404297\n",
      "steps per second: 0.50954\n",
      "step: 30763\n",
      "loss: 13.404293060302734\n",
      "steps per second: 0.52127\n",
      "step: 30764\n",
      "loss: 13.336235046386719\n",
      "steps per second: 0.47964\n",
      "step: 30765\n",
      "loss: 12.573699951171875\n",
      "steps per second: 0.56328\n",
      "step: 30766\n",
      "loss: 12.523758888244629\n",
      "steps per second: 0.53554\n",
      "step: 30767\n",
      "loss: 13.22998332977295\n",
      "steps per second: 0.56605\n",
      "step: 30768\n",
      "loss: 13.056757926940918\n",
      "steps per second: 0.52193\n",
      "step: 30769\n",
      "loss: 12.635623931884766\n",
      "steps per second: 0.48179\n",
      "step: 30770\n",
      "loss: 12.74601936340332\n",
      "steps per second: 0.54265\n",
      "step: 30771\n",
      "loss: 12.362394332885742\n",
      "steps per second: 0.56592\n",
      "step: 30772\n",
      "loss: 12.810580253601074\n",
      "steps per second: 0.56706\n",
      "step: 30773\n",
      "loss: 12.609169006347656\n",
      "steps per second: 0.56452\n",
      "step: 30774\n",
      "loss: 12.505452156066895\n",
      "steps per second: 0.50997\n",
      "step: 30775\n",
      "loss: 12.476367950439453\n",
      "steps per second: 0.53003\n",
      "step: 30776\n",
      "loss: 12.551694869995117\n",
      "steps per second: 0.51928\n",
      "step: 30777\n",
      "loss: 13.296072959899902\n",
      "steps per second: 0.53811\n",
      "step: 30778\n",
      "loss: 12.781408309936523\n",
      "steps per second: 0.54883\n",
      "step: 30779\n",
      "loss: 12.710886001586914\n",
      "steps per second: 0.53142\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8330926299095154, layer: 11\n",
      "saving at step 30779\n",
      "----------\n",
      "\n",
      "\n",
      "step: 30780\n",
      "loss: 12.878064155578613\n",
      "steps per second: 0.27609\n",
      "step: 30781\n",
      "loss: 12.893190383911133\n",
      "steps per second: 0.54821\n",
      "step: 30782\n",
      "loss: 12.56479263305664\n",
      "steps per second: 0.49978\n",
      "step: 30783\n",
      "loss: 12.333292961120605\n",
      "steps per second: 0.55156\n",
      "step: 30784\n",
      "loss: 12.921232223510742\n",
      "steps per second: 0.55737\n",
      "step: 30785\n",
      "loss: 12.815975189208984\n",
      "steps per second: 0.53352\n",
      "step: 30786\n",
      "loss: 13.163933753967285\n",
      "steps per second: 0.52366\n",
      "step: 30787\n",
      "loss: 12.085612297058105\n",
      "steps per second: 0.52149\n",
      "step: 30788\n",
      "loss: 12.607094764709473\n",
      "steps per second: 0.52254\n",
      "step: 30789\n",
      "loss: 12.301895141601562\n",
      "steps per second: 0.53940\n",
      "step: 30790\n",
      "loss: 12.721132278442383\n",
      "steps per second: 0.52239\n",
      "step: 30791\n",
      "loss: 13.031988143920898\n",
      "steps per second: 0.59334\n",
      "step: 30792\n",
      "loss: 12.575210571289062\n",
      "steps per second: 0.52017\n",
      "step: 30793\n",
      "loss: 13.091534614562988\n",
      "steps per second: 0.52140\n",
      "step: 30794\n",
      "loss: 13.140508651733398\n",
      "steps per second: 0.51945\n",
      "step: 30795\n",
      "loss: 12.280423164367676\n",
      "steps per second: 0.55511\n",
      "step: 30796\n",
      "loss: 12.755424499511719\n",
      "steps per second: 0.52879\n",
      "step: 30797\n",
      "loss: 12.473597526550293\n",
      "steps per second: 0.52208\n",
      "step: 30798\n",
      "loss: 12.17342472076416\n",
      "steps per second: 0.53666\n",
      "step: 30799\n",
      "loss: 13.007743835449219\n",
      "steps per second: 0.56974\n",
      "step: 30800\n",
      "loss: 13.120594024658203\n",
      "steps per second: 0.57145\n",
      "step: 30801\n",
      "loss: 12.746729850769043\n",
      "steps per second: 0.55348\n",
      "step: 30802\n",
      "loss: 12.711346626281738\n",
      "steps per second: 0.55821\n",
      "step: 30803\n",
      "loss: 12.800666809082031\n",
      "steps per second: 0.57402\n",
      "step: 30804\n",
      "loss: 12.618205070495605\n",
      "steps per second: 0.57934\n",
      "step: 30805\n",
      "loss: 13.162115097045898\n",
      "steps per second: 0.57368\n",
      "step: 30806\n",
      "loss: 12.928180694580078\n",
      "steps per second: 0.54607\n",
      "step: 30807\n",
      "loss: 13.087682723999023\n",
      "steps per second: 0.61422\n",
      "step: 30808\n",
      "loss: 13.29506778717041\n",
      "steps per second: 0.57191\n",
      "step: 30809\n",
      "loss: 13.113245010375977\n",
      "steps per second: 0.55015\n",
      "step: 30810\n",
      "loss: 13.050854682922363\n",
      "steps per second: 0.56054\n",
      "step: 30811\n",
      "loss: 12.949843406677246\n",
      "steps per second: 0.55116\n",
      "step: 30812\n",
      "loss: 13.035234451293945\n",
      "steps per second: 0.53874\n",
      "step: 30813\n",
      "loss: 12.954832077026367\n",
      "steps per second: 0.50628\n",
      "step: 30814\n",
      "loss: 13.07462215423584\n",
      "steps per second: 0.54471\n",
      "step: 30815\n",
      "loss: 12.607767105102539\n",
      "steps per second: 0.54662\n",
      "step: 30816\n",
      "loss: 12.677081108093262\n",
      "steps per second: 0.57683\n",
      "step: 30817\n",
      "loss: 13.162484169006348\n",
      "steps per second: 0.55369\n",
      "step: 30818\n",
      "loss: 12.955789566040039\n",
      "steps per second: 0.52516\n",
      "step: 30819\n",
      "loss: 12.716880798339844\n",
      "steps per second: 0.55045\n",
      "step: 30820\n",
      "loss: 12.843230247497559\n",
      "steps per second: 0.58434\n",
      "step: 30821\n",
      "loss: 13.097493171691895\n",
      "steps per second: 0.57462\n",
      "step: 30822\n",
      "loss: 12.863469123840332\n",
      "steps per second: 0.52902\n",
      "step: 30823\n",
      "loss: 12.99560546875\n",
      "steps per second: 0.51953\n",
      "step: 30824\n",
      "loss: 13.506695747375488\n",
      "steps per second: 0.55251\n",
      "step: 30825\n",
      "loss: 12.766757011413574\n",
      "steps per second: 0.52612\n",
      "step: 30826\n",
      "loss: 13.305180549621582\n",
      "steps per second: 0.61591\n",
      "step: 30827\n",
      "loss: 12.672805786132812\n",
      "steps per second: 0.52589\n",
      "step: 30828\n",
      "loss: 12.874655723571777\n",
      "steps per second: 0.53995\n",
      "step: 30829\n",
      "loss: 12.563949584960938\n",
      "steps per second: 0.55805\n",
      "step: 30830\n",
      "loss: 12.475220680236816\n",
      "steps per second: 0.49700\n",
      "step: 30831\n",
      "loss: 13.056621551513672\n",
      "steps per second: 0.56213\n",
      "step: 30832\n",
      "loss: 12.438261032104492\n",
      "steps per second: 0.56290\n",
      "step: 30833\n",
      "loss: 12.47733211517334\n",
      "steps per second: 0.56163\n",
      "step: 30834\n",
      "loss: 13.348404884338379\n",
      "steps per second: 0.55379\n",
      "step: 30835\n",
      "loss: 12.793997764587402\n",
      "steps per second: 0.53578\n",
      "step: 30836\n",
      "loss: 13.330923080444336\n",
      "steps per second: 0.54895\n",
      "step: 30837\n",
      "loss: 12.92209529876709\n",
      "steps per second: 0.50206\n",
      "step: 30838\n",
      "loss: 12.43839168548584\n",
      "steps per second: 0.57445\n",
      "step: 30839\n",
      "loss: 12.907248497009277\n",
      "steps per second: 0.58085\n",
      "step: 30840\n",
      "loss: 12.606547355651855\n",
      "steps per second: 0.56144\n",
      "step: 30841\n",
      "loss: 13.272852897644043\n",
      "steps per second: 0.60240\n",
      "step: 30842\n",
      "loss: 12.518962860107422\n",
      "steps per second: 0.51106\n",
      "step: 30843\n",
      "loss: 12.994694709777832\n",
      "steps per second: 0.55543\n",
      "step: 30844\n",
      "loss: 12.54393482208252\n",
      "steps per second: 0.55232\n",
      "step: 30845\n",
      "loss: 13.246906280517578\n",
      "steps per second: 0.55057\n",
      "step: 30846\n",
      "loss: 13.067234992980957\n",
      "steps per second: 0.53185\n",
      "step: 30847\n",
      "loss: 12.19699764251709\n",
      "steps per second: 0.52978\n",
      "step: 30848\n",
      "loss: 12.519591331481934\n",
      "steps per second: 0.55642\n",
      "step: 30849\n",
      "loss: 12.994431495666504\n",
      "steps per second: 0.52636\n",
      "step: 30850\n",
      "loss: 12.861991882324219\n",
      "steps per second: 0.55296\n",
      "step: 30851\n",
      "loss: 12.763692855834961\n",
      "steps per second: 0.55622\n",
      "step: 30852\n",
      "loss: 12.756916046142578\n",
      "steps per second: 0.59061\n",
      "step: 30853\n",
      "loss: 12.17056655883789\n",
      "steps per second: 0.58193\n",
      "step: 30854\n",
      "loss: 12.933839797973633\n",
      "steps per second: 0.57650\n",
      "step: 30855\n",
      "loss: 12.992648124694824\n",
      "steps per second: 0.57225\n",
      "step: 30856\n",
      "loss: 13.024787902832031\n",
      "steps per second: 0.54228\n",
      "step: 30857\n",
      "loss: 12.486899375915527\n",
      "steps per second: 0.56222\n",
      "step: 30858\n",
      "loss: 12.995000839233398\n",
      "steps per second: 0.53835\n",
      "step: 30859\n",
      "loss: 13.307076454162598\n",
      "steps per second: 0.55001\n",
      "step: 30860\n",
      "loss: 12.945642471313477\n",
      "steps per second: 0.51865\n",
      "step: 30861\n",
      "loss: 12.58468246459961\n",
      "steps per second: 0.53823\n",
      "step: 30862\n",
      "loss: 12.312860488891602\n",
      "steps per second: 0.55512\n",
      "step: 30863\n",
      "loss: 12.661806106567383\n",
      "steps per second: 0.53660\n",
      "step: 30864\n",
      "loss: 12.77035903930664\n",
      "steps per second: 0.57879\n",
      "step: 30865\n",
      "loss: 13.078227996826172\n",
      "steps per second: 0.54710\n",
      "step: 30866\n",
      "loss: 12.46484088897705\n",
      "steps per second: 0.54333\n",
      "step: 30867\n",
      "loss: 12.659668922424316\n",
      "steps per second: 0.56079\n",
      "step: 30868\n",
      "loss: 12.795543670654297\n",
      "steps per second: 0.54063\n",
      "step: 30869\n",
      "loss: 13.046567916870117\n",
      "steps per second: 0.53992\n",
      "step: 30870\n",
      "loss: 12.771815299987793\n",
      "steps per second: 0.58512\n",
      "step: 30871\n",
      "loss: 13.11735725402832\n",
      "steps per second: 0.57900\n",
      "step: 30872\n",
      "loss: 12.389495849609375\n",
      "steps per second: 0.55546\n",
      "step: 30873\n",
      "loss: 12.91928768157959\n",
      "steps per second: 0.58110\n",
      "step: 30874\n",
      "loss: 13.256820678710938\n",
      "steps per second: 0.51950\n",
      "step: 30875\n",
      "loss: 12.798559188842773\n",
      "steps per second: 0.51495\n",
      "step: 30876\n",
      "loss: 13.136666297912598\n",
      "steps per second: 0.55083\n",
      "step: 30877\n",
      "loss: 12.957486152648926\n",
      "steps per second: 0.55294\n",
      "step: 30878\n",
      "loss: 12.802817344665527\n",
      "steps per second: 0.57406\n",
      "step: 30879\n",
      "loss: 12.632216453552246\n",
      "steps per second: 0.61762\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7801054120063782, layer: 10\n",
      "saving at step 30879\n",
      "----------\n",
      "\n",
      "\n",
      "step: 30880\n",
      "loss: 12.61822509765625\n",
      "steps per second: 0.29805\n",
      "step: 30881\n",
      "loss: 12.893274307250977\n",
      "steps per second: 0.54239\n",
      "step: 30882\n",
      "loss: 12.886799812316895\n",
      "steps per second: 0.54591\n",
      "step: 30883\n",
      "loss: 12.651107788085938\n",
      "steps per second: 0.49436\n",
      "step: 30884\n",
      "loss: 12.926753997802734\n",
      "steps per second: 0.55963\n",
      "step: 30885\n",
      "loss: 12.739411354064941\n",
      "steps per second: 0.57620\n",
      "step: 30886\n",
      "loss: 12.677952766418457\n",
      "steps per second: 0.54044\n",
      "step: 30887\n",
      "loss: 12.148358345031738\n",
      "steps per second: 0.55148\n",
      "step: 30888\n",
      "loss: 12.533873558044434\n",
      "steps per second: 0.52825\n",
      "step: 30889\n",
      "loss: 12.491663932800293\n",
      "steps per second: 0.61803\n",
      "step: 30890\n",
      "loss: 12.82110595703125\n",
      "steps per second: 0.58067\n",
      "step: 30891\n",
      "loss: 12.484396934509277\n",
      "steps per second: 0.57021\n",
      "step: 30892\n",
      "loss: 12.722857475280762\n",
      "steps per second: 0.57841\n",
      "step: 30893\n",
      "loss: 12.832571983337402\n",
      "steps per second: 0.53800\n",
      "step: 30894\n",
      "loss: 12.727700233459473\n",
      "steps per second: 0.52487\n",
      "step: 30895\n",
      "loss: 13.095718383789062\n",
      "steps per second: 0.54322\n",
      "step: 30896\n",
      "loss: 12.53639030456543\n",
      "steps per second: 0.54928\n",
      "step: 30897\n",
      "loss: 12.784075736999512\n",
      "steps per second: 0.52366\n",
      "step: 30898\n",
      "loss: 12.958614349365234\n",
      "steps per second: 0.56700\n",
      "step: 30899\n",
      "loss: 12.55166244506836\n",
      "steps per second: 0.57554\n",
      "step: 30900\n",
      "loss: 12.841118812561035\n",
      "steps per second: 0.52121\n",
      "step: 30901\n",
      "loss: 12.767121315002441\n",
      "steps per second: 0.55138\n",
      "step: 30902\n",
      "loss: 12.520880699157715\n",
      "steps per second: 0.55126\n",
      "step: 30903\n",
      "loss: 12.883796691894531\n",
      "steps per second: 0.51733\n",
      "step: 30904\n",
      "loss: 12.561938285827637\n",
      "steps per second: 0.55694\n",
      "step: 30905\n",
      "loss: 12.91565227508545\n",
      "steps per second: 0.56250\n",
      "step: 30906\n",
      "loss: 13.155967712402344\n",
      "steps per second: 0.53611\n",
      "step: 30907\n",
      "loss: 12.568140029907227\n",
      "steps per second: 0.57450\n",
      "step: 30908\n",
      "loss: 13.118412017822266\n",
      "steps per second: 0.54443\n",
      "step: 30909\n",
      "loss: 12.159831047058105\n",
      "steps per second: 0.55379\n",
      "step: 30910\n",
      "loss: 12.883686065673828\n",
      "steps per second: 0.58025\n",
      "step: 30911\n",
      "loss: 12.677501678466797\n",
      "steps per second: 0.53336\n",
      "step: 30912\n",
      "loss: 12.60463809967041\n",
      "steps per second: 0.50029\n",
      "step: 30913\n",
      "loss: 12.743769645690918\n",
      "steps per second: 0.54069\n",
      "step: 30914\n",
      "loss: 12.563032150268555\n",
      "steps per second: 0.57805\n",
      "step: 30915\n",
      "loss: 12.788744926452637\n",
      "steps per second: 0.54913\n",
      "step: 30916\n",
      "loss: 12.806110382080078\n",
      "steps per second: 0.55712\n",
      "step: 30917\n",
      "loss: 12.780580520629883\n",
      "steps per second: 0.50902\n",
      "step: 30918\n",
      "loss: 12.55378532409668\n",
      "steps per second: 0.57044\n",
      "step: 30919\n",
      "loss: 12.857311248779297\n",
      "steps per second: 0.56480\n",
      "step: 30920\n",
      "loss: 12.627302169799805\n",
      "steps per second: 0.54537\n",
      "step: 30921\n",
      "loss: 12.733844757080078\n",
      "steps per second: 0.46141\n",
      "step: 30922\n",
      "loss: 12.15510082244873\n",
      "steps per second: 0.54043\n",
      "step: 30923\n",
      "loss: 12.908416748046875\n",
      "steps per second: 0.53462\n",
      "step: 30924\n",
      "loss: 12.984640121459961\n",
      "steps per second: 0.54606\n",
      "step: 30925\n",
      "loss: 13.042033195495605\n",
      "steps per second: 0.49841\n",
      "step: 30926\n",
      "loss: 13.263059616088867\n",
      "steps per second: 0.58236\n",
      "step: 30927\n",
      "loss: 13.033350944519043\n",
      "steps per second: 0.54572\n",
      "step: 30928\n",
      "loss: 12.783021926879883\n",
      "steps per second: 0.50491\n",
      "step: 30929\n",
      "loss: 12.57863998413086\n",
      "steps per second: 0.54057\n",
      "step: 30930\n",
      "loss: 13.010905265808105\n",
      "steps per second: 0.58725\n",
      "step: 30931\n",
      "loss: 12.829097747802734\n",
      "steps per second: 0.53129\n",
      "step: 30932\n",
      "loss: 13.095853805541992\n",
      "steps per second: 0.56193\n",
      "step: 30933\n",
      "loss: 12.435997009277344\n",
      "steps per second: 0.54840\n",
      "step: 30934\n",
      "loss: 12.905278205871582\n",
      "steps per second: 0.54178\n",
      "step: 30935\n",
      "loss: 13.201319694519043\n",
      "steps per second: 0.55015\n",
      "step: 30936\n",
      "loss: 13.079782485961914\n",
      "steps per second: 0.55093\n",
      "step: 30937\n",
      "loss: 13.094051361083984\n",
      "steps per second: 0.54451\n",
      "step: 30938\n",
      "loss: 12.882346153259277\n",
      "steps per second: 0.60173\n",
      "step: 30939\n",
      "loss: 12.984380722045898\n",
      "steps per second: 0.54110\n",
      "step: 30940\n",
      "loss: 12.423449516296387\n",
      "steps per second: 0.54527\n",
      "step: 30941\n",
      "loss: 13.215702056884766\n",
      "steps per second: 0.58493\n",
      "step: 30942\n",
      "loss: 13.333761215209961\n",
      "steps per second: 0.55063\n",
      "step: 30943\n",
      "loss: 13.195926666259766\n",
      "steps per second: 0.56100\n",
      "step: 30944\n",
      "loss: 12.696660041809082\n",
      "steps per second: 0.54240\n",
      "step: 30945\n",
      "loss: 12.834125518798828\n",
      "steps per second: 0.54566\n",
      "step: 30946\n",
      "loss: 13.174858093261719\n",
      "steps per second: 0.55491\n",
      "step: 30947\n",
      "loss: 12.700305938720703\n",
      "steps per second: 0.53551\n",
      "step: 30948\n",
      "loss: 12.553855895996094\n",
      "steps per second: 0.52992\n",
      "step: 30949\n",
      "loss: 12.931949615478516\n",
      "steps per second: 0.53855\n",
      "step: 30950\n",
      "loss: 12.578044891357422\n",
      "steps per second: 0.57911\n",
      "step: 30951\n",
      "loss: 12.533747673034668\n",
      "steps per second: 0.54691\n",
      "step: 30952\n",
      "loss: 12.820382118225098\n",
      "steps per second: 0.57284\n",
      "step: 30953\n",
      "loss: 13.478734016418457\n",
      "steps per second: 0.55009\n",
      "step: 30954\n",
      "loss: 12.725196838378906\n",
      "steps per second: 0.52916\n",
      "step: 30955\n",
      "loss: 13.081393241882324\n",
      "steps per second: 0.55564\n",
      "step: 30956\n",
      "loss: 12.71094799041748\n",
      "steps per second: 0.56011\n",
      "step: 30957\n",
      "loss: 13.759218215942383\n",
      "steps per second: 0.55690\n",
      "step: 30958\n",
      "loss: 13.179129600524902\n",
      "steps per second: 0.55302\n",
      "step: 30959\n",
      "loss: 12.849320411682129\n",
      "steps per second: 0.57723\n",
      "step: 30960\n",
      "loss: 12.211535453796387\n",
      "steps per second: 0.53531\n",
      "step: 30961\n",
      "loss: 12.557574272155762\n",
      "steps per second: 0.53859\n",
      "step: 30962\n",
      "loss: 13.163898468017578\n",
      "steps per second: 0.55983\n",
      "step: 30963\n",
      "loss: 12.546957969665527\n",
      "steps per second: 0.52881\n",
      "step: 30964\n",
      "loss: 12.073619842529297\n",
      "steps per second: 0.54927\n",
      "step: 30965\n",
      "loss: 13.014894485473633\n",
      "steps per second: 0.61724\n",
      "step: 30966\n",
      "loss: 13.184340476989746\n",
      "steps per second: 0.57212\n",
      "step: 30967\n",
      "loss: 12.764448165893555\n",
      "steps per second: 0.55136\n",
      "step: 30968\n",
      "loss: 12.628074645996094\n",
      "steps per second: 0.58590\n",
      "step: 30969\n",
      "loss: 12.378417015075684\n",
      "steps per second: 0.58580\n",
      "step: 30970\n",
      "loss: 12.920626640319824\n",
      "steps per second: 0.58531\n",
      "step: 30971\n",
      "loss: 12.592915534973145\n",
      "steps per second: 0.53146\n",
      "step: 30972\n",
      "loss: 12.564605712890625\n",
      "steps per second: 0.52723\n",
      "step: 30973\n",
      "loss: 13.087806701660156\n",
      "steps per second: 0.57765\n",
      "step: 30974\n",
      "loss: 13.203855514526367\n",
      "steps per second: 0.54805\n",
      "step: 30975\n",
      "loss: 12.648039817810059\n",
      "steps per second: 0.57049\n",
      "step: 30976\n",
      "loss: 12.884421348571777\n",
      "steps per second: 0.55652\n",
      "step: 30977\n",
      "loss: 12.81575870513916\n",
      "steps per second: 0.50563\n",
      "step: 30978\n",
      "loss: 13.398629188537598\n",
      "steps per second: 0.57553\n",
      "step: 30979\n",
      "loss: 13.12075138092041\n",
      "steps per second: 0.58408\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8356128334999084, layer: 11\n",
      "saving at step 30979\n",
      "----------\n",
      "\n",
      "\n",
      "step: 30980\n",
      "loss: 12.003947257995605\n",
      "steps per second: 0.28855\n",
      "step: 30981\n",
      "loss: 12.53938102722168\n",
      "steps per second: 0.54809\n",
      "step: 30982\n",
      "loss: 12.758600234985352\n",
      "steps per second: 0.52370\n",
      "step: 30983\n",
      "loss: 12.651779174804688\n",
      "steps per second: 0.55822\n",
      "step: 30984\n",
      "loss: 12.98688793182373\n",
      "steps per second: 0.52501\n",
      "step: 30985\n",
      "loss: 12.384573936462402\n",
      "steps per second: 0.51652\n",
      "step: 30986\n",
      "loss: 12.523420333862305\n",
      "steps per second: 0.55993\n",
      "step: 30987\n",
      "loss: 12.556660652160645\n",
      "steps per second: 0.57387\n",
      "step: 30988\n",
      "loss: 12.949589729309082\n",
      "steps per second: 0.52408\n",
      "step: 30989\n",
      "loss: 13.458076477050781\n",
      "steps per second: 0.54392\n",
      "step: 30990\n",
      "loss: 12.9951171875\n",
      "steps per second: 0.55135\n",
      "step: 30991\n",
      "loss: 12.589571952819824\n",
      "steps per second: 0.55342\n",
      "step: 30992\n",
      "loss: 13.016921997070312\n",
      "steps per second: 0.54346\n",
      "step: 30993\n",
      "loss: 12.476042747497559\n",
      "steps per second: 0.55613\n",
      "step: 30994\n",
      "loss: 12.616154670715332\n",
      "steps per second: 0.57674\n",
      "step: 30995\n",
      "loss: 12.845175743103027\n",
      "steps per second: 0.62060\n",
      "step: 30996\n",
      "loss: 12.87264633178711\n",
      "steps per second: 0.56623\n",
      "step: 30997\n",
      "loss: 12.953397750854492\n",
      "steps per second: 0.54929\n",
      "step: 30998\n",
      "loss: 12.615224838256836\n",
      "steps per second: 0.52336\n",
      "step: 30999\n",
      "loss: 12.691259384155273\n",
      "steps per second: 0.52255\n",
      "step: 31000\n",
      "loss: 12.229879379272461\n",
      "steps per second: 0.54753\n",
      "step: 31001\n",
      "loss: 12.762258529663086\n",
      "steps per second: 0.53223\n",
      "step: 31002\n",
      "loss: 13.375248908996582\n",
      "steps per second: 0.58498\n",
      "step: 31003\n",
      "loss: 12.746978759765625\n",
      "steps per second: 0.49418\n",
      "step: 31004\n",
      "loss: 12.982034683227539\n",
      "steps per second: 0.54808\n",
      "step: 31005\n",
      "loss: 12.560524940490723\n",
      "steps per second: 0.52546\n",
      "step: 31006\n",
      "loss: 12.695162773132324\n",
      "steps per second: 0.55243\n",
      "step: 31007\n",
      "loss: 13.325575828552246\n",
      "steps per second: 0.57469\n",
      "step: 31008\n",
      "loss: 13.73659896850586\n",
      "steps per second: 0.50160\n",
      "step: 31009\n",
      "loss: 12.565321922302246\n",
      "steps per second: 0.55009\n",
      "step: 31010\n",
      "loss: 12.547333717346191\n",
      "steps per second: 0.50811\n",
      "step: 31011\n",
      "loss: 12.669469833374023\n",
      "steps per second: 0.53235\n",
      "step: 31012\n",
      "loss: 13.029542922973633\n",
      "steps per second: 0.56092\n",
      "step: 31013\n",
      "loss: 12.960884094238281\n",
      "steps per second: 0.54398\n",
      "step: 31014\n",
      "loss: 12.676932334899902\n",
      "steps per second: 0.52264\n",
      "step: 31015\n",
      "loss: 12.98639965057373\n",
      "steps per second: 0.54962\n",
      "step: 31016\n",
      "loss: 12.864089965820312\n",
      "steps per second: 0.55388\n",
      "step: 31017\n",
      "loss: 12.67650032043457\n",
      "steps per second: 0.55279\n",
      "step: 31018\n",
      "loss: 13.078126907348633\n",
      "steps per second: 0.57871\n",
      "step: 31019\n",
      "loss: 12.533075332641602\n",
      "steps per second: 0.52232\n",
      "step: 31020\n",
      "loss: 12.74553108215332\n",
      "steps per second: 0.61256\n",
      "step: 31021\n",
      "loss: 13.1907320022583\n",
      "steps per second: 0.54908\n",
      "step: 31022\n",
      "loss: 13.059656143188477\n",
      "steps per second: 0.54115\n",
      "step: 31023\n",
      "loss: 12.718781471252441\n",
      "steps per second: 0.53558\n",
      "step: 31024\n",
      "loss: 12.752321243286133\n",
      "steps per second: 0.53636\n",
      "step: 31025\n",
      "loss: 12.732398986816406\n",
      "steps per second: 0.52355\n",
      "step: 31026\n",
      "loss: 13.338008880615234\n",
      "steps per second: 0.57034\n",
      "step: 31027\n",
      "loss: 12.81406307220459\n",
      "steps per second: 0.54376\n",
      "step: 31028\n",
      "loss: 12.31927490234375\n",
      "steps per second: 0.55702\n",
      "step: 31029\n",
      "loss: 12.471354484558105\n",
      "steps per second: 0.57975\n",
      "step: 31030\n",
      "loss: 12.91934871673584\n",
      "steps per second: 0.53806\n",
      "step: 31031\n",
      "loss: 12.8384428024292\n",
      "steps per second: 0.55599\n",
      "step: 31032\n",
      "loss: 12.815597534179688\n",
      "steps per second: 0.52511\n",
      "step: 31033\n",
      "loss: 13.039593696594238\n",
      "steps per second: 0.57371\n",
      "step: 31034\n",
      "loss: 12.680686950683594\n",
      "steps per second: 0.52388\n",
      "step: 31035\n",
      "loss: 12.45077896118164\n",
      "steps per second: 0.55205\n",
      "step: 31036\n",
      "loss: 13.085345268249512\n",
      "steps per second: 0.55535\n",
      "step: 31037\n",
      "loss: 12.78510856628418\n",
      "steps per second: 0.58522\n",
      "step: 31038\n",
      "loss: 12.966557502746582\n",
      "steps per second: 0.55250\n",
      "step: 31039\n",
      "loss: 12.921486854553223\n",
      "steps per second: 0.51034\n",
      "step: 31040\n",
      "loss: 12.793999671936035\n",
      "steps per second: 0.54834\n",
      "step: 31041\n",
      "loss: 13.293683052062988\n",
      "steps per second: 0.58581\n",
      "step: 31042\n",
      "loss: 12.710997581481934\n",
      "steps per second: 0.58312\n",
      "step: 31043\n",
      "loss: 12.874526023864746\n",
      "steps per second: 0.56375\n",
      "step: 31044\n",
      "loss: 13.393858909606934\n",
      "steps per second: 0.58579\n",
      "step: 31045\n",
      "loss: 12.670989036560059\n",
      "steps per second: 0.57475\n",
      "step: 31046\n",
      "loss: 12.914796829223633\n",
      "steps per second: 0.57730\n",
      "step: 31047\n",
      "loss: 12.809367179870605\n",
      "steps per second: 0.58955\n",
      "step: 31048\n",
      "loss: 12.803189277648926\n",
      "steps per second: 0.56338\n",
      "step: 31049\n",
      "loss: 13.273746490478516\n",
      "steps per second: 0.55733\n",
      "step: 31050\n",
      "loss: 13.031001091003418\n",
      "steps per second: 0.55841\n",
      "step: 31051\n",
      "loss: 12.675090789794922\n",
      "steps per second: 0.54704\n",
      "step: 31052\n",
      "loss: 12.819378852844238\n",
      "steps per second: 0.54561\n",
      "step: 31053\n",
      "loss: 13.113523483276367\n",
      "steps per second: 0.58226\n",
      "step: 31054\n",
      "loss: 12.729801177978516\n",
      "steps per second: 0.54822\n",
      "step: 31055\n",
      "loss: 12.857370376586914\n",
      "steps per second: 0.57409\n",
      "step: 31056\n",
      "loss: 13.428884506225586\n",
      "steps per second: 0.53477\n",
      "step: 31057\n",
      "loss: 12.75152587890625\n",
      "steps per second: 0.52776\n",
      "step: 31058\n",
      "loss: 12.71944522857666\n",
      "steps per second: 0.55395\n",
      "step: 31059\n",
      "loss: 13.136944770812988\n",
      "steps per second: 0.53379\n",
      "step: 31060\n",
      "loss: 12.655789375305176\n",
      "steps per second: 0.54647\n",
      "step: 31061\n",
      "loss: 12.337069511413574\n",
      "steps per second: 0.56799\n",
      "step: 31062\n",
      "loss: 12.947561264038086\n",
      "steps per second: 0.56010\n",
      "step: 31063\n",
      "loss: 12.90928840637207\n",
      "steps per second: 0.52935\n",
      "step: 31064\n",
      "loss: 12.457998275756836\n",
      "steps per second: 0.47302\n",
      "step: 31065\n",
      "loss: 13.293967247009277\n",
      "steps per second: 0.58424\n",
      "step: 31066\n",
      "loss: 11.988485336303711\n",
      "steps per second: 0.55150\n",
      "step: 31067\n",
      "loss: 13.22665786743164\n",
      "steps per second: 0.55187\n",
      "step: 31068\n",
      "loss: 13.181802749633789\n",
      "steps per second: 0.53994\n",
      "step: 31069\n",
      "loss: 12.821932792663574\n",
      "steps per second: 0.53880\n",
      "step: 31070\n",
      "loss: 13.213190078735352\n",
      "steps per second: 0.57752\n",
      "step: 31071\n",
      "loss: 12.897445678710938\n",
      "steps per second: 0.50814\n",
      "step: 31072\n",
      "loss: 12.808186531066895\n",
      "steps per second: 0.54307\n",
      "step: 31073\n",
      "loss: 12.778743743896484\n",
      "steps per second: 0.51468\n",
      "step: 31074\n",
      "loss: 12.999283790588379\n",
      "steps per second: 0.57450\n",
      "step: 31075\n",
      "loss: 12.829585075378418\n",
      "steps per second: 0.55778\n",
      "step: 31076\n",
      "loss: 12.309028625488281\n",
      "steps per second: 0.55073\n",
      "step: 31077\n",
      "loss: 12.8341064453125\n",
      "steps per second: 0.54764\n",
      "step: 31078\n",
      "loss: 12.95214557647705\n",
      "steps per second: 0.55827\n",
      "step: 31079\n",
      "loss: 13.429279327392578\n",
      "steps per second: 0.61692\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8837872743606567, layer: 10\n",
      "saving at step 31079\n",
      "----------\n",
      "\n",
      "\n",
      "step: 31080\n",
      "loss: 13.189626693725586\n",
      "steps per second: 0.30482\n",
      "step: 31081\n",
      "loss: 13.166753768920898\n",
      "steps per second: 0.58127\n",
      "step: 31082\n",
      "loss: 12.225164413452148\n",
      "steps per second: 0.52936\n",
      "step: 31083\n",
      "loss: 12.379963874816895\n",
      "steps per second: 0.58724\n",
      "step: 31084\n",
      "loss: 13.140892028808594\n",
      "steps per second: 0.56056\n",
      "step: 31085\n",
      "loss: 12.89013671875\n",
      "steps per second: 0.56117\n",
      "step: 31086\n",
      "loss: 12.752310752868652\n",
      "steps per second: 0.57849\n",
      "step: 31087\n",
      "loss: 12.824725151062012\n",
      "steps per second: 0.54346\n",
      "step: 31088\n",
      "loss: 12.146023750305176\n",
      "steps per second: 0.54795\n",
      "step: 31089\n",
      "loss: 12.207574844360352\n",
      "steps per second: 0.55536\n",
      "step: 31090\n",
      "loss: 13.232120513916016\n",
      "steps per second: 0.58691\n",
      "step: 31091\n",
      "loss: 12.796757698059082\n",
      "steps per second: 0.61487\n",
      "step: 31092\n",
      "loss: 13.182206153869629\n",
      "steps per second: 0.53496\n",
      "step: 31093\n",
      "loss: 13.11434555053711\n",
      "steps per second: 0.52618\n",
      "step: 31094\n",
      "loss: 12.413957595825195\n",
      "steps per second: 0.61590\n",
      "step: 31095\n",
      "loss: 12.309489250183105\n",
      "steps per second: 0.52601\n",
      "step: 31096\n",
      "loss: 12.85665225982666\n",
      "steps per second: 0.57109\n",
      "step: 31097\n",
      "loss: 12.336191177368164\n",
      "steps per second: 0.51174\n",
      "step: 31098\n",
      "loss: 12.84469223022461\n",
      "steps per second: 0.55212\n",
      "step: 31099\n",
      "loss: 12.860276222229004\n",
      "steps per second: 0.61202\n",
      "step: 31100\n",
      "loss: 13.097698211669922\n",
      "steps per second: 0.52561\n",
      "step: 31101\n",
      "loss: 12.701399803161621\n",
      "steps per second: 0.55723\n",
      "step: 31102\n",
      "loss: 13.179189682006836\n",
      "steps per second: 0.57831\n",
      "step: 31103\n",
      "loss: 12.83951187133789\n",
      "steps per second: 0.52640\n",
      "step: 31104\n",
      "loss: 12.904601097106934\n",
      "steps per second: 0.55229\n",
      "step: 31105\n",
      "loss: 12.892719268798828\n",
      "steps per second: 0.60950\n",
      "step: 31106\n",
      "loss: 12.835844039916992\n",
      "steps per second: 0.48069\n",
      "step: 31107\n",
      "loss: 12.624178886413574\n",
      "steps per second: 0.61007\n",
      "step: 31108\n",
      "loss: 12.779366493225098\n",
      "steps per second: 0.57669\n",
      "step: 31109\n",
      "loss: 13.147210121154785\n",
      "steps per second: 0.53505\n",
      "step: 31110\n",
      "loss: 12.697335243225098\n",
      "steps per second: 0.54298\n",
      "step: 31111\n",
      "loss: 12.804524421691895\n",
      "steps per second: 0.56967\n",
      "step: 31112\n",
      "loss: 13.169628143310547\n",
      "steps per second: 0.55002\n",
      "step: 31113\n",
      "loss: 12.660866737365723\n",
      "steps per second: 0.54304\n",
      "step: 31114\n",
      "loss: 12.545544624328613\n",
      "steps per second: 0.52076\n",
      "step: 31115\n",
      "loss: 13.007030487060547\n",
      "steps per second: 0.57260\n",
      "step: 31116\n",
      "loss: 12.819292068481445\n",
      "steps per second: 0.52344\n",
      "step: 31117\n",
      "loss: 12.638104438781738\n",
      "steps per second: 0.53279\n",
      "step: 31118\n",
      "loss: 12.969573974609375\n",
      "steps per second: 0.58644\n",
      "step: 31119\n",
      "loss: 12.66584587097168\n",
      "steps per second: 0.55077\n",
      "step: 31120\n",
      "loss: 13.320928573608398\n",
      "steps per second: 0.55261\n",
      "step: 31121\n",
      "loss: 12.911506652832031\n",
      "steps per second: 0.51998\n",
      "step: 31122\n",
      "loss: 12.542047500610352\n",
      "steps per second: 0.55232\n",
      "step: 31123\n",
      "loss: 13.028779983520508\n",
      "steps per second: 0.55486\n",
      "step: 31124\n",
      "loss: 12.621241569519043\n",
      "steps per second: 0.54141\n",
      "step: 31125\n",
      "loss: 13.07610034942627\n",
      "steps per second: 0.53762\n",
      "step: 31126\n",
      "loss: 13.09290885925293\n",
      "steps per second: 0.54419\n",
      "step: 31127\n",
      "loss: 12.79702377319336\n",
      "steps per second: 0.54967\n",
      "step: 31128\n",
      "loss: 12.322735786437988\n",
      "steps per second: 0.61921\n",
      "step: 31129\n",
      "loss: 12.48997974395752\n",
      "steps per second: 0.55573\n",
      "step: 31130\n",
      "loss: 13.20578384399414\n",
      "steps per second: 0.54172\n",
      "step: 31131\n",
      "loss: 12.63625717163086\n",
      "steps per second: 0.54376\n",
      "step: 31132\n",
      "loss: 12.741888046264648\n",
      "steps per second: 0.58419\n",
      "step: 31133\n",
      "loss: 12.552915573120117\n",
      "steps per second: 0.54053\n",
      "step: 31134\n",
      "loss: 12.818254470825195\n",
      "steps per second: 0.57103\n",
      "step: 31135\n",
      "loss: 13.286589622497559\n",
      "steps per second: 0.57001\n",
      "step: 31136\n",
      "loss: 12.860197067260742\n",
      "steps per second: 0.53438\n",
      "step: 31137\n",
      "loss: 12.523904800415039\n",
      "steps per second: 0.55440\n",
      "step: 31138\n",
      "loss: 12.680075645446777\n",
      "steps per second: 0.53727\n",
      "step: 31139\n",
      "loss: 12.088233947753906\n",
      "steps per second: 0.53175\n",
      "step: 31140\n",
      "loss: 13.025594711303711\n",
      "steps per second: 0.52403\n",
      "step: 31141\n",
      "loss: 12.957903861999512\n",
      "steps per second: 0.54023\n",
      "step: 31142\n",
      "loss: 12.8892240524292\n",
      "steps per second: 0.51940\n",
      "step: 31143\n",
      "loss: 13.073517799377441\n",
      "steps per second: 0.52101\n",
      "step: 31144\n",
      "loss: 12.765935897827148\n",
      "steps per second: 0.57289\n",
      "step: 31145\n",
      "loss: 12.323136329650879\n",
      "steps per second: 0.51360\n",
      "step: 31146\n",
      "loss: 12.592198371887207\n",
      "steps per second: 0.58432\n",
      "step: 31147\n",
      "loss: 12.635075569152832\n",
      "steps per second: 0.51603\n",
      "step: 31148\n",
      "loss: 12.930744171142578\n",
      "steps per second: 0.54045\n",
      "step: 31149\n",
      "loss: 13.087894439697266\n",
      "steps per second: 0.54336\n",
      "step: 31150\n",
      "loss: 12.691181182861328\n",
      "steps per second: 0.58044\n",
      "step: 31151\n",
      "loss: 13.081019401550293\n",
      "steps per second: 0.58055\n",
      "step: 31152\n",
      "loss: 12.664936065673828\n",
      "steps per second: 0.54007\n",
      "step: 31153\n",
      "loss: 12.76198673248291\n",
      "steps per second: 0.58548\n",
      "step: 31154\n",
      "loss: 13.006927490234375\n",
      "steps per second: 0.55247\n",
      "step: 31155\n",
      "loss: 12.995887756347656\n",
      "steps per second: 0.55028\n",
      "step: 31156\n",
      "loss: 12.926275253295898\n",
      "steps per second: 0.58443\n",
      "step: 31157\n",
      "loss: 12.581643104553223\n",
      "steps per second: 0.55848\n",
      "step: 31158\n",
      "loss: 12.71724796295166\n",
      "steps per second: 0.55839\n",
      "step: 31159\n",
      "loss: 12.973212242126465\n",
      "steps per second: 0.54852\n",
      "step: 31160\n",
      "loss: 12.931293487548828\n",
      "steps per second: 0.52779\n",
      "step: 31161\n",
      "loss: 12.837063789367676\n",
      "steps per second: 0.54203\n",
      "step: 31162\n",
      "loss: 12.641142845153809\n",
      "steps per second: 0.59163\n",
      "step: 31163\n",
      "loss: 13.177861213684082\n",
      "steps per second: 0.57470\n",
      "step: 31164\n",
      "loss: 13.149169921875\n",
      "steps per second: 0.57676\n",
      "step: 31165\n",
      "loss: 12.670601844787598\n",
      "steps per second: 0.55077\n",
      "step: 31166\n",
      "loss: 12.906392097473145\n",
      "steps per second: 0.54751\n",
      "step: 31167\n",
      "loss: 12.934029579162598\n",
      "steps per second: 0.57524\n",
      "step: 31168\n",
      "loss: 13.002420425415039\n",
      "steps per second: 0.53001\n",
      "step: 31169\n",
      "loss: 12.970277786254883\n",
      "steps per second: 0.54274\n",
      "step: 31170\n",
      "loss: 13.018190383911133\n",
      "steps per second: 0.57818\n",
      "step: 31171\n",
      "loss: 12.522651672363281\n",
      "steps per second: 0.55113\n",
      "step: 31172\n",
      "loss: 12.530828475952148\n",
      "steps per second: 0.54995\n",
      "step: 31173\n",
      "loss: 13.18725872039795\n",
      "steps per second: 0.55148\n",
      "step: 31174\n",
      "loss: 13.102177619934082\n",
      "steps per second: 0.52476\n",
      "step: 31175\n",
      "loss: 12.939336776733398\n",
      "steps per second: 0.55518\n",
      "step: 31176\n",
      "loss: 12.754899024963379\n",
      "steps per second: 0.56062\n",
      "step: 31177\n",
      "loss: 12.532279014587402\n",
      "steps per second: 0.55223\n",
      "step: 31178\n",
      "loss: 12.885499000549316\n",
      "steps per second: 0.57143\n",
      "step: 31179\n",
      "loss: 13.2804536819458\n",
      "steps per second: 0.61908\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8447100520133972, layer: 11\n",
      "saving at step 31179\n",
      "----------\n",
      "\n",
      "\n",
      "step: 31180\n",
      "loss: 13.110285758972168\n",
      "steps per second: 0.30550\n",
      "step: 31181\n",
      "loss: 12.69831657409668\n",
      "steps per second: 0.55462\n",
      "step: 31182\n",
      "loss: 12.794595718383789\n",
      "steps per second: 0.54731\n",
      "step: 31183\n",
      "loss: 13.29930305480957\n",
      "steps per second: 0.53924\n",
      "step: 31184\n",
      "loss: 12.621374130249023\n",
      "steps per second: 0.53901\n",
      "step: 31185\n",
      "loss: 12.86564826965332\n",
      "steps per second: 0.58130\n",
      "step: 31186\n",
      "loss: 12.584871292114258\n",
      "steps per second: 0.55420\n",
      "step: 31187\n",
      "loss: 13.410223960876465\n",
      "steps per second: 0.57625\n",
      "step: 31188\n",
      "loss: 12.569456100463867\n",
      "steps per second: 0.58359\n",
      "step: 31189\n",
      "loss: 12.104462623596191\n",
      "steps per second: 0.55364\n",
      "step: 31190\n",
      "loss: 12.458794593811035\n",
      "steps per second: 0.54651\n",
      "step: 31191\n",
      "loss: 12.442071914672852\n",
      "steps per second: 0.54863\n",
      "step: 31192\n",
      "loss: 12.98291301727295\n",
      "steps per second: 0.52048\n",
      "step: 31193\n",
      "loss: 12.643157958984375\n",
      "steps per second: 0.57674\n",
      "step: 31194\n",
      "loss: 12.878682136535645\n",
      "steps per second: 0.57086\n",
      "step: 31195\n",
      "loss: 13.124791145324707\n",
      "steps per second: 0.57202\n",
      "step: 31196\n",
      "loss: 12.908823013305664\n",
      "steps per second: 0.52638\n",
      "step: 31197\n",
      "loss: 12.489628791809082\n",
      "steps per second: 0.52376\n",
      "step: 31198\n",
      "loss: 13.08708381652832\n",
      "steps per second: 0.55603\n",
      "step: 31199\n",
      "loss: 12.77070426940918\n",
      "steps per second: 0.53734\n",
      "step: 31200\n",
      "loss: 12.87130355834961\n",
      "steps per second: 0.52299\n",
      "step: 31201\n",
      "loss: 12.39583683013916\n",
      "steps per second: 0.53822\n",
      "step: 31202\n",
      "loss: 13.481876373291016\n",
      "steps per second: 0.53666\n",
      "step: 31203\n",
      "loss: 12.444429397583008\n",
      "steps per second: 0.58337\n",
      "step: 31204\n",
      "loss: 12.547974586486816\n",
      "steps per second: 0.53125\n",
      "step: 31205\n",
      "loss: 13.125635147094727\n",
      "steps per second: 0.55622\n",
      "step: 31206\n",
      "loss: 12.743339538574219\n",
      "steps per second: 0.56198\n",
      "step: 31207\n",
      "loss: 12.617053031921387\n",
      "steps per second: 0.58326\n",
      "step: 31208\n",
      "loss: 12.617019653320312\n",
      "steps per second: 0.55520\n",
      "step: 31209\n",
      "loss: 13.088836669921875\n",
      "steps per second: 0.53904\n",
      "step: 31210\n",
      "loss: 12.809732437133789\n",
      "steps per second: 0.56772\n",
      "step: 31211\n",
      "loss: 13.180489540100098\n",
      "steps per second: 0.56222\n",
      "step: 31212\n",
      "loss: 13.461418151855469\n",
      "steps per second: 0.53154\n",
      "step: 31213\n",
      "loss: 12.792174339294434\n",
      "steps per second: 0.58563\n",
      "step: 31214\n",
      "loss: 12.908160209655762\n",
      "steps per second: 0.54935\n",
      "step: 31215\n",
      "loss: 12.912424087524414\n",
      "steps per second: 0.53744\n",
      "step: 31216\n",
      "loss: 12.8340425491333\n",
      "steps per second: 0.56559\n",
      "step: 31217\n",
      "loss: 12.428128242492676\n",
      "steps per second: 0.54190\n",
      "step: 31218\n",
      "loss: 13.159499168395996\n",
      "steps per second: 0.54740\n",
      "step: 31219\n",
      "loss: 12.615373611450195\n",
      "steps per second: 0.54564\n",
      "step: 31220\n",
      "loss: 12.766009330749512\n",
      "steps per second: 0.61451\n",
      "step: 31221\n",
      "loss: 12.830413818359375\n",
      "steps per second: 0.55258\n",
      "step: 31222\n",
      "loss: 13.00224781036377\n",
      "steps per second: 0.53632\n",
      "step: 31223\n",
      "loss: 12.787158012390137\n",
      "steps per second: 0.52777\n",
      "step: 31224\n",
      "loss: 12.429010391235352\n",
      "steps per second: 0.55667\n",
      "step: 31225\n",
      "loss: 12.709689140319824\n",
      "steps per second: 0.57884\n",
      "step: 31226\n",
      "loss: 12.652020454406738\n",
      "steps per second: 0.62088\n",
      "step: 31227\n",
      "loss: 12.8753080368042\n",
      "steps per second: 0.61646\n",
      "step: 31228\n",
      "loss: 13.070703506469727\n",
      "steps per second: 0.53064\n",
      "step: 31229\n",
      "loss: 12.679473876953125\n",
      "steps per second: 0.54634\n",
      "step: 31230\n",
      "loss: 13.011039733886719\n",
      "steps per second: 0.53221\n",
      "step: 31231\n",
      "loss: 13.558052062988281\n",
      "steps per second: 0.61946\n",
      "step: 31232\n",
      "loss: 12.864617347717285\n",
      "steps per second: 0.57950\n",
      "step: 31233\n",
      "loss: 13.091548919677734\n",
      "steps per second: 0.53046\n",
      "step: 31234\n",
      "loss: 12.186668395996094\n",
      "steps per second: 0.57212\n",
      "step: 31235\n",
      "loss: 12.993303298950195\n",
      "steps per second: 0.53472\n",
      "step: 31236\n",
      "loss: 13.081265449523926\n",
      "steps per second: 0.53273\n",
      "step: 31237\n",
      "loss: 12.219236373901367\n",
      "steps per second: 0.61163\n",
      "step: 31238\n",
      "loss: 12.893720626831055\n",
      "steps per second: 0.56396\n",
      "step: 31239\n",
      "loss: 12.703701972961426\n",
      "steps per second: 0.52250\n",
      "step: 31240\n",
      "loss: 13.055752754211426\n",
      "steps per second: 0.59039\n",
      "step: 31241\n",
      "loss: 12.872536659240723\n",
      "steps per second: 0.55071\n",
      "step: 31242\n",
      "loss: 12.666248321533203\n",
      "steps per second: 0.58786\n",
      "step: 31243\n",
      "loss: 12.470376014709473\n",
      "steps per second: 0.53685\n",
      "step: 31244\n",
      "loss: 12.502717971801758\n",
      "steps per second: 0.58446\n",
      "step: 31245\n",
      "loss: 13.15591049194336\n",
      "steps per second: 0.53412\n",
      "step: 31246\n",
      "loss: 13.409673690795898\n",
      "steps per second: 0.56082\n",
      "step: 31247\n",
      "loss: 12.69245719909668\n",
      "steps per second: 0.57816\n",
      "step: 31248\n",
      "loss: 12.6063232421875\n",
      "steps per second: 0.57487\n",
      "step: 31249\n",
      "loss: 12.396418571472168\n",
      "steps per second: 0.55395\n",
      "step: 31250\n",
      "loss: 12.912911415100098\n",
      "steps per second: 0.55759\n",
      "step: 31251\n",
      "loss: 12.964544296264648\n",
      "steps per second: 0.56250\n",
      "step: 31252\n",
      "loss: 12.639533042907715\n",
      "steps per second: 0.59180\n",
      "step: 31253\n",
      "loss: 12.186173439025879\n",
      "steps per second: 0.56093\n",
      "step: 31254\n",
      "loss: 12.569684028625488\n",
      "steps per second: 0.54048\n",
      "step: 31255\n",
      "loss: 13.20102596282959\n",
      "steps per second: 0.54126\n",
      "step: 31256\n",
      "loss: 12.918359756469727\n",
      "steps per second: 0.53093\n",
      "step: 31257\n",
      "loss: 13.306201934814453\n",
      "steps per second: 0.57014\n",
      "step: 31258\n",
      "loss: 12.71615982055664\n",
      "steps per second: 0.61591\n",
      "step: 31259\n",
      "loss: 12.918219566345215\n",
      "steps per second: 0.55658\n",
      "step: 31260\n",
      "loss: 13.281558990478516\n",
      "steps per second: 0.57446\n",
      "step: 31261\n",
      "loss: 12.221780776977539\n",
      "steps per second: 0.56393\n",
      "step: 31262\n",
      "loss: 12.734662055969238\n",
      "steps per second: 0.58557\n",
      "step: 31263\n",
      "loss: 12.62407112121582\n",
      "steps per second: 0.57704\n",
      "step: 31264\n",
      "loss: 12.804328918457031\n",
      "steps per second: 0.62374\n",
      "step: 31265\n",
      "loss: 13.061088562011719\n",
      "steps per second: 0.52843\n",
      "step: 31266\n",
      "loss: 12.631138801574707\n",
      "steps per second: 0.57740\n",
      "step: 31267\n",
      "loss: 12.793689727783203\n",
      "steps per second: 0.53801\n",
      "step: 31268\n",
      "loss: 12.786746978759766\n",
      "steps per second: 0.52955\n",
      "step: 31269\n",
      "loss: 12.634516716003418\n",
      "steps per second: 0.53371\n",
      "step: 31270\n",
      "loss: 12.207688331604004\n",
      "steps per second: 0.55974\n",
      "step: 31271\n",
      "loss: 12.713397026062012\n",
      "steps per second: 0.58259\n",
      "step: 31272\n",
      "loss: 12.803229331970215\n",
      "steps per second: 0.57552\n",
      "step: 31273\n",
      "loss: 12.43224811553955\n",
      "steps per second: 0.55914\n",
      "step: 31274\n",
      "loss: 13.448234558105469\n",
      "steps per second: 0.54133\n",
      "step: 31275\n",
      "loss: 12.731388092041016\n",
      "steps per second: 0.54423\n",
      "step: 31276\n",
      "loss: 12.990262031555176\n",
      "steps per second: 0.57569\n",
      "step: 31277\n",
      "loss: 12.991949081420898\n",
      "steps per second: 0.55789\n",
      "step: 31278\n",
      "loss: 12.700448036193848\n",
      "steps per second: 0.53393\n",
      "step: 31279\n",
      "loss: 12.821977615356445\n",
      "steps per second: 0.53403\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8324874639511108, layer: 11\n",
      "saving at step 31279\n",
      "----------\n",
      "\n",
      "\n",
      "step: 31280\n",
      "loss: 13.06260871887207\n",
      "steps per second: 0.28335\n",
      "step: 31281\n",
      "loss: 13.407809257507324\n",
      "steps per second: 0.53461\n",
      "step: 31282\n",
      "loss: 12.68785572052002\n",
      "steps per second: 0.54973\n",
      "step: 31283\n",
      "loss: 12.695145606994629\n",
      "steps per second: 0.55093\n",
      "step: 31284\n",
      "loss: 13.236924171447754\n",
      "steps per second: 0.55110\n",
      "step: 31285\n",
      "loss: 12.129045486450195\n",
      "steps per second: 0.54887\n",
      "step: 31286\n",
      "loss: 12.77479076385498\n",
      "steps per second: 0.55888\n",
      "step: 31287\n",
      "loss: 13.178744316101074\n",
      "steps per second: 0.55384\n",
      "step: 31288\n",
      "loss: 12.921721458435059\n",
      "steps per second: 0.50896\n",
      "step: 31289\n",
      "loss: 13.268147468566895\n",
      "steps per second: 0.61307\n",
      "step: 31290\n",
      "loss: 12.973397254943848\n",
      "steps per second: 0.57475\n",
      "step: 31291\n",
      "loss: 12.930814743041992\n",
      "steps per second: 0.56176\n",
      "step: 31292\n",
      "loss: 12.874117851257324\n",
      "steps per second: 0.58209\n",
      "step: 31293\n",
      "loss: 12.699455261230469\n",
      "steps per second: 0.60873\n",
      "step: 31294\n",
      "loss: 12.559925079345703\n",
      "steps per second: 0.54625\n",
      "step: 31295\n",
      "loss: 12.687932968139648\n",
      "steps per second: 0.55314\n",
      "step: 31296\n",
      "loss: 13.149450302124023\n",
      "steps per second: 0.56040\n",
      "step: 31297\n",
      "loss: 13.16987133026123\n",
      "steps per second: 0.58217\n",
      "step: 31298\n",
      "loss: 12.976398468017578\n",
      "steps per second: 0.52510\n",
      "step: 31299\n",
      "loss: 12.376850128173828\n",
      "steps per second: 0.56300\n",
      "step: 31300\n",
      "loss: 13.622262001037598\n",
      "steps per second: 0.58649\n",
      "step: 31301\n",
      "loss: 13.20425033569336\n",
      "steps per second: 0.58891\n",
      "step: 31302\n",
      "loss: 13.117620468139648\n",
      "steps per second: 0.51993\n",
      "step: 31303\n",
      "loss: 12.623211860656738\n",
      "steps per second: 0.53030\n",
      "step: 31304\n",
      "loss: 12.924737930297852\n",
      "steps per second: 0.55241\n",
      "step: 31305\n",
      "loss: 12.796355247497559\n",
      "steps per second: 0.58052\n",
      "step: 31306\n",
      "loss: 12.931713104248047\n",
      "steps per second: 0.59083\n",
      "step: 31307\n",
      "loss: 12.5934419631958\n",
      "steps per second: 0.57861\n",
      "step: 31308\n",
      "loss: 13.208643913269043\n",
      "steps per second: 0.54171\n",
      "step: 31309\n",
      "loss: 13.257406234741211\n",
      "steps per second: 0.51697\n",
      "step: 31310\n",
      "loss: 13.050288200378418\n",
      "steps per second: 0.54208\n",
      "step: 31311\n",
      "loss: 12.754340171813965\n",
      "steps per second: 0.57820\n",
      "step: 31312\n",
      "loss: 13.388288497924805\n",
      "steps per second: 0.56223\n",
      "step: 31313\n",
      "loss: 12.796127319335938\n",
      "steps per second: 0.56751\n",
      "step: 31314\n",
      "loss: 13.115999221801758\n",
      "steps per second: 0.50037\n",
      "step: 31315\n",
      "loss: 13.2056245803833\n",
      "steps per second: 0.55528\n",
      "step: 31316\n",
      "loss: 13.083719253540039\n",
      "steps per second: 0.58394\n",
      "step: 31317\n",
      "loss: 12.439207077026367\n",
      "steps per second: 0.58174\n",
      "step: 31318\n",
      "loss: 12.55693244934082\n",
      "steps per second: 0.55612\n",
      "step: 31319\n",
      "loss: 12.895058631896973\n",
      "steps per second: 0.55675\n",
      "step: 31320\n",
      "loss: 13.133241653442383\n",
      "steps per second: 0.58362\n",
      "step: 31321\n",
      "loss: 12.427631378173828\n",
      "steps per second: 0.47664\n",
      "step: 31322\n",
      "loss: 13.205096244812012\n",
      "steps per second: 0.53649\n",
      "step: 31323\n",
      "loss: 12.681044578552246\n",
      "steps per second: 0.51433\n",
      "step: 31324\n",
      "loss: 12.64207935333252\n",
      "steps per second: 0.53947\n",
      "step: 31325\n",
      "loss: 12.563817024230957\n",
      "steps per second: 0.51593\n",
      "step: 31326\n",
      "loss: 12.494192123413086\n",
      "steps per second: 0.54703\n",
      "step: 31327\n",
      "loss: 13.290298461914062\n",
      "steps per second: 0.54077\n",
      "step: 31328\n",
      "loss: 13.234837532043457\n",
      "steps per second: 0.55069\n",
      "step: 31329\n",
      "loss: 13.443028450012207\n",
      "steps per second: 0.56595\n",
      "step: 31330\n",
      "loss: 12.651597023010254\n",
      "steps per second: 0.55118\n",
      "step: 31331\n",
      "loss: 12.328862190246582\n",
      "steps per second: 0.53188\n",
      "step: 31332\n",
      "loss: 12.578316688537598\n",
      "steps per second: 0.54206\n",
      "step: 31333\n",
      "loss: 12.868327140808105\n",
      "steps per second: 0.54916\n",
      "step: 31334\n",
      "loss: 13.08410930633545\n",
      "steps per second: 0.55515\n",
      "step: 31335\n",
      "loss: 13.02099895477295\n",
      "steps per second: 0.58278\n",
      "step: 31336\n",
      "loss: 12.646095275878906\n",
      "steps per second: 0.58752\n",
      "step: 31337\n",
      "loss: 13.03912353515625\n",
      "steps per second: 0.55775\n",
      "step: 31338\n",
      "loss: 12.355746269226074\n",
      "steps per second: 0.56668\n",
      "step: 31339\n",
      "loss: 12.866575241088867\n",
      "steps per second: 0.56465\n",
      "step: 31340\n",
      "loss: 12.902750968933105\n",
      "steps per second: 0.58158\n",
      "step: 31341\n",
      "loss: 12.939590454101562\n",
      "steps per second: 0.55724\n",
      "step: 31342\n",
      "loss: 12.905679702758789\n",
      "steps per second: 0.55727\n",
      "step: 31343\n",
      "loss: 12.87496280670166\n",
      "steps per second: 0.53816\n",
      "step: 31344\n",
      "loss: 12.812423706054688\n",
      "steps per second: 0.52956\n",
      "step: 31345\n",
      "loss: 12.514066696166992\n",
      "steps per second: 0.55605\n",
      "step: 31346\n",
      "loss: 13.067397117614746\n",
      "steps per second: 0.54984\n",
      "step: 31347\n",
      "loss: 12.638693809509277\n",
      "steps per second: 0.53369\n",
      "step: 31348\n",
      "loss: 13.171514511108398\n",
      "steps per second: 0.52563\n",
      "step: 31349\n",
      "loss: 12.364974021911621\n",
      "steps per second: 0.56005\n",
      "step: 31350\n",
      "loss: 12.934931755065918\n",
      "steps per second: 0.61033\n",
      "step: 31351\n",
      "loss: 12.814619064331055\n",
      "steps per second: 0.54510\n",
      "step: 31352\n",
      "loss: 13.742537498474121\n",
      "steps per second: 0.57744\n",
      "step: 31353\n",
      "loss: 12.927992820739746\n",
      "steps per second: 0.58401\n",
      "step: 31354\n",
      "loss: 12.236044883728027\n",
      "steps per second: 0.56936\n",
      "step: 31355\n",
      "loss: 13.034090995788574\n",
      "steps per second: 0.58200\n",
      "step: 31356\n",
      "loss: 13.022456169128418\n",
      "steps per second: 0.54077\n",
      "step: 31357\n",
      "loss: 12.416105270385742\n",
      "steps per second: 0.54885\n",
      "step: 31358\n",
      "loss: 12.555477142333984\n",
      "steps per second: 0.57983\n",
      "step: 31359\n",
      "loss: 12.423809051513672\n",
      "steps per second: 0.56376\n",
      "step: 31360\n",
      "loss: 12.61169147491455\n",
      "steps per second: 0.57378\n",
      "step: 31361\n",
      "loss: 12.992033004760742\n",
      "steps per second: 0.55931\n",
      "step: 31362\n",
      "loss: 12.841691970825195\n",
      "steps per second: 0.58642\n",
      "step: 31363\n",
      "loss: 12.946983337402344\n",
      "steps per second: 0.58251\n",
      "step: 31364\n",
      "loss: 12.399279594421387\n",
      "steps per second: 0.56000\n",
      "step: 31365\n",
      "loss: 13.0324068069458\n",
      "steps per second: 0.54979\n",
      "step: 31366\n",
      "loss: 12.990816116333008\n",
      "steps per second: 0.56209\n",
      "step: 31367\n",
      "loss: 12.3478422164917\n",
      "steps per second: 0.55872\n",
      "step: 31368\n",
      "loss: 13.204422950744629\n",
      "steps per second: 0.55476\n",
      "step: 31369\n",
      "loss: 12.820561408996582\n",
      "steps per second: 0.55516\n",
      "step: 31370\n",
      "loss: 13.364806175231934\n",
      "steps per second: 0.58647\n",
      "step: 31371\n",
      "loss: 12.212830543518066\n",
      "steps per second: 0.60970\n",
      "step: 31372\n",
      "loss: 12.89193344116211\n",
      "steps per second: 0.55012\n",
      "step: 31373\n",
      "loss: 12.684284210205078\n",
      "steps per second: 0.55496\n",
      "step: 31374\n",
      "loss: 12.885323524475098\n",
      "steps per second: 0.54393\n",
      "step: 31375\n",
      "loss: 13.366726875305176\n",
      "steps per second: 0.58277\n",
      "step: 31376\n",
      "loss: 12.812294006347656\n",
      "steps per second: 0.53003\n",
      "step: 31377\n",
      "loss: 13.102603912353516\n",
      "steps per second: 0.53284\n",
      "step: 31378\n",
      "loss: 12.59794807434082\n",
      "steps per second: 0.58069\n",
      "step: 31379\n",
      "loss: 12.959107398986816\n",
      "steps per second: 0.54493\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8545792698860168, layer: 11\n",
      "saving at step 31379\n",
      "----------\n",
      "\n",
      "\n",
      "step: 31380\n",
      "loss: 12.857629776000977\n",
      "steps per second: 0.28291\n",
      "step: 31381\n",
      "loss: 12.228601455688477\n",
      "steps per second: 0.53940\n",
      "step: 31382\n",
      "loss: 13.0387544631958\n",
      "steps per second: 0.57224\n",
      "step: 31383\n",
      "loss: 12.546727180480957\n",
      "steps per second: 0.50729\n",
      "step: 31384\n",
      "loss: 12.700575828552246\n",
      "steps per second: 0.56140\n",
      "step: 31385\n",
      "loss: 12.924722671508789\n",
      "steps per second: 0.53299\n",
      "step: 31386\n",
      "loss: 12.364091873168945\n",
      "steps per second: 0.55969\n",
      "step: 31387\n",
      "loss: 12.886397361755371\n",
      "steps per second: 0.56751\n",
      "step: 31388\n",
      "loss: 13.04005241394043\n",
      "steps per second: 0.53071\n",
      "step: 31389\n",
      "loss: 13.082550048828125\n",
      "steps per second: 0.55958\n",
      "step: 31390\n",
      "loss: 12.779245376586914\n",
      "steps per second: 0.56165\n",
      "step: 31391\n",
      "loss: 13.267353057861328\n",
      "steps per second: 0.55919\n",
      "step: 31392\n",
      "loss: 12.90301513671875\n",
      "steps per second: 0.57286\n",
      "step: 31393\n",
      "loss: 12.947619438171387\n",
      "steps per second: 0.55880\n",
      "step: 31394\n",
      "loss: 12.77418327331543\n",
      "steps per second: 0.55992\n",
      "step: 31395\n",
      "loss: 13.129304885864258\n",
      "steps per second: 0.56263\n",
      "step: 31396\n",
      "loss: 12.660093307495117\n",
      "steps per second: 0.61504\n",
      "step: 31397\n",
      "loss: 12.982460021972656\n",
      "steps per second: 0.53662\n",
      "step: 31398\n",
      "loss: 12.059931755065918\n",
      "steps per second: 0.57314\n",
      "step: 31399\n",
      "loss: 12.982290267944336\n",
      "steps per second: 0.54115\n",
      "step: 31400\n",
      "loss: 12.617427825927734\n",
      "steps per second: 0.55303\n",
      "step: 31401\n",
      "loss: 12.959779739379883\n",
      "steps per second: 0.55639\n",
      "step: 31402\n",
      "loss: 12.562007904052734\n",
      "steps per second: 0.54757\n",
      "step: 31403\n",
      "loss: 12.93476676940918\n",
      "steps per second: 0.54882\n",
      "step: 31404\n",
      "loss: 13.168265342712402\n",
      "steps per second: 0.58170\n",
      "step: 31405\n",
      "loss: 13.53503131866455\n",
      "steps per second: 0.53744\n",
      "step: 31406\n",
      "loss: 12.637138366699219\n",
      "steps per second: 0.55550\n",
      "step: 31407\n",
      "loss: 12.765344619750977\n",
      "steps per second: 0.53211\n",
      "step: 31408\n",
      "loss: 12.79305648803711\n",
      "steps per second: 0.55768\n",
      "step: 31409\n",
      "loss: 12.971343994140625\n",
      "steps per second: 0.53811\n",
      "step: 31410\n",
      "loss: 12.848515510559082\n",
      "steps per second: 0.53225\n",
      "step: 31411\n",
      "loss: 12.793465614318848\n",
      "steps per second: 0.54860\n",
      "step: 31412\n",
      "loss: 12.864517211914062\n",
      "steps per second: 0.55345\n",
      "step: 31413\n",
      "loss: 12.575302124023438\n",
      "steps per second: 0.58139\n",
      "step: 31414\n",
      "loss: 12.873551368713379\n",
      "steps per second: 0.55150\n",
      "step: 31415\n",
      "loss: 12.41490364074707\n",
      "steps per second: 0.54059\n",
      "step: 31416\n",
      "loss: 12.404083251953125\n",
      "steps per second: 0.57636\n",
      "step: 31417\n",
      "loss: 12.902194023132324\n",
      "steps per second: 0.55778\n",
      "step: 31418\n",
      "loss: 13.035655975341797\n",
      "steps per second: 0.55279\n",
      "step: 31419\n",
      "loss: 12.772181510925293\n",
      "steps per second: 0.57486\n",
      "step: 31420\n",
      "loss: 12.799656867980957\n",
      "steps per second: 0.55405\n",
      "step: 31421\n",
      "loss: 12.465784072875977\n",
      "steps per second: 0.52269\n",
      "step: 31422\n",
      "loss: 12.830900192260742\n",
      "steps per second: 0.55940\n",
      "step: 31423\n",
      "loss: 12.539985656738281\n",
      "steps per second: 0.54466\n",
      "step: 31424\n",
      "loss: 12.98514175415039\n",
      "steps per second: 0.58414\n",
      "step: 31425\n",
      "loss: 12.563103675842285\n",
      "steps per second: 0.57569\n",
      "step: 31426\n",
      "loss: 12.874467849731445\n",
      "steps per second: 0.57293\n",
      "step: 31427\n",
      "loss: 12.886100769042969\n",
      "steps per second: 0.54233\n",
      "step: 31428\n",
      "loss: 12.65738296508789\n",
      "steps per second: 0.55150\n",
      "step: 31429\n",
      "loss: 12.87489128112793\n",
      "steps per second: 0.53522\n",
      "step: 31430\n",
      "loss: 12.90253734588623\n",
      "steps per second: 0.56247\n",
      "step: 31431\n",
      "loss: 12.50300407409668\n",
      "steps per second: 0.57094\n",
      "step: 31432\n",
      "loss: 12.96286392211914\n",
      "steps per second: 0.49671\n",
      "step: 31433\n",
      "loss: 13.342214584350586\n",
      "steps per second: 0.54520\n",
      "step: 31434\n",
      "loss: 12.78674602508545\n",
      "steps per second: 0.57890\n",
      "step: 31435\n",
      "loss: 12.624969482421875\n",
      "steps per second: 0.54832\n",
      "step: 31436\n",
      "loss: 12.187379837036133\n",
      "steps per second: 0.49942\n",
      "step: 31437\n",
      "loss: 13.400741577148438\n",
      "steps per second: 0.51370\n",
      "step: 31438\n",
      "loss: 12.374401092529297\n",
      "steps per second: 0.52316\n",
      "step: 31439\n",
      "loss: 12.966588973999023\n",
      "steps per second: 0.53204\n",
      "step: 31440\n",
      "loss: 12.118693351745605\n",
      "steps per second: 0.54393\n",
      "step: 31441\n",
      "loss: 12.592159271240234\n",
      "steps per second: 0.51662\n",
      "step: 31442\n",
      "loss: 12.639413833618164\n",
      "steps per second: 0.51222\n",
      "step: 31443\n",
      "loss: 12.766791343688965\n",
      "steps per second: 0.56299\n",
      "step: 31444\n",
      "loss: 12.7361478805542\n",
      "steps per second: 0.50767\n",
      "step: 31445\n",
      "loss: 12.76419448852539\n",
      "steps per second: 0.50062\n",
      "step: 31446\n",
      "loss: 12.74822998046875\n",
      "steps per second: 0.52087\n",
      "step: 31447\n",
      "loss: 12.824629783630371\n",
      "steps per second: 0.52486\n",
      "step: 31448\n",
      "loss: 13.408660888671875\n",
      "steps per second: 0.51288\n",
      "step: 31449\n",
      "loss: 12.638030052185059\n",
      "steps per second: 0.57387\n",
      "step: 31450\n",
      "loss: 12.5632963180542\n",
      "steps per second: 0.56257\n",
      "step: 31451\n",
      "loss: 13.382745742797852\n",
      "steps per second: 0.56570\n",
      "step: 31452\n",
      "loss: 12.615952491760254\n",
      "steps per second: 0.53201\n",
      "step: 31453\n",
      "loss: 12.641905784606934\n",
      "steps per second: 0.55729\n",
      "step: 31454\n",
      "loss: 12.730951309204102\n",
      "steps per second: 0.57420\n",
      "step: 31455\n",
      "loss: 12.19514274597168\n",
      "steps per second: 0.55660\n",
      "step: 31456\n",
      "loss: 12.661123275756836\n",
      "steps per second: 0.58049\n",
      "step: 31457\n",
      "loss: 13.123326301574707\n",
      "steps per second: 0.50160\n",
      "step: 31458\n",
      "loss: 12.856144905090332\n",
      "steps per second: 0.56240\n",
      "step: 31459\n",
      "loss: 12.850516319274902\n",
      "steps per second: 0.59007\n",
      "step: 31460\n",
      "loss: 12.865865707397461\n",
      "steps per second: 0.48681\n",
      "step: 31461\n",
      "loss: 12.89643383026123\n",
      "steps per second: 0.49568\n",
      "step: 31462\n",
      "loss: 12.936310768127441\n",
      "steps per second: 0.51954\n",
      "step: 31463\n",
      "loss: 12.977470397949219\n",
      "steps per second: 0.54910\n",
      "step: 31464\n",
      "loss: 12.818404197692871\n",
      "steps per second: 0.57003\n",
      "step: 31465\n",
      "loss: 12.670938491821289\n",
      "steps per second: 0.59327\n",
      "step: 31466\n",
      "loss: 12.741411209106445\n",
      "steps per second: 0.56518\n",
      "step: 31467\n",
      "loss: 12.810731887817383\n",
      "steps per second: 0.55374\n",
      "step: 31468\n",
      "loss: 12.77049732208252\n",
      "steps per second: 0.56349\n",
      "step: 31469\n",
      "loss: 12.425885200500488\n",
      "steps per second: 0.57584\n",
      "step: 31470\n",
      "loss: 13.319405555725098\n",
      "steps per second: 0.53759\n",
      "step: 31471\n",
      "loss: 13.23837661743164\n",
      "steps per second: 0.58903\n",
      "step: 31472\n",
      "loss: 13.058349609375\n",
      "steps per second: 0.56622\n",
      "step: 31473\n",
      "loss: 12.70695972442627\n",
      "steps per second: 0.56089\n",
      "step: 31474\n",
      "loss: 12.817743301391602\n",
      "steps per second: 0.55466\n",
      "step: 31475\n",
      "loss: 12.422113418579102\n",
      "steps per second: 0.56822\n",
      "step: 31476\n",
      "loss: 12.74685287475586\n",
      "steps per second: 0.54291\n",
      "step: 31477\n",
      "loss: 12.287775039672852\n",
      "steps per second: 0.58325\n",
      "step: 31478\n",
      "loss: 12.715548515319824\n",
      "steps per second: 0.55692\n",
      "step: 31479\n",
      "loss: 12.534513473510742\n",
      "steps per second: 0.55706\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7903504967689514, layer: 11\n",
      "saving at step 31479\n",
      "----------\n",
      "\n",
      "\n",
      "step: 31480\n",
      "loss: 12.196404457092285\n",
      "steps per second: 0.26881\n",
      "step: 31481\n",
      "loss: 12.425799369812012\n",
      "steps per second: 0.56939\n",
      "step: 31482\n",
      "loss: 12.822093963623047\n",
      "steps per second: 0.53862\n",
      "step: 31483\n",
      "loss: 13.216917037963867\n",
      "steps per second: 0.54182\n",
      "step: 31484\n",
      "loss: 12.86849594116211\n",
      "steps per second: 0.54430\n",
      "step: 31485\n",
      "loss: 13.550653457641602\n",
      "steps per second: 0.57419\n",
      "step: 31486\n",
      "loss: 12.622090339660645\n",
      "steps per second: 0.55461\n",
      "step: 31487\n",
      "loss: 12.883048057556152\n",
      "steps per second: 0.58952\n",
      "step: 31488\n",
      "loss: 12.444131851196289\n",
      "steps per second: 0.58677\n",
      "step: 31489\n",
      "loss: 12.631831169128418\n",
      "steps per second: 0.56256\n",
      "step: 31490\n",
      "loss: 12.362218856811523\n",
      "steps per second: 0.55842\n",
      "step: 31491\n",
      "loss: 12.649826049804688\n",
      "steps per second: 0.54194\n",
      "step: 31492\n",
      "loss: 13.257026672363281\n",
      "steps per second: 0.51796\n",
      "step: 31493\n",
      "loss: 12.992085456848145\n",
      "steps per second: 0.54462\n",
      "step: 31494\n",
      "loss: 12.606619834899902\n",
      "steps per second: 0.59552\n",
      "step: 31495\n",
      "loss: 12.707101821899414\n",
      "steps per second: 0.56133\n",
      "step: 31496\n",
      "loss: 13.140408515930176\n",
      "steps per second: 0.55206\n",
      "step: 31497\n",
      "loss: 13.492748260498047\n",
      "steps per second: 0.57475\n",
      "step: 31498\n",
      "loss: 12.455729484558105\n",
      "steps per second: 0.55982\n",
      "step: 31499\n",
      "loss: 12.835129737854004\n",
      "steps per second: 0.55381\n",
      "step: 31500\n",
      "loss: 12.347918510437012\n",
      "steps per second: 0.55119\n",
      "step: 31501\n",
      "loss: 12.777976989746094\n",
      "steps per second: 0.55263\n",
      "step: 31502\n",
      "loss: 12.63550090789795\n",
      "steps per second: 0.51992\n",
      "step: 31503\n",
      "loss: 13.139009475708008\n",
      "steps per second: 0.60952\n",
      "step: 31504\n",
      "loss: 12.368249893188477\n",
      "steps per second: 0.54788\n",
      "step: 31505\n",
      "loss: 12.612944602966309\n",
      "steps per second: 0.53396\n",
      "step: 31506\n",
      "loss: 12.489051818847656\n",
      "steps per second: 0.55817\n",
      "step: 31507\n",
      "loss: 13.117653846740723\n",
      "steps per second: 0.53855\n",
      "step: 31508\n",
      "loss: 12.667342185974121\n",
      "steps per second: 0.56310\n",
      "step: 31509\n",
      "loss: 12.771965026855469\n",
      "steps per second: 0.54410\n",
      "step: 31510\n",
      "loss: 13.109726905822754\n",
      "steps per second: 0.57163\n",
      "step: 31511\n",
      "loss: 12.701807975769043\n",
      "steps per second: 0.55758\n",
      "step: 31512\n",
      "loss: 12.765682220458984\n",
      "steps per second: 0.55089\n",
      "step: 31513\n",
      "loss: 12.283952713012695\n",
      "steps per second: 0.53651\n",
      "step: 31514\n",
      "loss: 12.221879959106445\n",
      "steps per second: 0.50916\n",
      "step: 31515\n",
      "loss: 12.99868392944336\n",
      "steps per second: 0.51175\n",
      "step: 31516\n",
      "loss: 12.54807186126709\n",
      "steps per second: 0.54183\n",
      "step: 31517\n",
      "loss: 12.526398658752441\n",
      "steps per second: 0.53200\n",
      "step: 31518\n",
      "loss: 12.56130599975586\n",
      "steps per second: 0.54955\n",
      "step: 31519\n",
      "loss: 13.377863883972168\n",
      "steps per second: 0.52952\n",
      "step: 31520\n",
      "loss: 12.641019821166992\n",
      "steps per second: 0.53165\n",
      "step: 31521\n",
      "loss: 12.235350608825684\n",
      "steps per second: 0.53328\n",
      "step: 31522\n",
      "loss: 12.745466232299805\n",
      "steps per second: 0.52815\n",
      "step: 31523\n",
      "loss: 13.04463005065918\n",
      "steps per second: 0.53012\n",
      "step: 31524\n",
      "loss: 13.419218063354492\n",
      "steps per second: 0.51329\n",
      "step: 31525\n",
      "loss: 12.455038070678711\n",
      "steps per second: 0.60816\n",
      "step: 31526\n",
      "loss: 12.817309379577637\n",
      "steps per second: 0.55951\n",
      "step: 31527\n",
      "loss: 12.979393005371094\n",
      "steps per second: 0.53637\n",
      "step: 31528\n",
      "loss: 12.566728591918945\n",
      "steps per second: 0.55591\n",
      "step: 31529\n",
      "loss: 12.883048057556152\n",
      "steps per second: 0.56954\n",
      "step: 31530\n",
      "loss: 12.319578170776367\n",
      "steps per second: 0.53547\n",
      "step: 31531\n",
      "loss: 13.33423137664795\n",
      "steps per second: 0.54241\n",
      "step: 31532\n",
      "loss: 12.929282188415527\n",
      "steps per second: 0.56716\n",
      "step: 31533\n",
      "loss: 12.893877029418945\n",
      "steps per second: 0.54674\n",
      "step: 31534\n",
      "loss: 12.909324645996094\n",
      "steps per second: 0.57497\n",
      "step: 31535\n",
      "loss: 12.669407844543457\n",
      "steps per second: 0.56868\n",
      "step: 31536\n",
      "loss: 13.198831558227539\n",
      "steps per second: 0.53986\n",
      "step: 31537\n",
      "loss: 13.124157905578613\n",
      "steps per second: 0.53889\n",
      "step: 31538\n",
      "loss: 13.230156898498535\n",
      "steps per second: 0.58387\n",
      "step: 31539\n",
      "loss: 12.993654251098633\n",
      "steps per second: 0.55673\n",
      "step: 31540\n",
      "loss: 12.846587181091309\n",
      "steps per second: 0.53360\n",
      "step: 31541\n",
      "loss: 13.104426383972168\n",
      "steps per second: 0.58176\n",
      "step: 31542\n",
      "loss: 12.862195014953613\n",
      "steps per second: 0.58762\n",
      "step: 31543\n",
      "loss: 13.120723724365234\n",
      "steps per second: 0.53263\n",
      "step: 31544\n",
      "loss: 12.434950828552246\n",
      "steps per second: 0.56394\n",
      "step: 31545\n",
      "loss: 12.913961410522461\n",
      "steps per second: 0.55742\n",
      "step: 31546\n",
      "loss: 12.895577430725098\n",
      "steps per second: 0.55525\n",
      "step: 31547\n",
      "loss: 12.815077781677246\n",
      "steps per second: 0.55835\n",
      "step: 31548\n",
      "loss: 12.75223159790039\n",
      "steps per second: 0.56510\n",
      "step: 31549\n",
      "loss: 12.793445587158203\n",
      "steps per second: 0.53582\n",
      "step: 31550\n",
      "loss: 13.04130744934082\n",
      "steps per second: 0.56301\n",
      "step: 31551\n",
      "loss: 12.585285186767578\n",
      "steps per second: 0.58245\n",
      "step: 31552\n",
      "loss: 13.325141906738281\n",
      "steps per second: 0.53676\n",
      "step: 31553\n",
      "loss: 12.538206100463867\n",
      "steps per second: 0.51696\n",
      "step: 31554\n",
      "loss: 12.438573837280273\n",
      "steps per second: 0.58658\n",
      "step: 31555\n",
      "loss: 12.903139114379883\n",
      "steps per second: 0.56821\n",
      "step: 31556\n",
      "loss: 13.074475288391113\n",
      "steps per second: 0.56707\n",
      "step: 31557\n",
      "loss: 12.927048683166504\n",
      "steps per second: 0.56891\n",
      "step: 31558\n",
      "loss: 13.152912139892578\n",
      "steps per second: 0.54897\n",
      "step: 31559\n",
      "loss: 12.761938095092773\n",
      "steps per second: 0.55471\n",
      "step: 31560\n",
      "loss: 12.456574440002441\n",
      "steps per second: 0.59141\n",
      "step: 31561\n",
      "loss: 13.160964965820312\n",
      "steps per second: 0.62273\n",
      "step: 31562\n",
      "loss: 12.985888481140137\n",
      "steps per second: 0.55959\n",
      "step: 31563\n",
      "loss: 13.230144500732422\n",
      "steps per second: 0.54485\n",
      "step: 31564\n",
      "loss: 12.435685157775879\n",
      "steps per second: 0.57931\n",
      "step: 31565\n",
      "loss: 12.875715255737305\n",
      "steps per second: 0.56734\n",
      "step: 31566\n",
      "loss: 12.89388370513916\n",
      "steps per second: 0.56483\n",
      "step: 31567\n",
      "loss: 13.222870826721191\n",
      "steps per second: 0.55818\n",
      "step: 31568\n",
      "loss: 13.205170631408691\n",
      "steps per second: 0.53898\n",
      "step: 31569\n",
      "loss: 12.611968040466309\n",
      "steps per second: 0.54908\n",
      "step: 31570\n",
      "loss: 12.746208190917969\n",
      "steps per second: 0.58939\n",
      "step: 31571\n",
      "loss: 12.490947723388672\n",
      "steps per second: 0.52714\n",
      "step: 31572\n",
      "loss: 12.727874755859375\n",
      "steps per second: 0.55053\n",
      "step: 31573\n",
      "loss: 12.43224811553955\n",
      "steps per second: 0.54352\n",
      "step: 31574\n",
      "loss: 13.347477912902832\n",
      "steps per second: 0.53593\n",
      "step: 31575\n",
      "loss: 12.503552436828613\n",
      "steps per second: 0.53186\n",
      "step: 31576\n",
      "loss: 12.411667823791504\n",
      "steps per second: 0.50188\n",
      "step: 31577\n",
      "loss: 12.54842472076416\n",
      "steps per second: 0.58067\n",
      "step: 31578\n",
      "loss: 12.665308952331543\n",
      "steps per second: 0.58306\n",
      "step: 31579\n",
      "loss: 12.723649978637695\n",
      "steps per second: 0.58247\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8091410994529724, layer: 11\n",
      "saving at step 31579\n",
      "----------\n",
      "\n",
      "\n",
      "step: 31580\n",
      "loss: 12.403177261352539\n",
      "steps per second: 0.29143\n",
      "step: 31581\n",
      "loss: 12.82620906829834\n",
      "steps per second: 0.54504\n",
      "step: 31582\n",
      "loss: 12.492579460144043\n",
      "steps per second: 0.56748\n",
      "step: 31583\n",
      "loss: 12.85467529296875\n",
      "steps per second: 0.57720\n",
      "step: 31584\n",
      "loss: 12.506114959716797\n",
      "steps per second: 0.56533\n",
      "step: 31585\n",
      "loss: 12.803645133972168\n",
      "steps per second: 0.51255\n",
      "step: 31586\n",
      "loss: 13.024449348449707\n",
      "steps per second: 0.55187\n",
      "step: 31587\n",
      "loss: 13.045639991760254\n",
      "steps per second: 0.55998\n",
      "step: 31588\n",
      "loss: 12.918010711669922\n",
      "steps per second: 0.54805\n",
      "step: 31589\n",
      "loss: 12.899754524230957\n",
      "steps per second: 0.50708\n",
      "step: 31590\n",
      "loss: 12.91331958770752\n",
      "steps per second: 0.53633\n",
      "step: 31591\n",
      "loss: 12.350199699401855\n",
      "steps per second: 0.55523\n",
      "step: 31592\n",
      "loss: 12.725075721740723\n",
      "steps per second: 0.58067\n",
      "step: 31593\n",
      "loss: 13.427477836608887\n",
      "steps per second: 0.58053\n",
      "step: 31594\n",
      "loss: 13.094649314880371\n",
      "steps per second: 0.54432\n",
      "step: 31595\n",
      "loss: 12.929046630859375\n",
      "steps per second: 0.49928\n",
      "step: 31596\n",
      "loss: 13.354959487915039\n",
      "steps per second: 0.57802\n",
      "step: 31597\n",
      "loss: 12.835336685180664\n",
      "steps per second: 0.55356\n",
      "step: 31598\n",
      "loss: 12.824605941772461\n",
      "steps per second: 0.53365\n",
      "step: 31599\n",
      "loss: 12.691317558288574\n",
      "steps per second: 0.56750\n",
      "step: 31600\n",
      "loss: 12.863327026367188\n",
      "steps per second: 0.54476\n",
      "step: 31601\n",
      "loss: 13.276224136352539\n",
      "steps per second: 0.60885\n",
      "step: 31602\n",
      "loss: 12.731504440307617\n",
      "steps per second: 0.49989\n",
      "step: 31603\n",
      "loss: 12.473997116088867\n",
      "steps per second: 0.55604\n",
      "step: 31604\n",
      "loss: 13.223828315734863\n",
      "steps per second: 0.54328\n",
      "step: 31605\n",
      "loss: 12.791632652282715\n",
      "steps per second: 0.54576\n",
      "step: 31606\n",
      "loss: 12.394789695739746\n",
      "steps per second: 0.58498\n",
      "step: 31607\n",
      "loss: 13.141166687011719\n",
      "steps per second: 0.56699\n",
      "step: 31608\n",
      "loss: 12.661651611328125\n",
      "steps per second: 0.56190\n",
      "step: 31609\n",
      "loss: 12.521193504333496\n",
      "steps per second: 0.54253\n",
      "step: 31610\n",
      "loss: 13.246376037597656\n",
      "steps per second: 0.55712\n",
      "step: 31611\n",
      "loss: 12.91028118133545\n",
      "steps per second: 0.56322\n",
      "step: 31612\n",
      "loss: 12.233585357666016\n",
      "steps per second: 0.52891\n",
      "step: 31613\n",
      "loss: 12.655142784118652\n",
      "steps per second: 0.55343\n",
      "step: 31614\n",
      "loss: 12.508399963378906\n",
      "steps per second: 0.61681\n",
      "step: 31615\n",
      "loss: 12.65712833404541\n",
      "steps per second: 0.52521\n",
      "step: 31616\n",
      "loss: 12.34945011138916\n",
      "steps per second: 0.58504\n",
      "step: 31617\n",
      "loss: 12.684723854064941\n",
      "steps per second: 0.55362\n",
      "step: 31618\n",
      "loss: 12.990118026733398\n",
      "steps per second: 0.53327\n",
      "step: 31619\n",
      "loss: 12.706719398498535\n",
      "steps per second: 0.58320\n",
      "step: 31620\n",
      "loss: 12.482572555541992\n",
      "steps per second: 0.58145\n",
      "step: 31621\n",
      "loss: 12.821925163269043\n",
      "steps per second: 0.58899\n",
      "step: 31622\n",
      "loss: 13.083739280700684\n",
      "steps per second: 0.53232\n",
      "step: 31623\n",
      "loss: 12.848102569580078\n",
      "steps per second: 0.54133\n",
      "step: 31624\n",
      "loss: 12.878414154052734\n",
      "steps per second: 0.55036\n",
      "step: 31625\n",
      "loss: 12.794343948364258\n",
      "steps per second: 0.58052\n",
      "step: 31626\n",
      "loss: 12.554550170898438\n",
      "steps per second: 0.56091\n",
      "step: 31627\n",
      "loss: 13.013877868652344\n",
      "steps per second: 0.55762\n",
      "step: 31628\n",
      "loss: 12.837308883666992\n",
      "steps per second: 0.56845\n",
      "step: 31629\n",
      "loss: 12.753029823303223\n",
      "steps per second: 0.56076\n",
      "step: 31630\n",
      "loss: 12.774359703063965\n",
      "steps per second: 0.58296\n",
      "step: 31631\n",
      "loss: 12.455031394958496\n",
      "steps per second: 0.57996\n",
      "step: 31632\n",
      "loss: 12.749860763549805\n",
      "steps per second: 0.57872\n",
      "step: 31633\n",
      "loss: 12.870794296264648\n",
      "steps per second: 0.57751\n",
      "step: 31634\n",
      "loss: 12.542694091796875\n",
      "steps per second: 0.56324\n",
      "step: 31635\n",
      "loss: 12.867350578308105\n",
      "steps per second: 0.58022\n",
      "step: 31636\n",
      "loss: 12.242539405822754\n",
      "steps per second: 0.62243\n",
      "step: 31637\n",
      "loss: 12.610548973083496\n",
      "steps per second: 0.56420\n",
      "step: 31638\n",
      "loss: 12.834070205688477\n",
      "steps per second: 0.54277\n",
      "step: 31639\n",
      "loss: 13.103864669799805\n",
      "steps per second: 0.53270\n",
      "step: 31640\n",
      "loss: 12.639835357666016\n",
      "steps per second: 0.56111\n",
      "step: 31641\n",
      "loss: 12.91635513305664\n",
      "steps per second: 0.54782\n",
      "step: 31642\n",
      "loss: 12.636543273925781\n",
      "steps per second: 0.57466\n",
      "step: 31643\n",
      "loss: 12.608553886413574\n",
      "steps per second: 0.54942\n",
      "step: 31644\n",
      "loss: 12.99412727355957\n",
      "steps per second: 0.53631\n",
      "step: 31645\n",
      "loss: 13.174257278442383\n",
      "steps per second: 0.62306\n",
      "step: 31646\n",
      "loss: 12.880224227905273\n",
      "steps per second: 0.51387\n",
      "step: 31647\n",
      "loss: 12.529422760009766\n",
      "steps per second: 0.56499\n",
      "step: 31648\n",
      "loss: 13.142870903015137\n",
      "steps per second: 0.54558\n",
      "step: 31649\n",
      "loss: 13.216015815734863\n",
      "steps per second: 0.55303\n",
      "step: 31650\n",
      "loss: 12.540802955627441\n",
      "steps per second: 0.56805\n",
      "step: 31651\n",
      "loss: 12.46955394744873\n",
      "steps per second: 0.59671\n",
      "step: 31652\n",
      "loss: 12.145292282104492\n",
      "steps per second: 0.55407\n",
      "step: 31653\n",
      "loss: 12.91313362121582\n",
      "steps per second: 0.54955\n",
      "step: 31654\n",
      "loss: 12.244792938232422\n",
      "steps per second: 0.53678\n",
      "step: 31655\n",
      "loss: 12.69454574584961\n",
      "steps per second: 0.55148\n",
      "step: 31656\n",
      "loss: 13.286016464233398\n",
      "steps per second: 0.55776\n",
      "step: 31657\n",
      "loss: 12.7728271484375\n",
      "steps per second: 0.56414\n",
      "step: 31658\n",
      "loss: 12.903613090515137\n",
      "steps per second: 0.52929\n",
      "step: 31659\n",
      "loss: 12.653664588928223\n",
      "steps per second: 0.58171\n",
      "step: 31660\n",
      "loss: 12.134723663330078\n",
      "steps per second: 0.54766\n",
      "step: 31661\n",
      "loss: 12.809739112854004\n",
      "steps per second: 0.51816\n",
      "step: 31662\n",
      "loss: 12.944853782653809\n",
      "steps per second: 0.55514\n",
      "step: 31663\n",
      "loss: 13.069219589233398\n",
      "steps per second: 0.51799\n",
      "step: 31664\n",
      "loss: 12.569089889526367\n",
      "steps per second: 0.50155\n",
      "step: 31665\n",
      "loss: 12.5309419631958\n",
      "steps per second: 0.56895\n",
      "step: 31666\n",
      "loss: 12.632830619812012\n",
      "steps per second: 0.52282\n",
      "step: 31667\n",
      "loss: 12.746223449707031\n",
      "steps per second: 0.53209\n",
      "step: 31668\n",
      "loss: 12.547017097473145\n",
      "steps per second: 0.55996\n",
      "step: 31669\n",
      "loss: 12.7144136428833\n",
      "steps per second: 0.53538\n",
      "step: 31670\n",
      "loss: 13.288435935974121\n",
      "steps per second: 0.57048\n",
      "step: 31671\n",
      "loss: 12.236802101135254\n",
      "steps per second: 0.61088\n",
      "step: 31672\n",
      "loss: 12.689298629760742\n",
      "steps per second: 0.54701\n",
      "step: 31673\n",
      "loss: 12.867291450500488\n",
      "steps per second: 0.52921\n",
      "step: 31674\n",
      "loss: 12.483793258666992\n",
      "steps per second: 0.54066\n",
      "step: 31675\n",
      "loss: 12.826857566833496\n",
      "steps per second: 0.51333\n",
      "step: 31676\n",
      "loss: 13.47279167175293\n",
      "steps per second: 0.56323\n",
      "step: 31677\n",
      "loss: 12.782342910766602\n",
      "steps per second: 0.50641\n",
      "step: 31678\n",
      "loss: 12.497903823852539\n",
      "steps per second: 0.54269\n",
      "step: 31679\n",
      "loss: 13.749996185302734\n",
      "steps per second: 0.52900\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.933404266834259, layer: 11\n",
      "saving at step 31679\n",
      "----------\n",
      "\n",
      "\n",
      "step: 31680\n",
      "loss: 12.237873077392578\n",
      "steps per second: 0.27399\n",
      "step: 31681\n",
      "loss: 12.76382064819336\n",
      "steps per second: 0.52931\n",
      "step: 31682\n",
      "loss: 12.786036491394043\n",
      "steps per second: 0.54475\n",
      "step: 31683\n",
      "loss: 12.379143714904785\n",
      "steps per second: 0.51296\n",
      "step: 31684\n",
      "loss: 12.908763885498047\n",
      "steps per second: 0.57233\n",
      "step: 31685\n",
      "loss: 12.50409984588623\n",
      "steps per second: 0.60850\n",
      "step: 31686\n",
      "loss: 13.009929656982422\n",
      "steps per second: 0.56846\n",
      "step: 31687\n",
      "loss: 12.546401023864746\n",
      "steps per second: 0.54049\n",
      "step: 31688\n",
      "loss: 12.555695533752441\n",
      "steps per second: 0.50111\n",
      "step: 31689\n",
      "loss: 12.483851432800293\n",
      "steps per second: 0.61222\n",
      "step: 31690\n",
      "loss: 12.63000774383545\n",
      "steps per second: 0.51426\n",
      "step: 31691\n",
      "loss: 13.3475923538208\n",
      "steps per second: 0.55876\n",
      "step: 31692\n",
      "loss: 12.606047630310059\n",
      "steps per second: 0.55998\n",
      "step: 31693\n",
      "loss: 12.848776817321777\n",
      "steps per second: 0.55939\n",
      "step: 31694\n",
      "loss: 12.445137023925781\n",
      "steps per second: 0.58359\n",
      "step: 31695\n",
      "loss: 13.1695556640625\n",
      "steps per second: 0.55776\n",
      "step: 31696\n",
      "loss: 13.009269714355469\n",
      "steps per second: 0.60956\n",
      "step: 31697\n",
      "loss: 13.016280174255371\n",
      "steps per second: 0.54197\n",
      "step: 31698\n",
      "loss: 12.231496810913086\n",
      "steps per second: 0.55679\n",
      "step: 31699\n",
      "loss: 13.014634132385254\n",
      "steps per second: 0.55209\n",
      "step: 31700\n",
      "loss: 12.783208847045898\n",
      "steps per second: 0.57918\n",
      "step: 31701\n",
      "loss: 13.140774726867676\n",
      "steps per second: 0.52596\n",
      "step: 31702\n",
      "loss: 12.815591812133789\n",
      "steps per second: 0.57913\n",
      "step: 31703\n",
      "loss: 12.54293441772461\n",
      "steps per second: 0.57770\n",
      "step: 31704\n",
      "loss: 12.942946434020996\n",
      "steps per second: 0.56471\n",
      "step: 31705\n",
      "loss: 13.203155517578125\n",
      "steps per second: 0.53833\n",
      "step: 31706\n",
      "loss: 12.428661346435547\n",
      "steps per second: 0.54253\n",
      "step: 31707\n",
      "loss: 12.976799964904785\n",
      "steps per second: 0.51657\n",
      "step: 31708\n",
      "loss: 13.35528564453125\n",
      "steps per second: 0.53034\n",
      "step: 31709\n",
      "loss: 13.146286964416504\n",
      "steps per second: 0.55884\n",
      "step: 31710\n",
      "loss: 12.73377799987793\n",
      "steps per second: 0.56335\n",
      "step: 31711\n",
      "loss: 13.083884239196777\n",
      "steps per second: 0.53228\n",
      "step: 31712\n",
      "loss: 12.216655731201172\n",
      "steps per second: 0.57764\n",
      "step: 31713\n",
      "loss: 12.630314826965332\n",
      "steps per second: 0.57526\n",
      "step: 31714\n",
      "loss: 12.840692520141602\n",
      "steps per second: 0.57430\n",
      "step: 31715\n",
      "loss: 12.702995300292969\n",
      "steps per second: 0.62151\n",
      "step: 31716\n",
      "loss: 12.838082313537598\n",
      "steps per second: 0.51403\n",
      "step: 31717\n",
      "loss: 12.648247718811035\n",
      "steps per second: 0.56851\n",
      "step: 31718\n",
      "loss: 12.88286304473877\n",
      "steps per second: 0.57244\n",
      "step: 31719\n",
      "loss: 13.180268287658691\n",
      "steps per second: 0.56118\n",
      "step: 31720\n",
      "loss: 13.168058395385742\n",
      "steps per second: 0.54702\n",
      "step: 31721\n",
      "loss: 13.131441116333008\n",
      "steps per second: 0.55476\n",
      "step: 31722\n",
      "loss: 12.932247161865234\n",
      "steps per second: 0.56216\n",
      "step: 31723\n",
      "loss: 12.970930099487305\n",
      "steps per second: 0.56154\n",
      "step: 31724\n",
      "loss: 12.779458999633789\n",
      "steps per second: 0.54050\n",
      "step: 31725\n",
      "loss: 13.161005020141602\n",
      "steps per second: 0.53911\n",
      "step: 31726\n",
      "loss: 13.073468208312988\n",
      "steps per second: 0.55773\n",
      "step: 31727\n",
      "loss: 12.737399101257324\n",
      "steps per second: 0.52598\n",
      "step: 31728\n",
      "loss: 12.235569953918457\n",
      "steps per second: 0.54524\n",
      "step: 31729\n",
      "loss: 12.54082202911377\n",
      "steps per second: 0.55211\n",
      "step: 31730\n",
      "loss: 13.486992835998535\n",
      "steps per second: 0.56252\n",
      "step: 31731\n",
      "loss: 12.819720268249512\n",
      "steps per second: 0.56250\n",
      "step: 31732\n",
      "loss: 13.040411949157715\n",
      "steps per second: 0.53337\n",
      "step: 31733\n",
      "loss: 12.891042709350586\n",
      "steps per second: 0.55533\n",
      "step: 31734\n",
      "loss: 13.059457778930664\n",
      "steps per second: 0.55216\n",
      "step: 31735\n",
      "loss: 13.050765037536621\n",
      "steps per second: 0.55284\n",
      "step: 31736\n",
      "loss: 13.090084075927734\n",
      "steps per second: 0.51109\n",
      "step: 31737\n",
      "loss: 13.052204132080078\n",
      "steps per second: 0.56718\n",
      "step: 31738\n",
      "loss: 13.15059757232666\n",
      "steps per second: 0.54398\n",
      "step: 31739\n",
      "loss: 13.215789794921875\n",
      "steps per second: 0.58511\n",
      "step: 31740\n",
      "loss: 13.10549545288086\n",
      "steps per second: 0.53349\n",
      "step: 31741\n",
      "loss: 12.593321800231934\n",
      "steps per second: 0.57801\n",
      "step: 31742\n",
      "loss: 12.622326850891113\n",
      "steps per second: 0.55093\n",
      "step: 31743\n",
      "loss: 12.151802062988281\n",
      "steps per second: 0.57895\n",
      "step: 31744\n",
      "loss: 13.322477340698242\n",
      "steps per second: 0.56688\n",
      "step: 31745\n",
      "loss: 12.801962852478027\n",
      "steps per second: 0.55605\n",
      "step: 31746\n",
      "loss: 12.719865798950195\n",
      "steps per second: 0.61087\n",
      "step: 31747\n",
      "loss: 13.0338773727417\n",
      "steps per second: 0.57479\n",
      "step: 31748\n",
      "loss: 12.418614387512207\n",
      "steps per second: 0.55409\n",
      "step: 31749\n",
      "loss: 12.858729362487793\n",
      "steps per second: 0.57136\n",
      "step: 31750\n",
      "loss: 12.776310920715332\n",
      "steps per second: 0.56590\n",
      "step: 31751\n",
      "loss: 12.208535194396973\n",
      "steps per second: 0.53009\n",
      "step: 31752\n",
      "loss: 12.702533721923828\n",
      "steps per second: 0.55423\n",
      "step: 31753\n",
      "loss: 12.687969207763672\n",
      "steps per second: 0.50427\n",
      "step: 31754\n",
      "loss: 12.629231452941895\n",
      "steps per second: 0.55922\n",
      "step: 31755\n",
      "loss: 13.036750793457031\n",
      "steps per second: 0.53231\n",
      "step: 31756\n",
      "loss: 12.395100593566895\n",
      "steps per second: 0.52971\n",
      "step: 31757\n",
      "loss: 12.599800109863281\n",
      "steps per second: 0.55042\n",
      "step: 31758\n",
      "loss: 12.88553524017334\n",
      "steps per second: 0.54663\n",
      "step: 31759\n",
      "loss: 12.848833084106445\n",
      "steps per second: 0.54168\n",
      "step: 31760\n",
      "loss: 12.221433639526367\n",
      "steps per second: 0.57919\n",
      "step: 31761\n",
      "loss: 12.491056442260742\n",
      "steps per second: 0.56414\n",
      "step: 31762\n",
      "loss: 12.940914154052734\n",
      "steps per second: 0.62133\n",
      "step: 31763\n",
      "loss: 12.857645988464355\n",
      "steps per second: 0.55366\n",
      "step: 31764\n",
      "loss: 12.950827598571777\n",
      "steps per second: 0.53519\n",
      "step: 31765\n",
      "loss: 12.581422805786133\n",
      "steps per second: 0.56357\n",
      "step: 31766\n",
      "loss: 12.440120697021484\n",
      "steps per second: 0.59828\n",
      "step: 31767\n",
      "loss: 12.832635879516602\n",
      "steps per second: 0.55437\n",
      "step: 31768\n",
      "loss: 12.45427131652832\n",
      "steps per second: 0.55713\n",
      "step: 31769\n",
      "loss: 13.160319328308105\n",
      "steps per second: 0.56546\n",
      "step: 31770\n",
      "loss: 12.99145793914795\n",
      "steps per second: 0.55499\n",
      "step: 31771\n",
      "loss: 12.64118766784668\n",
      "steps per second: 0.58921\n",
      "step: 31772\n",
      "loss: 12.473926544189453\n",
      "steps per second: 0.56292\n",
      "step: 31773\n",
      "loss: 12.5282621383667\n",
      "steps per second: 0.56203\n",
      "step: 31774\n",
      "loss: 12.657835006713867\n",
      "steps per second: 0.56121\n",
      "step: 31775\n",
      "loss: 13.077190399169922\n",
      "steps per second: 0.55126\n",
      "step: 31776\n",
      "loss: 12.597404479980469\n",
      "steps per second: 0.55975\n",
      "step: 31777\n",
      "loss: 12.892189979553223\n",
      "steps per second: 0.58225\n",
      "step: 31778\n",
      "loss: 12.589102745056152\n",
      "steps per second: 0.56283\n",
      "step: 31779\n",
      "loss: 12.299921989440918\n",
      "steps per second: 0.49413\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7655209302902222, layer: 10\n",
      "saving at step 31779\n",
      "----------\n",
      "\n",
      "\n",
      "step: 31780\n",
      "loss: 13.48983097076416\n",
      "steps per second: 0.26332\n",
      "step: 31781\n",
      "loss: 13.231230735778809\n",
      "steps per second: 0.62082\n",
      "step: 31782\n",
      "loss: 13.142327308654785\n",
      "steps per second: 0.56761\n",
      "step: 31783\n",
      "loss: 12.82616138458252\n",
      "steps per second: 0.55335\n",
      "step: 31784\n",
      "loss: 12.686676979064941\n",
      "steps per second: 0.56783\n",
      "step: 31785\n",
      "loss: 12.607131004333496\n",
      "steps per second: 0.58051\n",
      "step: 31786\n",
      "loss: 12.801430702209473\n",
      "steps per second: 0.56108\n",
      "step: 31787\n",
      "loss: 12.861211776733398\n",
      "steps per second: 0.53327\n",
      "step: 31788\n",
      "loss: 12.652746200561523\n",
      "steps per second: 0.53341\n",
      "step: 31789\n",
      "loss: 12.64172649383545\n",
      "steps per second: 0.55026\n",
      "step: 31790\n",
      "loss: 13.036964416503906\n",
      "steps per second: 0.62078\n",
      "step: 31791\n",
      "loss: 12.538359642028809\n",
      "steps per second: 0.55309\n",
      "step: 31792\n",
      "loss: 12.324947357177734\n",
      "steps per second: 0.56247\n",
      "step: 31793\n",
      "loss: 12.411741256713867\n",
      "steps per second: 0.58965\n",
      "step: 31794\n",
      "loss: 13.15320873260498\n",
      "steps per second: 0.53592\n",
      "step: 31795\n",
      "loss: 12.889472007751465\n",
      "steps per second: 0.57805\n",
      "step: 31796\n",
      "loss: 12.422247886657715\n",
      "steps per second: 0.55252\n",
      "step: 31797\n",
      "loss: 12.605639457702637\n",
      "steps per second: 0.56313\n",
      "step: 31798\n",
      "loss: 13.188827514648438\n",
      "steps per second: 0.58757\n",
      "step: 31799\n",
      "loss: 13.216565132141113\n",
      "steps per second: 0.61908\n",
      "step: 31800\n",
      "loss: 12.813485145568848\n",
      "steps per second: 0.57840\n",
      "step: 31801\n",
      "loss: 12.538895606994629\n",
      "steps per second: 0.55171\n",
      "step: 31802\n",
      "loss: 13.072407722473145\n",
      "steps per second: 0.56730\n",
      "step: 31803\n",
      "loss: 12.705931663513184\n",
      "steps per second: 0.56027\n",
      "step: 31804\n",
      "loss: 13.296588897705078\n",
      "steps per second: 0.56143\n",
      "step: 31805\n",
      "loss: 12.88251781463623\n",
      "steps per second: 0.56573\n",
      "step: 31806\n",
      "loss: 12.399560928344727\n",
      "steps per second: 0.58148\n",
      "step: 31807\n",
      "loss: 13.015860557556152\n",
      "steps per second: 0.55256\n",
      "step: 31808\n",
      "loss: 12.33955192565918\n",
      "steps per second: 0.62306\n",
      "step: 31809\n",
      "loss: 12.855324745178223\n",
      "steps per second: 0.56016\n",
      "step: 31810\n",
      "loss: 12.438356399536133\n",
      "steps per second: 0.56543\n",
      "step: 31811\n",
      "loss: 12.79819107055664\n",
      "steps per second: 0.56870\n",
      "step: 31812\n",
      "loss: 12.33747673034668\n",
      "steps per second: 0.56170\n",
      "step: 31813\n",
      "loss: 13.080301284790039\n",
      "steps per second: 0.56358\n",
      "step: 31814\n",
      "loss: 12.799043655395508\n",
      "steps per second: 0.58171\n",
      "step: 31815\n",
      "loss: 13.046164512634277\n",
      "steps per second: 0.54279\n",
      "step: 31816\n",
      "loss: 12.771233558654785\n",
      "steps per second: 0.62314\n",
      "step: 31817\n",
      "loss: 12.624777793884277\n",
      "steps per second: 0.56840\n",
      "step: 31818\n",
      "loss: 13.392487525939941\n",
      "steps per second: 0.54644\n",
      "step: 31819\n",
      "loss: 13.07436466217041\n",
      "steps per second: 0.55088\n",
      "step: 31820\n",
      "loss: 12.52221393585205\n",
      "steps per second: 0.59014\n",
      "step: 31821\n",
      "loss: 13.230219841003418\n",
      "steps per second: 0.55392\n",
      "step: 31822\n",
      "loss: 12.682611465454102\n",
      "steps per second: 0.61347\n",
      "step: 31823\n",
      "loss: 12.819099426269531\n",
      "steps per second: 0.54735\n",
      "step: 31824\n",
      "loss: 12.808428764343262\n",
      "steps per second: 0.61877\n",
      "step: 31825\n",
      "loss: 12.615135192871094\n",
      "steps per second: 0.53375\n",
      "step: 31826\n",
      "loss: 12.768380165100098\n",
      "steps per second: 0.57799\n",
      "step: 31827\n",
      "loss: 12.874764442443848\n",
      "steps per second: 0.45324\n",
      "step: 31828\n",
      "loss: 12.59140682220459\n",
      "steps per second: 0.56177\n",
      "step: 31829\n",
      "loss: 12.734359741210938\n",
      "steps per second: 0.48522\n",
      "step: 31830\n",
      "loss: 12.29106616973877\n",
      "steps per second: 0.55205\n",
      "step: 31831\n",
      "loss: 13.268310546875\n",
      "steps per second: 0.57032\n",
      "step: 31832\n",
      "loss: 12.504704475402832\n",
      "steps per second: 0.51554\n",
      "step: 31833\n",
      "loss: 13.208694458007812\n",
      "steps per second: 0.60981\n",
      "step: 31834\n",
      "loss: 12.346803665161133\n",
      "steps per second: 0.53859\n",
      "step: 31835\n",
      "loss: 13.378955841064453\n",
      "steps per second: 0.54170\n",
      "step: 31836\n",
      "loss: 12.917826652526855\n",
      "steps per second: 0.57660\n",
      "step: 31837\n",
      "loss: 13.69701862335205\n",
      "steps per second: 0.48822\n",
      "step: 31838\n",
      "loss: 12.608601570129395\n",
      "steps per second: 0.54920\n",
      "step: 31839\n",
      "loss: 13.000141143798828\n",
      "steps per second: 0.54376\n",
      "step: 31840\n",
      "loss: 13.213339805603027\n",
      "steps per second: 0.53422\n",
      "step: 31841\n",
      "loss: 12.381914138793945\n",
      "steps per second: 0.60393\n",
      "step: 31842\n",
      "loss: 12.779783248901367\n",
      "steps per second: 0.61357\n",
      "step: 31843\n",
      "loss: 12.482990264892578\n",
      "steps per second: 0.54848\n",
      "step: 31844\n",
      "loss: 12.261107444763184\n",
      "steps per second: 0.53605\n",
      "step: 31845\n",
      "loss: 12.880070686340332\n",
      "steps per second: 0.53656\n",
      "step: 31846\n",
      "loss: 12.730196952819824\n",
      "steps per second: 0.53765\n",
      "step: 31847\n",
      "loss: 12.392592430114746\n",
      "steps per second: 0.54628\n",
      "step: 31848\n",
      "loss: 12.555344581604004\n",
      "steps per second: 0.55049\n",
      "step: 31849\n",
      "loss: 12.75267505645752\n",
      "steps per second: 0.55338\n",
      "step: 31850\n",
      "loss: 13.10554313659668\n",
      "steps per second: 0.57710\n",
      "step: 31851\n",
      "loss: 13.212007522583008\n",
      "steps per second: 0.53470\n",
      "step: 31852\n",
      "loss: 12.428176879882812\n",
      "steps per second: 0.55724\n",
      "step: 31853\n",
      "loss: 13.112791061401367\n",
      "steps per second: 0.57701\n",
      "step: 31854\n",
      "loss: 12.686406135559082\n",
      "steps per second: 0.55348\n",
      "step: 31855\n",
      "loss: 13.077230453491211\n",
      "steps per second: 0.57793\n",
      "step: 31856\n",
      "loss: 12.645590782165527\n",
      "steps per second: 0.57587\n",
      "step: 31857\n",
      "loss: 13.052968978881836\n",
      "steps per second: 0.52138\n",
      "step: 31858\n",
      "loss: 13.12784481048584\n",
      "steps per second: 0.57399\n",
      "step: 31859\n",
      "loss: 12.820565223693848\n",
      "steps per second: 0.54538\n",
      "step: 31860\n",
      "loss: 13.025349617004395\n",
      "steps per second: 0.54965\n",
      "step: 31861\n",
      "loss: 13.492583274841309\n",
      "steps per second: 0.53645\n",
      "step: 31862\n",
      "loss: 12.141026496887207\n",
      "steps per second: 0.50087\n",
      "step: 31863\n",
      "loss: 12.728370666503906\n",
      "steps per second: 0.57628\n",
      "step: 31864\n",
      "loss: 12.839468955993652\n",
      "steps per second: 0.51988\n",
      "step: 31865\n",
      "loss: 12.787738800048828\n",
      "steps per second: 0.56999\n",
      "step: 31866\n",
      "loss: 12.299809455871582\n",
      "steps per second: 0.55546\n",
      "step: 31867\n",
      "loss: 12.719917297363281\n",
      "steps per second: 0.55544\n",
      "step: 31868\n",
      "loss: 12.582945823669434\n",
      "steps per second: 0.55347\n",
      "step: 31869\n",
      "loss: 13.207447052001953\n",
      "steps per second: 0.55674\n",
      "step: 31870\n",
      "loss: 12.667048454284668\n",
      "steps per second: 0.56854\n",
      "step: 31871\n",
      "loss: 12.725532531738281\n",
      "steps per second: 0.60853\n",
      "step: 31872\n",
      "loss: 12.645332336425781\n",
      "steps per second: 0.55123\n",
      "step: 31873\n",
      "loss: 12.826861381530762\n",
      "steps per second: 0.54990\n",
      "step: 31874\n",
      "loss: 12.789558410644531\n",
      "steps per second: 0.54272\n",
      "step: 31875\n",
      "loss: 12.633981704711914\n",
      "steps per second: 0.55046\n",
      "step: 31876\n",
      "loss: 12.76526927947998\n",
      "steps per second: 0.55553\n",
      "step: 31877\n",
      "loss: 12.242255210876465\n",
      "steps per second: 0.54376\n",
      "step: 31878\n",
      "loss: 12.614667892456055\n",
      "steps per second: 0.54390\n",
      "step: 31879\n",
      "loss: 12.727731704711914\n",
      "steps per second: 0.54073\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.801222026348114, layer: 11\n",
      "saving at step 31879\n",
      "----------\n",
      "\n",
      "\n",
      "step: 31880\n",
      "loss: 12.936779975891113\n",
      "steps per second: 0.28247\n",
      "step: 31881\n",
      "loss: 13.100318908691406\n",
      "steps per second: 0.56832\n",
      "step: 31882\n",
      "loss: 12.565918922424316\n",
      "steps per second: 0.51502\n",
      "step: 31883\n",
      "loss: 12.721664428710938\n",
      "steps per second: 0.54174\n",
      "step: 31884\n",
      "loss: 12.428095817565918\n",
      "steps per second: 0.55449\n",
      "step: 31885\n",
      "loss: 12.345114707946777\n",
      "steps per second: 0.49697\n",
      "step: 31886\n",
      "loss: 12.833292007446289\n",
      "steps per second: 0.54780\n",
      "step: 31887\n",
      "loss: 13.025351524353027\n",
      "steps per second: 0.57686\n",
      "step: 31888\n",
      "loss: 13.196924209594727\n",
      "steps per second: 0.55905\n",
      "step: 31889\n",
      "loss: 13.085485458374023\n",
      "steps per second: 0.52564\n",
      "step: 31890\n",
      "loss: 12.780110359191895\n",
      "steps per second: 0.48196\n",
      "step: 31891\n",
      "loss: 12.604620933532715\n",
      "steps per second: 0.55390\n",
      "step: 31892\n",
      "loss: 13.035425186157227\n",
      "steps per second: 0.52634\n",
      "step: 31893\n",
      "loss: 13.225359916687012\n",
      "steps per second: 0.57193\n",
      "step: 31894\n",
      "loss: 12.933805465698242\n",
      "steps per second: 0.57800\n",
      "step: 31895\n",
      "loss: 12.773839950561523\n",
      "steps per second: 0.55061\n",
      "step: 31896\n",
      "loss: 12.461963653564453\n",
      "steps per second: 0.54664\n",
      "step: 31897\n",
      "loss: 12.590588569641113\n",
      "steps per second: 0.54387\n",
      "step: 31898\n",
      "loss: 13.0276517868042\n",
      "steps per second: 0.57012\n",
      "step: 31899\n",
      "loss: 13.144842147827148\n",
      "steps per second: 0.52260\n",
      "step: 31900\n",
      "loss: 12.558993339538574\n",
      "steps per second: 0.54512\n",
      "step: 31901\n",
      "loss: 12.342226028442383\n",
      "steps per second: 0.56692\n",
      "step: 31902\n",
      "loss: 12.513590812683105\n",
      "steps per second: 0.55594\n",
      "step: 31903\n",
      "loss: 12.56102466583252\n",
      "steps per second: 0.55215\n",
      "step: 31904\n",
      "loss: 12.526784896850586\n",
      "steps per second: 0.56726\n",
      "step: 31905\n",
      "loss: 13.053328514099121\n",
      "steps per second: 0.54121\n",
      "step: 31906\n",
      "loss: 12.646977424621582\n",
      "steps per second: 0.54448\n",
      "step: 31907\n",
      "loss: 12.521086692810059\n",
      "steps per second: 0.49638\n",
      "step: 31908\n",
      "loss: 12.712560653686523\n",
      "steps per second: 0.51214\n",
      "step: 31909\n",
      "loss: 12.754050254821777\n",
      "steps per second: 0.55766\n",
      "step: 31910\n",
      "loss: 13.036232948303223\n",
      "steps per second: 0.55082\n",
      "step: 31911\n",
      "loss: 12.500288009643555\n",
      "steps per second: 0.56613\n",
      "step: 31912\n",
      "loss: 12.927456855773926\n",
      "steps per second: 0.54738\n",
      "step: 31913\n",
      "loss: 13.000550270080566\n",
      "steps per second: 0.54780\n",
      "step: 31914\n",
      "loss: 12.833542823791504\n",
      "steps per second: 0.47717\n",
      "step: 31915\n",
      "loss: 12.897958755493164\n",
      "steps per second: 0.49806\n",
      "step: 31916\n",
      "loss: 12.557785034179688\n",
      "steps per second: 0.53911\n",
      "step: 31917\n",
      "loss: 13.374561309814453\n",
      "steps per second: 0.53380\n",
      "step: 31918\n",
      "loss: 12.861984252929688\n",
      "steps per second: 0.55075\n",
      "step: 31919\n",
      "loss: 12.811156272888184\n",
      "steps per second: 0.57592\n",
      "step: 31920\n",
      "loss: 12.859602928161621\n",
      "steps per second: 0.55629\n",
      "step: 31921\n",
      "loss: 13.09342098236084\n",
      "steps per second: 0.57524\n",
      "step: 31922\n",
      "loss: 13.291826248168945\n",
      "steps per second: 0.54557\n",
      "step: 31923\n",
      "loss: 12.729643821716309\n",
      "steps per second: 0.56896\n",
      "step: 31924\n",
      "loss: 12.55341625213623\n",
      "steps per second: 0.56456\n",
      "step: 31925\n",
      "loss: 12.575511932373047\n",
      "steps per second: 0.53266\n",
      "step: 31926\n",
      "loss: 12.706559181213379\n",
      "steps per second: 0.56503\n",
      "step: 31927\n",
      "loss: 12.583285331726074\n",
      "steps per second: 0.57146\n",
      "step: 31928\n",
      "loss: 12.497036933898926\n",
      "steps per second: 0.53996\n",
      "step: 31929\n",
      "loss: 12.530105590820312\n",
      "steps per second: 0.53856\n",
      "step: 31930\n",
      "loss: 12.615102767944336\n",
      "steps per second: 0.54503\n",
      "step: 31931\n",
      "loss: 12.45044231414795\n",
      "steps per second: 0.54693\n",
      "step: 31932\n",
      "loss: 12.6707181930542\n",
      "steps per second: 0.54256\n",
      "step: 31933\n",
      "loss: 13.052898406982422\n",
      "steps per second: 0.53165\n",
      "step: 31934\n",
      "loss: 12.959965705871582\n",
      "steps per second: 0.53830\n",
      "step: 31935\n",
      "loss: 12.38730525970459\n",
      "steps per second: 0.52571\n",
      "step: 31936\n",
      "loss: 12.659188270568848\n",
      "steps per second: 0.57087\n",
      "step: 31937\n",
      "loss: 12.65963077545166\n",
      "steps per second: 0.55170\n",
      "step: 31938\n",
      "loss: 13.06882095336914\n",
      "steps per second: 0.55172\n",
      "step: 31939\n",
      "loss: 12.516485214233398\n",
      "steps per second: 0.55285\n",
      "step: 31940\n",
      "loss: 12.863274574279785\n",
      "steps per second: 0.56905\n",
      "step: 31941\n",
      "loss: 13.038585662841797\n",
      "steps per second: 0.55084\n",
      "step: 31942\n",
      "loss: 13.19632625579834\n",
      "steps per second: 0.55232\n",
      "step: 31943\n",
      "loss: 12.962825775146484\n",
      "steps per second: 0.52954\n",
      "step: 31944\n",
      "loss: 12.907302856445312\n",
      "steps per second: 0.53982\n",
      "step: 31945\n",
      "loss: 13.031898498535156\n",
      "steps per second: 0.54073\n",
      "step: 31946\n",
      "loss: 12.900636672973633\n",
      "steps per second: 0.58196\n",
      "step: 31947\n",
      "loss: 13.666271209716797\n",
      "steps per second: 0.50019\n",
      "step: 31948\n",
      "loss: 12.922507286071777\n",
      "steps per second: 0.54830\n",
      "step: 31949\n",
      "loss: 12.92973804473877\n",
      "steps per second: 0.60543\n",
      "step: 31950\n",
      "loss: 12.206928253173828\n",
      "steps per second: 0.56237\n",
      "step: 31951\n",
      "loss: 12.56222915649414\n",
      "steps per second: 0.54932\n",
      "step: 31952\n",
      "loss: 12.632567405700684\n",
      "steps per second: 0.53778\n",
      "step: 31953\n",
      "loss: 12.464646339416504\n",
      "steps per second: 0.54579\n",
      "step: 31954\n",
      "loss: 12.442333221435547\n",
      "steps per second: 0.55238\n",
      "step: 31955\n",
      "loss: 12.987083435058594\n",
      "steps per second: 0.53619\n",
      "step: 31956\n",
      "loss: 12.85067367553711\n",
      "steps per second: 0.54682\n",
      "step: 31957\n",
      "loss: 13.18693733215332\n",
      "steps per second: 0.52274\n",
      "step: 31958\n",
      "loss: 12.363814353942871\n",
      "steps per second: 0.51910\n",
      "step: 31959\n",
      "loss: 13.313834190368652\n",
      "steps per second: 0.52075\n",
      "step: 31960\n",
      "loss: 12.611083030700684\n",
      "steps per second: 0.55097\n",
      "step: 31961\n",
      "loss: 12.836560249328613\n",
      "steps per second: 0.56388\n",
      "step: 31962\n",
      "loss: 13.028265953063965\n",
      "steps per second: 0.54120\n",
      "step: 31963\n",
      "loss: 12.675491333007812\n",
      "steps per second: 0.54845\n",
      "step: 31964\n",
      "loss: 12.59959602355957\n",
      "steps per second: 0.60433\n",
      "step: 31965\n",
      "loss: 12.498940467834473\n",
      "steps per second: 0.57521\n",
      "step: 31966\n",
      "loss: 12.718210220336914\n",
      "steps per second: 0.54747\n",
      "step: 31967\n",
      "loss: 12.741809844970703\n",
      "steps per second: 0.53647\n",
      "step: 31968\n",
      "loss: 12.615052223205566\n",
      "steps per second: 0.53956\n",
      "step: 31969\n",
      "loss: 12.852421760559082\n",
      "steps per second: 0.54629\n",
      "step: 31970\n",
      "loss: 12.531743049621582\n",
      "steps per second: 0.54593\n",
      "step: 31971\n",
      "loss: 13.47274398803711\n",
      "steps per second: 0.53710\n",
      "step: 31972\n",
      "loss: 12.7595853805542\n",
      "steps per second: 0.49770\n",
      "step: 31973\n",
      "loss: 13.219693183898926\n",
      "steps per second: 0.56956\n",
      "step: 31974\n",
      "loss: 12.930496215820312\n",
      "steps per second: 0.56897\n",
      "step: 31975\n",
      "loss: 13.168431282043457\n",
      "steps per second: 0.59896\n",
      "step: 31976\n",
      "loss: 13.237114906311035\n",
      "steps per second: 0.55031\n",
      "step: 31977\n",
      "loss: 12.901971817016602\n",
      "steps per second: 0.54270\n",
      "step: 31978\n",
      "loss: 12.303812026977539\n",
      "steps per second: 0.57330\n",
      "step: 31979\n",
      "loss: 12.650684356689453\n",
      "steps per second: 0.54190\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8010784387588501, layer: 11\n",
      "saving at step 31979\n",
      "----------\n",
      "\n",
      "\n",
      "step: 31980\n",
      "loss: 12.195287704467773\n",
      "steps per second: 0.27357\n",
      "step: 31981\n",
      "loss: 12.483850479125977\n",
      "steps per second: 0.54383\n",
      "step: 31982\n",
      "loss: 13.077865600585938\n",
      "steps per second: 0.56153\n",
      "step: 31983\n",
      "loss: 13.387677192687988\n",
      "steps per second: 0.53468\n",
      "step: 31984\n",
      "loss: 12.757294654846191\n",
      "steps per second: 0.56589\n",
      "step: 31985\n",
      "loss: 12.975982666015625\n",
      "steps per second: 0.51115\n",
      "step: 31986\n",
      "loss: 12.613397598266602\n",
      "steps per second: 0.53258\n",
      "step: 31987\n",
      "loss: 12.413095474243164\n",
      "steps per second: 0.53378\n",
      "step: 31988\n",
      "loss: 12.743256568908691\n",
      "steps per second: 0.55137\n",
      "step: 31989\n",
      "loss: 12.54080581665039\n",
      "steps per second: 0.54972\n",
      "step: 31990\n",
      "loss: 13.035694122314453\n",
      "steps per second: 0.51746\n",
      "step: 31991\n",
      "loss: 13.035737037658691\n",
      "steps per second: 0.56465\n",
      "step: 31992\n",
      "loss: 12.462203979492188\n",
      "steps per second: 0.60741\n",
      "step: 31993\n",
      "loss: 12.424771308898926\n",
      "steps per second: 0.50835\n",
      "step: 31994\n",
      "loss: 12.615150451660156\n",
      "steps per second: 0.54542\n",
      "step: 31995\n",
      "loss: 12.899982452392578\n",
      "steps per second: 0.55469\n",
      "step: 31996\n",
      "loss: 13.091922760009766\n",
      "steps per second: 0.54658\n",
      "step: 31997\n",
      "loss: 12.435161590576172\n",
      "steps per second: 0.52888\n",
      "step: 31998\n",
      "loss: 13.259852409362793\n",
      "steps per second: 0.57634\n",
      "step: 31999\n",
      "loss: 12.160283088684082\n",
      "steps per second: 0.59251\n",
      "step: 32000\n",
      "loss: 12.740949630737305\n",
      "steps per second: 0.55418\n",
      "step: 32001\n",
      "loss: 12.520296096801758\n",
      "steps per second: 0.50007\n",
      "step: 32002\n",
      "loss: 13.02096176147461\n",
      "steps per second: 0.54065\n",
      "step: 32003\n",
      "loss: 12.847658157348633\n",
      "steps per second: 0.48283\n",
      "step: 32004\n",
      "loss: 12.965245246887207\n",
      "steps per second: 0.55249\n",
      "step: 32005\n",
      "loss: 12.946273803710938\n",
      "steps per second: 0.54705\n",
      "step: 32006\n",
      "loss: 12.867205619812012\n",
      "steps per second: 0.60039\n",
      "step: 32007\n",
      "loss: 13.113368034362793\n",
      "steps per second: 0.54197\n",
      "step: 32008\n",
      "loss: 12.642626762390137\n",
      "steps per second: 0.60673\n",
      "step: 32009\n",
      "loss: 13.51248550415039\n",
      "steps per second: 0.55281\n",
      "step: 32010\n",
      "loss: 12.661636352539062\n",
      "steps per second: 0.51785\n",
      "step: 32011\n",
      "loss: 12.366521835327148\n",
      "steps per second: 0.56831\n",
      "step: 32012\n",
      "loss: 12.660177230834961\n",
      "steps per second: 0.54710\n",
      "step: 32013\n",
      "loss: 12.527127265930176\n",
      "steps per second: 0.53296\n",
      "step: 32014\n",
      "loss: 13.33217716217041\n",
      "steps per second: 0.49675\n",
      "step: 32015\n",
      "loss: 13.093485832214355\n",
      "steps per second: 0.55648\n",
      "step: 32016\n",
      "loss: 12.558134078979492\n",
      "steps per second: 0.53034\n",
      "step: 32017\n",
      "loss: 12.535340309143066\n",
      "steps per second: 0.55078\n",
      "step: 32018\n",
      "loss: 13.108234405517578\n",
      "steps per second: 0.54131\n",
      "step: 32019\n",
      "loss: 12.303159713745117\n",
      "steps per second: 0.54798\n",
      "step: 32020\n",
      "loss: 12.561307907104492\n",
      "steps per second: 0.57722\n",
      "step: 32021\n",
      "loss: 13.34679889678955\n",
      "steps per second: 0.55320\n",
      "step: 32022\n",
      "loss: 12.642476081848145\n",
      "steps per second: 0.56857\n",
      "step: 32023\n",
      "loss: 12.113059043884277\n",
      "steps per second: 0.54535\n",
      "step: 32024\n",
      "loss: 12.995033264160156\n",
      "steps per second: 0.55159\n",
      "step: 32025\n",
      "loss: 12.561400413513184\n",
      "steps per second: 0.52517\n",
      "step: 32026\n",
      "loss: 12.312515258789062\n",
      "steps per second: 0.53469\n",
      "step: 32027\n",
      "loss: 12.799951553344727\n",
      "steps per second: 0.56445\n",
      "step: 32028\n",
      "loss: 12.618593215942383\n",
      "steps per second: 0.51083\n",
      "step: 32029\n",
      "loss: 12.93527603149414\n",
      "steps per second: 0.57519\n",
      "step: 32030\n",
      "loss: 13.151809692382812\n",
      "steps per second: 0.52343\n",
      "step: 32031\n",
      "loss: 12.745665550231934\n",
      "steps per second: 0.53180\n",
      "step: 32032\n",
      "loss: 12.809263229370117\n",
      "steps per second: 0.51971\n",
      "step: 32033\n",
      "loss: 12.858869552612305\n",
      "steps per second: 0.56157\n",
      "step: 32034\n",
      "loss: 12.78242015838623\n",
      "steps per second: 0.55169\n",
      "step: 32035\n",
      "loss: 12.409311294555664\n",
      "steps per second: 0.55252\n",
      "step: 32036\n",
      "loss: 12.730884552001953\n",
      "steps per second: 0.57480\n",
      "step: 32037\n",
      "loss: 12.281185150146484\n",
      "steps per second: 0.60733\n",
      "step: 32038\n",
      "loss: 12.306694984436035\n",
      "steps per second: 0.54828\n",
      "step: 32039\n",
      "loss: 12.552118301391602\n",
      "steps per second: 0.54722\n",
      "step: 32040\n",
      "loss: 12.99259090423584\n",
      "steps per second: 0.51938\n",
      "step: 32041\n",
      "loss: 12.726337432861328\n",
      "steps per second: 0.55000\n",
      "step: 32042\n",
      "loss: 13.058042526245117\n",
      "steps per second: 0.55332\n",
      "step: 32043\n",
      "loss: 12.191093444824219\n",
      "steps per second: 0.50104\n",
      "step: 32044\n",
      "loss: 12.244250297546387\n",
      "steps per second: 0.56501\n",
      "step: 32045\n",
      "loss: 13.072973251342773\n",
      "steps per second: 0.54779\n",
      "step: 32046\n",
      "loss: 12.677806854248047\n",
      "steps per second: 0.52171\n",
      "step: 32047\n",
      "loss: 13.088918685913086\n",
      "steps per second: 0.57153\n",
      "step: 32048\n",
      "loss: 13.375055313110352\n",
      "steps per second: 0.57128\n",
      "step: 32049\n",
      "loss: 13.101434707641602\n",
      "steps per second: 0.55155\n",
      "step: 32050\n",
      "loss: 12.85484504699707\n",
      "steps per second: 0.56604\n",
      "step: 32051\n",
      "loss: 13.04578685760498\n",
      "steps per second: 0.53889\n",
      "step: 32052\n",
      "loss: 13.474946022033691\n",
      "steps per second: 0.54625\n",
      "step: 32053\n",
      "loss: 12.789148330688477\n",
      "steps per second: 0.56326\n",
      "step: 32054\n",
      "loss: 12.636611938476562\n",
      "steps per second: 0.55079\n",
      "step: 32055\n",
      "loss: 13.060165405273438\n",
      "steps per second: 0.53364\n",
      "step: 32056\n",
      "loss: 12.89132308959961\n",
      "steps per second: 0.54709\n",
      "step: 32057\n",
      "loss: 13.31804084777832\n",
      "steps per second: 0.54453\n",
      "step: 32058\n",
      "loss: 13.046001434326172\n",
      "steps per second: 0.51982\n",
      "step: 32059\n",
      "loss: 13.326531410217285\n",
      "steps per second: 0.54717\n",
      "step: 32060\n",
      "loss: 12.255345344543457\n",
      "steps per second: 0.57442\n",
      "step: 32061\n",
      "loss: 12.913961410522461\n",
      "steps per second: 0.55617\n",
      "step: 32062\n",
      "loss: 12.369424819946289\n",
      "steps per second: 0.55099\n",
      "step: 32063\n",
      "loss: 12.7989501953125\n",
      "steps per second: 0.52201\n",
      "step: 32064\n",
      "loss: 12.407995223999023\n",
      "steps per second: 0.54267\n",
      "step: 32065\n",
      "loss: 12.855473518371582\n",
      "steps per second: 0.56780\n",
      "step: 32066\n",
      "loss: 12.818191528320312\n",
      "steps per second: 0.55400\n",
      "step: 32067\n",
      "loss: 12.312117576599121\n",
      "steps per second: 0.54721\n",
      "step: 32068\n",
      "loss: 13.218186378479004\n",
      "steps per second: 0.50975\n",
      "step: 32069\n",
      "loss: 12.578107833862305\n",
      "steps per second: 0.55325\n",
      "step: 32070\n",
      "loss: 12.917407989501953\n",
      "steps per second: 0.51968\n",
      "step: 32071\n",
      "loss: 13.065908432006836\n",
      "steps per second: 0.53224\n",
      "step: 32072\n",
      "loss: 12.930671691894531\n",
      "steps per second: 0.57455\n",
      "step: 32073\n",
      "loss: 12.802090644836426\n",
      "steps per second: 0.51972\n",
      "step: 32074\n",
      "loss: 13.442538261413574\n",
      "steps per second: 0.53952\n",
      "step: 32075\n",
      "loss: 12.947251319885254\n",
      "steps per second: 0.56644\n",
      "step: 32076\n",
      "loss: 12.531208038330078\n",
      "steps per second: 0.55441\n",
      "step: 32077\n",
      "loss: 12.362045288085938\n",
      "steps per second: 0.52823\n",
      "step: 32078\n",
      "loss: 13.13491439819336\n",
      "steps per second: 0.56582\n",
      "step: 32079\n",
      "loss: 13.049854278564453\n",
      "steps per second: 0.57642\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.853781521320343, layer: 11\n",
      "saving at step 32079\n",
      "----------\n",
      "\n",
      "\n",
      "step: 32080\n",
      "loss: 12.860772132873535\n",
      "steps per second: 0.27843\n",
      "step: 32081\n",
      "loss: 12.80343246459961\n",
      "steps per second: 0.51995\n",
      "step: 32082\n",
      "loss: 12.921333312988281\n",
      "steps per second: 0.55336\n",
      "step: 32083\n",
      "loss: 12.970284461975098\n",
      "steps per second: 0.57423\n",
      "step: 32084\n",
      "loss: 12.318222045898438\n",
      "steps per second: 0.53495\n",
      "step: 32085\n",
      "loss: 12.965319633483887\n",
      "steps per second: 0.60409\n",
      "step: 32086\n",
      "loss: 13.111315727233887\n",
      "steps per second: 0.53259\n",
      "step: 32087\n",
      "loss: 13.341297149658203\n",
      "steps per second: 0.54899\n",
      "step: 32088\n",
      "loss: 13.177488327026367\n",
      "steps per second: 0.53714\n",
      "step: 32089\n",
      "loss: 13.039689064025879\n",
      "steps per second: 0.54868\n",
      "step: 32090\n",
      "loss: 12.368528366088867\n",
      "steps per second: 0.54658\n",
      "step: 32091\n",
      "loss: 12.335354804992676\n",
      "steps per second: 0.55051\n",
      "step: 32092\n",
      "loss: 12.998106956481934\n",
      "steps per second: 0.55275\n",
      "step: 32093\n",
      "loss: 12.593949317932129\n",
      "steps per second: 0.50769\n",
      "step: 32094\n",
      "loss: 12.823731422424316\n",
      "steps per second: 0.54248\n",
      "step: 32095\n",
      "loss: 12.591604232788086\n",
      "steps per second: 0.54588\n",
      "step: 32096\n",
      "loss: 13.113710403442383\n",
      "steps per second: 0.55373\n",
      "step: 32097\n",
      "loss: 12.602045059204102\n",
      "steps per second: 0.55944\n",
      "step: 32098\n",
      "loss: 12.695406913757324\n",
      "steps per second: 0.53416\n",
      "step: 32099\n",
      "loss: 12.899176597595215\n",
      "steps per second: 0.52012\n",
      "step: 32100\n",
      "loss: 12.868278503417969\n",
      "steps per second: 0.49956\n",
      "step: 32101\n",
      "loss: 12.518004417419434\n",
      "steps per second: 0.57612\n",
      "step: 32102\n",
      "loss: 12.094503402709961\n",
      "steps per second: 0.53218\n",
      "step: 32103\n",
      "loss: 12.361635208129883\n",
      "steps per second: 0.49987\n",
      "step: 32104\n",
      "loss: 12.596067428588867\n",
      "steps per second: 0.57294\n",
      "step: 32105\n",
      "loss: 13.004287719726562\n",
      "steps per second: 0.57422\n",
      "step: 32106\n",
      "loss: 13.408072471618652\n",
      "steps per second: 0.54628\n",
      "step: 32107\n",
      "loss: 12.597681045532227\n",
      "steps per second: 0.56355\n",
      "step: 32108\n",
      "loss: 12.596345901489258\n",
      "steps per second: 0.54801\n",
      "step: 32109\n",
      "loss: 13.576715469360352\n",
      "steps per second: 0.54288\n",
      "step: 32110\n",
      "loss: 12.877679824829102\n",
      "steps per second: 0.57319\n",
      "step: 32111\n",
      "loss: 12.810354232788086\n",
      "steps per second: 0.56591\n",
      "step: 32112\n",
      "loss: 13.127069473266602\n",
      "steps per second: 0.53034\n",
      "step: 32113\n",
      "loss: 12.54456901550293\n",
      "steps per second: 0.54794\n",
      "step: 32114\n",
      "loss: 12.304388999938965\n",
      "steps per second: 0.54432\n",
      "step: 32115\n",
      "loss: 12.753279685974121\n",
      "steps per second: 0.53607\n",
      "step: 32116\n",
      "loss: 12.895437240600586\n",
      "steps per second: 0.54932\n",
      "step: 32117\n",
      "loss: 12.496156692504883\n",
      "steps per second: 0.53815\n",
      "step: 32118\n",
      "loss: 12.802996635437012\n",
      "steps per second: 0.54387\n",
      "step: 32119\n",
      "loss: 12.956067085266113\n",
      "steps per second: 0.48728\n",
      "step: 32120\n",
      "loss: 12.783324241638184\n",
      "steps per second: 0.47741\n",
      "step: 32121\n",
      "loss: 12.640934944152832\n",
      "steps per second: 0.54371\n",
      "step: 32122\n",
      "loss: 12.824772834777832\n",
      "steps per second: 0.52248\n",
      "step: 32123\n",
      "loss: 12.455646514892578\n",
      "steps per second: 0.54592\n",
      "step: 32124\n",
      "loss: 13.084229469299316\n",
      "steps per second: 0.48493\n",
      "step: 32125\n",
      "loss: 12.255073547363281\n",
      "steps per second: 0.53875\n",
      "step: 32126\n",
      "loss: 13.234698295593262\n",
      "steps per second: 0.55282\n",
      "step: 32127\n",
      "loss: 12.341875076293945\n",
      "steps per second: 0.54089\n",
      "step: 32128\n",
      "loss: 12.895615577697754\n",
      "steps per second: 0.51870\n",
      "step: 32129\n",
      "loss: 12.442116737365723\n",
      "steps per second: 0.60398\n",
      "step: 32130\n",
      "loss: 12.877238273620605\n",
      "steps per second: 0.55751\n",
      "step: 32131\n",
      "loss: 12.87248420715332\n",
      "steps per second: 0.53946\n",
      "step: 32132\n",
      "loss: 12.642094612121582\n",
      "steps per second: 0.56537\n",
      "step: 32133\n",
      "loss: 12.278604507446289\n",
      "steps per second: 0.53124\n",
      "step: 32134\n",
      "loss: 12.644261360168457\n",
      "steps per second: 0.52281\n",
      "step: 32135\n",
      "loss: 12.602636337280273\n",
      "steps per second: 0.54288\n",
      "step: 32136\n",
      "loss: 13.446202278137207\n",
      "steps per second: 0.47264\n",
      "step: 32137\n",
      "loss: 13.16088581085205\n",
      "steps per second: 0.53355\n",
      "step: 32138\n",
      "loss: 12.77235221862793\n",
      "steps per second: 0.56410\n",
      "step: 32139\n",
      "loss: 12.50924015045166\n",
      "steps per second: 0.55053\n",
      "step: 32140\n",
      "loss: 13.03321361541748\n",
      "steps per second: 0.55101\n",
      "step: 32141\n",
      "loss: 12.706436157226562\n",
      "steps per second: 0.60464\n",
      "step: 32142\n",
      "loss: 12.74640941619873\n",
      "steps per second: 0.52085\n",
      "step: 32143\n",
      "loss: 13.049345970153809\n",
      "steps per second: 0.52448\n",
      "step: 32144\n",
      "loss: 12.635428428649902\n",
      "steps per second: 0.53919\n",
      "step: 32145\n",
      "loss: 13.296159744262695\n",
      "steps per second: 0.56414\n",
      "step: 32146\n",
      "loss: 12.832954406738281\n",
      "steps per second: 0.53719\n",
      "step: 32147\n",
      "loss: 12.123175621032715\n",
      "steps per second: 0.53256\n",
      "step: 32148\n",
      "loss: 13.293339729309082\n",
      "steps per second: 0.59526\n",
      "step: 32149\n",
      "loss: 12.656458854675293\n",
      "steps per second: 0.54791\n",
      "step: 32150\n",
      "loss: 12.90981674194336\n",
      "steps per second: 0.56234\n",
      "step: 32151\n",
      "loss: 12.763252258300781\n",
      "steps per second: 0.57292\n",
      "step: 32152\n",
      "loss: 13.125678062438965\n",
      "steps per second: 0.52386\n",
      "step: 32153\n",
      "loss: 13.248662948608398\n",
      "steps per second: 0.56124\n",
      "step: 32154\n",
      "loss: 12.573573112487793\n",
      "steps per second: 0.56486\n",
      "step: 32155\n",
      "loss: 13.09891414642334\n",
      "steps per second: 0.54930\n",
      "step: 32156\n",
      "loss: 12.998015403747559\n",
      "steps per second: 0.53353\n",
      "step: 32157\n",
      "loss: 12.647700309753418\n",
      "steps per second: 0.53882\n",
      "step: 32158\n",
      "loss: 13.455467224121094\n",
      "steps per second: 0.55035\n",
      "step: 32159\n",
      "loss: 13.292708396911621\n",
      "steps per second: 0.55461\n",
      "step: 32160\n",
      "loss: 13.135986328125\n",
      "steps per second: 0.54928\n",
      "step: 32161\n",
      "loss: 12.68887710571289\n",
      "steps per second: 0.57119\n",
      "step: 32162\n",
      "loss: 12.482772827148438\n",
      "steps per second: 0.54228\n",
      "step: 32163\n",
      "loss: 12.670012474060059\n",
      "steps per second: 0.48204\n",
      "step: 32164\n",
      "loss: 12.560674667358398\n",
      "steps per second: 0.54426\n",
      "step: 32165\n",
      "loss: 13.373859405517578\n",
      "steps per second: 0.52752\n",
      "step: 32166\n",
      "loss: 12.335671424865723\n",
      "steps per second: 0.52027\n",
      "step: 32167\n",
      "loss: 13.240939140319824\n",
      "steps per second: 0.55300\n",
      "step: 32168\n",
      "loss: 13.41408634185791\n",
      "steps per second: 0.56713\n",
      "step: 32169\n",
      "loss: 12.69301986694336\n",
      "steps per second: 0.52378\n",
      "step: 32170\n",
      "loss: 12.802048683166504\n",
      "steps per second: 0.53381\n",
      "step: 32171\n",
      "loss: 13.001700401306152\n",
      "steps per second: 0.56148\n",
      "step: 32172\n",
      "loss: 12.748276710510254\n",
      "steps per second: 0.55652\n",
      "step: 32173\n",
      "loss: 12.693709373474121\n",
      "steps per second: 0.57082\n",
      "step: 32174\n",
      "loss: 12.92689323425293\n",
      "steps per second: 0.54622\n",
      "step: 32175\n",
      "loss: 12.647567749023438\n",
      "steps per second: 0.55740\n",
      "step: 32176\n",
      "loss: 13.258136749267578\n",
      "steps per second: 0.46274\n",
      "step: 32177\n",
      "loss: 12.948614120483398\n",
      "steps per second: 0.50843\n",
      "step: 32178\n",
      "loss: 13.097349166870117\n",
      "steps per second: 0.53967\n",
      "step: 32179\n",
      "loss: 12.857850074768066\n",
      "steps per second: 0.56957\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8443613648414612, layer: 11\n",
      "saving at step 32179\n",
      "----------\n",
      "\n",
      "\n",
      "step: 32180\n",
      "loss: 12.638541221618652\n",
      "steps per second: 0.27898\n",
      "step: 32181\n",
      "loss: 12.706188201904297\n",
      "steps per second: 0.56991\n",
      "step: 32182\n",
      "loss: 12.458248138427734\n",
      "steps per second: 0.54344\n",
      "step: 32183\n",
      "loss: 13.199698448181152\n",
      "steps per second: 0.55361\n",
      "step: 32184\n",
      "loss: 12.902860641479492\n",
      "steps per second: 0.54593\n",
      "step: 32185\n",
      "loss: 12.519536018371582\n",
      "steps per second: 0.54501\n",
      "step: 32186\n",
      "loss: 13.03772258758545\n",
      "steps per second: 0.55579\n",
      "step: 32187\n",
      "loss: 12.870660781860352\n",
      "steps per second: 0.52036\n",
      "step: 32188\n",
      "loss: 13.221335411071777\n",
      "steps per second: 0.53487\n",
      "step: 32189\n",
      "loss: 12.737862586975098\n",
      "steps per second: 0.54604\n",
      "step: 32190\n",
      "loss: 13.392251968383789\n",
      "steps per second: 0.56561\n",
      "step: 32191\n",
      "loss: 13.023262023925781\n",
      "steps per second: 0.53257\n",
      "step: 32192\n",
      "loss: 12.708782196044922\n",
      "steps per second: 0.54867\n",
      "step: 32193\n",
      "loss: 12.530938148498535\n",
      "steps per second: 0.54079\n",
      "step: 32194\n",
      "loss: 13.239255905151367\n",
      "steps per second: 0.54268\n",
      "step: 32195\n",
      "loss: 12.926265716552734\n",
      "steps per second: 0.53768\n",
      "step: 32196\n",
      "loss: 12.729557991027832\n",
      "steps per second: 0.53293\n",
      "step: 32197\n",
      "loss: 12.892992973327637\n",
      "steps per second: 0.55344\n",
      "step: 32198\n",
      "loss: 12.708124160766602\n",
      "steps per second: 0.52241\n",
      "step: 32199\n",
      "loss: 12.641441345214844\n",
      "steps per second: 0.60566\n",
      "step: 32200\n",
      "loss: 12.917835235595703\n",
      "steps per second: 0.53756\n",
      "step: 32201\n",
      "loss: 12.43499755859375\n",
      "steps per second: 0.54601\n",
      "step: 32202\n",
      "loss: 12.450023651123047\n",
      "steps per second: 0.55597\n",
      "step: 32203\n",
      "loss: 13.120063781738281\n",
      "steps per second: 0.54285\n",
      "step: 32204\n",
      "loss: 12.973390579223633\n",
      "steps per second: 0.55161\n",
      "step: 32205\n",
      "loss: 12.302168846130371\n",
      "steps per second: 0.54344\n",
      "step: 32206\n",
      "loss: 12.55263900756836\n",
      "steps per second: 0.55380\n",
      "step: 32207\n",
      "loss: 13.06104850769043\n",
      "steps per second: 0.56014\n",
      "step: 32208\n",
      "loss: 13.17965030670166\n",
      "steps per second: 0.55464\n",
      "step: 32209\n",
      "loss: 12.888386726379395\n",
      "steps per second: 0.54564\n",
      "step: 32210\n",
      "loss: 12.440281867980957\n",
      "steps per second: 0.56356\n",
      "step: 32211\n",
      "loss: 13.203373908996582\n",
      "steps per second: 0.54732\n",
      "step: 32212\n",
      "loss: 13.007925033569336\n",
      "steps per second: 0.54747\n",
      "step: 32213\n",
      "loss: 13.145102500915527\n",
      "steps per second: 0.56773\n",
      "step: 32214\n",
      "loss: 13.013655662536621\n",
      "steps per second: 0.56880\n",
      "step: 32215\n",
      "loss: 12.538297653198242\n",
      "steps per second: 0.55261\n",
      "step: 32216\n",
      "loss: 12.754120826721191\n",
      "steps per second: 0.52692\n",
      "step: 32217\n",
      "loss: 13.0420560836792\n",
      "steps per second: 0.54136\n",
      "step: 32218\n",
      "loss: 12.811076164245605\n",
      "steps per second: 0.55578\n",
      "step: 32219\n",
      "loss: 12.718728065490723\n",
      "steps per second: 0.54063\n",
      "step: 32220\n",
      "loss: 12.844795227050781\n",
      "steps per second: 0.54894\n",
      "step: 32221\n",
      "loss: 12.74135684967041\n",
      "steps per second: 0.54643\n",
      "step: 32222\n",
      "loss: 12.68769645690918\n",
      "steps per second: 0.49074\n",
      "step: 32223\n",
      "loss: 12.691367149353027\n",
      "steps per second: 0.57637\n",
      "step: 32224\n",
      "loss: 13.239747047424316\n",
      "steps per second: 0.55498\n",
      "step: 32225\n",
      "loss: 12.417505264282227\n",
      "steps per second: 0.53550\n",
      "step: 32226\n",
      "loss: 13.123213768005371\n",
      "steps per second: 0.54079\n",
      "step: 32227\n",
      "loss: 12.418850898742676\n",
      "steps per second: 0.49179\n",
      "step: 32228\n",
      "loss: 12.78498363494873\n",
      "steps per second: 0.52215\n",
      "step: 32229\n",
      "loss: 12.517498970031738\n",
      "steps per second: 0.52461\n",
      "step: 32230\n",
      "loss: 13.204025268554688\n",
      "steps per second: 0.57579\n",
      "step: 32231\n",
      "loss: 12.41787338256836\n",
      "steps per second: 0.53854\n",
      "step: 32232\n",
      "loss: 12.441407203674316\n",
      "steps per second: 0.53395\n",
      "step: 32233\n",
      "loss: 12.581653594970703\n",
      "steps per second: 0.54595\n",
      "step: 32234\n",
      "loss: 12.885163307189941\n",
      "steps per second: 0.53282\n",
      "step: 32235\n",
      "loss: 12.959101676940918\n",
      "steps per second: 0.53566\n",
      "step: 32236\n",
      "loss: 12.539793014526367\n",
      "steps per second: 0.54443\n",
      "step: 32237\n",
      "loss: 12.641100883483887\n",
      "steps per second: 0.50952\n",
      "step: 32238\n",
      "loss: 12.7883939743042\n",
      "steps per second: 0.55250\n",
      "step: 32239\n",
      "loss: 13.109874725341797\n",
      "steps per second: 0.54359\n",
      "step: 32240\n",
      "loss: 13.095405578613281\n",
      "steps per second: 0.55508\n",
      "step: 32241\n",
      "loss: 12.567390441894531\n",
      "steps per second: 0.54254\n",
      "step: 32242\n",
      "loss: 12.443840026855469\n",
      "steps per second: 0.60914\n",
      "step: 32243\n",
      "loss: 12.466507911682129\n",
      "steps per second: 0.57658\n",
      "step: 32244\n",
      "loss: 13.05571174621582\n",
      "steps per second: 0.53480\n",
      "step: 32245\n",
      "loss: 12.877257347106934\n",
      "steps per second: 0.49058\n",
      "step: 32246\n",
      "loss: 12.346925735473633\n",
      "steps per second: 0.54136\n",
      "step: 32247\n",
      "loss: 13.205451011657715\n",
      "steps per second: 0.53926\n",
      "step: 32248\n",
      "loss: 12.787361145019531\n",
      "steps per second: 0.54830\n",
      "step: 32249\n",
      "loss: 12.812956809997559\n",
      "steps per second: 0.53156\n",
      "step: 32250\n",
      "loss: 12.926288604736328\n",
      "steps per second: 0.53198\n",
      "step: 32251\n",
      "loss: 12.285327911376953\n",
      "steps per second: 0.54760\n",
      "step: 32252\n",
      "loss: 13.014991760253906\n",
      "steps per second: 0.55355\n",
      "step: 32253\n",
      "loss: 12.701043128967285\n",
      "steps per second: 0.55450\n",
      "step: 32254\n",
      "loss: 12.86370849609375\n",
      "steps per second: 0.53506\n",
      "step: 32255\n",
      "loss: 12.666015625\n",
      "steps per second: 0.51390\n",
      "step: 32256\n",
      "loss: 12.854707717895508\n",
      "steps per second: 0.57316\n",
      "step: 32257\n",
      "loss: 13.291894912719727\n",
      "steps per second: 0.57369\n",
      "step: 32258\n",
      "loss: 13.213303565979004\n",
      "steps per second: 0.54527\n",
      "step: 32259\n",
      "loss: 12.518564224243164\n",
      "steps per second: 0.53810\n",
      "step: 32260\n",
      "loss: 13.453503608703613\n",
      "steps per second: 0.54486\n",
      "step: 32261\n",
      "loss: 12.717199325561523\n",
      "steps per second: 0.56990\n",
      "step: 32262\n",
      "loss: 12.734405517578125\n",
      "steps per second: 0.57222\n",
      "step: 32263\n",
      "loss: 12.651509284973145\n",
      "steps per second: 0.60594\n",
      "step: 32264\n",
      "loss: 13.415658950805664\n",
      "steps per second: 0.56754\n",
      "step: 32265\n",
      "loss: 12.603734016418457\n",
      "steps per second: 0.54080\n",
      "step: 32266\n",
      "loss: 12.584564208984375\n",
      "steps per second: 0.54471\n",
      "step: 32267\n",
      "loss: 12.89673900604248\n",
      "steps per second: 0.56827\n",
      "step: 32268\n",
      "loss: 12.77684497833252\n",
      "steps per second: 0.60863\n",
      "step: 32269\n",
      "loss: 12.695781707763672\n",
      "steps per second: 0.50885\n",
      "step: 32270\n",
      "loss: 13.142193794250488\n",
      "steps per second: 0.57398\n",
      "step: 32271\n",
      "loss: 12.975050926208496\n",
      "steps per second: 0.45783\n",
      "step: 32272\n",
      "loss: 13.281408309936523\n",
      "steps per second: 0.56784\n",
      "step: 32273\n",
      "loss: 12.620491027832031\n",
      "steps per second: 0.53864\n",
      "step: 32274\n",
      "loss: 12.853165626525879\n",
      "steps per second: 0.54918\n",
      "step: 32275\n",
      "loss: 13.458683967590332\n",
      "steps per second: 0.51408\n",
      "step: 32276\n",
      "loss: 12.715597152709961\n",
      "steps per second: 0.54390\n",
      "step: 32277\n",
      "loss: 12.742959022521973\n",
      "steps per second: 0.55599\n",
      "step: 32278\n",
      "loss: 12.853296279907227\n",
      "steps per second: 0.53716\n",
      "step: 32279\n",
      "loss: 12.879911422729492\n",
      "steps per second: 0.57585\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8242416977882385, layer: 11\n",
      "saving at step 32279\n",
      "----------\n",
      "\n",
      "\n",
      "step: 32280\n",
      "loss: 12.68061351776123\n",
      "steps per second: 0.27784\n",
      "step: 32281\n",
      "loss: 12.101889610290527\n",
      "steps per second: 0.51940\n",
      "step: 32282\n",
      "loss: 12.508700370788574\n",
      "steps per second: 0.55829\n",
      "step: 32283\n",
      "loss: 12.602644920349121\n",
      "steps per second: 0.53131\n",
      "step: 32284\n",
      "loss: 12.418569564819336\n",
      "steps per second: 0.57209\n",
      "step: 32285\n",
      "loss: 13.053274154663086\n",
      "steps per second: 0.56261\n",
      "step: 32286\n",
      "loss: 12.743556022644043\n",
      "steps per second: 0.48827\n",
      "step: 32287\n",
      "loss: 12.783913612365723\n",
      "steps per second: 0.55736\n",
      "step: 32288\n",
      "loss: 12.737838745117188\n",
      "steps per second: 0.50343\n",
      "step: 32289\n",
      "loss: 12.514423370361328\n",
      "steps per second: 0.50746\n",
      "step: 32290\n",
      "loss: 12.738344192504883\n",
      "steps per second: 0.52313\n",
      "step: 32291\n",
      "loss: 12.767531394958496\n",
      "steps per second: 0.54113\n",
      "step: 32292\n",
      "loss: 12.44382381439209\n",
      "steps per second: 0.49413\n",
      "step: 32293\n",
      "loss: 12.41189956665039\n",
      "steps per second: 0.54257\n",
      "step: 32294\n",
      "loss: 12.878154754638672\n",
      "steps per second: 0.51405\n",
      "step: 32295\n",
      "loss: 12.712425231933594\n",
      "steps per second: 0.53974\n",
      "step: 32296\n",
      "loss: 12.173866271972656\n",
      "steps per second: 0.52372\n",
      "step: 32297\n",
      "loss: 12.594724655151367\n",
      "steps per second: 0.53738\n",
      "step: 32298\n",
      "loss: 12.962420463562012\n",
      "steps per second: 0.46915\n",
      "step: 32299\n",
      "loss: 12.527695655822754\n",
      "steps per second: 0.47057\n",
      "step: 32300\n",
      "loss: 13.483135223388672\n",
      "steps per second: 0.52354\n",
      "step: 32301\n",
      "loss: 12.004023551940918\n",
      "steps per second: 0.53974\n",
      "step: 32302\n",
      "loss: 12.178470611572266\n",
      "steps per second: 0.48650\n",
      "step: 32303\n",
      "loss: 12.76308822631836\n",
      "steps per second: 0.51497\n",
      "step: 32304\n",
      "loss: 12.480853080749512\n",
      "steps per second: 0.50664\n",
      "step: 32305\n",
      "loss: 12.42350959777832\n",
      "steps per second: 0.54960\n",
      "step: 32306\n",
      "loss: 12.564888000488281\n",
      "steps per second: 0.55356\n",
      "step: 32307\n",
      "loss: 12.941961288452148\n",
      "steps per second: 0.54475\n",
      "step: 32308\n",
      "loss: 12.28955364227295\n",
      "steps per second: 0.55985\n",
      "step: 32309\n",
      "loss: 12.770769119262695\n",
      "steps per second: 0.53505\n",
      "step: 32310\n",
      "loss: 13.027889251708984\n",
      "steps per second: 0.51386\n",
      "step: 32311\n",
      "loss: 13.086529731750488\n",
      "steps per second: 0.56156\n",
      "step: 32312\n",
      "loss: 12.921202659606934\n",
      "steps per second: 0.52971\n",
      "step: 32313\n",
      "loss: 13.015246391296387\n",
      "steps per second: 0.57076\n",
      "step: 32314\n",
      "loss: 12.706117630004883\n",
      "steps per second: 0.51363\n",
      "step: 32315\n",
      "loss: 12.9058198928833\n",
      "steps per second: 0.53786\n",
      "step: 32316\n",
      "loss: 13.357855796813965\n",
      "steps per second: 0.55262\n",
      "step: 32317\n",
      "loss: 12.787240028381348\n",
      "steps per second: 0.54446\n",
      "step: 32318\n",
      "loss: 13.148480415344238\n",
      "steps per second: 0.54571\n",
      "step: 32319\n",
      "loss: 13.032632827758789\n",
      "steps per second: 0.52835\n",
      "step: 32320\n",
      "loss: 12.584614753723145\n",
      "steps per second: 0.50707\n",
      "step: 32321\n",
      "loss: 12.66860580444336\n",
      "steps per second: 0.53644\n",
      "step: 32322\n",
      "loss: 12.41195011138916\n",
      "steps per second: 0.51239\n",
      "step: 32323\n",
      "loss: 12.548395156860352\n",
      "steps per second: 0.54526\n",
      "step: 32324\n",
      "loss: 13.291092872619629\n",
      "steps per second: 0.54882\n",
      "step: 32325\n",
      "loss: 12.75027084350586\n",
      "steps per second: 0.52914\n",
      "step: 32326\n",
      "loss: 13.468232154846191\n",
      "steps per second: 0.50359\n",
      "step: 32327\n",
      "loss: 13.430093765258789\n",
      "steps per second: 0.50494\n",
      "step: 32328\n",
      "loss: 13.002729415893555\n",
      "steps per second: 0.51870\n",
      "step: 32329\n",
      "loss: 12.75289535522461\n",
      "steps per second: 0.51133\n",
      "step: 32330\n",
      "loss: 13.031561851501465\n",
      "steps per second: 0.55292\n",
      "step: 32331\n",
      "loss: 12.148270606994629\n",
      "steps per second: 0.52169\n",
      "step: 32332\n",
      "loss: 13.154092788696289\n",
      "steps per second: 0.49232\n",
      "step: 32333\n",
      "loss: 12.87794017791748\n",
      "steps per second: 0.48130\n",
      "step: 32334\n",
      "loss: 12.683435440063477\n",
      "steps per second: 0.51250\n",
      "step: 32335\n",
      "loss: 12.728683471679688\n",
      "steps per second: 0.57870\n",
      "step: 32336\n",
      "loss: 12.2005033493042\n",
      "steps per second: 0.53446\n",
      "step: 32337\n",
      "loss: 12.983245849609375\n",
      "steps per second: 0.47482\n",
      "step: 32338\n",
      "loss: 12.794862747192383\n",
      "steps per second: 0.51407\n",
      "step: 32339\n",
      "loss: 12.920548439025879\n",
      "steps per second: 0.52530\n",
      "step: 32340\n",
      "loss: 12.880590438842773\n",
      "steps per second: 0.51000\n",
      "step: 32341\n",
      "loss: 12.998501777648926\n",
      "steps per second: 0.54549\n",
      "step: 32342\n",
      "loss: 12.657013893127441\n",
      "steps per second: 0.52221\n",
      "step: 32343\n",
      "loss: 12.97727108001709\n",
      "steps per second: 0.52712\n",
      "step: 32344\n",
      "loss: 12.776876449584961\n",
      "steps per second: 0.53033\n",
      "step: 32345\n",
      "loss: 13.206436157226562\n",
      "steps per second: 0.52276\n",
      "step: 32346\n",
      "loss: 12.649456024169922\n",
      "steps per second: 0.49023\n",
      "step: 32347\n",
      "loss: 12.062835693359375\n",
      "steps per second: 0.51748\n",
      "step: 32348\n",
      "loss: 12.990978240966797\n",
      "steps per second: 0.50304\n",
      "step: 32349\n",
      "loss: 13.125146865844727\n",
      "steps per second: 0.50767\n",
      "step: 32350\n",
      "loss: 12.652600288391113\n",
      "steps per second: 0.50461\n",
      "step: 32351\n",
      "loss: 12.9794921875\n",
      "steps per second: 0.56655\n",
      "step: 32352\n",
      "loss: 13.051104545593262\n",
      "steps per second: 0.51677\n",
      "step: 32353\n",
      "loss: 13.482404708862305\n",
      "steps per second: 0.50849\n",
      "step: 32354\n",
      "loss: 12.735404014587402\n",
      "steps per second: 0.54618\n",
      "step: 32355\n",
      "loss: 12.593891143798828\n",
      "steps per second: 0.54632\n",
      "step: 32356\n",
      "loss: 12.70071029663086\n",
      "steps per second: 0.54842\n",
      "step: 32357\n",
      "loss: 13.414225578308105\n",
      "steps per second: 0.55995\n",
      "step: 32358\n",
      "loss: 13.015159606933594\n",
      "steps per second: 0.51935\n",
      "step: 32359\n",
      "loss: 11.81030559539795\n",
      "steps per second: 0.52068\n",
      "step: 32360\n",
      "loss: 12.900182723999023\n",
      "steps per second: 0.53032\n",
      "step: 32361\n",
      "loss: 12.980506896972656\n",
      "steps per second: 0.54597\n",
      "step: 32362\n",
      "loss: 13.092619895935059\n",
      "steps per second: 0.49576\n",
      "step: 32363\n",
      "loss: 12.813191413879395\n",
      "steps per second: 0.52060\n",
      "step: 32364\n",
      "loss: 12.988726615905762\n",
      "steps per second: 0.53176\n",
      "step: 32365\n",
      "loss: 12.834257125854492\n",
      "steps per second: 0.51266\n",
      "step: 32366\n",
      "loss: 11.926706314086914\n",
      "steps per second: 0.52312\n",
      "step: 32367\n",
      "loss: 12.56806468963623\n",
      "steps per second: 0.51547\n",
      "step: 32368\n",
      "loss: 12.829540252685547\n",
      "steps per second: 0.52175\n",
      "step: 32369\n",
      "loss: 12.68753719329834\n",
      "steps per second: 0.55357\n",
      "step: 32370\n",
      "loss: 12.695921897888184\n",
      "steps per second: 0.54375\n",
      "step: 32371\n",
      "loss: 12.975674629211426\n",
      "steps per second: 0.48214\n",
      "step: 32372\n",
      "loss: 12.86073112487793\n",
      "steps per second: 0.52464\n",
      "step: 32373\n",
      "loss: 13.327751159667969\n",
      "steps per second: 0.52761\n",
      "step: 32374\n",
      "loss: 12.862133979797363\n",
      "steps per second: 0.53229\n",
      "step: 32375\n",
      "loss: 12.603304862976074\n",
      "steps per second: 0.51202\n",
      "step: 32376\n",
      "loss: 12.694424629211426\n",
      "steps per second: 0.52924\n",
      "step: 32377\n",
      "loss: 13.227572441101074\n",
      "steps per second: 0.52905\n",
      "step: 32378\n",
      "loss: 12.545050621032715\n",
      "steps per second: 0.49913\n",
      "step: 32379\n",
      "loss: 13.35820198059082\n",
      "steps per second: 0.56313\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8883377313613892, layer: 11\n",
      "saving at step 32379\n",
      "----------\n",
      "\n",
      "\n",
      "step: 32380\n",
      "loss: 12.525022506713867\n",
      "steps per second: 0.27130\n",
      "step: 32381\n",
      "loss: 12.931570053100586\n",
      "steps per second: 0.56262\n",
      "step: 32382\n",
      "loss: 12.936841011047363\n",
      "steps per second: 0.54896\n",
      "step: 32383\n",
      "loss: 12.678619384765625\n",
      "steps per second: 0.57464\n",
      "step: 32384\n",
      "loss: 12.842385292053223\n",
      "steps per second: 0.56817\n",
      "step: 32385\n",
      "loss: 13.004472732543945\n",
      "steps per second: 0.53772\n",
      "step: 32386\n",
      "loss: 12.366866111755371\n",
      "steps per second: 0.55059\n",
      "step: 32387\n",
      "loss: 12.729818344116211\n",
      "steps per second: 0.54245\n",
      "step: 32388\n",
      "loss: 12.741020202636719\n",
      "steps per second: 0.56149\n",
      "step: 32389\n",
      "loss: 12.776100158691406\n",
      "steps per second: 0.53627\n",
      "step: 32390\n",
      "loss: 12.572365760803223\n",
      "steps per second: 0.53582\n",
      "step: 32391\n",
      "loss: 12.82618522644043\n",
      "steps per second: 0.61031\n",
      "step: 32392\n",
      "loss: 12.927757263183594\n",
      "steps per second: 0.53459\n",
      "step: 32393\n",
      "loss: 13.212626457214355\n",
      "steps per second: 0.53942\n",
      "step: 32394\n",
      "loss: 12.988410949707031\n",
      "steps per second: 0.54669\n",
      "step: 32395\n",
      "loss: 12.81400203704834\n",
      "steps per second: 0.52391\n",
      "step: 32396\n",
      "loss: 13.06263542175293\n",
      "steps per second: 0.57790\n",
      "step: 32397\n",
      "loss: 12.960420608520508\n",
      "steps per second: 0.51770\n",
      "step: 32398\n",
      "loss: 12.784587860107422\n",
      "steps per second: 0.52396\n",
      "step: 32399\n",
      "loss: 12.800943374633789\n",
      "steps per second: 0.51982\n",
      "step: 32400\n",
      "loss: 12.596358299255371\n",
      "steps per second: 0.56866\n",
      "step: 32401\n",
      "loss: 13.060745239257812\n",
      "steps per second: 0.56388\n",
      "step: 32402\n",
      "loss: 12.166658401489258\n",
      "steps per second: 0.55470\n",
      "step: 32403\n",
      "loss: 12.259462356567383\n",
      "steps per second: 0.54093\n",
      "step: 32404\n",
      "loss: 12.89374828338623\n",
      "steps per second: 0.56486\n",
      "step: 32405\n",
      "loss: 12.536409378051758\n",
      "steps per second: 0.56060\n",
      "step: 32406\n",
      "loss: 12.562820434570312\n",
      "steps per second: 0.52388\n",
      "step: 32407\n",
      "loss: 12.314323425292969\n",
      "steps per second: 0.56959\n",
      "step: 32408\n",
      "loss: 12.10715389251709\n",
      "steps per second: 0.53304\n",
      "step: 32409\n",
      "loss: 13.4088716506958\n",
      "steps per second: 0.57391\n",
      "step: 32410\n",
      "loss: 12.942747116088867\n",
      "steps per second: 0.52752\n",
      "step: 32411\n",
      "loss: 12.606406211853027\n",
      "steps per second: 0.53024\n",
      "step: 32412\n",
      "loss: 12.689231872558594\n",
      "steps per second: 0.56870\n",
      "step: 32413\n",
      "loss: 13.508946418762207\n",
      "steps per second: 0.56603\n",
      "step: 32414\n",
      "loss: 12.596179008483887\n",
      "steps per second: 0.55481\n",
      "step: 32415\n",
      "loss: 13.1779203414917\n",
      "steps per second: 0.60484\n",
      "step: 32416\n",
      "loss: 12.510117530822754\n",
      "steps per second: 0.51876\n",
      "step: 32417\n",
      "loss: 12.869722366333008\n",
      "steps per second: 0.51840\n",
      "step: 32418\n",
      "loss: 13.221184730529785\n",
      "steps per second: 0.51726\n",
      "step: 32419\n",
      "loss: 12.496614456176758\n",
      "steps per second: 0.51569\n",
      "step: 32420\n",
      "loss: 11.992216110229492\n",
      "steps per second: 0.52773\n",
      "step: 32421\n",
      "loss: 12.826374053955078\n",
      "steps per second: 0.55544\n",
      "step: 32422\n",
      "loss: 12.545742988586426\n",
      "steps per second: 0.52579\n",
      "step: 32423\n",
      "loss: 13.41918659210205\n",
      "steps per second: 0.53374\n",
      "step: 32424\n",
      "loss: 12.554400444030762\n",
      "steps per second: 0.50888\n",
      "step: 32425\n",
      "loss: 12.476826667785645\n",
      "steps per second: 0.55901\n",
      "step: 32426\n",
      "loss: 12.958781242370605\n",
      "steps per second: 0.52161\n",
      "step: 32427\n",
      "loss: 12.954154968261719\n",
      "steps per second: 0.52735\n",
      "step: 32428\n",
      "loss: 12.659348487854004\n",
      "steps per second: 0.51534\n",
      "step: 32429\n",
      "loss: 12.859209060668945\n",
      "steps per second: 0.50590\n",
      "step: 32430\n",
      "loss: 13.38723087310791\n",
      "steps per second: 0.53765\n",
      "step: 32431\n",
      "loss: 12.442059516906738\n",
      "steps per second: 0.58226\n",
      "step: 32432\n",
      "loss: 12.94416332244873\n",
      "steps per second: 0.50355\n",
      "step: 32433\n",
      "loss: 12.814851760864258\n",
      "steps per second: 0.51145\n",
      "step: 32434\n",
      "loss: 12.585977554321289\n",
      "steps per second: 0.54288\n",
      "step: 32435\n",
      "loss: 12.589137077331543\n",
      "steps per second: 0.52809\n",
      "step: 32436\n",
      "loss: 13.019631385803223\n",
      "steps per second: 0.48004\n",
      "step: 32437\n",
      "loss: 12.52479362487793\n",
      "steps per second: 0.52820\n",
      "step: 32438\n",
      "loss: 12.833112716674805\n",
      "steps per second: 0.48208\n",
      "step: 32439\n",
      "loss: 12.710582733154297\n",
      "steps per second: 0.51526\n",
      "step: 32440\n",
      "loss: 12.719160079956055\n",
      "steps per second: 0.52143\n",
      "step: 32441\n",
      "loss: 13.194730758666992\n",
      "steps per second: 0.53153\n",
      "step: 32442\n",
      "loss: 13.109355926513672\n",
      "steps per second: 0.58216\n",
      "step: 32443\n",
      "loss: 12.274977684020996\n",
      "steps per second: 0.50588\n",
      "step: 32444\n",
      "loss: 12.934758186340332\n",
      "steps per second: 0.50263\n",
      "step: 32445\n",
      "loss: 13.134294509887695\n",
      "steps per second: 0.56870\n",
      "step: 32446\n",
      "loss: 12.283613204956055\n",
      "steps per second: 0.52656\n",
      "step: 32447\n",
      "loss: 12.847620010375977\n",
      "steps per second: 0.54263\n",
      "step: 32448\n",
      "loss: 12.807577133178711\n",
      "steps per second: 0.54343\n",
      "step: 32449\n",
      "loss: 12.291821479797363\n",
      "steps per second: 0.54056\n",
      "step: 32450\n",
      "loss: 12.432369232177734\n",
      "steps per second: 0.52883\n",
      "step: 32451\n",
      "loss: 12.98503589630127\n",
      "steps per second: 0.54295\n",
      "step: 32452\n",
      "loss: 12.436951637268066\n",
      "steps per second: 0.53239\n",
      "step: 32453\n",
      "loss: 13.056659698486328\n",
      "steps per second: 0.52296\n",
      "step: 32454\n",
      "loss: 12.653983116149902\n",
      "steps per second: 0.50544\n",
      "step: 32455\n",
      "loss: 12.782625198364258\n",
      "steps per second: 0.52600\n",
      "step: 32456\n",
      "loss: 13.01870059967041\n",
      "steps per second: 0.54501\n",
      "step: 32457\n",
      "loss: 13.00210189819336\n",
      "steps per second: 0.58139\n",
      "step: 32458\n",
      "loss: 12.702909469604492\n",
      "steps per second: 0.51771\n",
      "step: 32459\n",
      "loss: 13.09049129486084\n",
      "steps per second: 0.51449\n",
      "step: 32460\n",
      "loss: 12.822802543640137\n",
      "steps per second: 0.48309\n",
      "step: 32461\n",
      "loss: 13.080941200256348\n",
      "steps per second: 0.52126\n",
      "step: 32462\n",
      "loss: 12.89844799041748\n",
      "steps per second: 0.54338\n",
      "step: 32463\n",
      "loss: 12.468962669372559\n",
      "steps per second: 0.55568\n",
      "step: 32464\n",
      "loss: 12.274554252624512\n",
      "steps per second: 0.53261\n",
      "step: 32465\n",
      "loss: 13.522505760192871\n",
      "steps per second: 0.52583\n",
      "step: 32466\n",
      "loss: 12.766519546508789\n",
      "steps per second: 0.52495\n",
      "step: 32467\n",
      "loss: 12.743927001953125\n",
      "steps per second: 0.58294\n",
      "step: 32468\n",
      "loss: 12.375624656677246\n",
      "steps per second: 0.46836\n",
      "step: 32469\n",
      "loss: 12.65970230102539\n",
      "steps per second: 0.55183\n",
      "step: 32470\n",
      "loss: 13.140718460083008\n",
      "steps per second: 0.55018\n",
      "step: 32471\n",
      "loss: 12.557332992553711\n",
      "steps per second: 0.53400\n",
      "step: 32472\n",
      "loss: 12.30363655090332\n",
      "steps per second: 0.51395\n",
      "step: 32473\n",
      "loss: 12.695643424987793\n",
      "steps per second: 0.53105\n",
      "step: 32474\n",
      "loss: 13.478096008300781\n",
      "steps per second: 0.52575\n",
      "step: 32475\n",
      "loss: 12.788394927978516\n",
      "steps per second: 0.53965\n",
      "step: 32476\n",
      "loss: 13.182697296142578\n",
      "steps per second: 0.58255\n",
      "step: 32477\n",
      "loss: 12.501774787902832\n",
      "steps per second: 0.49133\n",
      "step: 32478\n",
      "loss: 12.84553337097168\n",
      "steps per second: 0.51623\n",
      "step: 32479\n",
      "loss: 13.114790916442871\n",
      "steps per second: 0.50404\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8615204691886902, layer: 11\n",
      "saving at step 32479\n",
      "----------\n",
      "\n",
      "\n",
      "step: 32480\n",
      "loss: 12.86950969696045\n",
      "steps per second: 0.27072\n",
      "step: 32481\n",
      "loss: 12.916547775268555\n",
      "steps per second: 0.56408\n",
      "step: 32482\n",
      "loss: 12.643779754638672\n",
      "steps per second: 0.55303\n",
      "step: 32483\n",
      "loss: 13.248190879821777\n",
      "steps per second: 0.53331\n",
      "step: 32484\n",
      "loss: 12.459230422973633\n",
      "steps per second: 0.57431\n",
      "step: 32485\n",
      "loss: 13.072694778442383\n",
      "steps per second: 0.54997\n",
      "step: 32486\n",
      "loss: 12.660297393798828\n",
      "steps per second: 0.55576\n",
      "step: 32487\n",
      "loss: 13.012564659118652\n",
      "steps per second: 0.49996\n",
      "step: 32488\n",
      "loss: 13.148749351501465\n",
      "steps per second: 0.55239\n",
      "step: 32489\n",
      "loss: 12.519573211669922\n",
      "steps per second: 0.49213\n",
      "step: 32490\n",
      "loss: 13.03754997253418\n",
      "steps per second: 0.51658\n",
      "step: 32491\n",
      "loss: 12.837566375732422\n",
      "steps per second: 0.49841\n",
      "step: 32492\n",
      "loss: 12.35604476928711\n",
      "steps per second: 0.60537\n",
      "step: 32493\n",
      "loss: 12.44875717163086\n",
      "steps per second: 0.54708\n",
      "step: 32494\n",
      "loss: 12.683321952819824\n",
      "steps per second: 0.55598\n",
      "step: 32495\n",
      "loss: 13.612163543701172\n",
      "steps per second: 0.56812\n",
      "step: 32496\n",
      "loss: 12.030167579650879\n",
      "steps per second: 0.55362\n",
      "step: 32497\n",
      "loss: 13.175158500671387\n",
      "steps per second: 0.53951\n",
      "step: 32498\n",
      "loss: 12.739974021911621\n",
      "steps per second: 0.59189\n",
      "step: 32499\n",
      "loss: 13.084531784057617\n",
      "steps per second: 0.54375\n",
      "step: 32500\n",
      "loss: 12.949311256408691\n",
      "steps per second: 0.53790\n",
      "step: 32501\n",
      "loss: 12.963517189025879\n",
      "steps per second: 0.53337\n",
      "step: 32502\n",
      "loss: 12.57463264465332\n",
      "steps per second: 0.55405\n",
      "step: 32503\n",
      "loss: 12.910872459411621\n",
      "steps per second: 0.57133\n",
      "step: 32504\n",
      "loss: 13.44970703125\n",
      "steps per second: 0.57589\n",
      "step: 32505\n",
      "loss: 12.948332786560059\n",
      "steps per second: 0.55198\n",
      "step: 32506\n",
      "loss: 12.629748344421387\n",
      "steps per second: 0.54654\n",
      "step: 32507\n",
      "loss: 12.741318702697754\n",
      "steps per second: 0.56577\n",
      "step: 32508\n",
      "loss: 13.320502281188965\n",
      "steps per second: 0.56839\n",
      "step: 32509\n",
      "loss: 12.306086540222168\n",
      "steps per second: 0.57477\n",
      "step: 32510\n",
      "loss: 13.019951820373535\n",
      "steps per second: 0.56224\n",
      "step: 32511\n",
      "loss: 12.908754348754883\n",
      "steps per second: 0.60743\n",
      "step: 32512\n",
      "loss: 12.565193176269531\n",
      "steps per second: 0.52497\n",
      "step: 32513\n",
      "loss: 12.831727981567383\n",
      "steps per second: 0.48499\n",
      "step: 32514\n",
      "loss: 12.97421646118164\n",
      "steps per second: 0.53589\n",
      "step: 32515\n",
      "loss: 12.840401649475098\n",
      "steps per second: 0.49329\n",
      "step: 32516\n",
      "loss: 12.480381965637207\n",
      "steps per second: 0.54390\n",
      "step: 32517\n",
      "loss: 12.842215538024902\n",
      "steps per second: 0.55690\n",
      "step: 32518\n",
      "loss: 13.074477195739746\n",
      "steps per second: 0.51283\n",
      "step: 32519\n",
      "loss: 12.644919395446777\n",
      "steps per second: 0.53343\n",
      "step: 32520\n",
      "loss: 13.211414337158203\n",
      "steps per second: 0.55295\n",
      "step: 32521\n",
      "loss: 12.837669372558594\n",
      "steps per second: 0.53085\n",
      "step: 32522\n",
      "loss: 12.30111026763916\n",
      "steps per second: 0.50349\n",
      "step: 32523\n",
      "loss: 12.015252113342285\n",
      "steps per second: 0.58132\n",
      "step: 32524\n",
      "loss: 12.672770500183105\n",
      "steps per second: 0.55297\n",
      "step: 32525\n",
      "loss: 12.215622901916504\n",
      "steps per second: 0.49650\n",
      "step: 32526\n",
      "loss: 13.049742698669434\n",
      "steps per second: 0.51026\n",
      "step: 32527\n",
      "loss: 12.46104907989502\n",
      "steps per second: 0.50222\n",
      "step: 32528\n",
      "loss: 13.133611679077148\n",
      "steps per second: 0.53537\n",
      "step: 32529\n",
      "loss: 12.969403266906738\n",
      "steps per second: 0.52124\n",
      "step: 32530\n",
      "loss: 12.826969146728516\n",
      "steps per second: 0.53662\n",
      "step: 32531\n",
      "loss: 13.091010093688965\n",
      "steps per second: 0.53959\n",
      "step: 32532\n",
      "loss: 13.064042091369629\n",
      "steps per second: 0.52983\n",
      "step: 32533\n",
      "loss: 13.172987937927246\n",
      "steps per second: 0.53501\n",
      "step: 32534\n",
      "loss: 12.50259017944336\n",
      "steps per second: 0.52322\n",
      "step: 32535\n",
      "loss: 12.661258697509766\n",
      "steps per second: 0.49653\n",
      "step: 32536\n",
      "loss: 12.465747833251953\n",
      "steps per second: 0.52638\n",
      "step: 32537\n",
      "loss: 12.935051918029785\n",
      "steps per second: 0.57660\n",
      "step: 32538\n",
      "loss: 12.877340316772461\n",
      "steps per second: 0.59839\n",
      "step: 32539\n",
      "loss: 13.360123634338379\n",
      "steps per second: 0.55257\n",
      "step: 32540\n",
      "loss: 12.565759658813477\n",
      "steps per second: 0.50009\n",
      "step: 32541\n",
      "loss: 13.24787425994873\n",
      "steps per second: 0.53719\n",
      "step: 32542\n",
      "loss: 12.845056533813477\n",
      "steps per second: 0.54389\n",
      "step: 32543\n",
      "loss: 12.387494087219238\n",
      "steps per second: 0.52223\n",
      "step: 32544\n",
      "loss: 12.752226829528809\n",
      "steps per second: 0.49647\n",
      "step: 32545\n",
      "loss: 12.805109024047852\n",
      "steps per second: 0.51073\n",
      "step: 32546\n",
      "loss: 13.21896743774414\n",
      "steps per second: 0.54624\n",
      "step: 32547\n",
      "loss: 12.641858100891113\n",
      "steps per second: 0.54261\n",
      "step: 32548\n",
      "loss: 13.04522705078125\n",
      "steps per second: 0.51946\n",
      "step: 32549\n",
      "loss: 12.370199203491211\n",
      "steps per second: 0.52567\n",
      "step: 32550\n",
      "loss: 12.727002143859863\n",
      "steps per second: 0.53198\n",
      "step: 32551\n",
      "loss: 13.116150856018066\n",
      "steps per second: 0.51185\n",
      "step: 32552\n",
      "loss: 12.736661911010742\n",
      "steps per second: 0.53317\n",
      "step: 32553\n",
      "loss: 13.240110397338867\n",
      "steps per second: 0.52706\n",
      "step: 32554\n",
      "loss: 13.36667537689209\n",
      "steps per second: 0.55641\n",
      "step: 32555\n",
      "loss: 13.078189849853516\n",
      "steps per second: 0.55148\n",
      "step: 32556\n",
      "loss: 12.862841606140137\n",
      "steps per second: 0.51649\n",
      "step: 32557\n",
      "loss: 12.661808013916016\n",
      "steps per second: 0.51764\n",
      "step: 32558\n",
      "loss: 12.092033386230469\n",
      "steps per second: 0.47354\n",
      "step: 32559\n",
      "loss: 13.399211883544922\n",
      "steps per second: 0.51592\n",
      "step: 32560\n",
      "loss: 12.516304016113281\n",
      "steps per second: 0.52703\n",
      "step: 32561\n",
      "loss: 13.373167991638184\n",
      "steps per second: 0.53731\n",
      "step: 32562\n",
      "loss: 12.645767211914062\n",
      "steps per second: 0.54530\n",
      "step: 32563\n",
      "loss: 12.949390411376953\n",
      "steps per second: 0.54716\n",
      "step: 32564\n",
      "loss: 13.125129699707031\n",
      "steps per second: 0.53774\n",
      "step: 32565\n",
      "loss: 12.556324005126953\n",
      "steps per second: 0.53300\n",
      "step: 32566\n",
      "loss: 12.683032989501953\n",
      "steps per second: 0.50817\n",
      "step: 32567\n",
      "loss: 13.134400367736816\n",
      "steps per second: 0.55000\n",
      "step: 32568\n",
      "loss: 12.924003601074219\n",
      "steps per second: 0.51113\n",
      "step: 32569\n",
      "loss: 12.746450424194336\n",
      "steps per second: 0.52586\n",
      "step: 32570\n",
      "loss: 13.531034469604492\n",
      "steps per second: 0.51690\n",
      "step: 32571\n",
      "loss: 12.991153717041016\n",
      "steps per second: 0.53277\n",
      "step: 32572\n",
      "loss: 12.141759872436523\n",
      "steps per second: 0.54767\n",
      "step: 32573\n",
      "loss: 13.00119400024414\n",
      "steps per second: 0.52442\n",
      "step: 32574\n",
      "loss: 12.857070922851562\n",
      "steps per second: 0.51688\n",
      "step: 32575\n",
      "loss: 12.870179176330566\n",
      "steps per second: 0.58448\n",
      "step: 32576\n",
      "loss: 12.911518096923828\n",
      "steps per second: 0.52597\n",
      "step: 32577\n",
      "loss: 12.764089584350586\n",
      "steps per second: 0.51858\n",
      "step: 32578\n",
      "loss: 13.320338249206543\n",
      "steps per second: 0.55214\n",
      "step: 32579\n",
      "loss: 12.131038665771484\n",
      "steps per second: 0.55071\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7636151313781738, layer: 11\n",
      "saving at step 32579\n",
      "----------\n",
      "\n",
      "\n",
      "step: 32580\n",
      "loss: 13.032761573791504\n",
      "steps per second: 0.27649\n",
      "step: 32581\n",
      "loss: 12.817342758178711\n",
      "steps per second: 0.56239\n",
      "step: 32582\n",
      "loss: 12.914640426635742\n",
      "steps per second: 0.54070\n",
      "step: 32583\n",
      "loss: 12.682365417480469\n",
      "steps per second: 0.52725\n",
      "step: 32584\n",
      "loss: 12.931756973266602\n",
      "steps per second: 0.56624\n",
      "step: 32585\n",
      "loss: 12.337180137634277\n",
      "steps per second: 0.55858\n",
      "step: 32586\n",
      "loss: 12.720017433166504\n",
      "steps per second: 0.51561\n",
      "step: 32587\n",
      "loss: 12.585360527038574\n",
      "steps per second: 0.56428\n",
      "step: 32588\n",
      "loss: 13.017934799194336\n",
      "steps per second: 0.55609\n",
      "step: 32589\n",
      "loss: 13.218820571899414\n",
      "steps per second: 0.49040\n",
      "step: 32590\n",
      "loss: 13.174952507019043\n",
      "steps per second: 0.47017\n",
      "step: 32591\n",
      "loss: 13.274972915649414\n",
      "steps per second: 0.56168\n",
      "step: 32592\n",
      "loss: 12.530351638793945\n",
      "steps per second: 0.53965\n",
      "step: 32593\n",
      "loss: 12.840787887573242\n",
      "steps per second: 0.57704\n",
      "step: 32594\n",
      "loss: 13.506643295288086\n",
      "steps per second: 0.54624\n",
      "step: 32595\n",
      "loss: 12.645752906799316\n",
      "steps per second: 0.53215\n",
      "step: 32596\n",
      "loss: 12.578635215759277\n",
      "steps per second: 0.50805\n",
      "step: 32597\n",
      "loss: 13.102559089660645\n",
      "steps per second: 0.55273\n",
      "step: 32598\n",
      "loss: 13.132328987121582\n",
      "steps per second: 0.56509\n",
      "step: 32599\n",
      "loss: 12.84013557434082\n",
      "steps per second: 0.54144\n",
      "step: 32600\n",
      "loss: 12.985457420349121\n",
      "steps per second: 0.55514\n",
      "step: 32601\n",
      "loss: 12.67430305480957\n",
      "steps per second: 0.54686\n",
      "step: 32602\n",
      "loss: 12.817987442016602\n",
      "steps per second: 0.51993\n",
      "step: 32603\n",
      "loss: 12.5670747756958\n",
      "steps per second: 0.47304\n",
      "step: 32604\n",
      "loss: 12.501848220825195\n",
      "steps per second: 0.57866\n",
      "step: 32605\n",
      "loss: 13.063043594360352\n",
      "steps per second: 0.56526\n",
      "step: 32606\n",
      "loss: 13.513917922973633\n",
      "steps per second: 0.52326\n",
      "step: 32607\n",
      "loss: 12.697181701660156\n",
      "steps per second: 0.54159\n",
      "step: 32608\n",
      "loss: 12.967257499694824\n",
      "steps per second: 0.54149\n",
      "step: 32609\n",
      "loss: 12.531905174255371\n",
      "steps per second: 0.50995\n",
      "step: 32610\n",
      "loss: 12.71155834197998\n",
      "steps per second: 0.60377\n",
      "step: 32611\n",
      "loss: 13.175689697265625\n",
      "steps per second: 0.53875\n",
      "step: 32612\n",
      "loss: 12.611398696899414\n",
      "steps per second: 0.55645\n",
      "step: 32613\n",
      "loss: 12.549378395080566\n",
      "steps per second: 0.60033\n",
      "step: 32614\n",
      "loss: 12.615309715270996\n",
      "steps per second: 0.53816\n",
      "step: 32615\n",
      "loss: 12.502471923828125\n",
      "steps per second: 0.52550\n",
      "step: 32616\n",
      "loss: 12.841451644897461\n",
      "steps per second: 0.61010\n",
      "step: 32617\n",
      "loss: 12.71841812133789\n",
      "steps per second: 0.56009\n",
      "step: 32618\n",
      "loss: 12.578654289245605\n",
      "steps per second: 0.56317\n",
      "step: 32619\n",
      "loss: 12.704682350158691\n",
      "steps per second: 0.60255\n",
      "step: 32620\n",
      "loss: 13.014145851135254\n",
      "steps per second: 0.53060\n",
      "step: 32621\n",
      "loss: 12.721857070922852\n",
      "steps per second: 0.53668\n",
      "step: 32622\n",
      "loss: 13.25643539428711\n",
      "steps per second: 0.56948\n",
      "step: 32623\n",
      "loss: 13.195825576782227\n",
      "steps per second: 0.57899\n",
      "step: 32624\n",
      "loss: 12.871264457702637\n",
      "steps per second: 0.53737\n",
      "step: 32625\n",
      "loss: 13.251408576965332\n",
      "steps per second: 0.54195\n",
      "step: 32626\n",
      "loss: 12.45598030090332\n",
      "steps per second: 0.55174\n",
      "step: 32627\n",
      "loss: 12.598226547241211\n",
      "steps per second: 0.52299\n",
      "step: 32628\n",
      "loss: 12.957389831542969\n",
      "steps per second: 0.52212\n",
      "step: 32629\n",
      "loss: 13.100481033325195\n",
      "steps per second: 0.54156\n",
      "step: 32630\n",
      "loss: 12.860682487487793\n",
      "steps per second: 0.55125\n",
      "step: 32631\n",
      "loss: 13.01616382598877\n",
      "steps per second: 0.53736\n",
      "step: 32632\n",
      "loss: 12.276535987854004\n",
      "steps per second: 0.51963\n",
      "step: 32633\n",
      "loss: 12.822565078735352\n",
      "steps per second: 0.50025\n",
      "step: 32634\n",
      "loss: 12.742345809936523\n",
      "steps per second: 0.52927\n",
      "step: 32635\n",
      "loss: 12.811551094055176\n",
      "steps per second: 0.49149\n",
      "step: 32636\n",
      "loss: 12.875341415405273\n",
      "steps per second: 0.52886\n",
      "step: 32637\n",
      "loss: 12.646194458007812\n",
      "steps per second: 0.53761\n",
      "step: 32638\n",
      "loss: 12.79678726196289\n",
      "steps per second: 0.49785\n",
      "step: 32639\n",
      "loss: 12.271985054016113\n",
      "steps per second: 0.50837\n",
      "step: 32640\n",
      "loss: 12.945331573486328\n",
      "steps per second: 0.52882\n",
      "step: 32641\n",
      "loss: 12.830646514892578\n",
      "steps per second: 0.51676\n",
      "step: 32642\n",
      "loss: 12.968457221984863\n",
      "steps per second: 0.60198\n",
      "step: 32643\n",
      "loss: 12.856985092163086\n",
      "steps per second: 0.52566\n",
      "step: 32644\n",
      "loss: 13.032870292663574\n",
      "steps per second: 0.49433\n",
      "step: 32645\n",
      "loss: 13.357442855834961\n",
      "steps per second: 0.56663\n",
      "step: 32646\n",
      "loss: 12.803319931030273\n",
      "steps per second: 0.55884\n",
      "step: 32647\n",
      "loss: 12.332469940185547\n",
      "steps per second: 0.55295\n",
      "step: 32648\n",
      "loss: 12.275267601013184\n",
      "steps per second: 0.53486\n",
      "step: 32649\n",
      "loss: 13.145535469055176\n",
      "steps per second: 0.54827\n",
      "step: 32650\n",
      "loss: 13.0419340133667\n",
      "steps per second: 0.52622\n",
      "step: 32651\n",
      "loss: 12.864179611206055\n",
      "steps per second: 0.54609\n",
      "step: 32652\n",
      "loss: 13.09616756439209\n",
      "steps per second: 0.54199\n",
      "step: 32653\n",
      "loss: 13.168542861938477\n",
      "steps per second: 0.55129\n",
      "step: 32654\n",
      "loss: 12.514892578125\n",
      "steps per second: 0.56847\n",
      "step: 32655\n",
      "loss: 13.054632186889648\n",
      "steps per second: 0.53398\n",
      "step: 32656\n",
      "loss: 12.325543403625488\n",
      "steps per second: 0.54967\n",
      "step: 32657\n",
      "loss: 12.634947776794434\n",
      "steps per second: 0.55093\n",
      "step: 32658\n",
      "loss: 12.476777076721191\n",
      "steps per second: 0.53016\n",
      "step: 32659\n",
      "loss: 12.574061393737793\n",
      "steps per second: 0.54996\n",
      "step: 32660\n",
      "loss: 12.680727005004883\n",
      "steps per second: 0.52310\n",
      "step: 32661\n",
      "loss: 12.87984848022461\n",
      "steps per second: 0.54783\n",
      "step: 32662\n",
      "loss: 12.692237854003906\n",
      "steps per second: 0.60928\n",
      "step: 32663\n",
      "loss: 13.349430084228516\n",
      "steps per second: 0.57145\n",
      "step: 32664\n",
      "loss: 13.18820858001709\n",
      "steps per second: 0.54812\n",
      "step: 32665\n",
      "loss: 12.57565975189209\n",
      "steps per second: 0.60752\n",
      "step: 32666\n",
      "loss: 12.674681663513184\n",
      "steps per second: 0.55621\n",
      "step: 32667\n",
      "loss: 12.394779205322266\n",
      "steps per second: 0.56038\n",
      "step: 32668\n",
      "loss: 12.892868995666504\n",
      "steps per second: 0.55120\n",
      "step: 32669\n",
      "loss: 13.097999572753906\n",
      "steps per second: 0.55625\n",
      "step: 32670\n",
      "loss: 12.707915306091309\n",
      "steps per second: 0.55435\n",
      "step: 32671\n",
      "loss: 13.08044147491455\n",
      "steps per second: 0.55440\n",
      "step: 32672\n",
      "loss: 12.215521812438965\n",
      "steps per second: 0.56855\n",
      "step: 32673\n",
      "loss: 12.74595832824707\n",
      "steps per second: 0.60065\n",
      "step: 32674\n",
      "loss: 13.518345832824707\n",
      "steps per second: 0.50445\n",
      "step: 32675\n",
      "loss: 12.811921119689941\n",
      "steps per second: 0.52518\n",
      "step: 32676\n",
      "loss: 12.675777435302734\n",
      "steps per second: 0.54727\n",
      "step: 32677\n",
      "loss: 12.661060333251953\n",
      "steps per second: 0.55084\n",
      "step: 32678\n",
      "loss: 12.485780715942383\n",
      "steps per second: 0.57149\n",
      "step: 32679\n",
      "loss: 12.835153579711914\n",
      "steps per second: 0.57087\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8586578965187073, layer: 11\n",
      "saving at step 32679\n",
      "----------\n",
      "\n",
      "\n",
      "step: 32680\n",
      "loss: 12.975252151489258\n",
      "steps per second: 0.28644\n",
      "step: 32681\n",
      "loss: 13.022573471069336\n",
      "steps per second: 0.53363\n",
      "step: 32682\n",
      "loss: 12.777287483215332\n",
      "steps per second: 0.52918\n",
      "step: 32683\n",
      "loss: 13.126514434814453\n",
      "steps per second: 0.57088\n",
      "step: 32684\n",
      "loss: 12.772777557373047\n",
      "steps per second: 0.56860\n",
      "step: 32685\n",
      "loss: 11.78685188293457\n",
      "steps per second: 0.54374\n",
      "step: 32686\n",
      "loss: 12.440135955810547\n",
      "steps per second: 0.55611\n",
      "step: 32687\n",
      "loss: 12.74950122833252\n",
      "steps per second: 0.51230\n",
      "step: 32688\n",
      "loss: 13.366765975952148\n",
      "steps per second: 0.56990\n",
      "step: 32689\n",
      "loss: 12.886245727539062\n",
      "steps per second: 0.60234\n",
      "step: 32690\n",
      "loss: 12.838923454284668\n",
      "steps per second: 0.53260\n",
      "step: 32691\n",
      "loss: 12.692203521728516\n",
      "steps per second: 0.49241\n",
      "step: 32692\n",
      "loss: 12.495140075683594\n",
      "steps per second: 0.53177\n",
      "step: 32693\n",
      "loss: 13.049845695495605\n",
      "steps per second: 0.54625\n",
      "step: 32694\n",
      "loss: 13.212648391723633\n",
      "steps per second: 0.53592\n",
      "step: 32695\n",
      "loss: 12.384342193603516\n",
      "steps per second: 0.52840\n",
      "step: 32696\n",
      "loss: 12.188464164733887\n",
      "steps per second: 0.55456\n",
      "step: 32697\n",
      "loss: 13.104208946228027\n",
      "steps per second: 0.53257\n",
      "step: 32698\n",
      "loss: 12.603004455566406\n",
      "steps per second: 0.57189\n",
      "step: 32699\n",
      "loss: 13.287280082702637\n",
      "steps per second: 0.53831\n",
      "step: 32700\n",
      "loss: 13.0852689743042\n",
      "steps per second: 0.53276\n",
      "step: 32701\n",
      "loss: 12.586343765258789\n",
      "steps per second: 0.56912\n",
      "step: 32702\n",
      "loss: 12.47189712524414\n",
      "steps per second: 0.56657\n",
      "step: 32703\n",
      "loss: 12.22731876373291\n",
      "steps per second: 0.56505\n",
      "step: 32704\n",
      "loss: 12.945548057556152\n",
      "steps per second: 0.53420\n",
      "step: 32705\n",
      "loss: 13.232977867126465\n",
      "steps per second: 0.57252\n",
      "step: 32706\n",
      "loss: 12.693519592285156\n",
      "steps per second: 0.52532\n",
      "step: 32707\n",
      "loss: 12.346853256225586\n",
      "steps per second: 0.56630\n",
      "step: 32708\n",
      "loss: 13.461369514465332\n",
      "steps per second: 0.54822\n",
      "step: 32709\n",
      "loss: 13.258813858032227\n",
      "steps per second: 0.55417\n",
      "step: 32710\n",
      "loss: 12.71544361114502\n",
      "steps per second: 0.56530\n",
      "step: 32711\n",
      "loss: 12.671185493469238\n",
      "steps per second: 0.52128\n",
      "step: 32712\n",
      "loss: 12.402154922485352\n",
      "steps per second: 0.52182\n",
      "step: 32713\n",
      "loss: 12.50319766998291\n",
      "steps per second: 0.56390\n",
      "step: 32714\n",
      "loss: 12.938909530639648\n",
      "steps per second: 0.54168\n",
      "step: 32715\n",
      "loss: 12.835172653198242\n",
      "steps per second: 0.53840\n",
      "step: 32716\n",
      "loss: 12.978654861450195\n",
      "steps per second: 0.53451\n",
      "step: 32717\n",
      "loss: 12.721635818481445\n",
      "steps per second: 0.51117\n",
      "step: 32718\n",
      "loss: 12.757525444030762\n",
      "steps per second: 0.49487\n",
      "step: 32719\n",
      "loss: 12.693615913391113\n",
      "steps per second: 0.53267\n",
      "step: 32720\n",
      "loss: 12.946723937988281\n",
      "steps per second: 0.56158\n",
      "step: 32721\n",
      "loss: 12.441861152648926\n",
      "steps per second: 0.57522\n",
      "step: 32722\n",
      "loss: 12.667659759521484\n",
      "steps per second: 0.53895\n",
      "step: 32723\n",
      "loss: 12.161322593688965\n",
      "steps per second: 0.57523\n",
      "step: 32724\n",
      "loss: 12.781841278076172\n",
      "steps per second: 0.60816\n",
      "step: 32725\n",
      "loss: 13.29158878326416\n",
      "steps per second: 0.60889\n",
      "step: 32726\n",
      "loss: 12.973584175109863\n",
      "steps per second: 0.56524\n",
      "step: 32727\n",
      "loss: 13.25843620300293\n",
      "steps per second: 0.53510\n",
      "step: 32728\n",
      "loss: 12.862556457519531\n",
      "steps per second: 0.55058\n",
      "step: 32729\n",
      "loss: 12.583906173706055\n",
      "steps per second: 0.54344\n",
      "step: 32730\n",
      "loss: 13.251338005065918\n",
      "steps per second: 0.55157\n",
      "step: 32731\n",
      "loss: 13.071321487426758\n",
      "steps per second: 0.60446\n",
      "step: 32732\n",
      "loss: 13.222354888916016\n",
      "steps per second: 0.56778\n",
      "step: 32733\n",
      "loss: 12.679582595825195\n",
      "steps per second: 0.54865\n",
      "step: 32734\n",
      "loss: 12.092001914978027\n",
      "steps per second: 0.53689\n",
      "step: 32735\n",
      "loss: 12.995177268981934\n",
      "steps per second: 0.53396\n",
      "step: 32736\n",
      "loss: 12.537982940673828\n",
      "steps per second: 0.54731\n",
      "step: 32737\n",
      "loss: 12.852865219116211\n",
      "steps per second: 0.55143\n",
      "step: 32738\n",
      "loss: 13.015175819396973\n",
      "steps per second: 0.52468\n",
      "step: 32739\n",
      "loss: 12.933565139770508\n",
      "steps per second: 0.55606\n",
      "step: 32740\n",
      "loss: 12.305136680603027\n",
      "steps per second: 0.52373\n",
      "step: 32741\n",
      "loss: 12.7673921585083\n",
      "steps per second: 0.52507\n",
      "step: 32742\n",
      "loss: 13.277593612670898\n",
      "steps per second: 0.54811\n",
      "step: 32743\n",
      "loss: 12.988269805908203\n",
      "steps per second: 0.52354\n",
      "step: 32744\n",
      "loss: 13.387039184570312\n",
      "steps per second: 0.60680\n",
      "step: 32745\n",
      "loss: 12.66859245300293\n",
      "steps per second: 0.54993\n",
      "step: 32746\n",
      "loss: 13.031479835510254\n",
      "steps per second: 0.57565\n",
      "step: 32747\n",
      "loss: 12.614435195922852\n",
      "steps per second: 0.56782\n",
      "step: 32748\n",
      "loss: 12.921611785888672\n",
      "steps per second: 0.57769\n",
      "step: 32749\n",
      "loss: 12.430509567260742\n",
      "steps per second: 0.57101\n",
      "step: 32750\n",
      "loss: 12.877111434936523\n",
      "steps per second: 0.54524\n",
      "step: 32751\n",
      "loss: 13.221010208129883\n",
      "steps per second: 0.53374\n",
      "step: 32752\n",
      "loss: 12.493025779724121\n",
      "steps per second: 0.58527\n",
      "step: 32753\n",
      "loss: 12.860782623291016\n",
      "steps per second: 0.55496\n",
      "step: 32754\n",
      "loss: 13.485933303833008\n",
      "steps per second: 0.55230\n",
      "step: 32755\n",
      "loss: 12.844927787780762\n",
      "steps per second: 0.49300\n",
      "step: 32756\n",
      "loss: 12.874507904052734\n",
      "steps per second: 0.55020\n",
      "step: 32757\n",
      "loss: 12.907177925109863\n",
      "steps per second: 0.52667\n",
      "step: 32758\n",
      "loss: 13.024503707885742\n",
      "steps per second: 0.56703\n",
      "step: 32759\n",
      "loss: 13.076700210571289\n",
      "steps per second: 0.53418\n",
      "step: 32760\n",
      "loss: 12.736074447631836\n",
      "steps per second: 0.56761\n",
      "step: 32761\n",
      "loss: 12.59111499786377\n",
      "steps per second: 0.53292\n",
      "step: 32762\n",
      "loss: 13.327337265014648\n",
      "steps per second: 0.57079\n",
      "step: 32763\n",
      "loss: 12.856892585754395\n",
      "steps per second: 0.53789\n",
      "step: 32764\n",
      "loss: 13.186476707458496\n",
      "steps per second: 0.57660\n",
      "step: 32765\n",
      "loss: 12.788074493408203\n",
      "steps per second: 0.57385\n",
      "step: 32766\n",
      "loss: 12.39993667602539\n",
      "steps per second: 0.55265\n",
      "step: 32767\n",
      "loss: 12.68310260772705\n",
      "steps per second: 0.55511\n",
      "step: 32768\n",
      "loss: 13.363933563232422\n",
      "steps per second: 0.49835\n",
      "step: 32769\n",
      "loss: 12.886737823486328\n",
      "steps per second: 0.53691\n",
      "step: 32770\n",
      "loss: 12.56628704071045\n",
      "steps per second: 0.54953\n",
      "step: 32771\n",
      "loss: 13.613996505737305\n",
      "steps per second: 0.54012\n",
      "step: 32772\n",
      "loss: 12.825843811035156\n",
      "steps per second: 0.57738\n",
      "step: 32773\n",
      "loss: 13.02480697631836\n",
      "steps per second: 0.57643\n",
      "step: 32774\n",
      "loss: 12.878889083862305\n",
      "steps per second: 0.54076\n",
      "step: 32775\n",
      "loss: 12.607442855834961\n",
      "steps per second: 0.52396\n",
      "step: 32776\n",
      "loss: 12.653636932373047\n",
      "steps per second: 0.60452\n",
      "step: 32777\n",
      "loss: 12.807271003723145\n",
      "steps per second: 0.55263\n",
      "step: 32778\n",
      "loss: 12.299490928649902\n",
      "steps per second: 0.55552\n",
      "step: 32779\n",
      "loss: 12.61854362487793\n",
      "steps per second: 0.54783\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8023622632026672, layer: 11\n",
      "saving at step 32779\n",
      "----------\n",
      "\n",
      "\n",
      "step: 32780\n",
      "loss: 13.182767868041992\n",
      "steps per second: 0.27600\n",
      "step: 32781\n",
      "loss: 12.71888256072998\n",
      "steps per second: 0.55579\n",
      "step: 32782\n",
      "loss: 12.415390968322754\n",
      "steps per second: 0.53488\n",
      "step: 32783\n",
      "loss: 12.877233505249023\n",
      "steps per second: 0.56754\n",
      "step: 32784\n",
      "loss: 13.051126480102539\n",
      "steps per second: 0.53267\n",
      "step: 32785\n",
      "loss: 12.634892463684082\n",
      "steps per second: 0.55739\n",
      "step: 32786\n",
      "loss: 12.903257369995117\n",
      "steps per second: 0.54136\n",
      "step: 32787\n",
      "loss: 12.845528602600098\n",
      "steps per second: 0.55035\n",
      "step: 32788\n",
      "loss: 12.44575309753418\n",
      "steps per second: 0.54876\n",
      "step: 32789\n",
      "loss: 12.491809844970703\n",
      "steps per second: 0.55047\n",
      "step: 32790\n",
      "loss: 12.8983736038208\n",
      "steps per second: 0.54567\n",
      "step: 32791\n",
      "loss: 12.237582206726074\n",
      "steps per second: 0.57388\n",
      "step: 32792\n",
      "loss: 12.901409149169922\n",
      "steps per second: 0.60786\n",
      "step: 32793\n",
      "loss: 12.755175590515137\n",
      "steps per second: 0.58387\n",
      "step: 32794\n",
      "loss: 13.365306854248047\n",
      "steps per second: 0.54719\n",
      "step: 32795\n",
      "loss: 12.750336647033691\n",
      "steps per second: 0.55260\n",
      "step: 32796\n",
      "loss: 13.51008129119873\n",
      "steps per second: 0.56449\n",
      "step: 32797\n",
      "loss: 13.121070861816406\n",
      "steps per second: 0.51718\n",
      "step: 32798\n",
      "loss: 13.164703369140625\n",
      "steps per second: 0.51116\n",
      "step: 32799\n",
      "loss: 12.920513153076172\n",
      "steps per second: 0.54089\n",
      "step: 32800\n",
      "loss: 12.35143756866455\n",
      "steps per second: 0.56900\n",
      "step: 32801\n",
      "loss: 13.100319862365723\n",
      "steps per second: 0.54267\n",
      "step: 32802\n",
      "loss: 12.485681533813477\n",
      "steps per second: 0.53618\n",
      "step: 32803\n",
      "loss: 12.582428932189941\n",
      "steps per second: 0.53225\n",
      "step: 32804\n",
      "loss: 12.209683418273926\n",
      "steps per second: 0.54238\n",
      "step: 32805\n",
      "loss: 12.765482902526855\n",
      "steps per second: 0.54928\n",
      "step: 32806\n",
      "loss: 12.646538734436035\n",
      "steps per second: 0.56468\n",
      "step: 32807\n",
      "loss: 13.216727256774902\n",
      "steps per second: 0.54321\n",
      "step: 32808\n",
      "loss: 13.293465614318848\n",
      "steps per second: 0.56917\n",
      "step: 32809\n",
      "loss: 12.903119087219238\n",
      "steps per second: 0.52280\n",
      "step: 32810\n",
      "loss: 12.53836727142334\n",
      "steps per second: 0.55753\n",
      "step: 32811\n",
      "loss: 12.903992652893066\n",
      "steps per second: 0.53549\n",
      "step: 32812\n",
      "loss: 12.610152244567871\n",
      "steps per second: 0.55322\n",
      "step: 32813\n",
      "loss: 12.808510780334473\n",
      "steps per second: 0.52640\n",
      "step: 32814\n",
      "loss: 12.91849136352539\n",
      "steps per second: 0.54817\n",
      "step: 32815\n",
      "loss: 12.988018989562988\n",
      "steps per second: 0.59412\n",
      "step: 32816\n",
      "loss: 12.79774284362793\n",
      "steps per second: 0.53428\n",
      "step: 32817\n",
      "loss: 12.551750183105469\n",
      "steps per second: 0.55756\n",
      "step: 32818\n",
      "loss: 12.83199405670166\n",
      "steps per second: 0.56157\n",
      "step: 32819\n",
      "loss: 12.869834899902344\n",
      "steps per second: 0.53355\n",
      "step: 32820\n",
      "loss: 12.999281883239746\n",
      "steps per second: 0.51909\n",
      "step: 32821\n",
      "loss: 12.4573974609375\n",
      "steps per second: 0.54160\n",
      "step: 32822\n",
      "loss: 12.905475616455078\n",
      "steps per second: 0.52329\n",
      "step: 32823\n",
      "loss: 12.63216781616211\n",
      "steps per second: 0.56143\n",
      "step: 32824\n",
      "loss: 12.966732025146484\n",
      "steps per second: 0.60991\n",
      "step: 32825\n",
      "loss: 12.340755462646484\n",
      "steps per second: 0.52558\n",
      "step: 32826\n",
      "loss: 12.87376880645752\n",
      "steps per second: 0.54855\n",
      "step: 32827\n",
      "loss: 12.721318244934082\n",
      "steps per second: 0.52431\n",
      "step: 32828\n",
      "loss: 12.742175102233887\n",
      "steps per second: 0.56816\n",
      "step: 32829\n",
      "loss: 12.858179092407227\n",
      "steps per second: 0.56705\n",
      "step: 32830\n",
      "loss: 12.822784423828125\n",
      "steps per second: 0.51670\n",
      "step: 32831\n",
      "loss: 12.267742156982422\n",
      "steps per second: 0.60894\n",
      "step: 32832\n",
      "loss: 12.76793384552002\n",
      "steps per second: 0.53412\n",
      "step: 32833\n",
      "loss: 12.618642807006836\n",
      "steps per second: 0.52164\n",
      "step: 32834\n",
      "loss: 12.342852592468262\n",
      "steps per second: 0.54819\n",
      "step: 32835\n",
      "loss: 12.801575660705566\n",
      "steps per second: 0.54465\n",
      "step: 32836\n",
      "loss: 12.523442268371582\n",
      "steps per second: 0.52298\n",
      "step: 32837\n",
      "loss: 12.51419448852539\n",
      "steps per second: 0.61235\n",
      "step: 32838\n",
      "loss: 13.360589027404785\n",
      "steps per second: 0.53594\n",
      "step: 32839\n",
      "loss: 13.230239868164062\n",
      "steps per second: 0.53490\n",
      "step: 32840\n",
      "loss: 12.521261215209961\n",
      "steps per second: 0.56855\n",
      "step: 32841\n",
      "loss: 12.838438034057617\n",
      "steps per second: 0.53945\n",
      "step: 32842\n",
      "loss: 12.25330638885498\n",
      "steps per second: 0.48309\n",
      "step: 32843\n",
      "loss: 12.418368339538574\n",
      "steps per second: 0.53124\n",
      "step: 32844\n",
      "loss: 12.183466911315918\n",
      "steps per second: 0.56342\n",
      "step: 32845\n",
      "loss: 13.149712562561035\n",
      "steps per second: 0.54598\n",
      "step: 32846\n",
      "loss: 13.131199836730957\n",
      "steps per second: 0.55709\n",
      "step: 32847\n",
      "loss: 12.95291519165039\n",
      "steps per second: 0.53727\n",
      "step: 32848\n",
      "loss: 12.840998649597168\n",
      "steps per second: 0.55953\n",
      "step: 32849\n",
      "loss: 13.038896560668945\n",
      "steps per second: 0.54180\n",
      "step: 32850\n",
      "loss: 12.419452667236328\n",
      "steps per second: 0.53714\n",
      "step: 32851\n",
      "loss: 13.294533729553223\n",
      "steps per second: 0.61000\n",
      "step: 32852\n",
      "loss: 12.635546684265137\n",
      "steps per second: 0.56510\n",
      "step: 32853\n",
      "loss: 12.545083999633789\n",
      "steps per second: 0.57593\n",
      "step: 32854\n",
      "loss: 12.699517250061035\n",
      "steps per second: 0.53487\n",
      "step: 32855\n",
      "loss: 13.18842601776123\n",
      "steps per second: 0.56922\n",
      "step: 32856\n",
      "loss: 13.066694259643555\n",
      "steps per second: 0.55592\n",
      "step: 32857\n",
      "loss: 12.392900466918945\n",
      "steps per second: 0.54523\n",
      "step: 32858\n",
      "loss: 12.867711067199707\n",
      "steps per second: 0.57046\n",
      "step: 32859\n",
      "loss: 12.551679611206055\n",
      "steps per second: 0.56683\n",
      "step: 32860\n",
      "loss: 12.640460968017578\n",
      "steps per second: 0.54934\n",
      "step: 32861\n",
      "loss: 13.070684432983398\n",
      "steps per second: 0.57854\n",
      "step: 32862\n",
      "loss: 13.417330741882324\n",
      "steps per second: 0.50976\n",
      "step: 32863\n",
      "loss: 12.700494766235352\n",
      "steps per second: 0.52108\n",
      "step: 32864\n",
      "loss: 12.924840927124023\n",
      "steps per second: 0.54262\n",
      "step: 32865\n",
      "loss: 12.419989585876465\n",
      "steps per second: 0.55666\n",
      "step: 32866\n",
      "loss: 12.541422843933105\n",
      "steps per second: 0.52327\n",
      "step: 32867\n",
      "loss: 12.546518325805664\n",
      "steps per second: 0.50065\n",
      "step: 32868\n",
      "loss: 13.345678329467773\n",
      "steps per second: 0.56696\n",
      "step: 32869\n",
      "loss: 12.4000825881958\n",
      "steps per second: 0.54186\n",
      "step: 32870\n",
      "loss: 12.632099151611328\n",
      "steps per second: 0.49138\n",
      "step: 32871\n",
      "loss: 12.691339492797852\n",
      "steps per second: 0.50794\n",
      "step: 32872\n",
      "loss: 13.316374778747559\n",
      "steps per second: 0.53358\n",
      "step: 32873\n",
      "loss: 13.718209266662598\n",
      "steps per second: 0.56934\n",
      "step: 32874\n",
      "loss: 12.860529899597168\n",
      "steps per second: 0.55045\n",
      "step: 32875\n",
      "loss: 13.015822410583496\n",
      "steps per second: 0.55649\n",
      "step: 32876\n",
      "loss: 12.6398286819458\n",
      "steps per second: 0.55746\n",
      "step: 32877\n",
      "loss: 12.759255409240723\n",
      "steps per second: 0.53357\n",
      "step: 32878\n",
      "loss: 13.073358535766602\n",
      "steps per second: 0.56629\n",
      "step: 32879\n",
      "loss: 13.501401901245117\n",
      "steps per second: 0.54165\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.901572585105896, layer: 11\n",
      "saving at step 32879\n",
      "----------\n",
      "\n",
      "\n",
      "step: 32880\n",
      "loss: 12.772257804870605\n",
      "steps per second: 0.28652\n",
      "step: 32881\n",
      "loss: 12.143424987792969\n",
      "steps per second: 0.56739\n",
      "step: 32882\n",
      "loss: 12.661124229431152\n",
      "steps per second: 0.54938\n",
      "step: 32883\n",
      "loss: 12.621838569641113\n",
      "steps per second: 0.53677\n",
      "step: 32884\n",
      "loss: 12.969963073730469\n",
      "steps per second: 0.53489\n",
      "step: 32885\n",
      "loss: 12.902626037597656\n",
      "steps per second: 0.56974\n",
      "step: 32886\n",
      "loss: 12.749469757080078\n",
      "steps per second: 0.52833\n",
      "step: 32887\n",
      "loss: 12.896746635437012\n",
      "steps per second: 0.57699\n",
      "step: 32888\n",
      "loss: 13.244211196899414\n",
      "steps per second: 0.56996\n",
      "step: 32889\n",
      "loss: 12.488394737243652\n",
      "steps per second: 0.55429\n",
      "step: 32890\n",
      "loss: 12.83531665802002\n",
      "steps per second: 0.52371\n",
      "step: 32891\n",
      "loss: 12.799979209899902\n",
      "steps per second: 0.55407\n",
      "step: 32892\n",
      "loss: 13.098465919494629\n",
      "steps per second: 0.55628\n",
      "step: 32893\n",
      "loss: 12.813050270080566\n",
      "steps per second: 0.52417\n",
      "step: 32894\n",
      "loss: 13.114559173583984\n",
      "steps per second: 0.53416\n",
      "step: 32895\n",
      "loss: 13.052726745605469\n",
      "steps per second: 0.52115\n",
      "step: 32896\n",
      "loss: 12.82707405090332\n",
      "steps per second: 0.51039\n",
      "step: 32897\n",
      "loss: 12.713862419128418\n",
      "steps per second: 0.56941\n",
      "step: 32898\n",
      "loss: 12.7052583694458\n",
      "steps per second: 0.52375\n",
      "step: 32899\n",
      "loss: 12.720098495483398\n",
      "steps per second: 0.60631\n",
      "step: 32900\n",
      "loss: 13.187967300415039\n",
      "steps per second: 0.53568\n",
      "step: 32901\n",
      "loss: 12.95631217956543\n",
      "steps per second: 0.55600\n",
      "step: 32902\n",
      "loss: 12.595306396484375\n",
      "steps per second: 0.52376\n",
      "step: 32903\n",
      "loss: 13.018919944763184\n",
      "steps per second: 0.51117\n",
      "step: 32904\n",
      "loss: 13.30916690826416\n",
      "steps per second: 0.55603\n",
      "step: 32905\n",
      "loss: 12.566551208496094\n",
      "steps per second: 0.55704\n",
      "step: 32906\n",
      "loss: 12.604156494140625\n",
      "steps per second: 0.60598\n",
      "step: 32907\n",
      "loss: 12.973690032958984\n",
      "steps per second: 0.57865\n",
      "step: 32908\n",
      "loss: 12.923194885253906\n",
      "steps per second: 0.55614\n",
      "step: 32909\n",
      "loss: 13.378569602966309\n",
      "steps per second: 0.48876\n",
      "step: 32910\n",
      "loss: 12.610269546508789\n",
      "steps per second: 0.54497\n",
      "step: 32911\n",
      "loss: 13.138476371765137\n",
      "steps per second: 0.53517\n",
      "step: 32912\n",
      "loss: 12.561874389648438\n",
      "steps per second: 0.55507\n",
      "step: 32913\n",
      "loss: 13.069116592407227\n",
      "steps per second: 0.54870\n",
      "step: 32914\n",
      "loss: 13.361480712890625\n",
      "steps per second: 0.55057\n",
      "step: 32915\n",
      "loss: 12.553224563598633\n",
      "steps per second: 0.55638\n",
      "step: 32916\n",
      "loss: 12.788572311401367\n",
      "steps per second: 0.53447\n",
      "step: 32917\n",
      "loss: 12.836183547973633\n",
      "steps per second: 0.55030\n",
      "step: 32918\n",
      "loss: 12.765559196472168\n",
      "steps per second: 0.60911\n",
      "step: 32919\n",
      "loss: 12.62627124786377\n",
      "steps per second: 0.57610\n",
      "step: 32920\n",
      "loss: 12.746094703674316\n",
      "steps per second: 0.56772\n",
      "step: 32921\n",
      "loss: 12.755331039428711\n",
      "steps per second: 0.51821\n",
      "step: 32922\n",
      "loss: 12.56151294708252\n",
      "steps per second: 0.56474\n",
      "step: 32923\n",
      "loss: 13.211642265319824\n",
      "steps per second: 0.58194\n",
      "step: 32924\n",
      "loss: 12.477577209472656\n",
      "steps per second: 0.55515\n",
      "step: 32925\n",
      "loss: 12.886617660522461\n",
      "steps per second: 0.56898\n",
      "step: 32926\n",
      "loss: 12.77767276763916\n",
      "steps per second: 0.55018\n",
      "step: 32927\n",
      "loss: 12.41301155090332\n",
      "steps per second: 0.55556\n",
      "step: 32928\n",
      "loss: 12.616060256958008\n",
      "steps per second: 0.56135\n",
      "step: 32929\n",
      "loss: 12.280350685119629\n",
      "steps per second: 0.51989\n",
      "step: 32930\n",
      "loss: 12.706730842590332\n",
      "steps per second: 0.54860\n",
      "step: 32931\n",
      "loss: 11.96985149383545\n",
      "steps per second: 0.56710\n",
      "step: 32932\n",
      "loss: 12.9388427734375\n",
      "steps per second: 0.46940\n",
      "step: 32933\n",
      "loss: 12.10203742980957\n",
      "steps per second: 0.55668\n",
      "step: 32934\n",
      "loss: 13.274154663085938\n",
      "steps per second: 0.52554\n",
      "step: 32935\n",
      "loss: 13.298975944519043\n",
      "steps per second: 0.54211\n",
      "step: 32936\n",
      "loss: 13.23188304901123\n",
      "steps per second: 0.49024\n",
      "step: 32937\n",
      "loss: 13.360494613647461\n",
      "steps per second: 0.49838\n",
      "step: 32938\n",
      "loss: 12.225783348083496\n",
      "steps per second: 0.60696\n",
      "step: 32939\n",
      "loss: 12.287461280822754\n",
      "steps per second: 0.54913\n",
      "step: 32940\n",
      "loss: 12.629143714904785\n",
      "steps per second: 0.51002\n",
      "step: 32941\n",
      "loss: 12.686271667480469\n",
      "steps per second: 0.60801\n",
      "step: 32942\n",
      "loss: 12.8229398727417\n",
      "steps per second: 0.55699\n",
      "step: 32943\n",
      "loss: 12.664860725402832\n",
      "steps per second: 0.54691\n",
      "step: 32944\n",
      "loss: 12.71222972869873\n",
      "steps per second: 0.54057\n",
      "step: 32945\n",
      "loss: 12.178343772888184\n",
      "steps per second: 0.56659\n",
      "step: 32946\n",
      "loss: 12.423898696899414\n",
      "steps per second: 0.50773\n",
      "step: 32947\n",
      "loss: 12.808826446533203\n",
      "steps per second: 0.53384\n",
      "step: 32948\n",
      "loss: 12.101764678955078\n",
      "steps per second: 0.49704\n",
      "step: 32949\n",
      "loss: 12.79813289642334\n",
      "steps per second: 0.55680\n",
      "step: 32950\n",
      "loss: 12.282160758972168\n",
      "steps per second: 0.54863\n",
      "step: 32951\n",
      "loss: 12.618269920349121\n",
      "steps per second: 0.55723\n",
      "step: 32952\n",
      "loss: 12.837008476257324\n",
      "steps per second: 0.53341\n",
      "step: 32953\n",
      "loss: 12.967199325561523\n",
      "steps per second: 0.54130\n",
      "step: 32954\n",
      "loss: 13.239007949829102\n",
      "steps per second: 0.52266\n",
      "step: 32955\n",
      "loss: 13.408491134643555\n",
      "steps per second: 0.60221\n",
      "step: 32956\n",
      "loss: 12.743583679199219\n",
      "steps per second: 0.54684\n",
      "step: 32957\n",
      "loss: 12.42496109008789\n",
      "steps per second: 0.56822\n",
      "step: 32958\n",
      "loss: 12.722564697265625\n",
      "steps per second: 0.54079\n",
      "step: 32959\n",
      "loss: 12.98631763458252\n",
      "steps per second: 0.53093\n",
      "step: 32960\n",
      "loss: 12.866414070129395\n",
      "steps per second: 0.55035\n",
      "step: 32961\n",
      "loss: 12.803366661071777\n",
      "steps per second: 0.52326\n",
      "step: 32962\n",
      "loss: 13.632855415344238\n",
      "steps per second: 0.54884\n",
      "step: 32963\n",
      "loss: 12.594072341918945\n",
      "steps per second: 0.54643\n",
      "step: 32964\n",
      "loss: 12.90217113494873\n",
      "steps per second: 0.53304\n",
      "step: 32965\n",
      "loss: 12.918081283569336\n",
      "steps per second: 0.50999\n",
      "step: 32966\n",
      "loss: 12.577281951904297\n",
      "steps per second: 0.56704\n",
      "step: 32967\n",
      "loss: 12.882634162902832\n",
      "steps per second: 0.53330\n",
      "step: 32968\n",
      "loss: 12.823187828063965\n",
      "steps per second: 0.54283\n",
      "step: 32969\n",
      "loss: 13.240693092346191\n",
      "steps per second: 0.52353\n",
      "step: 32970\n",
      "loss: 12.589829444885254\n",
      "steps per second: 0.57624\n",
      "step: 32971\n",
      "loss: 13.21035099029541\n",
      "steps per second: 0.52398\n",
      "step: 32972\n",
      "loss: 12.428372383117676\n",
      "steps per second: 0.50864\n",
      "step: 32973\n",
      "loss: 13.351283073425293\n",
      "steps per second: 0.54986\n",
      "step: 32974\n",
      "loss: 12.7865629196167\n",
      "steps per second: 0.53334\n",
      "step: 32975\n",
      "loss: 12.18588924407959\n",
      "steps per second: 0.55197\n",
      "step: 32976\n",
      "loss: 12.684359550476074\n",
      "steps per second: 0.52138\n",
      "step: 32977\n",
      "loss: 12.487272262573242\n",
      "steps per second: 0.57875\n",
      "step: 32978\n",
      "loss: 13.027527809143066\n",
      "steps per second: 0.54642\n",
      "step: 32979\n",
      "loss: 12.644043922424316\n",
      "steps per second: 0.57622\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8213218450546265, layer: 12\n",
      "saving at step 32979\n",
      "----------\n",
      "\n",
      "\n",
      "step: 32980\n",
      "loss: 13.02590560913086\n",
      "steps per second: 0.28465\n",
      "step: 32981\n",
      "loss: 13.181280136108398\n",
      "steps per second: 0.56034\n",
      "step: 32982\n",
      "loss: 13.093158721923828\n",
      "steps per second: 0.54859\n",
      "step: 32983\n",
      "loss: 12.82309627532959\n",
      "steps per second: 0.52171\n",
      "step: 32984\n",
      "loss: 12.620967864990234\n",
      "steps per second: 0.61004\n",
      "step: 32985\n",
      "loss: 12.912884712219238\n",
      "steps per second: 0.55668\n",
      "step: 32986\n",
      "loss: 12.814175605773926\n",
      "steps per second: 0.52411\n",
      "step: 32987\n",
      "loss: 12.82757568359375\n",
      "steps per second: 0.54351\n",
      "step: 32988\n",
      "loss: 12.762541770935059\n",
      "steps per second: 0.54619\n",
      "step: 32989\n",
      "loss: 13.217185020446777\n",
      "steps per second: 0.54215\n",
      "step: 32990\n",
      "loss: 12.856224060058594\n",
      "steps per second: 0.54538\n",
      "step: 32991\n",
      "loss: 11.928296089172363\n",
      "steps per second: 0.53368\n",
      "step: 32992\n",
      "loss: 12.927845001220703\n",
      "steps per second: 0.54596\n",
      "step: 32993\n",
      "loss: 12.355691909790039\n",
      "steps per second: 0.58307\n",
      "step: 32994\n",
      "loss: 13.004573822021484\n",
      "steps per second: 0.56415\n",
      "step: 32995\n",
      "loss: 13.133282661437988\n",
      "steps per second: 0.55484\n",
      "step: 32996\n",
      "loss: 13.0342378616333\n",
      "steps per second: 0.57040\n",
      "step: 32997\n",
      "loss: 12.928472518920898\n",
      "steps per second: 0.54682\n",
      "step: 32998\n",
      "loss: 12.527637481689453\n",
      "steps per second: 0.55271\n",
      "step: 32999\n",
      "loss: 12.443836212158203\n",
      "steps per second: 0.56818\n",
      "step: 33000\n",
      "loss: 12.754545211791992\n",
      "steps per second: 0.54783\n",
      "step: 33001\n",
      "loss: 12.57762622833252\n",
      "steps per second: 0.56494\n",
      "step: 33002\n",
      "loss: 12.863317489624023\n",
      "steps per second: 0.51156\n",
      "step: 33003\n",
      "loss: 13.201526641845703\n",
      "steps per second: 0.55358\n",
      "step: 33004\n",
      "loss: 13.069540023803711\n",
      "steps per second: 0.55733\n",
      "step: 33005\n",
      "loss: 12.994744300842285\n",
      "steps per second: 0.61134\n",
      "step: 33006\n",
      "loss: 12.616670608520508\n",
      "steps per second: 0.50999\n",
      "step: 33007\n",
      "loss: 12.459280014038086\n",
      "steps per second: 0.54114\n",
      "step: 33008\n",
      "loss: 12.695548057556152\n",
      "steps per second: 0.54673\n",
      "step: 33009\n",
      "loss: 12.555648803710938\n",
      "steps per second: 0.54909\n",
      "step: 33010\n",
      "loss: 12.842561721801758\n",
      "steps per second: 0.53182\n",
      "step: 33011\n",
      "loss: 13.011201858520508\n",
      "steps per second: 0.49251\n",
      "step: 33012\n",
      "loss: 12.826268196105957\n",
      "steps per second: 0.52390\n",
      "step: 33013\n",
      "loss: 13.290907859802246\n",
      "steps per second: 0.53438\n",
      "step: 33014\n",
      "loss: 12.494738578796387\n",
      "steps per second: 0.53226\n",
      "step: 33015\n",
      "loss: 12.299773216247559\n",
      "steps per second: 0.60586\n",
      "step: 33016\n",
      "loss: 12.941642761230469\n",
      "steps per second: 0.55220\n",
      "step: 33017\n",
      "loss: 12.710654258728027\n",
      "steps per second: 0.61232\n",
      "step: 33018\n",
      "loss: 13.007789611816406\n",
      "steps per second: 0.52456\n",
      "step: 33019\n",
      "loss: 12.569660186767578\n",
      "steps per second: 0.52360\n",
      "step: 33020\n",
      "loss: 12.905431747436523\n",
      "steps per second: 0.57461\n",
      "step: 33021\n",
      "loss: 13.14100456237793\n",
      "steps per second: 0.55500\n",
      "step: 33022\n",
      "loss: 13.01944637298584\n",
      "steps per second: 0.55559\n",
      "step: 33023\n",
      "loss: 12.793731689453125\n",
      "steps per second: 0.54831\n",
      "step: 33024\n",
      "loss: 12.187647819519043\n",
      "steps per second: 0.53389\n",
      "step: 33025\n",
      "loss: 12.933199882507324\n",
      "steps per second: 0.55471\n",
      "step: 33026\n",
      "loss: 12.501583099365234\n",
      "steps per second: 0.49312\n",
      "step: 33027\n",
      "loss: 13.263152122497559\n",
      "steps per second: 0.52257\n",
      "step: 33028\n",
      "loss: 13.030529975891113\n",
      "steps per second: 0.55508\n",
      "step: 33029\n",
      "loss: 12.940836906433105\n",
      "steps per second: 0.52583\n",
      "step: 33030\n",
      "loss: 13.156920433044434\n",
      "steps per second: 0.54148\n",
      "step: 33031\n",
      "loss: 12.588174819946289\n",
      "steps per second: 0.55370\n",
      "step: 33032\n",
      "loss: 12.075535774230957\n",
      "steps per second: 0.54225\n",
      "step: 33033\n",
      "loss: 12.5299711227417\n",
      "steps per second: 0.55828\n",
      "step: 33034\n",
      "loss: 12.693511009216309\n",
      "steps per second: 0.56716\n",
      "step: 33035\n",
      "loss: 13.054165840148926\n",
      "steps per second: 0.53511\n",
      "step: 33036\n",
      "loss: 12.540458679199219\n",
      "steps per second: 0.53930\n",
      "step: 33037\n",
      "loss: 12.997396469116211\n",
      "steps per second: 0.52505\n",
      "step: 33038\n",
      "loss: 12.61223030090332\n",
      "steps per second: 0.55691\n",
      "step: 33039\n",
      "loss: 12.749332427978516\n",
      "steps per second: 0.54295\n",
      "step: 33040\n",
      "loss: 13.054588317871094\n",
      "steps per second: 0.53007\n",
      "step: 33041\n",
      "loss: 12.57131290435791\n",
      "steps per second: 0.52499\n",
      "step: 33042\n",
      "loss: 13.13235092163086\n",
      "steps per second: 0.53646\n",
      "step: 33043\n",
      "loss: 12.244328498840332\n",
      "steps per second: 0.53715\n",
      "step: 33044\n",
      "loss: 12.743714332580566\n",
      "steps per second: 0.52607\n",
      "step: 33045\n",
      "loss: 12.896947860717773\n",
      "steps per second: 0.50797\n",
      "step: 33046\n",
      "loss: 12.50745964050293\n",
      "steps per second: 0.55809\n",
      "step: 33047\n",
      "loss: 12.394805908203125\n",
      "steps per second: 0.54974\n",
      "step: 33048\n",
      "loss: 12.618456840515137\n",
      "steps per second: 0.54331\n",
      "step: 33049\n",
      "loss: 12.360288619995117\n",
      "steps per second: 0.55281\n",
      "step: 33050\n",
      "loss: 13.130758285522461\n",
      "steps per second: 0.53378\n",
      "step: 33051\n",
      "loss: 13.168314933776855\n",
      "steps per second: 0.54667\n",
      "step: 33052\n",
      "loss: 12.61349868774414\n",
      "steps per second: 0.52521\n",
      "step: 33053\n",
      "loss: 12.690675735473633\n",
      "steps per second: 0.54981\n",
      "step: 33054\n",
      "loss: 12.66675090789795\n",
      "steps per second: 0.54487\n",
      "step: 33055\n",
      "loss: 12.29914665222168\n",
      "steps per second: 0.54770\n",
      "step: 33056\n",
      "loss: 13.000462532043457\n",
      "steps per second: 0.54290\n",
      "step: 33057\n",
      "loss: 12.542789459228516\n",
      "steps per second: 0.57114\n",
      "step: 33058\n",
      "loss: 12.747649192810059\n",
      "steps per second: 0.56768\n",
      "step: 33059\n",
      "loss: 12.474618911743164\n",
      "steps per second: 0.55588\n",
      "step: 33060\n",
      "loss: 13.024615287780762\n",
      "steps per second: 0.51018\n",
      "step: 33061\n",
      "loss: 12.845499992370605\n",
      "steps per second: 0.54877\n",
      "step: 33062\n",
      "loss: 13.166475296020508\n",
      "steps per second: 0.57542\n",
      "step: 33063\n",
      "loss: 12.457343101501465\n",
      "steps per second: 0.53854\n",
      "step: 33064\n",
      "loss: 12.570651054382324\n",
      "steps per second: 0.55122\n",
      "step: 33065\n",
      "loss: 12.976987838745117\n",
      "steps per second: 0.54915\n",
      "step: 33066\n",
      "loss: 12.675774574279785\n",
      "steps per second: 0.54063\n",
      "step: 33067\n",
      "loss: 12.759711265563965\n",
      "steps per second: 0.55162\n",
      "step: 33068\n",
      "loss: 12.482972145080566\n",
      "steps per second: 0.54169\n",
      "step: 33069\n",
      "loss: 12.817365646362305\n",
      "steps per second: 0.55493\n",
      "step: 33070\n",
      "loss: 12.524828910827637\n",
      "steps per second: 0.55775\n",
      "step: 33071\n",
      "loss: 12.527518272399902\n",
      "steps per second: 0.57665\n",
      "step: 33072\n",
      "loss: 13.08578872680664\n",
      "steps per second: 0.53862\n",
      "step: 33073\n",
      "loss: 12.86431884765625\n",
      "steps per second: 0.54988\n",
      "step: 33074\n",
      "loss: 12.500201225280762\n",
      "steps per second: 0.61247\n",
      "step: 33075\n",
      "loss: 12.474693298339844\n",
      "steps per second: 0.53309\n",
      "step: 33076\n",
      "loss: 13.429513931274414\n",
      "steps per second: 0.55600\n",
      "step: 33077\n",
      "loss: 13.434561729431152\n",
      "steps per second: 0.55697\n",
      "step: 33078\n",
      "loss: 12.371418952941895\n",
      "steps per second: 0.57695\n",
      "step: 33079\n",
      "loss: 12.748400688171387\n",
      "steps per second: 0.49392\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8245643377304077, layer: 10\n",
      "saving at step 33079\n",
      "----------\n",
      "\n",
      "\n",
      "step: 33080\n",
      "loss: 12.645325660705566\n",
      "steps per second: 0.26661\n",
      "step: 33081\n",
      "loss: 12.654195785522461\n",
      "steps per second: 0.55601\n",
      "step: 33082\n",
      "loss: 12.626792907714844\n",
      "steps per second: 0.57063\n",
      "step: 33083\n",
      "loss: 12.995872497558594\n",
      "steps per second: 0.54101\n",
      "step: 33084\n",
      "loss: 12.582290649414062\n",
      "steps per second: 0.53378\n",
      "step: 33085\n",
      "loss: 12.32618236541748\n",
      "steps per second: 0.60661\n",
      "step: 33086\n",
      "loss: 12.927324295043945\n",
      "steps per second: 0.52633\n",
      "step: 33087\n",
      "loss: 12.863608360290527\n",
      "steps per second: 0.54107\n",
      "step: 33088\n",
      "loss: 13.063311576843262\n",
      "steps per second: 0.55661\n",
      "step: 33089\n",
      "loss: 13.218804359436035\n",
      "steps per second: 0.52395\n",
      "step: 33090\n",
      "loss: 12.291881561279297\n",
      "steps per second: 0.56797\n",
      "step: 33091\n",
      "loss: 12.384332656860352\n",
      "steps per second: 0.54672\n",
      "step: 33092\n",
      "loss: 13.135931968688965\n",
      "steps per second: 0.52318\n",
      "step: 33093\n",
      "loss: 12.611007690429688\n",
      "steps per second: 0.56843\n",
      "step: 33094\n",
      "loss: 12.885944366455078\n",
      "steps per second: 0.57990\n",
      "step: 33095\n",
      "loss: 12.857272148132324\n",
      "steps per second: 0.54724\n",
      "step: 33096\n",
      "loss: 12.442285537719727\n",
      "steps per second: 0.54805\n",
      "step: 33097\n",
      "loss: 12.621637344360352\n",
      "steps per second: 0.54284\n",
      "step: 33098\n",
      "loss: 12.645075798034668\n",
      "steps per second: 0.53460\n",
      "step: 33099\n",
      "loss: 12.616741180419922\n",
      "steps per second: 0.53221\n",
      "step: 33100\n",
      "loss: 12.801559448242188\n",
      "steps per second: 0.56922\n",
      "step: 33101\n",
      "loss: 12.679694175720215\n",
      "steps per second: 0.56938\n",
      "step: 33102\n",
      "loss: 12.505609512329102\n",
      "steps per second: 0.57785\n",
      "step: 33103\n",
      "loss: 12.861150741577148\n",
      "steps per second: 0.56994\n",
      "step: 33104\n",
      "loss: 13.081180572509766\n",
      "steps per second: 0.55490\n",
      "step: 33105\n",
      "loss: 12.323627471923828\n",
      "steps per second: 0.54859\n",
      "step: 33106\n",
      "loss: 12.93339729309082\n",
      "steps per second: 0.55671\n",
      "step: 33107\n",
      "loss: 12.50238037109375\n",
      "steps per second: 0.54645\n",
      "step: 33108\n",
      "loss: 13.0482816696167\n",
      "steps per second: 0.58117\n",
      "step: 33109\n",
      "loss: 12.895477294921875\n",
      "steps per second: 0.53458\n",
      "step: 33110\n",
      "loss: 12.890857696533203\n",
      "steps per second: 0.56668\n",
      "step: 33111\n",
      "loss: 12.774930000305176\n",
      "steps per second: 0.57250\n",
      "step: 33112\n",
      "loss: 12.452981948852539\n",
      "steps per second: 0.56642\n",
      "step: 33113\n",
      "loss: 12.482925415039062\n",
      "steps per second: 0.54333\n",
      "step: 33114\n",
      "loss: 12.490628242492676\n",
      "steps per second: 0.52195\n",
      "step: 33115\n",
      "loss: 12.635316848754883\n",
      "steps per second: 0.49674\n",
      "step: 33116\n",
      "loss: 12.869804382324219\n",
      "steps per second: 0.58416\n",
      "step: 33117\n",
      "loss: 12.976208686828613\n",
      "steps per second: 0.57062\n",
      "step: 33118\n",
      "loss: 13.238723754882812\n",
      "steps per second: 0.55627\n",
      "step: 33119\n",
      "loss: 13.113022804260254\n",
      "steps per second: 0.54089\n",
      "step: 33120\n",
      "loss: 12.502420425415039\n",
      "steps per second: 0.56404\n",
      "step: 33121\n",
      "loss: 12.878645896911621\n",
      "steps per second: 0.54684\n",
      "step: 33122\n",
      "loss: 13.070267677307129\n",
      "steps per second: 0.54675\n",
      "step: 33123\n",
      "loss: 12.332067489624023\n",
      "steps per second: 0.53294\n",
      "step: 33124\n",
      "loss: 12.723577499389648\n",
      "steps per second: 0.60551\n",
      "step: 33125\n",
      "loss: 12.757492065429688\n",
      "steps per second: 0.54904\n",
      "step: 33126\n",
      "loss: 13.360795021057129\n",
      "steps per second: 0.50040\n",
      "step: 33127\n",
      "loss: 12.788846015930176\n",
      "steps per second: 0.57566\n",
      "step: 33128\n",
      "loss: 13.089215278625488\n",
      "steps per second: 0.54077\n",
      "step: 33129\n",
      "loss: 12.497350692749023\n",
      "steps per second: 0.53181\n",
      "step: 33130\n",
      "loss: 12.464771270751953\n",
      "steps per second: 0.54826\n",
      "step: 33131\n",
      "loss: 12.983765602111816\n",
      "steps per second: 0.52116\n",
      "step: 33132\n",
      "loss: 12.401762962341309\n",
      "steps per second: 0.55722\n",
      "step: 33133\n",
      "loss: 13.280094146728516\n",
      "steps per second: 0.55026\n",
      "step: 33134\n",
      "loss: 12.539366722106934\n",
      "steps per second: 0.55505\n",
      "step: 33135\n",
      "loss: 13.202085494995117\n",
      "steps per second: 0.54302\n",
      "step: 33136\n",
      "loss: 13.299132347106934\n",
      "steps per second: 0.49564\n",
      "step: 33137\n",
      "loss: 13.348416328430176\n",
      "steps per second: 0.57492\n",
      "step: 33138\n",
      "loss: 12.856868743896484\n",
      "steps per second: 0.54444\n",
      "step: 33139\n",
      "loss: 13.36721134185791\n",
      "steps per second: 0.50988\n",
      "step: 33140\n",
      "loss: 13.17318058013916\n",
      "steps per second: 0.53571\n",
      "step: 33141\n",
      "loss: 12.798956871032715\n",
      "steps per second: 0.57476\n",
      "step: 33142\n",
      "loss: 13.100911140441895\n",
      "steps per second: 0.55462\n",
      "step: 33143\n",
      "loss: 12.58780288696289\n",
      "steps per second: 0.54194\n",
      "step: 33144\n",
      "loss: 12.909720420837402\n",
      "steps per second: 0.55482\n",
      "step: 33145\n",
      "loss: 12.398727416992188\n",
      "steps per second: 0.52397\n",
      "step: 33146\n",
      "loss: 12.343265533447266\n",
      "steps per second: 0.54139\n",
      "step: 33147\n",
      "loss: 13.323126792907715\n",
      "steps per second: 0.52495\n",
      "step: 33148\n",
      "loss: 12.596474647521973\n",
      "steps per second: 0.49520\n",
      "step: 33149\n",
      "loss: 12.701004981994629\n",
      "steps per second: 0.49728\n",
      "step: 33150\n",
      "loss: 13.324496269226074\n",
      "steps per second: 0.55520\n",
      "step: 33151\n",
      "loss: 13.363903045654297\n",
      "steps per second: 0.54153\n",
      "step: 33152\n",
      "loss: 13.222114562988281\n",
      "steps per second: 0.60889\n",
      "step: 33153\n",
      "loss: 12.78390121459961\n",
      "steps per second: 0.58082\n",
      "step: 33154\n",
      "loss: 13.222447395324707\n",
      "steps per second: 0.54388\n",
      "step: 33155\n",
      "loss: 13.440261840820312\n",
      "steps per second: 0.56485\n",
      "step: 33156\n",
      "loss: 12.902015686035156\n",
      "steps per second: 0.54605\n",
      "step: 33157\n",
      "loss: 13.430084228515625\n",
      "steps per second: 0.54349\n",
      "step: 33158\n",
      "loss: 13.147333145141602\n",
      "steps per second: 0.57340\n",
      "step: 33159\n",
      "loss: 12.833694458007812\n",
      "steps per second: 0.57648\n",
      "step: 33160\n",
      "loss: 12.811606407165527\n",
      "steps per second: 0.56394\n",
      "step: 33161\n",
      "loss: 12.728194236755371\n",
      "steps per second: 0.56287\n",
      "step: 33162\n",
      "loss: 12.958319664001465\n",
      "steps per second: 0.54365\n",
      "step: 33163\n",
      "loss: 12.398016929626465\n",
      "steps per second: 0.52007\n",
      "step: 33164\n",
      "loss: 13.198020935058594\n",
      "steps per second: 0.54778\n",
      "step: 33165\n",
      "loss: 12.744640350341797\n",
      "steps per second: 0.60646\n",
      "step: 33166\n",
      "loss: 12.55481243133545\n",
      "steps per second: 0.51784\n",
      "step: 33167\n",
      "loss: 12.525508880615234\n",
      "steps per second: 0.54437\n",
      "step: 33168\n",
      "loss: 13.251764297485352\n",
      "steps per second: 0.55450\n",
      "step: 33169\n",
      "loss: 12.703722953796387\n",
      "steps per second: 0.56729\n",
      "step: 33170\n",
      "loss: 12.749311447143555\n",
      "steps per second: 0.56381\n",
      "step: 33171\n",
      "loss: 12.469791412353516\n",
      "steps per second: 0.52143\n",
      "step: 33172\n",
      "loss: 13.19162368774414\n",
      "steps per second: 0.56663\n",
      "step: 33173\n",
      "loss: 12.64910888671875\n",
      "steps per second: 0.57602\n",
      "step: 33174\n",
      "loss: 13.144680976867676\n",
      "steps per second: 0.56052\n",
      "step: 33175\n",
      "loss: 12.627411842346191\n",
      "steps per second: 0.55513\n",
      "step: 33176\n",
      "loss: 13.08375358581543\n",
      "steps per second: 0.51917\n",
      "step: 33177\n",
      "loss: 12.773109436035156\n",
      "steps per second: 0.60577\n",
      "step: 33178\n",
      "loss: 12.534149169921875\n",
      "steps per second: 0.52638\n",
      "step: 33179\n",
      "loss: 12.367281913757324\n",
      "steps per second: 0.53660\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8112590312957764, layer: 10\n",
      "saving at step 33179\n",
      "----------\n",
      "\n",
      "\n",
      "step: 33180\n",
      "loss: 12.782102584838867\n",
      "steps per second: 0.27459\n",
      "step: 33181\n",
      "loss: 13.260042190551758\n",
      "steps per second: 0.51985\n",
      "step: 33182\n",
      "loss: 13.462916374206543\n",
      "steps per second: 0.56933\n",
      "step: 33183\n",
      "loss: 13.122498512268066\n",
      "steps per second: 0.55397\n",
      "step: 33184\n",
      "loss: 12.863964080810547\n",
      "steps per second: 0.57275\n",
      "step: 33185\n",
      "loss: 12.684440612792969\n",
      "steps per second: 0.60432\n",
      "step: 33186\n",
      "loss: 12.755054473876953\n",
      "steps per second: 0.56791\n",
      "step: 33187\n",
      "loss: 12.206178665161133\n",
      "steps per second: 0.53076\n",
      "step: 33188\n",
      "loss: 12.588159561157227\n",
      "steps per second: 0.51875\n",
      "step: 33189\n",
      "loss: 12.723851203918457\n",
      "steps per second: 0.56648\n",
      "step: 33190\n",
      "loss: 13.407068252563477\n",
      "steps per second: 0.55648\n",
      "step: 33191\n",
      "loss: 13.168150901794434\n",
      "steps per second: 0.54680\n",
      "step: 33192\n",
      "loss: 12.718668937683105\n",
      "steps per second: 0.54573\n",
      "step: 33193\n",
      "loss: 13.76355266571045\n",
      "steps per second: 0.51607\n",
      "step: 33194\n",
      "loss: 12.695649147033691\n",
      "steps per second: 0.56632\n",
      "step: 33195\n",
      "loss: 13.38115406036377\n",
      "steps per second: 0.53163\n",
      "step: 33196\n",
      "loss: 13.043327331542969\n",
      "steps per second: 0.52590\n",
      "step: 33197\n",
      "loss: 12.673373222351074\n",
      "steps per second: 0.61030\n",
      "step: 33198\n",
      "loss: 12.991214752197266\n",
      "steps per second: 0.54563\n",
      "step: 33199\n",
      "loss: 12.682239532470703\n",
      "steps per second: 0.56774\n",
      "step: 33200\n",
      "loss: 13.011326789855957\n",
      "steps per second: 0.55250\n",
      "step: 33201\n",
      "loss: 12.76546859741211\n",
      "steps per second: 0.54617\n",
      "step: 33202\n",
      "loss: 12.391813278198242\n",
      "steps per second: 0.52229\n",
      "step: 33203\n",
      "loss: 12.764867782592773\n",
      "steps per second: 0.56621\n",
      "step: 33204\n",
      "loss: 13.081274032592773\n",
      "steps per second: 0.52203\n",
      "step: 33205\n",
      "loss: 12.54117488861084\n",
      "steps per second: 0.49779\n",
      "step: 33206\n",
      "loss: 12.810626029968262\n",
      "steps per second: 0.55661\n",
      "step: 33207\n",
      "loss: 12.762642860412598\n",
      "steps per second: 0.53668\n",
      "step: 33208\n",
      "loss: 12.741558074951172\n",
      "steps per second: 0.60900\n",
      "step: 33209\n",
      "loss: 12.940459251403809\n",
      "steps per second: 0.54014\n",
      "step: 33210\n",
      "loss: 12.65695858001709\n",
      "steps per second: 0.51651\n",
      "step: 33211\n",
      "loss: 13.282257080078125\n",
      "steps per second: 0.54526\n",
      "step: 33212\n",
      "loss: 12.910118103027344\n",
      "steps per second: 0.52856\n",
      "step: 33213\n",
      "loss: 13.05215072631836\n",
      "steps per second: 0.52914\n",
      "step: 33214\n",
      "loss: 12.589994430541992\n",
      "steps per second: 0.58329\n",
      "step: 33215\n",
      "loss: 12.507415771484375\n",
      "steps per second: 0.55253\n",
      "step: 33216\n",
      "loss: 13.11512565612793\n",
      "steps per second: 0.53658\n",
      "step: 33217\n",
      "loss: 12.969915390014648\n",
      "steps per second: 0.53595\n",
      "step: 33218\n",
      "loss: 12.994726181030273\n",
      "steps per second: 0.55568\n",
      "step: 33219\n",
      "loss: 13.108529090881348\n",
      "steps per second: 0.54589\n",
      "step: 33220\n",
      "loss: 12.742658615112305\n",
      "steps per second: 0.54007\n",
      "step: 33221\n",
      "loss: 13.127660751342773\n",
      "steps per second: 0.58284\n",
      "step: 33222\n",
      "loss: 12.866189956665039\n",
      "steps per second: 0.58115\n",
      "step: 33223\n",
      "loss: 12.717167854309082\n",
      "steps per second: 0.55328\n",
      "step: 33224\n",
      "loss: 12.67646312713623\n",
      "steps per second: 0.55678\n",
      "step: 33225\n",
      "loss: 13.20707893371582\n",
      "steps per second: 0.55259\n",
      "step: 33226\n",
      "loss: 12.690553665161133\n",
      "steps per second: 0.50815\n",
      "step: 33227\n",
      "loss: 12.229207038879395\n",
      "steps per second: 0.51648\n",
      "step: 33228\n",
      "loss: 13.355052947998047\n",
      "steps per second: 0.50063\n",
      "step: 33229\n",
      "loss: 13.50909423828125\n",
      "steps per second: 0.56829\n",
      "step: 33230\n",
      "loss: 13.58602523803711\n",
      "steps per second: 0.55604\n",
      "step: 33231\n",
      "loss: 12.360827445983887\n",
      "steps per second: 0.48229\n",
      "step: 33232\n",
      "loss: 12.890890121459961\n",
      "steps per second: 0.55276\n",
      "step: 33233\n",
      "loss: 12.725272178649902\n",
      "steps per second: 0.54373\n",
      "step: 33234\n",
      "loss: 12.623005867004395\n",
      "steps per second: 0.50139\n",
      "step: 33235\n",
      "loss: 12.858022689819336\n",
      "steps per second: 0.50983\n",
      "step: 33236\n",
      "loss: 13.082788467407227\n",
      "steps per second: 0.54418\n",
      "step: 33237\n",
      "loss: 12.890009880065918\n",
      "steps per second: 0.55494\n",
      "step: 33238\n",
      "loss: 12.683996200561523\n",
      "steps per second: 0.53646\n",
      "step: 33239\n",
      "loss: 12.833487510681152\n",
      "steps per second: 0.53003\n",
      "step: 33240\n",
      "loss: 12.747279167175293\n",
      "steps per second: 0.56872\n",
      "step: 33241\n",
      "loss: 12.668127059936523\n",
      "steps per second: 0.51790\n",
      "step: 33242\n",
      "loss: 12.891080856323242\n",
      "steps per second: 0.56842\n",
      "step: 33243\n",
      "loss: 12.983595848083496\n",
      "steps per second: 0.51729\n",
      "step: 33244\n",
      "loss: 13.133305549621582\n",
      "steps per second: 0.54414\n",
      "step: 33245\n",
      "loss: 13.118922233581543\n",
      "steps per second: 0.55531\n",
      "step: 33246\n",
      "loss: 12.578459739685059\n",
      "steps per second: 0.54478\n",
      "step: 33247\n",
      "loss: 12.82272720336914\n",
      "steps per second: 0.61034\n",
      "step: 33248\n",
      "loss: 12.57225227355957\n",
      "steps per second: 0.51725\n",
      "step: 33249\n",
      "loss: 12.761551856994629\n",
      "steps per second: 0.52967\n",
      "step: 33250\n",
      "loss: 12.722402572631836\n",
      "steps per second: 0.57549\n",
      "step: 33251\n",
      "loss: 12.840093612670898\n",
      "steps per second: 0.56898\n",
      "step: 33252\n",
      "loss: 12.874433517456055\n",
      "steps per second: 0.60253\n",
      "step: 33253\n",
      "loss: 13.162659645080566\n",
      "steps per second: 0.53359\n",
      "step: 33254\n",
      "loss: 12.279847145080566\n",
      "steps per second: 0.55384\n",
      "step: 33255\n",
      "loss: 12.690019607543945\n",
      "steps per second: 0.52763\n",
      "step: 33256\n",
      "loss: 12.211882591247559\n",
      "steps per second: 0.55872\n",
      "step: 33257\n",
      "loss: 12.752193450927734\n",
      "steps per second: 0.54609\n",
      "step: 33258\n",
      "loss: 12.913309097290039\n",
      "steps per second: 0.55086\n",
      "step: 33259\n",
      "loss: 12.478965759277344\n",
      "steps per second: 0.53619\n",
      "step: 33260\n",
      "loss: 12.5380277633667\n",
      "steps per second: 0.55640\n",
      "step: 33261\n",
      "loss: 12.645124435424805\n",
      "steps per second: 0.56624\n",
      "step: 33262\n",
      "loss: 13.202345848083496\n",
      "steps per second: 0.52559\n",
      "step: 33263\n",
      "loss: 13.029829978942871\n",
      "steps per second: 0.51839\n",
      "step: 33264\n",
      "loss: 13.247268676757812\n",
      "steps per second: 0.54514\n",
      "step: 33265\n",
      "loss: 12.369790077209473\n",
      "steps per second: 0.53523\n",
      "step: 33266\n",
      "loss: 12.846663475036621\n",
      "steps per second: 0.57018\n",
      "step: 33267\n",
      "loss: 12.508646011352539\n",
      "steps per second: 0.51220\n",
      "step: 33268\n",
      "loss: 13.229048728942871\n",
      "steps per second: 0.54148\n",
      "step: 33269\n",
      "loss: 12.674610137939453\n",
      "steps per second: 0.54777\n",
      "step: 33270\n",
      "loss: 12.850384712219238\n",
      "steps per second: 0.54061\n",
      "step: 33271\n",
      "loss: 12.93459415435791\n",
      "steps per second: 0.48535\n",
      "step: 33272\n",
      "loss: 12.948400497436523\n",
      "steps per second: 0.57459\n",
      "step: 33273\n",
      "loss: 13.100637435913086\n",
      "steps per second: 0.55578\n",
      "step: 33274\n",
      "loss: 12.459317207336426\n",
      "steps per second: 0.51466\n",
      "step: 33275\n",
      "loss: 13.054332733154297\n",
      "steps per second: 0.53361\n",
      "step: 33276\n",
      "loss: 12.514095306396484\n",
      "steps per second: 0.56627\n",
      "step: 33277\n",
      "loss: 12.485480308532715\n",
      "steps per second: 0.53661\n",
      "step: 33278\n",
      "loss: 12.671501159667969\n",
      "steps per second: 0.50644\n",
      "step: 33279\n",
      "loss: 12.835959434509277\n",
      "steps per second: 0.52047\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7701480984687805, layer: 11\n",
      "saving at step 33279\n",
      "----------\n",
      "\n",
      "\n",
      "step: 33280\n",
      "loss: 12.597171783447266\n",
      "steps per second: 0.27662\n",
      "step: 33281\n",
      "loss: 12.993159294128418\n",
      "steps per second: 0.57519\n",
      "step: 33282\n",
      "loss: 12.70717716217041\n",
      "steps per second: 0.54197\n",
      "step: 33283\n",
      "loss: 12.421334266662598\n",
      "steps per second: 0.56766\n",
      "step: 33284\n",
      "loss: 12.404582977294922\n",
      "steps per second: 0.53083\n",
      "step: 33285\n",
      "loss: 12.732707023620605\n",
      "steps per second: 0.56730\n",
      "step: 33286\n",
      "loss: 12.571919441223145\n",
      "steps per second: 0.53661\n",
      "step: 33287\n",
      "loss: 13.002694129943848\n",
      "steps per second: 0.56615\n",
      "step: 33288\n",
      "loss: 12.651291847229004\n",
      "steps per second: 0.53441\n",
      "step: 33289\n",
      "loss: 12.798049926757812\n",
      "steps per second: 0.60652\n",
      "step: 33290\n",
      "loss: 13.308076858520508\n",
      "steps per second: 0.60417\n",
      "step: 33291\n",
      "loss: 12.287357330322266\n",
      "steps per second: 0.51240\n",
      "step: 33292\n",
      "loss: 12.698781967163086\n",
      "steps per second: 0.56857\n",
      "step: 33293\n",
      "loss: 12.804617881774902\n",
      "steps per second: 0.56699\n",
      "step: 33294\n",
      "loss: 12.60599136352539\n",
      "steps per second: 0.54504\n",
      "step: 33295\n",
      "loss: 13.280293464660645\n",
      "steps per second: 0.54570\n",
      "step: 33296\n",
      "loss: 12.676000595092773\n",
      "steps per second: 0.52823\n",
      "step: 33297\n",
      "loss: 12.833815574645996\n",
      "steps per second: 0.52972\n",
      "step: 33298\n",
      "loss: 13.088496208190918\n",
      "steps per second: 0.57449\n",
      "step: 33299\n",
      "loss: 13.018645286560059\n",
      "steps per second: 0.56424\n",
      "step: 33300\n",
      "loss: 12.804960250854492\n",
      "steps per second: 0.54143\n",
      "step: 33301\n",
      "loss: 12.833623886108398\n",
      "steps per second: 0.57153\n",
      "step: 33302\n",
      "loss: 12.77259635925293\n",
      "steps per second: 0.50030\n",
      "step: 33303\n",
      "loss: 13.126678466796875\n",
      "steps per second: 0.53552\n",
      "step: 33304\n",
      "loss: 12.611309051513672\n",
      "steps per second: 0.51333\n",
      "step: 33305\n",
      "loss: 12.903457641601562\n",
      "steps per second: 0.51685\n",
      "step: 33306\n",
      "loss: 12.901839256286621\n",
      "steps per second: 0.51358\n",
      "step: 33307\n",
      "loss: 13.08919620513916\n",
      "steps per second: 0.54743\n",
      "step: 33308\n",
      "loss: 13.049911499023438\n",
      "steps per second: 0.57241\n",
      "step: 33309\n",
      "loss: 12.52053165435791\n",
      "steps per second: 0.52067\n",
      "step: 33310\n",
      "loss: 13.071500778198242\n",
      "steps per second: 0.52511\n",
      "step: 33311\n",
      "loss: 12.77369213104248\n",
      "steps per second: 0.54387\n",
      "step: 33312\n",
      "loss: 12.739617347717285\n",
      "steps per second: 0.56830\n",
      "step: 33313\n",
      "loss: 12.443039894104004\n",
      "steps per second: 0.57545\n",
      "step: 33314\n",
      "loss: 12.310893058776855\n",
      "steps per second: 0.57560\n",
      "step: 33315\n",
      "loss: 13.025907516479492\n",
      "steps per second: 0.56607\n",
      "step: 33316\n",
      "loss: 12.916521072387695\n",
      "steps per second: 0.54409\n",
      "step: 33317\n",
      "loss: 12.634965896606445\n",
      "steps per second: 0.51574\n",
      "step: 33318\n",
      "loss: 12.767354965209961\n",
      "steps per second: 0.54377\n",
      "step: 33319\n",
      "loss: 12.394164085388184\n",
      "steps per second: 0.60832\n",
      "step: 33320\n",
      "loss: 13.19172477722168\n",
      "steps per second: 0.57063\n",
      "step: 33321\n",
      "loss: 12.11645793914795\n",
      "steps per second: 0.55460\n",
      "step: 33322\n",
      "loss: 12.683272361755371\n",
      "steps per second: 0.52481\n",
      "step: 33323\n",
      "loss: 12.782078742980957\n",
      "steps per second: 0.56421\n",
      "step: 33324\n",
      "loss: 12.573324203491211\n",
      "steps per second: 0.51798\n",
      "step: 33325\n",
      "loss: 12.893548011779785\n",
      "steps per second: 0.48119\n",
      "step: 33326\n",
      "loss: 12.924653053283691\n",
      "steps per second: 0.49580\n",
      "step: 33327\n",
      "loss: 12.996692657470703\n",
      "steps per second: 0.52697\n",
      "step: 33328\n",
      "loss: 12.940608024597168\n",
      "steps per second: 0.53758\n",
      "step: 33329\n",
      "loss: 12.716251373291016\n",
      "steps per second: 0.55637\n",
      "step: 33330\n",
      "loss: 12.581242561340332\n",
      "steps per second: 0.56862\n",
      "step: 33331\n",
      "loss: 12.342950820922852\n",
      "steps per second: 0.53573\n",
      "step: 33332\n",
      "loss: 12.894996643066406\n",
      "steps per second: 0.56665\n",
      "step: 33333\n",
      "loss: 12.89356517791748\n",
      "steps per second: 0.54550\n",
      "step: 33334\n",
      "loss: 12.773697853088379\n",
      "steps per second: 0.53788\n",
      "step: 33335\n",
      "loss: 12.717133522033691\n",
      "steps per second: 0.55064\n",
      "step: 33336\n",
      "loss: 12.762713432312012\n",
      "steps per second: 0.51401\n",
      "step: 33337\n",
      "loss: 13.426578521728516\n",
      "steps per second: 0.52779\n",
      "step: 33338\n",
      "loss: 12.864714622497559\n",
      "steps per second: 0.54454\n",
      "step: 33339\n",
      "loss: 12.857621192932129\n",
      "steps per second: 0.52841\n",
      "step: 33340\n",
      "loss: 13.113374710083008\n",
      "steps per second: 0.51868\n",
      "step: 33341\n",
      "loss: 13.234174728393555\n",
      "steps per second: 0.51844\n",
      "step: 33342\n",
      "loss: 13.320963859558105\n",
      "steps per second: 0.60679\n",
      "step: 33343\n",
      "loss: 12.730345726013184\n",
      "steps per second: 0.51683\n",
      "step: 33344\n",
      "loss: 12.584501266479492\n",
      "steps per second: 0.51772\n",
      "step: 33345\n",
      "loss: 12.605696678161621\n",
      "steps per second: 0.51913\n",
      "step: 33346\n",
      "loss: 13.353422164916992\n",
      "steps per second: 0.54436\n",
      "step: 33347\n",
      "loss: 12.836816787719727\n",
      "steps per second: 0.53629\n",
      "step: 33348\n",
      "loss: 13.049402236938477\n",
      "steps per second: 0.55366\n",
      "step: 33349\n",
      "loss: 12.907235145568848\n",
      "steps per second: 0.56049\n",
      "step: 33350\n",
      "loss: 12.773401260375977\n",
      "steps per second: 0.53204\n",
      "step: 33351\n",
      "loss: 12.580737113952637\n",
      "steps per second: 0.56690\n",
      "step: 33352\n",
      "loss: 12.474939346313477\n",
      "steps per second: 0.53882\n",
      "step: 33353\n",
      "loss: 13.03209114074707\n",
      "steps per second: 0.57006\n",
      "step: 33354\n",
      "loss: 13.01496410369873\n",
      "steps per second: 0.57048\n",
      "step: 33355\n",
      "loss: 12.955665588378906\n",
      "steps per second: 0.57385\n",
      "step: 33356\n",
      "loss: 12.321599960327148\n",
      "steps per second: 0.51473\n",
      "step: 33357\n",
      "loss: 12.676945686340332\n",
      "steps per second: 0.56427\n",
      "step: 33358\n",
      "loss: 13.046229362487793\n",
      "steps per second: 0.51175\n",
      "step: 33359\n",
      "loss: 13.172911643981934\n",
      "steps per second: 0.55930\n",
      "step: 33360\n",
      "loss: 12.729784965515137\n",
      "steps per second: 0.54195\n",
      "step: 33361\n",
      "loss: 12.757254600524902\n",
      "steps per second: 0.56732\n",
      "step: 33362\n",
      "loss: 13.002385139465332\n",
      "steps per second: 0.55487\n",
      "step: 33363\n",
      "loss: 12.752222061157227\n",
      "steps per second: 0.49663\n",
      "step: 33364\n",
      "loss: 12.828460693359375\n",
      "steps per second: 0.54303\n",
      "step: 33365\n",
      "loss: 12.536348342895508\n",
      "steps per second: 0.55424\n",
      "step: 33366\n",
      "loss: 12.77462100982666\n",
      "steps per second: 0.46929\n",
      "step: 33367\n",
      "loss: 12.66926097869873\n",
      "steps per second: 0.57483\n",
      "step: 33368\n",
      "loss: 12.610076904296875\n",
      "steps per second: 0.52706\n",
      "step: 33369\n",
      "loss: 12.560900688171387\n",
      "steps per second: 0.55523\n",
      "step: 33370\n",
      "loss: 13.06071949005127\n",
      "steps per second: 0.56830\n",
      "step: 33371\n",
      "loss: 12.47407341003418\n",
      "steps per second: 0.54431\n",
      "step: 33372\n",
      "loss: 12.656895637512207\n",
      "steps per second: 0.53502\n",
      "step: 33373\n",
      "loss: 12.61202335357666\n",
      "steps per second: 0.54396\n",
      "step: 33374\n",
      "loss: 12.703357696533203\n",
      "steps per second: 0.60600\n",
      "step: 33375\n",
      "loss: 13.284625053405762\n",
      "steps per second: 0.52831\n",
      "step: 33376\n",
      "loss: 13.188752174377441\n",
      "steps per second: 0.57111\n",
      "step: 33377\n",
      "loss: 12.700041770935059\n",
      "steps per second: 0.53418\n",
      "step: 33378\n",
      "loss: 12.61328125\n",
      "steps per second: 0.53351\n",
      "step: 33379\n",
      "loss: 12.62049674987793\n",
      "steps per second: 0.51908\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8141816854476929, layer: 11\n",
      "saving at step 33379\n",
      "----------\n",
      "\n",
      "\n",
      "step: 33380\n",
      "loss: 12.776266098022461\n",
      "steps per second: 0.28129\n",
      "step: 33381\n",
      "loss: 12.449342727661133\n",
      "steps per second: 0.60424\n",
      "step: 33382\n",
      "loss: 12.39417839050293\n",
      "steps per second: 0.56855\n",
      "step: 33383\n",
      "loss: 12.827486038208008\n",
      "steps per second: 0.52932\n",
      "step: 33384\n",
      "loss: 12.402567863464355\n",
      "steps per second: 0.54241\n",
      "step: 33385\n",
      "loss: 12.179930686950684\n",
      "steps per second: 0.57712\n",
      "step: 33386\n",
      "loss: 12.56516170501709\n",
      "steps per second: 0.55601\n",
      "step: 33387\n",
      "loss: 13.285022735595703\n",
      "steps per second: 0.54286\n",
      "step: 33388\n",
      "loss: 12.900115966796875\n",
      "steps per second: 0.54300\n",
      "step: 33389\n",
      "loss: 12.787328720092773\n",
      "steps per second: 0.49950\n",
      "step: 33390\n",
      "loss: 12.74655818939209\n",
      "steps per second: 0.60277\n",
      "step: 33391\n",
      "loss: 12.964107513427734\n",
      "steps per second: 0.52784\n",
      "step: 33392\n",
      "loss: 12.71124267578125\n",
      "steps per second: 0.51515\n",
      "step: 33393\n",
      "loss: 12.38229751586914\n",
      "steps per second: 0.56975\n",
      "step: 33394\n",
      "loss: 12.820329666137695\n",
      "steps per second: 0.55442\n",
      "step: 33395\n",
      "loss: 13.059194564819336\n",
      "steps per second: 0.60718\n",
      "step: 33396\n",
      "loss: 12.480094909667969\n",
      "steps per second: 0.51066\n",
      "step: 33397\n",
      "loss: 13.134049415588379\n",
      "steps per second: 0.55691\n",
      "step: 33398\n",
      "loss: 12.409253120422363\n",
      "steps per second: 0.55585\n",
      "step: 33399\n",
      "loss: 13.530437469482422\n",
      "steps per second: 0.57543\n",
      "step: 33400\n",
      "loss: 12.762601852416992\n",
      "steps per second: 0.55268\n",
      "step: 33401\n",
      "loss: 12.998032569885254\n",
      "steps per second: 0.54267\n",
      "step: 33402\n",
      "loss: 12.874764442443848\n",
      "steps per second: 0.60642\n",
      "step: 33403\n",
      "loss: 12.99450969696045\n",
      "steps per second: 0.52765\n",
      "step: 33404\n",
      "loss: 12.962030410766602\n",
      "steps per second: 0.52696\n",
      "step: 33405\n",
      "loss: 12.584318161010742\n",
      "steps per second: 0.51746\n",
      "step: 33406\n",
      "loss: 12.530570030212402\n",
      "steps per second: 0.55456\n",
      "step: 33407\n",
      "loss: 12.594058990478516\n",
      "steps per second: 0.49520\n",
      "step: 33408\n",
      "loss: 12.87755012512207\n",
      "steps per second: 0.54102\n",
      "step: 33409\n",
      "loss: 12.666351318359375\n",
      "steps per second: 0.53494\n",
      "step: 33410\n",
      "loss: 12.490283966064453\n",
      "steps per second: 0.53532\n",
      "step: 33411\n",
      "loss: 12.949711799621582\n",
      "steps per second: 0.61121\n",
      "step: 33412\n",
      "loss: 12.867305755615234\n",
      "steps per second: 0.55391\n",
      "step: 33413\n",
      "loss: 12.760937690734863\n",
      "steps per second: 0.55603\n",
      "step: 33414\n",
      "loss: 12.433265686035156\n",
      "steps per second: 0.52408\n",
      "step: 33415\n",
      "loss: 12.712820053100586\n",
      "steps per second: 0.53923\n",
      "step: 33416\n",
      "loss: 13.091156959533691\n",
      "steps per second: 0.22063\n",
      "step: 33417\n",
      "loss: 12.58163833618164\n",
      "steps per second: 0.56157\n",
      "step: 33418\n",
      "loss: 12.935072898864746\n",
      "steps per second: 0.56632\n",
      "step: 33419\n",
      "loss: 11.872088432312012\n",
      "steps per second: 0.52975\n",
      "step: 33420\n",
      "loss: 13.39241886138916\n",
      "steps per second: 0.56372\n",
      "step: 33421\n",
      "loss: 13.061321258544922\n",
      "steps per second: 0.50633\n",
      "step: 33422\n",
      "loss: 12.536028861999512\n",
      "steps per second: 0.56906\n",
      "step: 33423\n",
      "loss: 13.35472297668457\n",
      "steps per second: 0.52718\n",
      "step: 33424\n",
      "loss: 12.607770919799805\n",
      "steps per second: 0.54392\n",
      "step: 33425\n",
      "loss: 13.104292869567871\n",
      "steps per second: 0.54550\n",
      "step: 33426\n",
      "loss: 13.038010597229004\n",
      "steps per second: 0.51276\n",
      "step: 33427\n",
      "loss: 13.005403518676758\n",
      "steps per second: 0.50589\n",
      "step: 33428\n",
      "loss: 13.12618637084961\n",
      "steps per second: 0.59578\n",
      "step: 33429\n",
      "loss: 12.483970642089844\n",
      "steps per second: 0.53944\n",
      "step: 33430\n",
      "loss: 12.936328887939453\n",
      "steps per second: 0.56041\n",
      "step: 33431\n",
      "loss: 12.85469913482666\n",
      "steps per second: 0.56368\n",
      "step: 33432\n",
      "loss: 12.291727066040039\n",
      "steps per second: 0.56343\n",
      "step: 33433\n",
      "loss: 12.745668411254883\n",
      "steps per second: 0.56423\n",
      "step: 33434\n",
      "loss: 12.902170181274414\n",
      "steps per second: 0.54034\n",
      "step: 33435\n",
      "loss: 12.49020767211914\n",
      "steps per second: 0.54126\n",
      "step: 33436\n",
      "loss: 12.478754997253418\n",
      "steps per second: 0.56168\n",
      "step: 33437\n",
      "loss: 13.1356201171875\n",
      "steps per second: 0.55823\n",
      "step: 33438\n",
      "loss: 13.051100730895996\n",
      "steps per second: 0.52488\n",
      "step: 33439\n",
      "loss: 12.581748008728027\n",
      "steps per second: 0.53593\n",
      "step: 33440\n",
      "loss: 12.330667495727539\n",
      "steps per second: 0.55888\n",
      "step: 33441\n",
      "loss: 13.354024887084961\n",
      "steps per second: 0.54696\n",
      "step: 33442\n",
      "loss: 13.755884170532227\n",
      "steps per second: 0.56327\n",
      "step: 33443\n",
      "loss: 13.15252685546875\n",
      "steps per second: 0.53154\n",
      "step: 33444\n",
      "loss: 12.557770729064941\n",
      "steps per second: 0.55840\n",
      "step: 33445\n",
      "loss: 12.765315055847168\n",
      "steps per second: 0.50511\n",
      "step: 33446\n",
      "loss: 12.864604949951172\n",
      "steps per second: 0.52484\n",
      "step: 33447\n",
      "loss: 12.980384826660156\n",
      "steps per second: 0.51901\n",
      "step: 33448\n",
      "loss: 12.731734275817871\n",
      "steps per second: 0.51673\n",
      "step: 33449\n",
      "loss: 13.074180603027344\n",
      "steps per second: 0.52263\n",
      "step: 33450\n",
      "loss: 12.329444885253906\n",
      "steps per second: 0.55979\n",
      "step: 33451\n",
      "loss: 12.511012077331543\n",
      "steps per second: 0.53550\n",
      "step: 33452\n",
      "loss: 12.66344928741455\n",
      "steps per second: 0.56026\n",
      "step: 33453\n",
      "loss: 13.239997863769531\n",
      "steps per second: 0.54906\n",
      "step: 33454\n",
      "loss: 13.098942756652832\n",
      "steps per second: 0.54168\n",
      "step: 33455\n",
      "loss: 12.838873863220215\n",
      "steps per second: 0.54941\n",
      "step: 33456\n",
      "loss: 13.278286933898926\n",
      "steps per second: 0.53407\n",
      "step: 33457\n",
      "loss: 13.037575721740723\n",
      "steps per second: 0.53539\n",
      "step: 33458\n",
      "loss: 12.646918296813965\n",
      "steps per second: 0.52812\n",
      "step: 33459\n",
      "loss: 12.718667030334473\n",
      "steps per second: 0.54888\n",
      "step: 33460\n",
      "loss: 12.908766746520996\n",
      "steps per second: 0.51571\n",
      "step: 33461\n",
      "loss: 13.300376892089844\n",
      "steps per second: 0.54628\n",
      "step: 33462\n",
      "loss: 12.403913497924805\n",
      "steps per second: 0.54814\n",
      "step: 33463\n",
      "loss: 12.52425479888916\n",
      "steps per second: 0.51810\n",
      "step: 33464\n",
      "loss: 13.147347450256348\n",
      "steps per second: 0.53474\n",
      "step: 33465\n",
      "loss: 12.863028526306152\n",
      "steps per second: 0.53955\n",
      "step: 33466\n",
      "loss: 12.785582542419434\n",
      "steps per second: 0.51664\n",
      "step: 33467\n",
      "loss: 12.761138916015625\n",
      "steps per second: 0.54641\n",
      "step: 33468\n",
      "loss: 12.95273208618164\n",
      "steps per second: 0.49467\n",
      "step: 33469\n",
      "loss: 12.776382446289062\n",
      "steps per second: 0.54336\n",
      "step: 33470\n",
      "loss: 13.472159385681152\n",
      "steps per second: 0.51809\n",
      "step: 33471\n",
      "loss: 13.298480987548828\n",
      "steps per second: 0.53771\n",
      "step: 33472\n",
      "loss: 12.901533126831055\n",
      "steps per second: 0.53226\n",
      "step: 33473\n",
      "loss: 12.74722957611084\n",
      "steps per second: 0.51335\n",
      "step: 33474\n",
      "loss: 12.487799644470215\n",
      "steps per second: 0.54153\n",
      "step: 33475\n",
      "loss: 12.44303035736084\n",
      "steps per second: 0.55953\n",
      "step: 33476\n",
      "loss: 12.955509185791016\n",
      "steps per second: 0.54712\n",
      "step: 33477\n",
      "loss: 12.29062557220459\n",
      "steps per second: 0.53142\n",
      "step: 33478\n",
      "loss: 12.904726028442383\n",
      "steps per second: 0.52960\n",
      "step: 33479\n",
      "loss: 12.333635330200195\n",
      "steps per second: 0.50329\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7790542244911194, layer: 12\n",
      "saving at step 33479\n",
      "----------\n",
      "\n",
      "\n",
      "step: 33480\n",
      "loss: 12.415578842163086\n",
      "steps per second: 0.23854\n",
      "step: 33481\n",
      "loss: 12.65475082397461\n",
      "steps per second: 0.51009\n",
      "step: 33482\n",
      "loss: 12.667596817016602\n",
      "steps per second: 0.52831\n",
      "step: 33483\n",
      "loss: 12.640312194824219\n",
      "steps per second: 0.53762\n",
      "step: 33484\n",
      "loss: 13.037050247192383\n",
      "steps per second: 0.53737\n",
      "step: 33485\n",
      "loss: 12.593549728393555\n",
      "steps per second: 0.53488\n",
      "step: 33486\n",
      "loss: 12.501653671264648\n",
      "steps per second: 0.51857\n",
      "step: 33487\n",
      "loss: 12.909822463989258\n",
      "steps per second: 0.51689\n",
      "step: 33488\n",
      "loss: 12.614153861999512\n",
      "steps per second: 0.56657\n",
      "step: 33489\n",
      "loss: 12.641246795654297\n",
      "steps per second: 0.57687\n",
      "step: 33490\n",
      "loss: 12.949456214904785\n",
      "steps per second: 0.59181\n",
      "step: 33491\n",
      "loss: 13.018632888793945\n",
      "steps per second: 0.59101\n",
      "step: 33492\n",
      "loss: 13.373900413513184\n",
      "steps per second: 0.54365\n",
      "step: 33493\n",
      "loss: 12.840648651123047\n",
      "steps per second: 0.48646\n",
      "step: 33494\n",
      "loss: 12.752415657043457\n",
      "steps per second: 0.51414\n",
      "step: 33495\n",
      "loss: 12.77490520477295\n",
      "steps per second: 0.50638\n",
      "step: 33496\n",
      "loss: 12.82334041595459\n",
      "steps per second: 0.52874\n",
      "step: 33497\n",
      "loss: 12.598891258239746\n",
      "steps per second: 0.52772\n",
      "step: 33498\n",
      "loss: 12.745542526245117\n",
      "steps per second: 0.52484\n",
      "step: 33499\n",
      "loss: 12.952905654907227\n",
      "steps per second: 0.54121\n",
      "step: 33500\n",
      "loss: 12.485855102539062\n",
      "steps per second: 0.53469\n",
      "step: 33501\n",
      "loss: 12.934873580932617\n",
      "steps per second: 0.52593\n",
      "step: 33502\n",
      "loss: 13.860538482666016\n",
      "steps per second: 0.53806\n",
      "step: 33503\n",
      "loss: 13.001287460327148\n",
      "steps per second: 0.53946\n",
      "step: 33504\n",
      "loss: 13.075811386108398\n",
      "steps per second: 0.52147\n",
      "step: 33505\n",
      "loss: 13.548497200012207\n",
      "steps per second: 0.56143\n",
      "step: 33506\n",
      "loss: 12.585719108581543\n",
      "steps per second: 0.53781\n",
      "step: 33507\n",
      "loss: 12.687164306640625\n",
      "steps per second: 0.54712\n",
      "step: 33508\n",
      "loss: 12.836557388305664\n",
      "steps per second: 0.53259\n",
      "step: 33509\n",
      "loss: 12.928441047668457\n",
      "steps per second: 0.51468\n",
      "step: 33510\n",
      "loss: 13.023338317871094\n",
      "steps per second: 0.50850\n",
      "step: 33511\n",
      "loss: 13.035913467407227\n",
      "steps per second: 0.55830\n",
      "step: 33512\n",
      "loss: 12.86664867401123\n",
      "steps per second: 0.54302\n",
      "step: 33513\n",
      "loss: 12.354429244995117\n",
      "steps per second: 0.54035\n",
      "step: 33514\n",
      "loss: 12.495400428771973\n",
      "steps per second: 0.59733\n",
      "step: 33515\n",
      "loss: 12.433538436889648\n",
      "steps per second: 0.51785\n",
      "step: 33516\n",
      "loss: 13.194759368896484\n",
      "steps per second: 0.56088\n",
      "step: 33517\n",
      "loss: 12.764569282531738\n",
      "steps per second: 0.49903\n",
      "step: 33518\n",
      "loss: 12.83835506439209\n",
      "steps per second: 0.54730\n",
      "step: 33519\n",
      "loss: 12.453306198120117\n",
      "steps per second: 0.54474\n",
      "step: 33520\n",
      "loss: 13.291922569274902\n",
      "steps per second: 0.50717\n",
      "step: 33521\n",
      "loss: 13.061848640441895\n",
      "steps per second: 0.55809\n",
      "step: 33522\n",
      "loss: 13.067353248596191\n",
      "steps per second: 0.54173\n",
      "step: 33523\n",
      "loss: 12.866432189941406\n",
      "steps per second: 0.54068\n",
      "step: 33524\n",
      "loss: 12.707316398620605\n",
      "steps per second: 0.54237\n",
      "step: 33525\n",
      "loss: 13.018647193908691\n",
      "steps per second: 0.54751\n",
      "step: 33526\n",
      "loss: 12.8653564453125\n",
      "steps per second: 0.52712\n",
      "step: 33527\n",
      "loss: 13.308965682983398\n",
      "steps per second: 0.51529\n",
      "step: 33528\n",
      "loss: 12.476919174194336\n",
      "steps per second: 0.53919\n",
      "step: 33529\n",
      "loss: 12.926168441772461\n",
      "steps per second: 0.55720\n",
      "step: 33530\n",
      "loss: 13.047982215881348\n",
      "steps per second: 0.56347\n",
      "step: 33531\n",
      "loss: 12.79423999786377\n",
      "steps per second: 0.54327\n",
      "step: 33532\n",
      "loss: 12.797042846679688\n",
      "steps per second: 0.55689\n",
      "step: 33533\n",
      "loss: 13.204075813293457\n",
      "steps per second: 0.55970\n",
      "step: 33534\n",
      "loss: 13.049695014953613\n",
      "steps per second: 0.51551\n",
      "step: 33535\n",
      "loss: 12.550959587097168\n",
      "steps per second: 0.51395\n",
      "step: 33536\n",
      "loss: 13.498568534851074\n",
      "steps per second: 0.53299\n",
      "step: 33537\n",
      "loss: 12.50915813446045\n",
      "steps per second: 0.51698\n",
      "step: 33538\n",
      "loss: 12.858903884887695\n",
      "steps per second: 0.55824\n",
      "step: 33539\n",
      "loss: 12.964651107788086\n",
      "steps per second: 0.55214\n",
      "step: 33540\n",
      "loss: 12.747875213623047\n",
      "steps per second: 0.50471\n",
      "step: 33541\n",
      "loss: 12.97275161743164\n",
      "steps per second: 0.54587\n",
      "step: 33542\n",
      "loss: 12.344277381896973\n",
      "steps per second: 0.56816\n",
      "step: 33543\n",
      "loss: 13.237446784973145\n",
      "steps per second: 0.56623\n",
      "step: 33544\n",
      "loss: 12.439814567565918\n",
      "steps per second: 0.55930\n",
      "step: 33545\n",
      "loss: 12.553677558898926\n",
      "steps per second: 0.54127\n",
      "step: 33546\n",
      "loss: 12.44066047668457\n",
      "steps per second: 0.50293\n",
      "step: 33547\n",
      "loss: 12.982508659362793\n",
      "steps per second: 0.59689\n",
      "step: 33548\n",
      "loss: 13.563206672668457\n",
      "steps per second: 0.54002\n",
      "step: 33549\n",
      "loss: 13.015596389770508\n",
      "steps per second: 0.54384\n",
      "step: 33550\n",
      "loss: 12.647819519042969\n",
      "steps per second: 0.54581\n",
      "step: 33551\n",
      "loss: 13.153631210327148\n",
      "steps per second: 0.51336\n",
      "step: 33552\n",
      "loss: 13.396486282348633\n",
      "steps per second: 0.54265\n",
      "step: 33553\n",
      "loss: 12.715580940246582\n",
      "steps per second: 0.53529\n",
      "step: 33554\n",
      "loss: 13.137374877929688\n",
      "steps per second: 0.54400\n",
      "step: 33555\n",
      "loss: 12.4785737991333\n",
      "steps per second: 0.56399\n",
      "step: 33556\n",
      "loss: 13.581774711608887\n",
      "steps per second: 0.58553\n",
      "step: 33557\n",
      "loss: 12.82302188873291\n",
      "steps per second: 0.53086\n",
      "step: 33558\n",
      "loss: 12.70148754119873\n",
      "steps per second: 0.52748\n",
      "step: 33559\n",
      "loss: 13.16866397857666\n",
      "steps per second: 0.54802\n",
      "step: 33560\n",
      "loss: 12.970453262329102\n",
      "steps per second: 0.54570\n",
      "step: 33561\n",
      "loss: 13.135462760925293\n",
      "steps per second: 0.54614\n",
      "step: 33562\n",
      "loss: 12.743433952331543\n",
      "steps per second: 0.54781\n",
      "step: 33563\n",
      "loss: 12.414335250854492\n",
      "steps per second: 0.53883\n",
      "step: 33564\n",
      "loss: 12.777606964111328\n",
      "steps per second: 0.53271\n",
      "step: 33565\n",
      "loss: 12.988423347473145\n",
      "steps per second: 0.54641\n",
      "step: 33566\n",
      "loss: 12.17592716217041\n",
      "steps per second: 0.55474\n",
      "step: 33567\n",
      "loss: 13.142951011657715\n",
      "steps per second: 0.53159\n",
      "step: 33568\n",
      "loss: 12.64200210571289\n",
      "steps per second: 0.53411\n",
      "step: 33569\n",
      "loss: 13.092362403869629\n",
      "steps per second: 0.58445\n",
      "step: 33570\n",
      "loss: 12.967239379882812\n",
      "steps per second: 0.51650\n",
      "step: 33571\n",
      "loss: 13.236204147338867\n",
      "steps per second: 0.53517\n",
      "step: 33572\n",
      "loss: 12.353739738464355\n",
      "steps per second: 0.51970\n",
      "step: 33573\n",
      "loss: 12.687018394470215\n",
      "steps per second: 0.55815\n",
      "step: 33574\n",
      "loss: 12.880873680114746\n",
      "steps per second: 0.55723\n",
      "step: 33575\n",
      "loss: 12.4357328414917\n",
      "steps per second: 0.53813\n",
      "step: 33576\n",
      "loss: 13.118671417236328\n",
      "steps per second: 0.55884\n",
      "step: 33577\n",
      "loss: 12.748773574829102\n",
      "steps per second: 0.51720\n",
      "step: 33578\n",
      "loss: 12.765220642089844\n",
      "steps per second: 0.54010\n",
      "step: 33579\n",
      "loss: 13.043757438659668\n",
      "steps per second: 0.56474\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8425724506378174, layer: 10\n",
      "saving at step 33579\n",
      "----------\n",
      "\n",
      "\n",
      "step: 33580\n",
      "loss: 12.323037147521973\n",
      "steps per second: 0.29144\n",
      "step: 33581\n",
      "loss: 12.69202995300293\n",
      "steps per second: 0.56527\n",
      "step: 33582\n",
      "loss: 12.621536254882812\n",
      "steps per second: 0.57256\n",
      "step: 33583\n",
      "loss: 13.290034294128418\n",
      "steps per second: 0.59368\n",
      "step: 33584\n",
      "loss: 12.20200252532959\n",
      "steps per second: 0.52760\n",
      "step: 33585\n",
      "loss: 12.836712837219238\n",
      "steps per second: 0.54534\n",
      "step: 33586\n",
      "loss: 12.489420890808105\n",
      "steps per second: 0.55904\n",
      "step: 33587\n",
      "loss: 12.86121654510498\n",
      "steps per second: 0.54037\n",
      "step: 33588\n",
      "loss: 13.354388236999512\n",
      "steps per second: 0.54431\n",
      "step: 33589\n",
      "loss: 12.520459175109863\n",
      "steps per second: 0.54167\n",
      "step: 33590\n",
      "loss: 13.0919828414917\n",
      "steps per second: 0.54091\n",
      "step: 33591\n",
      "loss: 13.045062065124512\n",
      "steps per second: 0.52955\n",
      "step: 33592\n",
      "loss: 12.618431091308594\n",
      "steps per second: 0.52774\n",
      "step: 33593\n",
      "loss: 12.830780982971191\n",
      "steps per second: 0.55599\n",
      "step: 33594\n",
      "loss: 12.48214340209961\n",
      "steps per second: 0.54182\n",
      "step: 33595\n",
      "loss: 12.415715217590332\n",
      "steps per second: 0.53513\n",
      "step: 33596\n",
      "loss: 12.323746681213379\n",
      "steps per second: 0.54114\n",
      "step: 33597\n",
      "loss: 12.77271842956543\n",
      "steps per second: 0.55946\n",
      "step: 33598\n",
      "loss: 12.63696575164795\n",
      "steps per second: 0.51631\n",
      "step: 33599\n",
      "loss: 13.04808521270752\n",
      "steps per second: 0.54838\n",
      "step: 33600\n",
      "loss: 12.879393577575684\n",
      "steps per second: 0.54826\n",
      "step: 33601\n",
      "loss: 12.851588249206543\n",
      "steps per second: 0.56563\n",
      "step: 33602\n",
      "loss: 12.68373966217041\n",
      "steps per second: 0.51513\n",
      "step: 33603\n",
      "loss: 12.408238410949707\n",
      "steps per second: 0.54005\n",
      "step: 33604\n",
      "loss: 12.998934745788574\n",
      "steps per second: 0.54637\n",
      "step: 33605\n",
      "loss: 12.906078338623047\n",
      "steps per second: 0.55631\n",
      "step: 33606\n",
      "loss: 12.659146308898926\n",
      "steps per second: 0.55333\n",
      "step: 33607\n",
      "loss: 12.76131820678711\n",
      "steps per second: 0.53324\n",
      "step: 33608\n",
      "loss: 12.53441047668457\n",
      "steps per second: 0.54683\n",
      "step: 33609\n",
      "loss: 13.113121032714844\n",
      "steps per second: 0.51878\n",
      "step: 33610\n",
      "loss: 12.75238037109375\n",
      "steps per second: 0.55736\n",
      "step: 33611\n",
      "loss: 12.883641242980957\n",
      "steps per second: 0.55043\n",
      "step: 33612\n",
      "loss: 12.301587104797363\n",
      "steps per second: 0.54112\n",
      "step: 33613\n",
      "loss: 13.302262306213379\n",
      "steps per second: 0.56622\n",
      "step: 33614\n",
      "loss: 12.88472843170166\n",
      "steps per second: 0.50692\n",
      "step: 33615\n",
      "loss: 12.59325885772705\n",
      "steps per second: 0.48910\n",
      "step: 33616\n",
      "loss: 12.360295295715332\n",
      "steps per second: 0.53835\n",
      "step: 33617\n",
      "loss: 12.868185997009277\n",
      "steps per second: 0.54638\n",
      "step: 33618\n",
      "loss: 13.015287399291992\n",
      "steps per second: 0.53382\n",
      "step: 33619\n",
      "loss: 12.95470142364502\n",
      "steps per second: 0.57515\n",
      "step: 33620\n",
      "loss: 12.818967819213867\n",
      "steps per second: 0.56467\n",
      "step: 33621\n",
      "loss: 12.76353931427002\n",
      "steps per second: 0.52829\n",
      "step: 33622\n",
      "loss: 12.783361434936523\n",
      "steps per second: 0.54772\n",
      "step: 33623\n",
      "loss: 12.712745666503906\n",
      "steps per second: 0.49782\n",
      "step: 33624\n",
      "loss: 12.804757118225098\n",
      "steps per second: 0.54351\n",
      "step: 33625\n",
      "loss: 12.475858688354492\n",
      "steps per second: 0.54523\n",
      "step: 33626\n",
      "loss: 13.075946807861328\n",
      "steps per second: 0.51248\n",
      "step: 33627\n",
      "loss: 12.803147315979004\n",
      "steps per second: 0.53994\n",
      "step: 33628\n",
      "loss: 13.002220153808594\n",
      "steps per second: 0.51670\n",
      "step: 33629\n",
      "loss: 12.89142894744873\n",
      "steps per second: 0.55145\n",
      "step: 33630\n",
      "loss: 13.03883171081543\n",
      "steps per second: 0.56531\n",
      "step: 33631\n",
      "loss: 12.562641143798828\n",
      "steps per second: 0.55830\n",
      "step: 33632\n",
      "loss: 12.970919609069824\n",
      "steps per second: 0.53361\n",
      "step: 33633\n",
      "loss: 12.44133472442627\n",
      "steps per second: 0.54634\n",
      "step: 33634\n",
      "loss: 12.956687927246094\n",
      "steps per second: 0.52765\n",
      "step: 33635\n",
      "loss: 13.034501075744629\n",
      "steps per second: 0.53899\n",
      "step: 33636\n",
      "loss: 12.701157569885254\n",
      "steps per second: 0.51434\n",
      "step: 33637\n",
      "loss: 12.682225227355957\n",
      "steps per second: 0.55350\n",
      "step: 33638\n",
      "loss: 12.747709274291992\n",
      "steps per second: 0.53746\n",
      "step: 33639\n",
      "loss: 12.71886920928955\n",
      "steps per second: 0.51525\n",
      "step: 33640\n",
      "loss: 12.854330062866211\n",
      "steps per second: 0.54573\n",
      "step: 33641\n",
      "loss: 12.5031156539917\n",
      "steps per second: 0.51533\n",
      "step: 33642\n",
      "loss: 12.72790241241455\n",
      "steps per second: 0.51609\n",
      "step: 33643\n",
      "loss: 12.460990905761719\n",
      "steps per second: 0.56350\n",
      "step: 33644\n",
      "loss: 12.550944328308105\n",
      "steps per second: 0.52793\n",
      "step: 33645\n",
      "loss: 12.690873146057129\n",
      "steps per second: 0.54325\n",
      "step: 33646\n",
      "loss: 12.670795440673828\n",
      "steps per second: 0.51739\n",
      "step: 33647\n",
      "loss: 12.631174087524414\n",
      "steps per second: 0.53840\n",
      "step: 33648\n",
      "loss: 13.15764331817627\n",
      "steps per second: 0.53487\n",
      "step: 33649\n",
      "loss: 12.572924613952637\n",
      "steps per second: 0.59550\n",
      "step: 33650\n",
      "loss: 12.691295623779297\n",
      "steps per second: 0.50711\n",
      "step: 33651\n",
      "loss: 13.223129272460938\n",
      "steps per second: 0.54816\n",
      "step: 33652\n",
      "loss: 13.157732009887695\n",
      "steps per second: 0.54527\n",
      "step: 33653\n",
      "loss: 12.96487808227539\n",
      "steps per second: 0.52488\n",
      "step: 33654\n",
      "loss: 13.015413284301758\n",
      "steps per second: 0.52724\n",
      "step: 33655\n",
      "loss: 13.716968536376953\n",
      "steps per second: 0.55232\n",
      "step: 33656\n",
      "loss: 12.653342247009277\n",
      "steps per second: 0.56549\n",
      "step: 33657\n",
      "loss: 12.253575325012207\n",
      "steps per second: 0.56287\n",
      "step: 33658\n",
      "loss: 13.346302032470703\n",
      "steps per second: 0.49807\n",
      "step: 33659\n",
      "loss: 12.504045486450195\n",
      "steps per second: 0.55412\n",
      "step: 33660\n",
      "loss: 12.520340919494629\n",
      "steps per second: 0.52824\n",
      "step: 33661\n",
      "loss: 13.004855155944824\n",
      "steps per second: 0.54219\n",
      "step: 33662\n",
      "loss: 13.009160995483398\n",
      "steps per second: 0.56580\n",
      "step: 33663\n",
      "loss: 12.59339427947998\n",
      "steps per second: 0.52390\n",
      "step: 33664\n",
      "loss: 12.603373527526855\n",
      "steps per second: 0.55918\n",
      "step: 33665\n",
      "loss: 13.486982345581055\n",
      "steps per second: 0.54584\n",
      "step: 33666\n",
      "loss: 12.77157974243164\n",
      "steps per second: 0.54169\n",
      "step: 33667\n",
      "loss: 12.890329360961914\n",
      "steps per second: 0.51703\n",
      "step: 33668\n",
      "loss: 12.470656394958496\n",
      "steps per second: 0.53400\n",
      "step: 33669\n",
      "loss: 12.320367813110352\n",
      "steps per second: 0.53635\n",
      "step: 33670\n",
      "loss: 13.016778945922852\n",
      "steps per second: 0.51667\n",
      "step: 33671\n",
      "loss: 12.500489234924316\n",
      "steps per second: 0.55769\n",
      "step: 33672\n",
      "loss: 13.17918872833252\n",
      "steps per second: 0.56442\n",
      "step: 33673\n",
      "loss: 12.431048393249512\n",
      "steps per second: 0.53958\n",
      "step: 33674\n",
      "loss: 12.557138442993164\n",
      "steps per second: 0.59927\n",
      "step: 33675\n",
      "loss: 12.46702766418457\n",
      "steps per second: 0.53420\n",
      "step: 33676\n",
      "loss: 12.842703819274902\n",
      "steps per second: 0.49062\n",
      "step: 33677\n",
      "loss: 12.903647422790527\n",
      "steps per second: 0.49830\n",
      "step: 33678\n",
      "loss: 13.094143867492676\n",
      "steps per second: 0.48750\n",
      "step: 33679\n",
      "loss: 12.578520774841309\n",
      "steps per second: 0.55474\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7769771218299866, layer: 11\n",
      "saving at step 33679\n",
      "----------\n",
      "\n",
      "\n",
      "step: 33680\n",
      "loss: 12.821560859680176\n",
      "steps per second: 0.28395\n",
      "step: 33681\n",
      "loss: 13.177245140075684\n",
      "steps per second: 0.51325\n",
      "step: 33682\n",
      "loss: 13.190094947814941\n",
      "steps per second: 0.51734\n",
      "step: 33683\n",
      "loss: 12.73948860168457\n",
      "steps per second: 0.56453\n",
      "step: 33684\n",
      "loss: 12.840066909790039\n",
      "steps per second: 0.54726\n",
      "step: 33685\n",
      "loss: 12.778890609741211\n",
      "steps per second: 0.53537\n",
      "step: 33686\n",
      "loss: 12.622607231140137\n",
      "steps per second: 0.55949\n",
      "step: 33687\n",
      "loss: 13.076875686645508\n",
      "steps per second: 0.54336\n",
      "step: 33688\n",
      "loss: 13.101737022399902\n",
      "steps per second: 0.53991\n",
      "step: 33689\n",
      "loss: 12.647695541381836\n",
      "steps per second: 0.53503\n",
      "step: 33690\n",
      "loss: 12.966349601745605\n",
      "steps per second: 0.51586\n",
      "step: 33691\n",
      "loss: 13.724496841430664\n",
      "steps per second: 0.49025\n",
      "step: 33692\n",
      "loss: 12.339404106140137\n",
      "steps per second: 0.53410\n",
      "step: 33693\n",
      "loss: 12.160122871398926\n",
      "steps per second: 0.54821\n",
      "step: 33694\n",
      "loss: 13.129345893859863\n",
      "steps per second: 0.53129\n",
      "step: 33695\n",
      "loss: 12.49571704864502\n",
      "steps per second: 0.53844\n",
      "step: 33696\n",
      "loss: 12.592655181884766\n",
      "steps per second: 0.53715\n",
      "step: 33697\n",
      "loss: 13.347755432128906\n",
      "steps per second: 0.53707\n",
      "step: 33698\n",
      "loss: 13.077537536621094\n",
      "steps per second: 0.48762\n",
      "step: 33699\n",
      "loss: 12.6893949508667\n",
      "steps per second: 0.54337\n",
      "step: 33700\n",
      "loss: 12.435829162597656\n",
      "steps per second: 0.54603\n",
      "step: 33701\n",
      "loss: 12.919771194458008\n",
      "steps per second: 0.49356\n",
      "step: 33702\n",
      "loss: 12.937695503234863\n",
      "steps per second: 0.51631\n",
      "step: 33703\n",
      "loss: 13.379769325256348\n",
      "steps per second: 0.56795\n",
      "step: 33704\n",
      "loss: 11.794360160827637\n",
      "steps per second: 0.53319\n",
      "step: 33705\n",
      "loss: 12.733978271484375\n",
      "steps per second: 0.56693\n",
      "step: 33706\n",
      "loss: 12.710204124450684\n",
      "steps per second: 0.56429\n",
      "step: 33707\n",
      "loss: 12.706974983215332\n",
      "steps per second: 0.52747\n",
      "step: 33708\n",
      "loss: 12.516475677490234\n",
      "steps per second: 0.54780\n",
      "step: 33709\n",
      "loss: 13.118642807006836\n",
      "steps per second: 0.53351\n",
      "step: 33710\n",
      "loss: 12.985101699829102\n",
      "steps per second: 0.52297\n",
      "step: 33711\n",
      "loss: 12.741653442382812\n",
      "steps per second: 0.53351\n",
      "step: 33712\n",
      "loss: 12.78233814239502\n",
      "steps per second: 0.54221\n",
      "step: 33713\n",
      "loss: 13.066269874572754\n",
      "steps per second: 0.50561\n",
      "step: 33714\n",
      "loss: 12.420889854431152\n",
      "steps per second: 0.55791\n",
      "step: 33715\n",
      "loss: 12.969991683959961\n",
      "steps per second: 0.53498\n",
      "step: 33716\n",
      "loss: 13.093799591064453\n",
      "steps per second: 0.51336\n",
      "step: 33717\n",
      "loss: 12.900625228881836\n",
      "steps per second: 0.60105\n",
      "step: 33718\n",
      "loss: 12.713972091674805\n",
      "steps per second: 0.54244\n",
      "step: 33719\n",
      "loss: 12.584477424621582\n",
      "steps per second: 0.54162\n",
      "step: 33720\n",
      "loss: 12.90962028503418\n",
      "steps per second: 0.52868\n",
      "step: 33721\n",
      "loss: 12.549918174743652\n",
      "steps per second: 0.56639\n",
      "step: 33722\n",
      "loss: 12.783562660217285\n",
      "steps per second: 0.46967\n",
      "step: 33723\n",
      "loss: 12.978231430053711\n",
      "steps per second: 0.52979\n",
      "step: 33724\n",
      "loss: 13.307430267333984\n",
      "steps per second: 0.54710\n",
      "step: 33725\n",
      "loss: 12.520096778869629\n",
      "steps per second: 0.54044\n",
      "step: 33726\n",
      "loss: 12.65148639678955\n",
      "steps per second: 0.52982\n",
      "step: 33727\n",
      "loss: 12.441610336303711\n",
      "steps per second: 0.53441\n",
      "step: 33728\n",
      "loss: 12.873716354370117\n",
      "steps per second: 0.55878\n",
      "step: 33729\n",
      "loss: 12.325506210327148\n",
      "steps per second: 0.51584\n",
      "step: 33730\n",
      "loss: 12.6354341506958\n",
      "steps per second: 0.59874\n",
      "step: 33731\n",
      "loss: 12.613655090332031\n",
      "steps per second: 0.55747\n",
      "step: 33732\n",
      "loss: 13.132205963134766\n",
      "steps per second: 0.55839\n",
      "step: 33733\n",
      "loss: 12.607205390930176\n",
      "steps per second: 0.52743\n",
      "step: 33734\n",
      "loss: 12.387899398803711\n",
      "steps per second: 0.60005\n",
      "step: 33735\n",
      "loss: 12.236227989196777\n",
      "steps per second: 0.56696\n",
      "step: 33736\n",
      "loss: 12.619895935058594\n",
      "steps per second: 0.51842\n",
      "step: 33737\n",
      "loss: 13.220569610595703\n",
      "steps per second: 0.51512\n",
      "step: 33738\n",
      "loss: 12.387735366821289\n",
      "steps per second: 0.49716\n",
      "step: 33739\n",
      "loss: 12.284442901611328\n",
      "steps per second: 0.51591\n",
      "step: 33740\n",
      "loss: 13.291862487792969\n",
      "steps per second: 0.48641\n",
      "step: 33741\n",
      "loss: 12.842422485351562\n",
      "steps per second: 0.52221\n",
      "step: 33742\n",
      "loss: 12.970670700073242\n",
      "steps per second: 0.54124\n",
      "step: 33743\n",
      "loss: 12.551549911499023\n",
      "steps per second: 0.59845\n",
      "step: 33744\n",
      "loss: 13.11641788482666\n",
      "steps per second: 0.51575\n",
      "step: 33745\n",
      "loss: 13.232355117797852\n",
      "steps per second: 0.54119\n",
      "step: 33746\n",
      "loss: 12.582362174987793\n",
      "steps per second: 0.56659\n",
      "step: 33747\n",
      "loss: 13.005401611328125\n",
      "steps per second: 0.55893\n",
      "step: 33748\n",
      "loss: 12.863139152526855\n",
      "steps per second: 0.48741\n",
      "step: 33749\n",
      "loss: 12.730606079101562\n",
      "steps per second: 0.54575\n",
      "step: 33750\n",
      "loss: 12.904151916503906\n",
      "steps per second: 0.55854\n",
      "step: 33751\n",
      "loss: 12.965890884399414\n",
      "steps per second: 0.54677\n",
      "step: 33752\n",
      "loss: 12.903295516967773\n",
      "steps per second: 0.54665\n",
      "step: 33753\n",
      "loss: 12.91366195678711\n",
      "steps per second: 0.55962\n",
      "step: 33754\n",
      "loss: 12.673358917236328\n",
      "steps per second: 0.53273\n",
      "step: 33755\n",
      "loss: 13.039600372314453\n",
      "steps per second: 0.56366\n",
      "step: 33756\n",
      "loss: 12.73753833770752\n",
      "steps per second: 0.53472\n",
      "step: 33757\n",
      "loss: 13.412612915039062\n",
      "steps per second: 0.53363\n",
      "step: 33758\n",
      "loss: 12.832200050354004\n",
      "steps per second: 0.54114\n",
      "step: 33759\n",
      "loss: 12.777409553527832\n",
      "steps per second: 0.51845\n",
      "step: 33760\n",
      "loss: 12.910908699035645\n",
      "steps per second: 0.56899\n",
      "step: 33761\n",
      "loss: 12.931882858276367\n",
      "steps per second: 0.54673\n",
      "step: 33762\n",
      "loss: 13.210103034973145\n",
      "steps per second: 0.54235\n",
      "step: 33763\n",
      "loss: 12.668290138244629\n",
      "steps per second: 0.54374\n",
      "step: 33764\n",
      "loss: 12.856756210327148\n",
      "steps per second: 0.53366\n",
      "step: 33765\n",
      "loss: 12.887900352478027\n",
      "steps per second: 0.55604\n",
      "step: 33766\n",
      "loss: 12.468551635742188\n",
      "steps per second: 0.53389\n",
      "step: 33767\n",
      "loss: 13.042824745178223\n",
      "steps per second: 0.51315\n",
      "step: 33768\n",
      "loss: 12.275022506713867\n",
      "steps per second: 0.49280\n",
      "step: 33769\n",
      "loss: 13.22912883758545\n",
      "steps per second: 0.55930\n",
      "step: 33770\n",
      "loss: 12.730852127075195\n",
      "steps per second: 0.52767\n",
      "step: 33771\n",
      "loss: 12.69520092010498\n",
      "steps per second: 0.54382\n",
      "step: 33772\n",
      "loss: 12.872045516967773\n",
      "steps per second: 0.51531\n",
      "step: 33773\n",
      "loss: 13.10813045501709\n",
      "steps per second: 0.52579\n",
      "step: 33774\n",
      "loss: 12.632848739624023\n",
      "steps per second: 0.51861\n",
      "step: 33775\n",
      "loss: 12.859383583068848\n",
      "steps per second: 0.50701\n",
      "step: 33776\n",
      "loss: 12.704523086547852\n",
      "steps per second: 0.54743\n",
      "step: 33777\n",
      "loss: 12.555702209472656\n",
      "steps per second: 0.49616\n",
      "step: 33778\n",
      "loss: 12.994744300842285\n",
      "steps per second: 0.48762\n",
      "step: 33779\n",
      "loss: 12.66473388671875\n",
      "steps per second: 0.53433\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7937371730804443, layer: 10\n",
      "saving at step 33779\n",
      "----------\n",
      "\n",
      "\n",
      "step: 33780\n",
      "loss: 12.64682388305664\n",
      "steps per second: 0.27567\n",
      "step: 33781\n",
      "loss: 13.28734302520752\n",
      "steps per second: 0.53629\n",
      "step: 33782\n",
      "loss: 12.749717712402344\n",
      "steps per second: 0.49246\n",
      "step: 33783\n",
      "loss: 12.756437301635742\n",
      "steps per second: 0.52272\n",
      "step: 33784\n",
      "loss: 12.735546112060547\n",
      "steps per second: 0.59070\n",
      "step: 33785\n",
      "loss: 13.140485763549805\n",
      "steps per second: 0.49392\n",
      "step: 33786\n",
      "loss: 12.358672142028809\n",
      "steps per second: 0.54099\n",
      "step: 33787\n",
      "loss: 12.926068305969238\n",
      "steps per second: 0.56727\n",
      "step: 33788\n",
      "loss: 12.839700698852539\n",
      "steps per second: 0.56040\n",
      "step: 33789\n",
      "loss: 12.157020568847656\n",
      "steps per second: 0.60041\n",
      "step: 33790\n",
      "loss: 12.585061073303223\n",
      "steps per second: 0.49499\n",
      "step: 33791\n",
      "loss: 12.61092472076416\n",
      "steps per second: 0.59839\n",
      "step: 33792\n",
      "loss: 12.673062324523926\n",
      "steps per second: 0.54549\n",
      "step: 33793\n",
      "loss: 12.544007301330566\n",
      "steps per second: 0.55894\n",
      "step: 33794\n",
      "loss: 13.149290084838867\n",
      "steps per second: 0.54017\n",
      "step: 33795\n",
      "loss: 12.943446159362793\n",
      "steps per second: 0.56528\n",
      "step: 33796\n",
      "loss: 12.796259880065918\n",
      "steps per second: 0.53994\n",
      "step: 33797\n",
      "loss: 13.152485847473145\n",
      "steps per second: 0.52393\n",
      "step: 33798\n",
      "loss: 13.019091606140137\n",
      "steps per second: 0.53775\n",
      "step: 33799\n",
      "loss: 12.83051872253418\n",
      "steps per second: 0.55625\n",
      "step: 33800\n",
      "loss: 13.124975204467773\n",
      "steps per second: 0.53325\n",
      "step: 33801\n",
      "loss: 11.748574256896973\n",
      "steps per second: 0.54163\n",
      "step: 33802\n",
      "loss: 13.167887687683105\n",
      "steps per second: 0.54024\n",
      "step: 33803\n",
      "loss: 12.3629789352417\n",
      "steps per second: 0.54170\n",
      "step: 33804\n",
      "loss: 13.104654312133789\n",
      "steps per second: 0.56481\n",
      "step: 33805\n",
      "loss: 12.302284240722656\n",
      "steps per second: 0.48189\n",
      "step: 33806\n",
      "loss: 12.774253845214844\n",
      "steps per second: 0.53261\n",
      "step: 33807\n",
      "loss: 12.944536209106445\n",
      "steps per second: 0.52712\n",
      "step: 33808\n",
      "loss: 12.802817344665527\n",
      "steps per second: 0.52746\n",
      "step: 33809\n",
      "loss: 12.489376068115234\n",
      "steps per second: 0.52838\n",
      "step: 33810\n",
      "loss: 12.625612258911133\n",
      "steps per second: 0.48763\n",
      "step: 33811\n",
      "loss: 12.981992721557617\n",
      "steps per second: 0.55444\n",
      "step: 33812\n",
      "loss: 12.809449195861816\n",
      "steps per second: 0.53395\n",
      "step: 33813\n",
      "loss: 12.952454566955566\n",
      "steps per second: 0.53634\n",
      "step: 33814\n",
      "loss: 13.5217866897583\n",
      "steps per second: 0.53176\n",
      "step: 33815\n",
      "loss: 12.089017868041992\n",
      "steps per second: 0.53155\n",
      "step: 33816\n",
      "loss: 13.173380851745605\n",
      "steps per second: 0.53868\n",
      "step: 33817\n",
      "loss: 13.123433113098145\n",
      "steps per second: 0.59720\n",
      "step: 33818\n",
      "loss: 13.004839897155762\n",
      "steps per second: 0.53896\n",
      "step: 33819\n",
      "loss: 12.401662826538086\n",
      "steps per second: 0.59942\n",
      "step: 33820\n",
      "loss: 13.116259574890137\n",
      "steps per second: 0.56100\n",
      "step: 33821\n",
      "loss: 12.739930152893066\n",
      "steps per second: 0.57446\n",
      "step: 33822\n",
      "loss: 12.336822509765625\n",
      "steps per second: 0.54534\n",
      "step: 33823\n",
      "loss: 12.864285469055176\n",
      "steps per second: 0.52295\n",
      "step: 33824\n",
      "loss: 12.786092758178711\n",
      "steps per second: 0.54349\n",
      "step: 33825\n",
      "loss: 12.73818588256836\n",
      "steps per second: 0.54406\n",
      "step: 33826\n",
      "loss: 13.265298843383789\n",
      "steps per second: 0.59637\n",
      "step: 33827\n",
      "loss: 12.716292381286621\n",
      "steps per second: 0.54225\n",
      "step: 33828\n",
      "loss: 12.768750190734863\n",
      "steps per second: 0.56666\n",
      "step: 33829\n",
      "loss: 12.435632705688477\n",
      "steps per second: 0.53783\n",
      "step: 33830\n",
      "loss: 12.307165145874023\n",
      "steps per second: 0.57299\n",
      "step: 33831\n",
      "loss: 12.836538314819336\n",
      "steps per second: 0.56619\n",
      "step: 33832\n",
      "loss: 12.470715522766113\n",
      "steps per second: 0.56749\n",
      "step: 33833\n",
      "loss: 13.017160415649414\n",
      "steps per second: 0.54851\n",
      "step: 33834\n",
      "loss: 12.84749698638916\n",
      "steps per second: 0.54553\n",
      "step: 33835\n",
      "loss: 12.703951835632324\n",
      "steps per second: 0.51634\n",
      "step: 33836\n",
      "loss: 12.758612632751465\n",
      "steps per second: 0.52668\n",
      "step: 33837\n",
      "loss: 12.343310356140137\n",
      "steps per second: 0.54677\n",
      "step: 33838\n",
      "loss: 13.202020645141602\n",
      "steps per second: 0.54550\n",
      "step: 33839\n",
      "loss: 12.871459007263184\n",
      "steps per second: 0.56131\n",
      "step: 33840\n",
      "loss: 12.95079517364502\n",
      "steps per second: 0.51823\n",
      "step: 33841\n",
      "loss: 12.801627159118652\n",
      "steps per second: 0.54154\n",
      "step: 33842\n",
      "loss: 13.096075057983398\n",
      "steps per second: 0.55966\n",
      "step: 33843\n",
      "loss: 12.55186653137207\n",
      "steps per second: 0.54033\n",
      "step: 33844\n",
      "loss: 13.049379348754883\n",
      "steps per second: 0.54481\n",
      "step: 33845\n",
      "loss: 12.96744441986084\n",
      "steps per second: 0.52726\n",
      "step: 33846\n",
      "loss: 12.782258987426758\n",
      "steps per second: 0.54608\n",
      "step: 33847\n",
      "loss: 13.036096572875977\n",
      "steps per second: 0.54732\n",
      "step: 33848\n",
      "loss: 13.011380195617676\n",
      "steps per second: 0.52442\n",
      "step: 33849\n",
      "loss: 12.970413208007812\n",
      "steps per second: 0.55684\n",
      "step: 33850\n",
      "loss: 12.573376655578613\n",
      "steps per second: 0.54447\n",
      "step: 33851\n",
      "loss: 12.902165412902832\n",
      "steps per second: 0.51095\n",
      "step: 33852\n",
      "loss: 13.39717960357666\n",
      "steps per second: 0.59952\n",
      "step: 33853\n",
      "loss: 12.445773124694824\n",
      "steps per second: 0.54303\n",
      "step: 33854\n",
      "loss: 12.849333763122559\n",
      "steps per second: 0.53583\n",
      "step: 33855\n",
      "loss: 12.964486122131348\n",
      "steps per second: 0.51807\n",
      "step: 33856\n",
      "loss: 13.371749877929688\n",
      "steps per second: 0.55821\n",
      "step: 33857\n",
      "loss: 12.924921035766602\n",
      "steps per second: 0.54286\n",
      "step: 33858\n",
      "loss: 12.086769104003906\n",
      "steps per second: 0.49887\n",
      "step: 33859\n",
      "loss: 12.92380428314209\n",
      "steps per second: 0.53425\n",
      "step: 33860\n",
      "loss: 12.736967086791992\n",
      "steps per second: 0.51874\n",
      "step: 33861\n",
      "loss: 13.267993927001953\n",
      "steps per second: 0.53343\n",
      "step: 33862\n",
      "loss: 12.930058479309082\n",
      "steps per second: 0.54630\n",
      "step: 33863\n",
      "loss: 12.98061752319336\n",
      "steps per second: 0.52607\n",
      "step: 33864\n",
      "loss: 12.95919418334961\n",
      "steps per second: 0.55562\n",
      "step: 33865\n",
      "loss: 13.20561695098877\n",
      "steps per second: 0.55779\n",
      "step: 33866\n",
      "loss: 12.942805290222168\n",
      "steps per second: 0.54544\n",
      "step: 33867\n",
      "loss: 12.691165924072266\n",
      "steps per second: 0.51490\n",
      "step: 33868\n",
      "loss: 12.437141418457031\n",
      "steps per second: 0.52750\n",
      "step: 33869\n",
      "loss: 12.641312599182129\n",
      "steps per second: 0.54236\n",
      "step: 33870\n",
      "loss: 13.105778694152832\n",
      "steps per second: 0.52579\n",
      "step: 33871\n",
      "loss: 12.947198867797852\n",
      "steps per second: 0.56005\n",
      "step: 33872\n",
      "loss: 12.861909866333008\n",
      "steps per second: 0.59436\n",
      "step: 33873\n",
      "loss: 12.406981468200684\n",
      "steps per second: 0.54036\n",
      "step: 33874\n",
      "loss: 12.945657730102539\n",
      "steps per second: 0.54259\n",
      "step: 33875\n",
      "loss: 12.755779266357422\n",
      "steps per second: 0.52858\n",
      "step: 33876\n",
      "loss: 13.16537857055664\n",
      "steps per second: 0.59346\n",
      "step: 33877\n",
      "loss: 12.619402885437012\n",
      "steps per second: 0.55188\n",
      "step: 33878\n",
      "loss: 12.854195594787598\n",
      "steps per second: 0.53255\n",
      "step: 33879\n",
      "loss: 12.360967636108398\n",
      "steps per second: 0.51561\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7858923673629761, layer: 11\n",
      "saving at step 33879\n",
      "----------\n",
      "\n",
      "\n",
      "step: 33880\n",
      "loss: 12.302716255187988\n",
      "steps per second: 0.27750\n",
      "step: 33881\n",
      "loss: 12.63354206085205\n",
      "steps per second: 0.52755\n",
      "step: 33882\n",
      "loss: 12.927205085754395\n",
      "steps per second: 0.48729\n",
      "step: 33883\n",
      "loss: 12.87225341796875\n",
      "steps per second: 0.54641\n",
      "step: 33884\n",
      "loss: 13.150854110717773\n",
      "steps per second: 0.52622\n",
      "step: 33885\n",
      "loss: 12.860647201538086\n",
      "steps per second: 0.54019\n",
      "step: 33886\n",
      "loss: 12.912516593933105\n",
      "steps per second: 0.53480\n",
      "step: 33887\n",
      "loss: 12.93179702758789\n",
      "steps per second: 0.59778\n",
      "step: 33888\n",
      "loss: 13.103343963623047\n",
      "steps per second: 0.52778\n",
      "step: 33889\n",
      "loss: 12.411547660827637\n",
      "steps per second: 0.53140\n",
      "step: 33890\n",
      "loss: 13.030927658081055\n",
      "steps per second: 0.51570\n",
      "step: 33891\n",
      "loss: 13.119860649108887\n",
      "steps per second: 0.57077\n",
      "step: 33892\n",
      "loss: 13.073033332824707\n",
      "steps per second: 0.54184\n",
      "step: 33893\n",
      "loss: 12.775687217712402\n",
      "steps per second: 0.53470\n",
      "step: 33894\n",
      "loss: 13.302875518798828\n",
      "steps per second: 0.51626\n",
      "step: 33895\n",
      "loss: 13.223408699035645\n",
      "steps per second: 0.55753\n",
      "step: 33896\n",
      "loss: 12.965002059936523\n",
      "steps per second: 0.54582\n",
      "step: 33897\n",
      "loss: 13.160018920898438\n",
      "steps per second: 0.54026\n",
      "step: 33898\n",
      "loss: 12.891073226928711\n",
      "steps per second: 0.51529\n",
      "step: 33899\n",
      "loss: 13.78898811340332\n",
      "steps per second: 0.52755\n",
      "step: 33900\n",
      "loss: 12.92629337310791\n",
      "steps per second: 0.55944\n",
      "step: 33901\n",
      "loss: 12.74862003326416\n",
      "steps per second: 0.50832\n",
      "step: 33902\n",
      "loss: 13.288337707519531\n",
      "steps per second: 0.55643\n",
      "step: 33903\n",
      "loss: 12.58106517791748\n",
      "steps per second: 0.53255\n",
      "step: 33904\n",
      "loss: 12.541135787963867\n",
      "steps per second: 0.54184\n",
      "step: 33905\n",
      "loss: 12.916596412658691\n",
      "steps per second: 0.54706\n",
      "step: 33906\n",
      "loss: 12.368633270263672\n",
      "steps per second: 0.55924\n",
      "step: 33907\n",
      "loss: 12.235733032226562\n",
      "steps per second: 0.53524\n",
      "step: 33908\n",
      "loss: 13.103760719299316\n",
      "steps per second: 0.55321\n",
      "step: 33909\n",
      "loss: 12.497658729553223\n",
      "steps per second: 0.51302\n",
      "step: 33910\n",
      "loss: 13.165696144104004\n",
      "steps per second: 0.56644\n",
      "step: 33911\n",
      "loss: 13.140080451965332\n",
      "steps per second: 0.55503\n",
      "step: 33912\n",
      "loss: 12.616240501403809\n",
      "steps per second: 0.59465\n",
      "step: 33913\n",
      "loss: 12.932050704956055\n",
      "steps per second: 0.48674\n",
      "step: 33914\n",
      "loss: 11.833051681518555\n",
      "steps per second: 0.54034\n",
      "step: 33915\n",
      "loss: 12.87525463104248\n",
      "steps per second: 0.53973\n",
      "step: 33916\n",
      "loss: 12.925962448120117\n",
      "steps per second: 0.52473\n",
      "step: 33917\n",
      "loss: 12.926712036132812\n",
      "steps per second: 0.54662\n",
      "step: 33918\n",
      "loss: 12.841668128967285\n",
      "steps per second: 0.53298\n",
      "step: 33919\n",
      "loss: 12.934906959533691\n",
      "steps per second: 0.55901\n",
      "step: 33920\n",
      "loss: 13.043424606323242\n",
      "steps per second: 0.53474\n",
      "step: 33921\n",
      "loss: 12.927586555480957\n",
      "steps per second: 0.52691\n",
      "step: 33922\n",
      "loss: 12.508150100708008\n",
      "steps per second: 0.54154\n",
      "step: 33923\n",
      "loss: 13.218883514404297\n",
      "steps per second: 0.53337\n",
      "step: 33924\n",
      "loss: 12.632280349731445\n",
      "steps per second: 0.54050\n",
      "step: 33925\n",
      "loss: 13.054082870483398\n",
      "steps per second: 0.53034\n",
      "step: 33926\n",
      "loss: 12.743152618408203\n",
      "steps per second: 0.52074\n",
      "step: 33927\n",
      "loss: 12.289387702941895\n",
      "steps per second: 0.54507\n",
      "step: 33928\n",
      "loss: 12.637750625610352\n",
      "steps per second: 0.53286\n",
      "step: 33929\n",
      "loss: 11.966022491455078\n",
      "steps per second: 0.52393\n",
      "step: 33930\n",
      "loss: 12.861030578613281\n",
      "steps per second: 0.54335\n",
      "step: 33931\n",
      "loss: 12.099575996398926\n",
      "steps per second: 0.53808\n",
      "step: 33932\n",
      "loss: 13.028177261352539\n",
      "steps per second: 0.54181\n",
      "step: 33933\n",
      "loss: 12.571125030517578\n",
      "steps per second: 0.54168\n",
      "step: 33934\n",
      "loss: 13.108755111694336\n",
      "steps per second: 0.53156\n",
      "step: 33935\n",
      "loss: 12.57106876373291\n",
      "steps per second: 0.54147\n",
      "step: 33936\n",
      "loss: 12.658273696899414\n",
      "steps per second: 0.55793\n",
      "step: 33937\n",
      "loss: 12.320683479309082\n",
      "steps per second: 0.56700\n",
      "step: 33938\n",
      "loss: 12.570767402648926\n",
      "steps per second: 0.51679\n",
      "step: 33939\n",
      "loss: 12.76742172241211\n",
      "steps per second: 0.54556\n",
      "step: 33940\n",
      "loss: 12.425529479980469\n",
      "steps per second: 0.51434\n",
      "step: 33941\n",
      "loss: 12.475926399230957\n",
      "steps per second: 0.54534\n",
      "step: 33942\n",
      "loss: 12.956022262573242\n",
      "steps per second: 0.56799\n",
      "step: 33943\n",
      "loss: 12.345757484436035\n",
      "steps per second: 0.56002\n",
      "step: 33944\n",
      "loss: 12.626267433166504\n",
      "steps per second: 0.54676\n",
      "step: 33945\n",
      "loss: 12.728800773620605\n",
      "steps per second: 0.51765\n",
      "step: 33946\n",
      "loss: 12.312973022460938\n",
      "steps per second: 0.52869\n",
      "step: 33947\n",
      "loss: 13.176994323730469\n",
      "steps per second: 0.54504\n",
      "step: 33948\n",
      "loss: 12.932388305664062\n",
      "steps per second: 0.46523\n",
      "step: 33949\n",
      "loss: 12.77318000793457\n",
      "steps per second: 0.54063\n",
      "step: 33950\n",
      "loss: 12.715010643005371\n",
      "steps per second: 0.53365\n",
      "step: 33951\n",
      "loss: 12.971051216125488\n",
      "steps per second: 0.55772\n",
      "step: 33952\n",
      "loss: 13.358599662780762\n",
      "steps per second: 0.54773\n",
      "step: 33953\n",
      "loss: 12.743169784545898\n",
      "steps per second: 0.55975\n",
      "step: 33954\n",
      "loss: 12.430195808410645\n",
      "steps per second: 0.56430\n",
      "step: 33955\n",
      "loss: 12.814151763916016\n",
      "steps per second: 0.54176\n",
      "step: 33956\n",
      "loss: 12.689482688903809\n",
      "steps per second: 0.51850\n",
      "step: 33957\n",
      "loss: 12.144998550415039\n",
      "steps per second: 0.49299\n",
      "step: 33958\n",
      "loss: 13.245506286621094\n",
      "steps per second: 0.54497\n",
      "step: 33959\n",
      "loss: 12.835516929626465\n",
      "steps per second: 0.52801\n",
      "step: 33960\n",
      "loss: 12.548040390014648\n",
      "steps per second: 0.55490\n",
      "step: 33961\n",
      "loss: 13.301755905151367\n",
      "steps per second: 0.51239\n",
      "step: 33962\n",
      "loss: 12.276761054992676\n",
      "steps per second: 0.53830\n",
      "step: 33963\n",
      "loss: 13.272586822509766\n",
      "steps per second: 0.52096\n",
      "step: 33964\n",
      "loss: 12.593240737915039\n",
      "steps per second: 0.49849\n",
      "step: 33965\n",
      "loss: 12.466718673706055\n",
      "steps per second: 0.52763\n",
      "step: 33966\n",
      "loss: 12.634698867797852\n",
      "steps per second: 0.53328\n",
      "step: 33967\n",
      "loss: 12.620404243469238\n",
      "steps per second: 0.49302\n",
      "step: 33968\n",
      "loss: 12.766473770141602\n",
      "steps per second: 0.55790\n",
      "step: 33969\n",
      "loss: 13.15040397644043\n",
      "steps per second: 0.56895\n",
      "step: 33970\n",
      "loss: 12.926527976989746\n",
      "steps per second: 0.54067\n",
      "step: 33971\n",
      "loss: 12.280767440795898\n",
      "steps per second: 0.53863\n",
      "step: 33972\n",
      "loss: 12.75768756866455\n",
      "steps per second: 0.54311\n",
      "step: 33973\n",
      "loss: 12.694547653198242\n",
      "steps per second: 0.54776\n",
      "step: 33974\n",
      "loss: 12.980687141418457\n",
      "steps per second: 0.56082\n",
      "step: 33975\n",
      "loss: 12.766938209533691\n",
      "steps per second: 0.54227\n",
      "step: 33976\n",
      "loss: 12.650486946105957\n",
      "steps per second: 0.52283\n",
      "step: 33977\n",
      "loss: 13.122185707092285\n",
      "steps per second: 0.53667\n",
      "step: 33978\n",
      "loss: 12.392476081848145\n",
      "steps per second: 0.53177\n",
      "step: 33979\n",
      "loss: 13.269098281860352\n",
      "steps per second: 0.53469\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.9287378191947937, layer: 11\n",
      "saving at step 33979\n",
      "----------\n",
      "\n",
      "\n",
      "step: 33980\n",
      "loss: 12.69825553894043\n",
      "steps per second: 0.28892\n",
      "step: 33981\n",
      "loss: 12.346074104309082\n",
      "steps per second: 0.54344\n",
      "step: 33982\n",
      "loss: 12.767216682434082\n",
      "steps per second: 0.54526\n",
      "step: 33983\n",
      "loss: 12.597594261169434\n",
      "steps per second: 0.56068\n",
      "step: 33984\n",
      "loss: 12.804289817810059\n",
      "steps per second: 0.56515\n",
      "step: 33985\n",
      "loss: 13.154329299926758\n",
      "steps per second: 0.54140\n",
      "step: 33986\n",
      "loss: 12.759847640991211\n",
      "steps per second: 0.51948\n",
      "step: 33987\n",
      "loss: 13.61365032196045\n",
      "steps per second: 0.54622\n",
      "step: 33988\n",
      "loss: 12.473289489746094\n",
      "steps per second: 0.53421\n",
      "step: 33989\n",
      "loss: 13.159430503845215\n",
      "steps per second: 0.53521\n",
      "step: 33990\n",
      "loss: 13.13325309753418\n",
      "steps per second: 0.56592\n",
      "step: 33991\n",
      "loss: 12.529641151428223\n",
      "steps per second: 0.52133\n",
      "step: 33992\n",
      "loss: 12.907224655151367\n",
      "steps per second: 0.56008\n",
      "step: 33993\n",
      "loss: 12.909444808959961\n",
      "steps per second: 0.53376\n",
      "step: 33994\n",
      "loss: 13.148846626281738\n",
      "steps per second: 0.54602\n",
      "step: 33995\n",
      "loss: 13.036028861999512\n",
      "steps per second: 0.53441\n",
      "step: 33996\n",
      "loss: 13.005735397338867\n",
      "steps per second: 0.53557\n",
      "step: 33997\n",
      "loss: 12.694966316223145\n",
      "steps per second: 0.59127\n",
      "step: 33998\n",
      "loss: 12.474832534790039\n",
      "steps per second: 0.47833\n",
      "step: 33999\n",
      "loss: 12.439148902893066\n",
      "steps per second: 0.53091\n",
      "step: 34000\n",
      "loss: 12.821799278259277\n",
      "steps per second: 0.55698\n",
      "step: 34001\n",
      "loss: 12.690375328063965\n",
      "steps per second: 0.55962\n",
      "step: 34002\n",
      "loss: 12.290399551391602\n",
      "steps per second: 0.54368\n",
      "step: 34003\n",
      "loss: 13.008885383605957\n",
      "steps per second: 0.55673\n",
      "step: 34004\n",
      "loss: 13.485170364379883\n",
      "steps per second: 0.56574\n",
      "step: 34005\n",
      "loss: 13.020691871643066\n",
      "steps per second: 0.53419\n",
      "step: 34006\n",
      "loss: 12.661239624023438\n",
      "steps per second: 0.56014\n",
      "step: 34007\n",
      "loss: 12.76570987701416\n",
      "steps per second: 0.53509\n",
      "step: 34008\n",
      "loss: 12.95409107208252\n",
      "steps per second: 0.54638\n",
      "step: 34009\n",
      "loss: 12.795649528503418\n",
      "steps per second: 0.53752\n",
      "step: 34010\n",
      "loss: 12.853185653686523\n",
      "steps per second: 0.60112\n",
      "step: 34011\n",
      "loss: 13.510936737060547\n",
      "steps per second: 0.55707\n",
      "step: 34012\n",
      "loss: 13.170605659484863\n",
      "steps per second: 0.55931\n",
      "step: 34013\n",
      "loss: 12.970968246459961\n",
      "steps per second: 0.56068\n",
      "step: 34014\n",
      "loss: 12.60944652557373\n",
      "steps per second: 0.55084\n",
      "step: 34015\n",
      "loss: 12.942254066467285\n",
      "steps per second: 0.54104\n",
      "step: 34016\n",
      "loss: 12.936407089233398\n",
      "steps per second: 0.55719\n",
      "step: 34017\n",
      "loss: 12.582955360412598\n",
      "steps per second: 0.56426\n",
      "step: 34018\n",
      "loss: 12.93420696258545\n",
      "steps per second: 0.49569\n",
      "step: 34019\n",
      "loss: 12.650315284729004\n",
      "steps per second: 0.57388\n",
      "step: 34020\n",
      "loss: 12.832706451416016\n",
      "steps per second: 0.52799\n",
      "step: 34021\n",
      "loss: 12.74029541015625\n",
      "steps per second: 0.53132\n",
      "step: 34022\n",
      "loss: 12.953417778015137\n",
      "steps per second: 0.52614\n",
      "step: 34023\n",
      "loss: 12.6920804977417\n",
      "steps per second: 0.55766\n",
      "step: 34024\n",
      "loss: 12.994626998901367\n",
      "steps per second: 0.55864\n",
      "step: 34025\n",
      "loss: 13.197911262512207\n",
      "steps per second: 0.51430\n",
      "step: 34026\n",
      "loss: 13.071755409240723\n",
      "steps per second: 0.53102\n",
      "step: 34027\n",
      "loss: 12.909165382385254\n",
      "steps per second: 0.51791\n",
      "step: 34028\n",
      "loss: 13.131143569946289\n",
      "steps per second: 0.53541\n",
      "step: 34029\n",
      "loss: 12.509310722351074\n",
      "steps per second: 0.53300\n",
      "step: 34030\n",
      "loss: 13.025535583496094\n",
      "steps per second: 0.53500\n",
      "step: 34031\n",
      "loss: 12.409038543701172\n",
      "steps per second: 0.54152\n",
      "step: 34032\n",
      "loss: 12.721006393432617\n",
      "steps per second: 0.53133\n",
      "step: 34033\n",
      "loss: 12.878918647766113\n",
      "steps per second: 0.54932\n",
      "step: 34034\n",
      "loss: 13.285444259643555\n",
      "steps per second: 0.51385\n",
      "step: 34035\n",
      "loss: 11.944060325622559\n",
      "steps per second: 0.53504\n",
      "step: 34036\n",
      "loss: 12.869956016540527\n",
      "steps per second: 0.52551\n",
      "step: 34037\n",
      "loss: 12.645058631896973\n",
      "steps per second: 0.56613\n",
      "step: 34038\n",
      "loss: 12.500289916992188\n",
      "steps per second: 0.54220\n",
      "step: 34039\n",
      "loss: 12.617247581481934\n",
      "steps per second: 0.54516\n",
      "step: 34040\n",
      "loss: 12.754613876342773\n",
      "steps per second: 0.53739\n",
      "step: 34041\n",
      "loss: 13.150848388671875\n",
      "steps per second: 0.52617\n",
      "step: 34042\n",
      "loss: 12.552783012390137\n",
      "steps per second: 0.56015\n",
      "step: 34043\n",
      "loss: 13.2374906539917\n",
      "steps per second: 0.55752\n",
      "step: 34044\n",
      "loss: 12.817264556884766\n",
      "steps per second: 0.56828\n",
      "step: 34045\n",
      "loss: 12.366972923278809\n",
      "steps per second: 0.54455\n",
      "step: 34046\n",
      "loss: 12.550565719604492\n",
      "steps per second: 0.55705\n",
      "step: 34047\n",
      "loss: 12.66844367980957\n",
      "steps per second: 0.51309\n",
      "step: 34048\n",
      "loss: 12.804359436035156\n",
      "steps per second: 0.52617\n",
      "step: 34049\n",
      "loss: 12.345982551574707\n",
      "steps per second: 0.54068\n",
      "step: 34050\n",
      "loss: 12.713924407958984\n",
      "steps per second: 0.52628\n",
      "step: 34051\n",
      "loss: 13.072339057922363\n",
      "steps per second: 0.51956\n",
      "step: 34052\n",
      "loss: 12.693387031555176\n",
      "steps per second: 0.52613\n",
      "step: 34053\n",
      "loss: 12.559367179870605\n",
      "steps per second: 0.54423\n",
      "step: 34054\n",
      "loss: 12.33558177947998\n",
      "steps per second: 0.56618\n",
      "step: 34055\n",
      "loss: 12.851545333862305\n",
      "steps per second: 0.51758\n",
      "step: 34056\n",
      "loss: 12.566178321838379\n",
      "steps per second: 0.51582\n",
      "step: 34057\n",
      "loss: 12.642925262451172\n",
      "steps per second: 0.53488\n",
      "step: 34058\n",
      "loss: 12.972743034362793\n",
      "steps per second: 0.54180\n",
      "step: 34059\n",
      "loss: 12.371159553527832\n",
      "steps per second: 0.56594\n",
      "step: 34060\n",
      "loss: 13.530733108520508\n",
      "steps per second: 0.56409\n",
      "step: 34061\n",
      "loss: 12.922191619873047\n",
      "steps per second: 0.54596\n",
      "step: 34062\n",
      "loss: 12.954347610473633\n",
      "steps per second: 0.54027\n",
      "step: 34063\n",
      "loss: 12.226834297180176\n",
      "steps per second: 0.51508\n",
      "step: 34064\n",
      "loss: 12.166885375976562\n",
      "steps per second: 0.49083\n",
      "step: 34065\n",
      "loss: 12.48178482055664\n",
      "steps per second: 0.53901\n",
      "step: 34066\n",
      "loss: 13.461899757385254\n",
      "steps per second: 0.54111\n",
      "step: 34067\n",
      "loss: 12.842461585998535\n",
      "steps per second: 0.55267\n",
      "step: 34068\n",
      "loss: 12.3223295211792\n",
      "steps per second: 0.53800\n",
      "step: 34069\n",
      "loss: 13.08639144897461\n",
      "steps per second: 0.50573\n",
      "step: 34070\n",
      "loss: 12.794294357299805\n",
      "steps per second: 0.46611\n",
      "step: 34071\n",
      "loss: 12.918696403503418\n",
      "steps per second: 0.52322\n",
      "step: 34072\n",
      "loss: 13.1421480178833\n",
      "steps per second: 0.53946\n",
      "step: 34073\n",
      "loss: 12.660479545593262\n",
      "steps per second: 0.51840\n",
      "step: 34074\n",
      "loss: 12.882447242736816\n",
      "steps per second: 0.51747\n",
      "step: 34075\n",
      "loss: 12.633182525634766\n",
      "steps per second: 0.52518\n",
      "step: 34076\n",
      "loss: 13.01008415222168\n",
      "steps per second: 0.52774\n",
      "step: 34077\n",
      "loss: 12.831509590148926\n",
      "steps per second: 0.54718\n",
      "step: 34078\n",
      "loss: 12.952444076538086\n",
      "steps per second: 0.54533\n",
      "step: 34079\n",
      "loss: 13.419594764709473\n",
      "steps per second: 0.51391\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8753455877304077, layer: 10\n",
      "saving at step 34079\n",
      "----------\n",
      "\n",
      "\n",
      "step: 34080\n",
      "loss: 13.523197174072266\n",
      "steps per second: 0.26358\n",
      "step: 34081\n",
      "loss: 12.956175804138184\n",
      "steps per second: 0.51738\n",
      "step: 34082\n",
      "loss: 12.676409721374512\n",
      "steps per second: 0.54313\n",
      "step: 34083\n",
      "loss: 12.587937355041504\n",
      "steps per second: 0.56712\n",
      "step: 34084\n",
      "loss: 12.579903602600098\n",
      "steps per second: 0.52519\n",
      "step: 34085\n",
      "loss: 12.671608924865723\n",
      "steps per second: 0.53287\n",
      "step: 34086\n",
      "loss: 12.529983520507812\n",
      "steps per second: 0.51665\n",
      "step: 34087\n",
      "loss: 12.884031295776367\n",
      "steps per second: 0.54719\n",
      "step: 34088\n",
      "loss: 12.873270988464355\n",
      "steps per second: 0.52810\n",
      "step: 34089\n",
      "loss: 13.135704040527344\n",
      "steps per second: 0.55840\n",
      "step: 34090\n",
      "loss: 13.01445484161377\n",
      "steps per second: 0.52590\n",
      "step: 34091\n",
      "loss: 12.883647918701172\n",
      "steps per second: 0.54039\n",
      "step: 34092\n",
      "loss: 13.054187774658203\n",
      "steps per second: 0.54746\n",
      "step: 34093\n",
      "loss: 13.18261432647705\n",
      "steps per second: 0.54735\n",
      "step: 34094\n",
      "loss: 12.717205047607422\n",
      "steps per second: 0.56593\n",
      "step: 34095\n",
      "loss: 12.590896606445312\n",
      "steps per second: 0.56125\n",
      "step: 34096\n",
      "loss: 13.031890869140625\n",
      "steps per second: 0.51985\n",
      "step: 34097\n",
      "loss: 12.638054847717285\n",
      "steps per second: 0.60049\n",
      "step: 34098\n",
      "loss: 13.132362365722656\n",
      "steps per second: 0.55625\n",
      "step: 34099\n",
      "loss: 12.869621276855469\n",
      "steps per second: 0.54542\n",
      "step: 34100\n",
      "loss: 12.419501304626465\n",
      "steps per second: 0.51484\n",
      "step: 34101\n",
      "loss: 12.646347045898438\n",
      "steps per second: 0.52013\n",
      "step: 34102\n",
      "loss: 12.42839241027832\n",
      "steps per second: 0.54215\n",
      "step: 34103\n",
      "loss: 12.909192085266113\n",
      "steps per second: 0.46866\n",
      "step: 34104\n",
      "loss: 13.15206527709961\n",
      "steps per second: 0.50633\n",
      "step: 34105\n",
      "loss: 12.8706693649292\n",
      "steps per second: 0.49920\n",
      "step: 34106\n",
      "loss: 12.561996459960938\n",
      "steps per second: 0.52387\n",
      "step: 34107\n",
      "loss: 13.427549362182617\n",
      "steps per second: 0.53949\n",
      "step: 34108\n",
      "loss: 12.321746826171875\n",
      "steps per second: 0.51734\n",
      "step: 34109\n",
      "loss: 13.062493324279785\n",
      "steps per second: 0.54371\n",
      "step: 34110\n",
      "loss: 13.442955017089844\n",
      "steps per second: 0.59546\n",
      "step: 34111\n",
      "loss: 12.782902717590332\n",
      "steps per second: 0.60209\n",
      "step: 34112\n",
      "loss: 12.965254783630371\n",
      "steps per second: 0.54598\n",
      "step: 34113\n",
      "loss: 12.832332611083984\n",
      "steps per second: 0.54797\n",
      "step: 34114\n",
      "loss: 12.536888122558594\n",
      "steps per second: 0.54376\n",
      "step: 34115\n",
      "loss: 12.292298316955566\n",
      "steps per second: 0.51351\n",
      "step: 34116\n",
      "loss: 12.958706855773926\n",
      "steps per second: 0.53299\n",
      "step: 34117\n",
      "loss: 12.438437461853027\n",
      "steps per second: 0.54138\n",
      "step: 34118\n",
      "loss: 13.049034118652344\n",
      "steps per second: 0.51781\n",
      "step: 34119\n",
      "loss: 12.940335273742676\n",
      "steps per second: 0.56451\n",
      "step: 34120\n",
      "loss: 12.714738845825195\n",
      "steps per second: 0.52795\n",
      "step: 34121\n",
      "loss: 12.46202564239502\n",
      "steps per second: 0.48052\n",
      "step: 34122\n",
      "loss: 12.82877254486084\n",
      "steps per second: 0.56841\n",
      "step: 34123\n",
      "loss: 12.696739196777344\n",
      "steps per second: 0.54891\n",
      "step: 34124\n",
      "loss: 12.456604957580566\n",
      "steps per second: 0.56897\n",
      "step: 34125\n",
      "loss: 13.309867858886719\n",
      "steps per second: 0.49947\n",
      "step: 34126\n",
      "loss: 12.821812629699707\n",
      "steps per second: 0.51485\n",
      "step: 34127\n",
      "loss: 12.527837753295898\n",
      "steps per second: 0.54941\n",
      "step: 34128\n",
      "loss: 12.98080825805664\n",
      "steps per second: 0.50406\n",
      "step: 34129\n",
      "loss: 12.285515785217285\n",
      "steps per second: 0.56988\n",
      "step: 34130\n",
      "loss: 12.997401237487793\n",
      "steps per second: 0.57288\n",
      "step: 34131\n",
      "loss: 13.29613208770752\n",
      "steps per second: 0.54153\n",
      "step: 34132\n",
      "loss: 13.04609203338623\n",
      "steps per second: 0.55077\n",
      "step: 34133\n",
      "loss: 12.43191146850586\n",
      "steps per second: 0.53568\n",
      "step: 34134\n",
      "loss: 13.065744400024414\n",
      "steps per second: 0.55852\n",
      "step: 34135\n",
      "loss: 12.518513679504395\n",
      "steps per second: 0.60826\n",
      "step: 34136\n",
      "loss: 12.810158729553223\n",
      "steps per second: 0.54297\n",
      "step: 34137\n",
      "loss: 12.074142456054688\n",
      "steps per second: 0.53504\n",
      "step: 34138\n",
      "loss: 13.311491012573242\n",
      "steps per second: 0.52723\n",
      "step: 34139\n",
      "loss: 12.807857513427734\n",
      "steps per second: 0.54342\n",
      "step: 34140\n",
      "loss: 12.744196891784668\n",
      "steps per second: 0.54272\n",
      "step: 34141\n",
      "loss: 12.903373718261719\n",
      "steps per second: 0.57031\n",
      "step: 34142\n",
      "loss: 13.294075012207031\n",
      "steps per second: 0.54691\n",
      "step: 34143\n",
      "loss: 13.265436172485352\n",
      "steps per second: 0.60937\n",
      "step: 34144\n",
      "loss: 12.580074310302734\n",
      "steps per second: 0.55682\n",
      "step: 34145\n",
      "loss: 12.906713485717773\n",
      "steps per second: 0.55087\n",
      "step: 34146\n",
      "loss: 12.524978637695312\n",
      "steps per second: 0.56967\n",
      "step: 34147\n",
      "loss: 13.080387115478516\n",
      "steps per second: 0.55746\n",
      "step: 34148\n",
      "loss: 12.820889472961426\n",
      "steps per second: 0.58277\n",
      "step: 34149\n",
      "loss: 12.689664840698242\n",
      "steps per second: 0.55415\n",
      "step: 34150\n",
      "loss: 12.967264175415039\n",
      "steps per second: 0.55002\n",
      "step: 34151\n",
      "loss: 13.121500015258789\n",
      "steps per second: 0.57729\n",
      "step: 34152\n",
      "loss: 13.032498359680176\n",
      "steps per second: 0.54359\n",
      "step: 34153\n",
      "loss: 13.13713550567627\n",
      "steps per second: 0.52428\n",
      "step: 34154\n",
      "loss: 13.08350944519043\n",
      "steps per second: 0.55489\n",
      "step: 34155\n",
      "loss: 12.912959098815918\n",
      "steps per second: 0.53500\n",
      "step: 34156\n",
      "loss: 12.886384010314941\n",
      "steps per second: 0.54879\n",
      "step: 34157\n",
      "loss: 13.138530731201172\n",
      "steps per second: 0.57817\n",
      "step: 34158\n",
      "loss: 12.246336936950684\n",
      "steps per second: 0.57467\n",
      "step: 34159\n",
      "loss: 13.215409278869629\n",
      "steps per second: 0.60980\n",
      "step: 34160\n",
      "loss: 13.002663612365723\n",
      "steps per second: 0.49270\n",
      "step: 34161\n",
      "loss: 12.784319877624512\n",
      "steps per second: 0.52124\n",
      "step: 34162\n",
      "loss: 12.580580711364746\n",
      "steps per second: 0.53528\n",
      "step: 34163\n",
      "loss: 12.81313419342041\n",
      "steps per second: 0.55191\n",
      "step: 34164\n",
      "loss: 12.433597564697266\n",
      "steps per second: 0.53181\n",
      "step: 34165\n",
      "loss: 12.67502212524414\n",
      "steps per second: 0.54094\n",
      "step: 34166\n",
      "loss: 12.196310043334961\n",
      "steps per second: 0.57440\n",
      "step: 34167\n",
      "loss: 12.749370574951172\n",
      "steps per second: 0.56874\n",
      "step: 34168\n",
      "loss: 12.648171424865723\n",
      "steps per second: 0.55742\n",
      "step: 34169\n",
      "loss: 13.507528305053711\n",
      "steps per second: 0.55859\n",
      "step: 34170\n",
      "loss: 13.008522033691406\n",
      "steps per second: 0.53907\n",
      "step: 34171\n",
      "loss: 12.87923526763916\n",
      "steps per second: 0.54284\n",
      "step: 34172\n",
      "loss: 13.039019584655762\n",
      "steps per second: 0.52117\n",
      "step: 34173\n",
      "loss: 13.027615547180176\n",
      "steps per second: 0.54774\n",
      "step: 34174\n",
      "loss: 12.961828231811523\n",
      "steps per second: 0.56949\n",
      "step: 34175\n",
      "loss: 13.064849853515625\n",
      "steps per second: 0.55709\n",
      "step: 34176\n",
      "loss: 12.56511402130127\n",
      "steps per second: 0.53376\n",
      "step: 34177\n",
      "loss: 12.706559181213379\n",
      "steps per second: 0.57082\n",
      "step: 34178\n",
      "loss: 13.036147117614746\n",
      "steps per second: 0.55148\n",
      "step: 34179\n",
      "loss: 12.484784126281738\n",
      "steps per second: 0.54978\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8285838961601257, layer: 11\n",
      "saving at step 34179\n",
      "----------\n",
      "\n",
      "\n",
      "step: 34180\n",
      "loss: 13.143857955932617\n",
      "steps per second: 0.27823\n",
      "step: 34181\n",
      "loss: 12.738883018493652\n",
      "steps per second: 0.55123\n",
      "step: 34182\n",
      "loss: 13.283133506774902\n",
      "steps per second: 0.54905\n",
      "step: 34183\n",
      "loss: 12.711692810058594\n",
      "steps per second: 0.56860\n",
      "step: 34184\n",
      "loss: 13.000397682189941\n",
      "steps per second: 0.55744\n",
      "step: 34185\n",
      "loss: 12.996865272521973\n",
      "steps per second: 0.54438\n",
      "step: 34186\n",
      "loss: 12.562459945678711\n",
      "steps per second: 0.55079\n",
      "step: 34187\n",
      "loss: 12.549559593200684\n",
      "steps per second: 0.52485\n",
      "step: 34188\n",
      "loss: 12.33421802520752\n",
      "steps per second: 0.56599\n",
      "step: 34189\n",
      "loss: 12.579172134399414\n",
      "steps per second: 0.51511\n",
      "step: 34190\n",
      "loss: 12.711065292358398\n",
      "steps per second: 0.56907\n",
      "step: 34191\n",
      "loss: 12.86134147644043\n",
      "steps per second: 0.54287\n",
      "step: 34192\n",
      "loss: 12.528882026672363\n",
      "steps per second: 0.56500\n",
      "step: 34193\n",
      "loss: 13.098917007446289\n",
      "steps per second: 0.56740\n",
      "step: 34194\n",
      "loss: 12.926334381103516\n",
      "steps per second: 0.55141\n",
      "step: 34195\n",
      "loss: 13.084693908691406\n",
      "steps per second: 0.57458\n",
      "step: 34196\n",
      "loss: 12.45198917388916\n",
      "steps per second: 0.54279\n",
      "step: 34197\n",
      "loss: 12.331427574157715\n",
      "steps per second: 0.55023\n",
      "step: 34198\n",
      "loss: 12.644614219665527\n",
      "steps per second: 0.57293\n",
      "step: 34199\n",
      "loss: 12.993783950805664\n",
      "steps per second: 0.48842\n",
      "step: 34200\n",
      "loss: 12.959726333618164\n",
      "steps per second: 0.52444\n",
      "step: 34201\n",
      "loss: 12.826321601867676\n",
      "steps per second: 0.52657\n",
      "step: 34202\n",
      "loss: 12.967756271362305\n",
      "steps per second: 0.54958\n",
      "step: 34203\n",
      "loss: 12.919251441955566\n",
      "steps per second: 0.55137\n",
      "step: 34204\n",
      "loss: 12.465291976928711\n",
      "steps per second: 0.54221\n",
      "step: 34205\n",
      "loss: 12.003317832946777\n",
      "steps per second: 0.55188\n",
      "step: 34206\n",
      "loss: 12.808990478515625\n",
      "steps per second: 0.54837\n",
      "step: 34207\n",
      "loss: 12.921209335327148\n",
      "steps per second: 0.55102\n",
      "step: 34208\n",
      "loss: 12.835166931152344\n",
      "steps per second: 0.51950\n",
      "step: 34209\n",
      "loss: 13.248676300048828\n",
      "steps per second: 0.54862\n",
      "step: 34210\n",
      "loss: 12.925440788269043\n",
      "steps per second: 0.55482\n",
      "step: 34211\n",
      "loss: 12.566302299499512\n",
      "steps per second: 0.54883\n",
      "step: 34212\n",
      "loss: 13.121787071228027\n",
      "steps per second: 0.60709\n",
      "step: 34213\n",
      "loss: 12.620524406433105\n",
      "steps per second: 0.51600\n",
      "step: 34214\n",
      "loss: 12.344208717346191\n",
      "steps per second: 0.52362\n",
      "step: 34215\n",
      "loss: 12.887961387634277\n",
      "steps per second: 0.57088\n",
      "step: 34216\n",
      "loss: 12.443357467651367\n",
      "steps per second: 0.53072\n",
      "step: 34217\n",
      "loss: 13.010132789611816\n",
      "steps per second: 0.56440\n",
      "step: 34218\n",
      "loss: 13.138741493225098\n",
      "steps per second: 0.56868\n",
      "step: 34219\n",
      "loss: 12.388418197631836\n",
      "steps per second: 0.54946\n",
      "step: 34220\n",
      "loss: 12.956629753112793\n",
      "steps per second: 0.52380\n",
      "step: 34221\n",
      "loss: 12.788553237915039\n",
      "steps per second: 0.50239\n",
      "step: 34222\n",
      "loss: 12.461628913879395\n",
      "steps per second: 0.51604\n",
      "step: 34223\n",
      "loss: 12.685338020324707\n",
      "steps per second: 0.53004\n",
      "step: 34224\n",
      "loss: 12.97339153289795\n",
      "steps per second: 0.50110\n",
      "step: 34225\n",
      "loss: 12.909147262573242\n",
      "steps per second: 0.52993\n",
      "step: 34226\n",
      "loss: 12.855838775634766\n",
      "steps per second: 0.54858\n",
      "step: 34227\n",
      "loss: 13.44699478149414\n",
      "steps per second: 0.51886\n",
      "step: 34228\n",
      "loss: 13.295248985290527\n",
      "steps per second: 0.51345\n",
      "step: 34229\n",
      "loss: 12.862428665161133\n",
      "steps per second: 0.50982\n",
      "step: 34230\n",
      "loss: 12.808368682861328\n",
      "steps per second: 0.48683\n",
      "step: 34231\n",
      "loss: 13.093420028686523\n",
      "steps per second: 0.49784\n",
      "step: 34232\n",
      "loss: 13.010601997375488\n",
      "steps per second: 0.48557\n",
      "step: 34233\n",
      "loss: 12.370621681213379\n",
      "steps per second: 0.54268\n",
      "step: 34234\n",
      "loss: 12.57456111907959\n",
      "steps per second: 0.51674\n",
      "step: 34235\n",
      "loss: 13.257991790771484\n",
      "steps per second: 0.49202\n",
      "step: 34236\n",
      "loss: 12.865724563598633\n",
      "steps per second: 0.52067\n",
      "step: 34237\n",
      "loss: 12.833183288574219\n",
      "steps per second: 0.53385\n",
      "step: 34238\n",
      "loss: 12.517319679260254\n",
      "steps per second: 0.49895\n",
      "step: 34239\n",
      "loss: 12.822186470031738\n",
      "steps per second: 0.49224\n",
      "step: 34240\n",
      "loss: 13.127946853637695\n",
      "steps per second: 0.53840\n",
      "step: 34241\n",
      "loss: 12.791851997375488\n",
      "steps per second: 0.56636\n",
      "step: 34242\n",
      "loss: 12.614086151123047\n",
      "steps per second: 0.50728\n",
      "step: 34243\n",
      "loss: 12.727136611938477\n",
      "steps per second: 0.51669\n",
      "step: 34244\n",
      "loss: 12.651944160461426\n",
      "steps per second: 0.51395\n",
      "step: 34245\n",
      "loss: 12.884408950805664\n",
      "steps per second: 0.55529\n",
      "step: 34246\n",
      "loss: 12.173616409301758\n",
      "steps per second: 0.52551\n",
      "step: 34247\n",
      "loss: 12.534976959228516\n",
      "steps per second: 0.55651\n",
      "step: 34248\n",
      "loss: 12.794438362121582\n",
      "steps per second: 0.51455\n",
      "step: 34249\n",
      "loss: 12.84600830078125\n",
      "steps per second: 0.50009\n",
      "step: 34250\n",
      "loss: 12.580427169799805\n",
      "steps per second: 0.56969\n",
      "step: 34251\n",
      "loss: 13.160400390625\n",
      "steps per second: 0.55233\n",
      "step: 34252\n",
      "loss: 13.193603515625\n",
      "steps per second: 0.57708\n",
      "step: 34253\n",
      "loss: 12.597624778747559\n",
      "steps per second: 0.55822\n",
      "step: 34254\n",
      "loss: 12.913402557373047\n",
      "steps per second: 0.54290\n",
      "step: 34255\n",
      "loss: 12.924198150634766\n",
      "steps per second: 0.54619\n",
      "step: 34256\n",
      "loss: 12.573151588439941\n",
      "steps per second: 0.54582\n",
      "step: 34257\n",
      "loss: 12.683418273925781\n",
      "steps per second: 0.55381\n",
      "step: 34258\n",
      "loss: 12.607573509216309\n",
      "steps per second: 0.52733\n",
      "step: 34259\n",
      "loss: 13.161935806274414\n",
      "steps per second: 0.52389\n",
      "step: 34260\n",
      "loss: 12.700992584228516\n",
      "steps per second: 0.57188\n",
      "step: 34261\n",
      "loss: 12.534485816955566\n",
      "steps per second: 0.54434\n",
      "step: 34262\n",
      "loss: 13.692142486572266\n",
      "steps per second: 0.52847\n",
      "step: 34263\n",
      "loss: 12.85169506072998\n",
      "steps per second: 0.48375\n",
      "step: 34264\n",
      "loss: 12.978538513183594\n",
      "steps per second: 0.53669\n",
      "step: 34265\n",
      "loss: 12.825214385986328\n",
      "steps per second: 0.55688\n",
      "step: 34266\n",
      "loss: 12.899175643920898\n",
      "steps per second: 0.51294\n",
      "step: 34267\n",
      "loss: 12.896932601928711\n",
      "steps per second: 0.55260\n",
      "step: 34268\n",
      "loss: 12.854735374450684\n",
      "steps per second: 0.57277\n",
      "step: 34269\n",
      "loss: 12.696039199829102\n",
      "steps per second: 0.56847\n",
      "step: 34270\n",
      "loss: 13.010695457458496\n",
      "steps per second: 0.58201\n",
      "step: 34271\n",
      "loss: 13.402965545654297\n",
      "steps per second: 0.51413\n",
      "step: 34272\n",
      "loss: 12.52779769897461\n",
      "steps per second: 0.56752\n",
      "step: 34273\n",
      "loss: 13.279245376586914\n",
      "steps per second: 0.55468\n",
      "step: 34274\n",
      "loss: 12.362674713134766\n",
      "steps per second: 0.55321\n",
      "step: 34275\n",
      "loss: 12.801590919494629\n",
      "steps per second: 0.54049\n",
      "step: 34276\n",
      "loss: 12.551194190979004\n",
      "steps per second: 0.55697\n",
      "step: 34277\n",
      "loss: 12.795698165893555\n",
      "steps per second: 0.56144\n",
      "step: 34278\n",
      "loss: 12.407662391662598\n",
      "steps per second: 0.54911\n",
      "step: 34279\n",
      "loss: 12.345005989074707\n",
      "steps per second: 0.56878\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8143353462219238, layer: 12\n",
      "saving at step 34279\n",
      "----------\n",
      "\n",
      "\n",
      "step: 34280\n",
      "loss: 13.302132606506348\n",
      "steps per second: 0.28373\n",
      "step: 34281\n",
      "loss: 12.914369583129883\n",
      "steps per second: 0.55494\n",
      "step: 34282\n",
      "loss: 13.024059295654297\n",
      "steps per second: 0.55614\n",
      "step: 34283\n",
      "loss: 12.54765796661377\n",
      "steps per second: 0.54122\n",
      "step: 34284\n",
      "loss: 12.879286766052246\n",
      "steps per second: 0.60060\n",
      "step: 34285\n",
      "loss: 12.527010917663574\n",
      "steps per second: 0.56509\n",
      "step: 34286\n",
      "loss: 13.687228202819824\n",
      "steps per second: 0.54317\n",
      "step: 34287\n",
      "loss: 12.4364652633667\n",
      "steps per second: 0.54263\n",
      "step: 34288\n",
      "loss: 13.555123329162598\n",
      "steps per second: 0.57320\n",
      "step: 34289\n",
      "loss: 12.972776412963867\n",
      "steps per second: 0.54545\n",
      "step: 34290\n",
      "loss: 12.578189849853516\n",
      "steps per second: 0.51723\n",
      "step: 34291\n",
      "loss: 13.064393997192383\n",
      "steps per second: 0.55159\n",
      "step: 34292\n",
      "loss: 13.098302841186523\n",
      "steps per second: 0.54664\n",
      "step: 34293\n",
      "loss: 12.664568901062012\n",
      "steps per second: 0.50570\n",
      "step: 34294\n",
      "loss: 12.71579647064209\n",
      "steps per second: 0.53298\n",
      "step: 34295\n",
      "loss: 13.844069480895996\n",
      "steps per second: 0.53660\n",
      "step: 34296\n",
      "loss: 12.806398391723633\n",
      "steps per second: 0.54246\n",
      "step: 34297\n",
      "loss: 12.640549659729004\n",
      "steps per second: 0.52752\n",
      "step: 34298\n",
      "loss: 12.523445129394531\n",
      "steps per second: 0.52641\n",
      "step: 34299\n",
      "loss: 12.825767517089844\n",
      "steps per second: 0.52112\n",
      "step: 34300\n",
      "loss: 13.055811882019043\n",
      "steps per second: 0.57850\n",
      "step: 34301\n",
      "loss: 13.009835243225098\n",
      "steps per second: 0.51593\n",
      "step: 34302\n",
      "loss: 13.524189949035645\n",
      "steps per second: 0.51634\n",
      "step: 34303\n",
      "loss: 12.870009422302246\n",
      "steps per second: 0.51612\n",
      "step: 34304\n",
      "loss: 12.473245620727539\n",
      "steps per second: 0.51893\n",
      "step: 34305\n",
      "loss: 12.703206062316895\n",
      "steps per second: 0.54013\n",
      "step: 34306\n",
      "loss: 12.504302024841309\n",
      "steps per second: 0.49996\n",
      "step: 34307\n",
      "loss: 12.627108573913574\n",
      "steps per second: 0.52992\n",
      "step: 34308\n",
      "loss: 12.554347038269043\n",
      "steps per second: 0.52572\n",
      "step: 34309\n",
      "loss: 12.834921836853027\n",
      "steps per second: 0.53514\n",
      "step: 34310\n",
      "loss: 13.163472175598145\n",
      "steps per second: 0.46546\n",
      "step: 34311\n",
      "loss: 12.737752914428711\n",
      "steps per second: 0.50595\n",
      "step: 34312\n",
      "loss: 13.15610408782959\n",
      "steps per second: 0.53420\n",
      "step: 34313\n",
      "loss: 12.756179809570312\n",
      "steps per second: 0.54438\n",
      "step: 34314\n",
      "loss: 13.147064208984375\n",
      "steps per second: 0.53857\n",
      "step: 34315\n",
      "loss: 12.941641807556152\n",
      "steps per second: 0.51115\n",
      "step: 34316\n",
      "loss: 13.245068550109863\n",
      "steps per second: 0.52635\n",
      "step: 34317\n",
      "loss: 12.835230827331543\n",
      "steps per second: 0.55906\n",
      "step: 34318\n",
      "loss: 12.700886726379395\n",
      "steps per second: 0.51726\n",
      "step: 34319\n",
      "loss: 12.359824180603027\n",
      "steps per second: 0.53615\n",
      "step: 34320\n",
      "loss: 12.910764694213867\n",
      "steps per second: 0.48322\n",
      "step: 34321\n",
      "loss: 12.364668846130371\n",
      "steps per second: 0.58756\n",
      "step: 34322\n",
      "loss: 12.648208618164062\n",
      "steps per second: 0.53726\n",
      "step: 34323\n",
      "loss: 12.73400592803955\n",
      "steps per second: 0.51891\n",
      "step: 34324\n",
      "loss: 13.38419246673584\n",
      "steps per second: 0.53591\n",
      "step: 34325\n",
      "loss: 13.29957389831543\n",
      "steps per second: 0.52882\n",
      "step: 34326\n",
      "loss: 13.17159366607666\n",
      "steps per second: 0.53144\n",
      "step: 34327\n",
      "loss: 12.272880554199219\n",
      "steps per second: 0.50361\n",
      "step: 34328\n",
      "loss: 12.685404777526855\n",
      "steps per second: 0.49835\n",
      "step: 34329\n",
      "loss: 12.612380981445312\n",
      "steps per second: 0.52026\n",
      "step: 34330\n",
      "loss: 12.55069637298584\n",
      "steps per second: 0.52324\n",
      "step: 34331\n",
      "loss: 12.63615608215332\n",
      "steps per second: 0.54874\n",
      "step: 34332\n",
      "loss: 12.844843864440918\n",
      "steps per second: 0.58514\n",
      "step: 34333\n",
      "loss: 13.102677345275879\n",
      "steps per second: 0.54825\n",
      "step: 34334\n",
      "loss: 12.667420387268066\n",
      "steps per second: 0.51760\n",
      "step: 34335\n",
      "loss: 12.041489601135254\n",
      "steps per second: 0.56189\n",
      "step: 34336\n",
      "loss: 12.638373374938965\n",
      "steps per second: 0.58432\n",
      "step: 34337\n",
      "loss: 12.660945892333984\n",
      "steps per second: 0.54420\n",
      "step: 34338\n",
      "loss: 12.382244110107422\n",
      "steps per second: 0.51518\n",
      "step: 34339\n",
      "loss: 12.478263854980469\n",
      "steps per second: 0.51832\n",
      "step: 34340\n",
      "loss: 12.5507230758667\n",
      "steps per second: 0.52853\n",
      "step: 34341\n",
      "loss: 12.868741989135742\n",
      "steps per second: 0.50248\n",
      "step: 34342\n",
      "loss: 12.690314292907715\n",
      "steps per second: 0.52780\n",
      "step: 34343\n",
      "loss: 12.435242652893066\n",
      "steps per second: 0.53264\n",
      "step: 34344\n",
      "loss: 12.816668510437012\n",
      "steps per second: 0.50498\n",
      "step: 34345\n",
      "loss: 12.91535758972168\n",
      "steps per second: 0.51711\n",
      "step: 34346\n",
      "loss: 13.273412704467773\n",
      "steps per second: 0.54737\n",
      "step: 34347\n",
      "loss: 12.36791706085205\n",
      "steps per second: 0.50336\n",
      "step: 34348\n",
      "loss: 12.574674606323242\n",
      "steps per second: 0.51998\n",
      "step: 34349\n",
      "loss: 12.552483558654785\n",
      "steps per second: 0.53038\n",
      "step: 34350\n",
      "loss: 12.934831619262695\n",
      "steps per second: 0.54154\n",
      "step: 34351\n",
      "loss: 13.01768970489502\n",
      "steps per second: 0.53371\n",
      "step: 34352\n",
      "loss: 12.51893424987793\n",
      "steps per second: 0.60733\n",
      "step: 34353\n",
      "loss: 12.601563453674316\n",
      "steps per second: 0.55111\n",
      "step: 34354\n",
      "loss: 12.386699676513672\n",
      "steps per second: 0.52712\n",
      "step: 34355\n",
      "loss: 12.745505332946777\n",
      "steps per second: 0.51717\n",
      "step: 34356\n",
      "loss: 12.780874252319336\n",
      "steps per second: 0.58021\n",
      "step: 34357\n",
      "loss: 13.165056228637695\n",
      "steps per second: 0.61140\n",
      "step: 34358\n",
      "loss: 13.194418907165527\n",
      "steps per second: 0.55569\n",
      "step: 34359\n",
      "loss: 12.715240478515625\n",
      "steps per second: 0.53824\n",
      "step: 34360\n",
      "loss: 12.338495254516602\n",
      "steps per second: 0.57161\n",
      "step: 34361\n",
      "loss: 12.263970375061035\n",
      "steps per second: 0.54587\n",
      "step: 34362\n",
      "loss: 13.252413749694824\n",
      "steps per second: 0.52843\n",
      "step: 34363\n",
      "loss: 13.142391204833984\n",
      "steps per second: 0.55327\n",
      "step: 34364\n",
      "loss: 12.991328239440918\n",
      "steps per second: 0.49242\n",
      "step: 34365\n",
      "loss: 12.725910186767578\n",
      "steps per second: 0.53599\n",
      "step: 34366\n",
      "loss: 12.830805778503418\n",
      "steps per second: 0.50114\n",
      "step: 34367\n",
      "loss: 12.522130012512207\n",
      "steps per second: 0.51851\n",
      "step: 34368\n",
      "loss: 12.973284721374512\n",
      "steps per second: 0.53698\n",
      "step: 34369\n",
      "loss: 13.237942695617676\n",
      "steps per second: 0.54717\n",
      "step: 34370\n",
      "loss: 13.705754280090332\n",
      "steps per second: 0.55901\n",
      "step: 34371\n",
      "loss: 12.711493492126465\n",
      "steps per second: 0.53083\n",
      "step: 34372\n",
      "loss: 12.436139106750488\n",
      "steps per second: 0.54253\n",
      "step: 34373\n",
      "loss: 12.935235977172852\n",
      "steps per second: 0.55958\n",
      "step: 34374\n",
      "loss: 12.951264381408691\n",
      "steps per second: 0.49865\n",
      "step: 34375\n",
      "loss: 13.033506393432617\n",
      "steps per second: 0.54472\n",
      "step: 34376\n",
      "loss: 12.649033546447754\n",
      "steps per second: 0.56094\n",
      "step: 34377\n",
      "loss: 13.07186508178711\n",
      "steps per second: 0.57462\n",
      "step: 34378\n",
      "loss: 12.645975112915039\n",
      "steps per second: 0.55454\n",
      "step: 34379\n",
      "loss: 12.554825782775879\n",
      "steps per second: 0.53869\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8120141625404358, layer: 10\n",
      "saving at step 34379\n",
      "----------\n",
      "\n",
      "\n",
      "step: 34380\n",
      "loss: 12.909645080566406\n",
      "steps per second: 0.27057\n",
      "step: 34381\n",
      "loss: 12.909066200256348\n",
      "steps per second: 0.54508\n",
      "step: 34382\n",
      "loss: 12.731120109558105\n",
      "steps per second: 0.54458\n",
      "step: 34383\n",
      "loss: 13.678333282470703\n",
      "steps per second: 0.55873\n",
      "step: 34384\n",
      "loss: 12.743145942687988\n",
      "steps per second: 0.55189\n",
      "step: 34385\n",
      "loss: 13.043244361877441\n",
      "steps per second: 0.61233\n",
      "step: 34386\n",
      "loss: 12.9219970703125\n",
      "steps per second: 0.57644\n",
      "step: 34387\n",
      "loss: 12.640406608581543\n",
      "steps per second: 0.49163\n",
      "step: 34388\n",
      "loss: 12.71234130859375\n",
      "steps per second: 0.54061\n",
      "step: 34389\n",
      "loss: 12.860432624816895\n",
      "steps per second: 0.55797\n",
      "step: 34390\n",
      "loss: 12.696759223937988\n",
      "steps per second: 0.57820\n",
      "step: 34391\n",
      "loss: 12.803336143493652\n",
      "steps per second: 0.53743\n",
      "step: 34392\n",
      "loss: 12.321012496948242\n",
      "steps per second: 0.55770\n",
      "step: 34393\n",
      "loss: 12.867362022399902\n",
      "steps per second: 0.54935\n",
      "step: 34394\n",
      "loss: 12.537943840026855\n",
      "steps per second: 0.52697\n",
      "step: 34395\n",
      "loss: 12.516884803771973\n",
      "steps per second: 0.55681\n",
      "step: 34396\n",
      "loss: 12.241822242736816\n",
      "steps per second: 0.49769\n",
      "step: 34397\n",
      "loss: 12.1851167678833\n",
      "steps per second: 0.55142\n",
      "step: 34398\n",
      "loss: 12.965694427490234\n",
      "steps per second: 0.57154\n",
      "step: 34399\n",
      "loss: 13.081226348876953\n",
      "steps per second: 0.53155\n",
      "step: 34400\n",
      "loss: 12.858193397521973\n",
      "steps per second: 0.55384\n",
      "step: 34401\n",
      "loss: 12.861165046691895\n",
      "steps per second: 0.54532\n",
      "step: 34402\n",
      "loss: 13.58315372467041\n",
      "steps per second: 0.51802\n",
      "step: 34403\n",
      "loss: 12.886340141296387\n",
      "steps per second: 0.52304\n",
      "step: 34404\n",
      "loss: 12.97669792175293\n",
      "steps per second: 0.49276\n",
      "step: 34405\n",
      "loss: 13.048274993896484\n",
      "steps per second: 0.56371\n",
      "step: 34406\n",
      "loss: 12.858457565307617\n",
      "steps per second: 0.54421\n",
      "step: 34407\n",
      "loss: 13.15184497833252\n",
      "steps per second: 0.54510\n",
      "step: 34408\n",
      "loss: 13.389840126037598\n",
      "steps per second: 0.55348\n",
      "step: 34409\n",
      "loss: 12.609670639038086\n",
      "steps per second: 0.57078\n",
      "step: 34410\n",
      "loss: 12.883223533630371\n",
      "steps per second: 0.54179\n",
      "step: 34411\n",
      "loss: 12.235841751098633\n",
      "steps per second: 0.52112\n",
      "step: 34412\n",
      "loss: 13.121100425720215\n",
      "steps per second: 0.57201\n",
      "step: 34413\n",
      "loss: 12.421850204467773\n",
      "steps per second: 0.55030\n",
      "step: 34414\n",
      "loss: 12.226736068725586\n",
      "steps per second: 0.55161\n",
      "step: 34415\n",
      "loss: 13.193680763244629\n",
      "steps per second: 0.54273\n",
      "step: 34416\n",
      "loss: 12.870135307312012\n",
      "steps per second: 0.60371\n",
      "step: 34417\n",
      "loss: 12.781068801879883\n",
      "steps per second: 0.55690\n",
      "step: 34418\n",
      "loss: 12.795711517333984\n",
      "steps per second: 0.55107\n",
      "step: 34419\n",
      "loss: 13.020572662353516\n",
      "steps per second: 0.52693\n",
      "step: 34420\n",
      "loss: 12.656142234802246\n",
      "steps per second: 0.55202\n",
      "step: 34421\n",
      "loss: 12.586891174316406\n",
      "steps per second: 0.53526\n",
      "step: 34422\n",
      "loss: 12.8541898727417\n",
      "steps per second: 0.55027\n",
      "step: 34423\n",
      "loss: 12.644701957702637\n",
      "steps per second: 0.56641\n",
      "step: 34424\n",
      "loss: 13.041065216064453\n",
      "steps per second: 0.55895\n",
      "step: 34425\n",
      "loss: 13.369857788085938\n",
      "steps per second: 0.55437\n",
      "step: 34426\n",
      "loss: 12.935018539428711\n",
      "steps per second: 0.56970\n",
      "step: 34427\n",
      "loss: 12.66999626159668\n",
      "steps per second: 0.55204\n",
      "step: 34428\n",
      "loss: 13.014969825744629\n",
      "steps per second: 0.54548\n",
      "step: 34429\n",
      "loss: 12.829388618469238\n",
      "steps per second: 0.48301\n",
      "step: 34430\n",
      "loss: 12.89008617401123\n",
      "steps per second: 0.57614\n",
      "step: 34431\n",
      "loss: 12.707975387573242\n",
      "steps per second: 0.54204\n",
      "step: 34432\n",
      "loss: 12.189322471618652\n",
      "steps per second: 0.53712\n",
      "step: 34433\n",
      "loss: 12.73211669921875\n",
      "steps per second: 0.54113\n",
      "step: 34434\n",
      "loss: 12.412829399108887\n",
      "steps per second: 0.60450\n",
      "step: 34435\n",
      "loss: 12.934344291687012\n",
      "steps per second: 0.52671\n",
      "step: 34436\n",
      "loss: 12.937682151794434\n",
      "steps per second: 0.57837\n",
      "step: 34437\n",
      "loss: 12.879377365112305\n",
      "steps per second: 0.57038\n",
      "step: 34438\n",
      "loss: 12.647629737854004\n",
      "steps per second: 0.56815\n",
      "step: 34439\n",
      "loss: 12.718920707702637\n",
      "steps per second: 0.52552\n",
      "step: 34440\n",
      "loss: 13.580854415893555\n",
      "steps per second: 0.55609\n",
      "step: 34441\n",
      "loss: 12.591691017150879\n",
      "steps per second: 0.52981\n",
      "step: 34442\n",
      "loss: 13.143830299377441\n",
      "steps per second: 0.55119\n",
      "step: 34443\n",
      "loss: 12.974574089050293\n",
      "steps per second: 0.52424\n",
      "step: 34444\n",
      "loss: 12.386351585388184\n",
      "steps per second: 0.54164\n",
      "step: 34445\n",
      "loss: 13.377110481262207\n",
      "steps per second: 0.55054\n",
      "step: 34446\n",
      "loss: 12.750906944274902\n",
      "steps per second: 0.55752\n",
      "step: 34447\n",
      "loss: 12.888164520263672\n",
      "steps per second: 0.52687\n",
      "step: 34448\n",
      "loss: 12.95175552368164\n",
      "steps per second: 0.56765\n",
      "step: 34449\n",
      "loss: 12.603273391723633\n",
      "steps per second: 0.52618\n",
      "step: 34450\n",
      "loss: 12.170268058776855\n",
      "steps per second: 0.53577\n",
      "step: 34451\n",
      "loss: 13.002509117126465\n",
      "steps per second: 0.55900\n",
      "step: 34452\n",
      "loss: 12.878280639648438\n",
      "steps per second: 0.53364\n",
      "step: 34453\n",
      "loss: 12.357134819030762\n",
      "steps per second: 0.55079\n",
      "step: 34454\n",
      "loss: 12.67567253112793\n",
      "steps per second: 0.54925\n",
      "step: 34455\n",
      "loss: 13.136037826538086\n",
      "steps per second: 0.54293\n",
      "step: 34456\n",
      "loss: 12.67939281463623\n",
      "steps per second: 0.52308\n",
      "step: 34457\n",
      "loss: 13.252667427062988\n",
      "steps per second: 0.53367\n",
      "step: 34458\n",
      "loss: 12.63592529296875\n",
      "steps per second: 0.55353\n",
      "step: 34459\n",
      "loss: 13.344696044921875\n",
      "steps per second: 0.52658\n",
      "step: 34460\n",
      "loss: 12.656291007995605\n",
      "steps per second: 0.56907\n",
      "step: 34461\n",
      "loss: 12.903472900390625\n",
      "steps per second: 0.54926\n",
      "step: 34462\n",
      "loss: 12.918961524963379\n",
      "steps per second: 0.54186\n",
      "step: 34463\n",
      "loss: 12.449843406677246\n",
      "steps per second: 0.57063\n",
      "step: 34464\n",
      "loss: 12.86422348022461\n",
      "steps per second: 0.53754\n",
      "step: 34465\n",
      "loss: 12.791935920715332\n",
      "steps per second: 0.53410\n",
      "step: 34466\n",
      "loss: 13.007357597351074\n",
      "steps per second: 0.54440\n",
      "step: 34467\n",
      "loss: 13.2521390914917\n",
      "steps per second: 0.56512\n",
      "step: 34468\n",
      "loss: 12.242453575134277\n",
      "steps per second: 0.50387\n",
      "step: 34469\n",
      "loss: 12.260377883911133\n",
      "steps per second: 0.54401\n",
      "step: 34470\n",
      "loss: 12.250937461853027\n",
      "steps per second: 0.54404\n",
      "step: 34471\n",
      "loss: 12.275681495666504\n",
      "steps per second: 0.55746\n",
      "step: 34472\n",
      "loss: 12.883332252502441\n",
      "steps per second: 0.55472\n",
      "step: 34473\n",
      "loss: 13.308225631713867\n",
      "steps per second: 0.51567\n",
      "step: 34474\n",
      "loss: 12.85483455657959\n",
      "steps per second: 0.55994\n",
      "step: 34475\n",
      "loss: 12.78423023223877\n",
      "steps per second: 0.55823\n",
      "step: 34476\n",
      "loss: 12.60511302947998\n",
      "steps per second: 0.53510\n",
      "step: 34477\n",
      "loss: 13.152867317199707\n",
      "steps per second: 0.54947\n",
      "step: 34478\n",
      "loss: 12.892074584960938\n",
      "steps per second: 0.56966\n",
      "step: 34479\n",
      "loss: 13.094654083251953\n",
      "steps per second: 0.55789\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8632693886756897, layer: 12\n",
      "saving at step 34479\n",
      "----------\n",
      "\n",
      "\n",
      "step: 34480\n",
      "loss: 13.446873664855957\n",
      "steps per second: 0.27726\n",
      "step: 34481\n",
      "loss: 12.67291259765625\n",
      "steps per second: 0.56877\n",
      "step: 34482\n",
      "loss: 13.045437812805176\n",
      "steps per second: 0.55087\n",
      "step: 34483\n",
      "loss: 12.943942070007324\n",
      "steps per second: 0.52640\n",
      "step: 34484\n",
      "loss: 13.6014986038208\n",
      "steps per second: 0.54454\n",
      "step: 34485\n",
      "loss: 12.182697296142578\n",
      "steps per second: 0.55017\n",
      "step: 34486\n",
      "loss: 13.801546096801758\n",
      "steps per second: 0.54725\n",
      "step: 34487\n",
      "loss: 12.49169635772705\n",
      "steps per second: 0.56927\n",
      "step: 34488\n",
      "loss: 13.345270156860352\n",
      "steps per second: 0.56940\n",
      "step: 34489\n",
      "loss: 13.450725555419922\n",
      "steps per second: 0.56563\n",
      "step: 34490\n",
      "loss: 13.232044219970703\n",
      "steps per second: 0.57703\n",
      "step: 34491\n",
      "loss: 12.296602249145508\n",
      "steps per second: 0.55122\n",
      "step: 34492\n",
      "loss: 13.000028610229492\n",
      "steps per second: 0.55114\n",
      "step: 34493\n",
      "loss: 13.061678886413574\n",
      "steps per second: 0.48773\n",
      "step: 34494\n",
      "loss: 12.992804527282715\n",
      "steps per second: 0.56873\n",
      "step: 34495\n",
      "loss: 12.490044593811035\n",
      "steps per second: 0.55582\n",
      "step: 34496\n",
      "loss: 12.535527229309082\n",
      "steps per second: 0.55136\n",
      "step: 34497\n",
      "loss: 13.04420280456543\n",
      "steps per second: 0.54278\n",
      "step: 34498\n",
      "loss: 12.741596221923828\n",
      "steps per second: 0.55690\n",
      "step: 34499\n",
      "loss: 13.2453031539917\n",
      "steps per second: 0.53473\n",
      "step: 34500\n",
      "loss: 12.95322036743164\n",
      "steps per second: 0.55097\n",
      "step: 34501\n",
      "loss: 12.979339599609375\n",
      "steps per second: 0.56314\n",
      "step: 34502\n",
      "loss: 12.30065631866455\n",
      "steps per second: 0.53479\n",
      "step: 34503\n",
      "loss: 12.447997093200684\n",
      "steps per second: 0.55392\n",
      "step: 34504\n",
      "loss: 12.294656753540039\n",
      "steps per second: 0.55740\n",
      "step: 34505\n",
      "loss: 12.77658462524414\n",
      "steps per second: 0.55079\n",
      "step: 34506\n",
      "loss: 12.91864013671875\n",
      "steps per second: 0.55578\n",
      "step: 34507\n",
      "loss: 12.723155975341797\n",
      "steps per second: 0.55688\n",
      "step: 34508\n",
      "loss: 12.550332069396973\n",
      "steps per second: 0.53198\n",
      "step: 34509\n",
      "loss: 12.909975051879883\n",
      "steps per second: 0.57183\n",
      "step: 34510\n",
      "loss: 12.876524925231934\n",
      "steps per second: 0.55820\n",
      "step: 34511\n",
      "loss: 13.323140144348145\n",
      "steps per second: 0.53373\n",
      "step: 34512\n",
      "loss: 12.603926658630371\n",
      "steps per second: 0.57452\n",
      "step: 34513\n",
      "loss: 13.13807201385498\n",
      "steps per second: 0.52399\n",
      "step: 34514\n",
      "loss: 12.62570858001709\n",
      "steps per second: 0.53456\n",
      "step: 34515\n",
      "loss: 12.763336181640625\n",
      "steps per second: 0.55698\n",
      "step: 34516\n",
      "loss: 12.94495677947998\n",
      "steps per second: 0.56541\n",
      "step: 34517\n",
      "loss: 12.65789794921875\n",
      "steps per second: 0.56918\n",
      "step: 34518\n",
      "loss: 12.762685775756836\n",
      "steps per second: 0.55075\n",
      "step: 34519\n",
      "loss: 12.434903144836426\n",
      "steps per second: 0.52288\n",
      "step: 34520\n",
      "loss: 12.94494915008545\n",
      "steps per second: 0.55681\n",
      "step: 34521\n",
      "loss: 12.794958114624023\n",
      "steps per second: 0.57576\n",
      "step: 34522\n",
      "loss: 12.633676528930664\n",
      "steps per second: 0.54244\n",
      "step: 34523\n",
      "loss: 12.856343269348145\n",
      "steps per second: 0.55095\n",
      "step: 34524\n",
      "loss: 12.099993705749512\n",
      "steps per second: 0.53495\n",
      "step: 34525\n",
      "loss: 13.101707458496094\n",
      "steps per second: 0.56609\n",
      "step: 34526\n",
      "loss: 12.435534477233887\n",
      "steps per second: 0.58283\n",
      "step: 34527\n",
      "loss: 12.859040260314941\n",
      "steps per second: 0.50127\n",
      "step: 34528\n",
      "loss: 12.694803237915039\n",
      "steps per second: 0.53434\n",
      "step: 34529\n",
      "loss: 12.91153335571289\n",
      "steps per second: 0.56726\n",
      "step: 34530\n",
      "loss: 12.8148775100708\n",
      "steps per second: 0.54086\n",
      "step: 34531\n",
      "loss: 13.005294799804688\n",
      "steps per second: 0.49784\n",
      "step: 34532\n",
      "loss: 13.008363723754883\n",
      "steps per second: 0.48679\n",
      "step: 34533\n",
      "loss: 13.393956184387207\n",
      "steps per second: 0.53638\n",
      "step: 34534\n",
      "loss: 12.756077766418457\n",
      "steps per second: 0.54136\n",
      "step: 34535\n",
      "loss: 12.549752235412598\n",
      "steps per second: 0.55769\n",
      "step: 34536\n",
      "loss: 12.600476264953613\n",
      "steps per second: 0.54893\n",
      "step: 34537\n",
      "loss: 12.74402904510498\n",
      "steps per second: 0.60311\n",
      "step: 34538\n",
      "loss: 12.685483932495117\n",
      "steps per second: 0.54956\n",
      "step: 34539\n",
      "loss: 12.691445350646973\n",
      "steps per second: 0.57478\n",
      "step: 34540\n",
      "loss: 13.128273963928223\n",
      "steps per second: 0.53384\n",
      "step: 34541\n",
      "loss: 13.117287635803223\n",
      "steps per second: 0.48841\n",
      "step: 34542\n",
      "loss: 13.239611625671387\n",
      "steps per second: 0.56563\n",
      "step: 34543\n",
      "loss: 12.84736442565918\n",
      "steps per second: 0.55075\n",
      "step: 34544\n",
      "loss: 13.341404914855957\n",
      "steps per second: 0.56809\n",
      "step: 34545\n",
      "loss: 12.507742881774902\n",
      "steps per second: 0.52534\n",
      "step: 34546\n",
      "loss: 12.730623245239258\n",
      "steps per second: 0.52399\n",
      "step: 34547\n",
      "loss: 12.527217864990234\n",
      "steps per second: 0.53293\n",
      "step: 34548\n",
      "loss: 12.431151390075684\n",
      "steps per second: 0.57703\n",
      "step: 34549\n",
      "loss: 12.948723793029785\n",
      "steps per second: 0.55552\n",
      "step: 34550\n",
      "loss: 13.183091163635254\n",
      "steps per second: 0.53333\n",
      "step: 34551\n",
      "loss: 12.823660850524902\n",
      "steps per second: 0.53524\n",
      "step: 34552\n",
      "loss: 12.748191833496094\n",
      "steps per second: 0.53416\n",
      "step: 34553\n",
      "loss: 12.73155689239502\n",
      "steps per second: 0.55487\n",
      "step: 34554\n",
      "loss: 12.056840896606445\n",
      "steps per second: 0.56845\n",
      "step: 34555\n",
      "loss: 12.795029640197754\n",
      "steps per second: 0.54124\n",
      "step: 34556\n",
      "loss: 12.458513259887695\n",
      "steps per second: 0.51947\n",
      "step: 34557\n",
      "loss: 12.828283309936523\n",
      "steps per second: 0.54843\n",
      "step: 34558\n",
      "loss: 12.838756561279297\n",
      "steps per second: 0.52631\n",
      "step: 34559\n",
      "loss: 12.71074390411377\n",
      "steps per second: 0.55456\n",
      "step: 34560\n",
      "loss: 12.799405097961426\n",
      "steps per second: 0.52439\n",
      "step: 34561\n",
      "loss: 13.00688648223877\n",
      "steps per second: 0.51550\n",
      "step: 34562\n",
      "loss: 12.265250205993652\n",
      "steps per second: 0.53315\n",
      "step: 34563\n",
      "loss: 12.668946266174316\n",
      "steps per second: 0.54329\n",
      "step: 34564\n",
      "loss: 12.999394416809082\n",
      "steps per second: 0.57989\n",
      "step: 34565\n",
      "loss: 12.982721328735352\n",
      "steps per second: 0.54320\n",
      "step: 34566\n",
      "loss: 13.201719284057617\n",
      "steps per second: 0.57008\n",
      "step: 34567\n",
      "loss: 12.940078735351562\n",
      "steps per second: 0.56599\n",
      "step: 34568\n",
      "loss: 12.893721580505371\n",
      "steps per second: 0.56536\n",
      "step: 34569\n",
      "loss: 12.348698616027832\n",
      "steps per second: 0.55435\n",
      "step: 34570\n",
      "loss: 12.419387817382812\n",
      "steps per second: 0.60799\n",
      "step: 34571\n",
      "loss: 13.14352035522461\n",
      "steps per second: 0.52666\n",
      "step: 34572\n",
      "loss: 12.876190185546875\n",
      "steps per second: 0.55669\n",
      "step: 34573\n",
      "loss: 12.929750442504883\n",
      "steps per second: 0.56673\n",
      "step: 34574\n",
      "loss: 12.710738182067871\n",
      "steps per second: 0.53472\n",
      "step: 34575\n",
      "loss: 12.761250495910645\n",
      "steps per second: 0.55353\n",
      "step: 34576\n",
      "loss: 12.929781913757324\n",
      "steps per second: 0.54711\n",
      "step: 34577\n",
      "loss: 13.206271171569824\n",
      "steps per second: 0.52152\n",
      "step: 34578\n",
      "loss: 12.754717826843262\n",
      "steps per second: 0.54735\n",
      "step: 34579\n",
      "loss: 12.808662414550781\n",
      "steps per second: 0.49836\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.794572114944458, layer: 11\n",
      "saving at step 34579\n",
      "----------\n",
      "\n",
      "\n",
      "step: 34580\n",
      "loss: 12.739444732666016\n",
      "steps per second: 0.26075\n",
      "step: 34581\n",
      "loss: 13.033841133117676\n",
      "steps per second: 0.55755\n",
      "step: 34582\n",
      "loss: 12.789091110229492\n",
      "steps per second: 0.56959\n",
      "step: 34583\n",
      "loss: 12.975042343139648\n",
      "steps per second: 0.56209\n",
      "step: 34584\n",
      "loss: 13.104179382324219\n",
      "steps per second: 0.55680\n",
      "step: 34585\n",
      "loss: 13.049690246582031\n",
      "steps per second: 0.53363\n",
      "step: 34586\n",
      "loss: 12.701310157775879\n",
      "steps per second: 0.57777\n",
      "step: 34587\n",
      "loss: 13.081307411193848\n",
      "steps per second: 0.52459\n",
      "step: 34588\n",
      "loss: 12.728374481201172\n",
      "steps per second: 0.54143\n",
      "step: 34589\n",
      "loss: 13.136285781860352\n",
      "steps per second: 0.55220\n",
      "step: 34590\n",
      "loss: 13.140419960021973\n",
      "steps per second: 0.57367\n",
      "step: 34591\n",
      "loss: 12.127320289611816\n",
      "steps per second: 0.58004\n",
      "step: 34592\n",
      "loss: 13.060830116271973\n",
      "steps per second: 0.53352\n",
      "step: 34593\n",
      "loss: 12.981249809265137\n",
      "steps per second: 0.57647\n",
      "step: 34594\n",
      "loss: 12.56171989440918\n",
      "steps per second: 0.54419\n",
      "step: 34595\n",
      "loss: 12.43822193145752\n",
      "steps per second: 0.55336\n",
      "step: 34596\n",
      "loss: 13.095117568969727\n",
      "steps per second: 0.55319\n",
      "step: 34597\n",
      "loss: 13.006683349609375\n",
      "steps per second: 0.54989\n",
      "step: 34598\n",
      "loss: 12.442313194274902\n",
      "steps per second: 0.54934\n",
      "step: 34599\n",
      "loss: 12.804869651794434\n",
      "steps per second: 0.53346\n",
      "step: 34600\n",
      "loss: 12.749385833740234\n",
      "steps per second: 0.53699\n",
      "step: 34601\n",
      "loss: 13.081000328063965\n",
      "steps per second: 0.55092\n",
      "step: 34602\n",
      "loss: 12.793272018432617\n",
      "steps per second: 0.55513\n",
      "step: 34603\n",
      "loss: 12.44364070892334\n",
      "steps per second: 0.53435\n",
      "step: 34604\n",
      "loss: 12.442115783691406\n",
      "steps per second: 0.52484\n",
      "step: 34605\n",
      "loss: 13.045376777648926\n",
      "steps per second: 0.52912\n",
      "step: 34606\n",
      "loss: 13.576343536376953\n",
      "steps per second: 0.54795\n",
      "step: 34607\n",
      "loss: 13.370770454406738\n",
      "steps per second: 0.53841\n",
      "step: 34608\n",
      "loss: 12.769861221313477\n",
      "steps per second: 0.53505\n",
      "step: 34609\n",
      "loss: 13.505135536193848\n",
      "steps per second: 0.52331\n",
      "step: 34610\n",
      "loss: 12.551583290100098\n",
      "steps per second: 0.56796\n",
      "step: 34611\n",
      "loss: 13.330856323242188\n",
      "steps per second: 0.53432\n",
      "step: 34612\n",
      "loss: 12.3805513381958\n",
      "steps per second: 0.55177\n",
      "step: 34613\n",
      "loss: 12.730772972106934\n",
      "steps per second: 0.54828\n",
      "step: 34614\n",
      "loss: 12.647072792053223\n",
      "steps per second: 0.53440\n",
      "step: 34615\n",
      "loss: 12.783732414245605\n",
      "steps per second: 0.61081\n",
      "step: 34616\n",
      "loss: 12.938050270080566\n",
      "steps per second: 0.56234\n",
      "step: 34617\n",
      "loss: 12.24113941192627\n",
      "steps per second: 0.52286\n",
      "step: 34618\n",
      "loss: 12.736075401306152\n",
      "steps per second: 0.60678\n",
      "step: 34619\n",
      "loss: 13.15024185180664\n",
      "steps per second: 0.52337\n",
      "step: 34620\n",
      "loss: 12.954379081726074\n",
      "steps per second: 0.55204\n",
      "step: 34621\n",
      "loss: 13.011635780334473\n",
      "steps per second: 0.55142\n",
      "step: 34622\n",
      "loss: 12.736555099487305\n",
      "steps per second: 0.55218\n",
      "step: 34623\n",
      "loss: 12.106355667114258\n",
      "steps per second: 0.55383\n",
      "step: 34624\n",
      "loss: 12.509366035461426\n",
      "steps per second: 0.54302\n",
      "step: 34625\n",
      "loss: 12.854768753051758\n",
      "steps per second: 0.53981\n",
      "step: 34626\n",
      "loss: 12.620737075805664\n",
      "steps per second: 0.49546\n",
      "step: 34627\n",
      "loss: 12.81100845336914\n",
      "steps per second: 0.52777\n",
      "step: 34628\n",
      "loss: 12.900520324707031\n",
      "steps per second: 0.55506\n",
      "step: 34629\n",
      "loss: 12.06557846069336\n",
      "steps per second: 0.57785\n",
      "step: 34630\n",
      "loss: 13.001827239990234\n",
      "steps per second: 0.59546\n",
      "step: 34631\n",
      "loss: 13.218647956848145\n",
      "steps per second: 0.54268\n",
      "step: 34632\n",
      "loss: 12.295828819274902\n",
      "steps per second: 0.54029\n",
      "step: 34633\n",
      "loss: 13.106752395629883\n",
      "steps per second: 0.60576\n",
      "step: 34634\n",
      "loss: 12.529117584228516\n",
      "steps per second: 0.57279\n",
      "step: 34635\n",
      "loss: 12.537314414978027\n",
      "steps per second: 0.53791\n",
      "step: 34636\n",
      "loss: 12.536073684692383\n",
      "steps per second: 0.54019\n",
      "step: 34637\n",
      "loss: 12.644269943237305\n",
      "steps per second: 0.54064\n",
      "step: 34638\n",
      "loss: 12.389791488647461\n",
      "steps per second: 0.54988\n",
      "step: 34639\n",
      "loss: 12.940594673156738\n",
      "steps per second: 0.54192\n",
      "step: 34640\n",
      "loss: 12.669038772583008\n",
      "steps per second: 0.52702\n",
      "step: 34641\n",
      "loss: 13.02508544921875\n",
      "steps per second: 0.52226\n",
      "step: 34642\n",
      "loss: 12.967366218566895\n",
      "steps per second: 0.58759\n",
      "step: 34643\n",
      "loss: 12.526740074157715\n",
      "steps per second: 0.53817\n",
      "step: 34644\n",
      "loss: 13.130037307739258\n",
      "steps per second: 0.54977\n",
      "step: 34645\n",
      "loss: 12.443193435668945\n",
      "steps per second: 0.53069\n",
      "step: 34646\n",
      "loss: 13.088363647460938\n",
      "steps per second: 0.54953\n",
      "step: 34647\n",
      "loss: 12.910928726196289\n",
      "steps per second: 0.49976\n",
      "step: 34648\n",
      "loss: 13.091500282287598\n",
      "steps per second: 0.55160\n",
      "step: 34649\n",
      "loss: 13.332914352416992\n",
      "steps per second: 0.54131\n",
      "step: 34650\n",
      "loss: 13.105424880981445\n",
      "steps per second: 0.58305\n",
      "step: 34651\n",
      "loss: 13.133624076843262\n",
      "steps per second: 0.56673\n",
      "step: 34652\n",
      "loss: 13.205814361572266\n",
      "steps per second: 0.53370\n",
      "step: 34653\n",
      "loss: 12.658464431762695\n",
      "steps per second: 0.57636\n",
      "step: 34654\n",
      "loss: 13.09824275970459\n",
      "steps per second: 0.55439\n",
      "step: 34655\n",
      "loss: 12.577470779418945\n",
      "steps per second: 0.54317\n",
      "step: 34656\n",
      "loss: 13.152230262756348\n",
      "steps per second: 0.54920\n",
      "step: 34657\n",
      "loss: 13.118711471557617\n",
      "steps per second: 0.56737\n",
      "step: 34658\n",
      "loss: 12.663745880126953\n",
      "steps per second: 0.54149\n",
      "step: 34659\n",
      "loss: 12.830901145935059\n",
      "steps per second: 0.56833\n",
      "step: 34660\n",
      "loss: 12.847001075744629\n",
      "steps per second: 0.48816\n",
      "step: 34661\n",
      "loss: 12.72995662689209\n",
      "steps per second: 0.52260\n",
      "step: 34662\n",
      "loss: 12.656332969665527\n",
      "steps per second: 0.54789\n",
      "step: 34663\n",
      "loss: 12.645017623901367\n",
      "steps per second: 0.55027\n",
      "step: 34664\n",
      "loss: 12.399459838867188\n",
      "steps per second: 0.50217\n",
      "step: 34665\n",
      "loss: 13.131563186645508\n",
      "steps per second: 0.54094\n",
      "step: 34666\n",
      "loss: 12.422253608703613\n",
      "steps per second: 0.57378\n",
      "step: 34667\n",
      "loss: 13.221759796142578\n",
      "steps per second: 0.55113\n",
      "step: 34668\n",
      "loss: 12.798574447631836\n",
      "steps per second: 0.55298\n",
      "step: 34669\n",
      "loss: 13.733416557312012\n",
      "steps per second: 0.53576\n",
      "step: 34670\n",
      "loss: 12.974059104919434\n",
      "steps per second: 0.55252\n",
      "step: 34671\n",
      "loss: 13.205218315124512\n",
      "steps per second: 0.57764\n",
      "step: 34672\n",
      "loss: 13.177879333496094\n",
      "steps per second: 0.54937\n",
      "step: 34673\n",
      "loss: 12.255507469177246\n",
      "steps per second: 0.56816\n",
      "step: 34674\n",
      "loss: 13.170804023742676\n",
      "steps per second: 0.52458\n",
      "step: 34675\n",
      "loss: 12.639784812927246\n",
      "steps per second: 0.57610\n",
      "step: 34676\n",
      "loss: 13.41578483581543\n",
      "steps per second: 0.55721\n",
      "step: 34677\n",
      "loss: 12.69917106628418\n",
      "steps per second: 0.58628\n",
      "step: 34678\n",
      "loss: 13.060927391052246\n",
      "steps per second: 0.49110\n",
      "step: 34679\n",
      "loss: 12.774831771850586\n",
      "steps per second: 0.55805\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8210886120796204, layer: 11\n",
      "saving at step 34679\n",
      "----------\n",
      "\n",
      "\n",
      "step: 34680\n",
      "loss: 12.962503433227539\n",
      "steps per second: 0.29051\n",
      "step: 34681\n",
      "loss: 12.670848846435547\n",
      "steps per second: 0.52756\n",
      "step: 34682\n",
      "loss: 12.231460571289062\n",
      "steps per second: 0.55909\n",
      "step: 34683\n",
      "loss: 13.275096893310547\n",
      "steps per second: 0.56849\n",
      "step: 34684\n",
      "loss: 12.731034278869629\n",
      "steps per second: 0.54942\n",
      "step: 34685\n",
      "loss: 12.811793327331543\n",
      "steps per second: 0.52614\n",
      "step: 34686\n",
      "loss: 13.312175750732422\n",
      "steps per second: 0.49627\n",
      "step: 34687\n",
      "loss: 13.111684799194336\n",
      "steps per second: 0.60769\n",
      "step: 34688\n",
      "loss: 12.98037338256836\n",
      "steps per second: 0.55273\n",
      "step: 34689\n",
      "loss: 12.561873435974121\n",
      "steps per second: 0.52745\n",
      "step: 34690\n",
      "loss: 13.065576553344727\n",
      "steps per second: 0.54160\n",
      "step: 34691\n",
      "loss: 13.225471496582031\n",
      "steps per second: 0.53898\n",
      "step: 34692\n",
      "loss: 12.091657638549805\n",
      "steps per second: 0.55616\n",
      "step: 34693\n",
      "loss: 12.794246673583984\n",
      "steps per second: 0.56920\n",
      "step: 34694\n",
      "loss: 12.372082710266113\n",
      "steps per second: 0.53049\n",
      "step: 34695\n",
      "loss: 12.800753593444824\n",
      "steps per second: 0.60829\n",
      "step: 34696\n",
      "loss: 13.110116004943848\n",
      "steps per second: 0.52511\n",
      "step: 34697\n",
      "loss: 12.643163681030273\n",
      "steps per second: 0.52399\n",
      "step: 34698\n",
      "loss: 13.273797035217285\n",
      "steps per second: 0.53463\n",
      "step: 34699\n",
      "loss: 12.166558265686035\n",
      "steps per second: 0.57695\n",
      "step: 34700\n",
      "loss: 12.976346015930176\n",
      "steps per second: 0.54885\n",
      "step: 34701\n",
      "loss: 12.969992637634277\n",
      "steps per second: 0.54242\n",
      "step: 34702\n",
      "loss: 12.03021240234375\n",
      "steps per second: 0.55681\n",
      "step: 34703\n",
      "loss: 12.764996528625488\n",
      "steps per second: 0.52458\n",
      "step: 34704\n",
      "loss: 12.840044975280762\n",
      "steps per second: 0.57789\n",
      "step: 34705\n",
      "loss: 12.676582336425781\n",
      "steps per second: 0.57021\n",
      "step: 34706\n",
      "loss: 13.001078605651855\n",
      "steps per second: 0.53934\n",
      "step: 34707\n",
      "loss: 12.107879638671875\n",
      "steps per second: 0.60830\n",
      "step: 34708\n",
      "loss: 12.827441215515137\n",
      "steps per second: 0.52495\n",
      "step: 34709\n",
      "loss: 13.296154022216797\n",
      "steps per second: 0.55698\n",
      "step: 34710\n",
      "loss: 12.892644882202148\n",
      "steps per second: 0.49947\n",
      "step: 34711\n",
      "loss: 12.369723320007324\n",
      "steps per second: 0.49999\n",
      "step: 34712\n",
      "loss: 13.10262393951416\n",
      "steps per second: 0.51484\n",
      "step: 34713\n",
      "loss: 12.96587085723877\n",
      "steps per second: 0.52286\n",
      "step: 34714\n",
      "loss: 12.712218284606934\n",
      "steps per second: 0.57261\n",
      "step: 34715\n",
      "loss: 12.764853477478027\n",
      "steps per second: 0.54320\n",
      "step: 34716\n",
      "loss: 13.160892486572266\n",
      "steps per second: 0.52738\n",
      "step: 34717\n",
      "loss: 13.090110778808594\n",
      "steps per second: 0.54205\n",
      "step: 34718\n",
      "loss: 12.187274932861328\n",
      "steps per second: 0.60582\n",
      "step: 34719\n",
      "loss: 12.64555835723877\n",
      "steps per second: 0.52291\n",
      "step: 34720\n",
      "loss: 12.652693748474121\n",
      "steps per second: 0.53087\n",
      "step: 34721\n",
      "loss: 13.388284683227539\n",
      "steps per second: 0.54044\n",
      "step: 34722\n",
      "loss: 12.685648918151855\n",
      "steps per second: 0.54692\n",
      "step: 34723\n",
      "loss: 12.83637809753418\n",
      "steps per second: 0.52006\n",
      "step: 34724\n",
      "loss: 12.72957992553711\n",
      "steps per second: 0.60838\n",
      "step: 34725\n",
      "loss: 12.76369571685791\n",
      "steps per second: 0.56871\n",
      "step: 34726\n",
      "loss: 12.766927719116211\n",
      "steps per second: 0.52546\n",
      "step: 34727\n",
      "loss: 12.823995590209961\n",
      "steps per second: 0.53350\n",
      "step: 34728\n",
      "loss: 12.95830249786377\n",
      "steps per second: 0.57557\n",
      "step: 34729\n",
      "loss: 12.979411125183105\n",
      "steps per second: 0.52238\n",
      "step: 34730\n",
      "loss: 12.051687240600586\n",
      "steps per second: 0.56618\n",
      "step: 34731\n",
      "loss: 12.690306663513184\n",
      "steps per second: 0.55484\n",
      "step: 34732\n",
      "loss: 12.327199935913086\n",
      "steps per second: 0.52331\n",
      "step: 34733\n",
      "loss: 12.71940803527832\n",
      "steps per second: 0.55759\n",
      "step: 34734\n",
      "loss: 13.10835075378418\n",
      "steps per second: 0.58367\n",
      "step: 34735\n",
      "loss: 11.995888710021973\n",
      "steps per second: 0.52537\n",
      "step: 34736\n",
      "loss: 12.821382522583008\n",
      "steps per second: 0.56560\n",
      "step: 34737\n",
      "loss: 12.740355491638184\n",
      "steps per second: 0.53945\n",
      "step: 34738\n",
      "loss: 13.179085731506348\n",
      "steps per second: 0.54662\n",
      "step: 34739\n",
      "loss: 12.926405906677246\n",
      "steps per second: 0.57032\n",
      "step: 34740\n",
      "loss: 12.800958633422852\n",
      "steps per second: 0.55033\n",
      "step: 34741\n",
      "loss: 12.959367752075195\n",
      "steps per second: 0.52728\n",
      "step: 34742\n",
      "loss: 13.063457489013672\n",
      "steps per second: 0.55437\n",
      "step: 34743\n",
      "loss: 13.251837730407715\n",
      "steps per second: 0.56313\n",
      "step: 34744\n",
      "loss: 12.653645515441895\n",
      "steps per second: 0.57720\n",
      "step: 34745\n",
      "loss: 13.499951362609863\n",
      "steps per second: 0.60912\n",
      "step: 34746\n",
      "loss: 12.994068145751953\n",
      "steps per second: 0.55744\n",
      "step: 34747\n",
      "loss: 12.50719165802002\n",
      "steps per second: 0.56762\n",
      "step: 34748\n",
      "loss: 12.054756164550781\n",
      "steps per second: 0.53448\n",
      "step: 34749\n",
      "loss: 13.111946105957031\n",
      "steps per second: 0.55118\n",
      "step: 34750\n",
      "loss: 12.638978004455566\n",
      "steps per second: 0.55521\n",
      "step: 34751\n",
      "loss: 12.419120788574219\n",
      "steps per second: 0.55683\n",
      "step: 34752\n",
      "loss: 13.029566764831543\n",
      "steps per second: 0.55068\n",
      "step: 34753\n",
      "loss: 12.658682823181152\n",
      "steps per second: 0.53850\n",
      "step: 34754\n",
      "loss: 12.641632080078125\n",
      "steps per second: 0.56910\n",
      "step: 34755\n",
      "loss: 12.75169849395752\n",
      "steps per second: 0.54447\n",
      "step: 34756\n",
      "loss: 13.200017929077148\n",
      "steps per second: 0.57390\n",
      "step: 34757\n",
      "loss: 12.945914268493652\n",
      "steps per second: 0.55539\n",
      "step: 34758\n",
      "loss: 13.446052551269531\n",
      "steps per second: 0.54923\n",
      "step: 34759\n",
      "loss: 12.103967666625977\n",
      "steps per second: 0.54930\n",
      "step: 34760\n",
      "loss: 12.028573036193848\n",
      "steps per second: 0.57515\n",
      "step: 34761\n",
      "loss: 12.997505187988281\n",
      "steps per second: 0.56528\n",
      "step: 34762\n",
      "loss: 12.71413803100586\n",
      "steps per second: 0.54750\n",
      "step: 34763\n",
      "loss: 12.55135440826416\n",
      "steps per second: 0.53847\n",
      "step: 34764\n",
      "loss: 13.427278518676758\n",
      "steps per second: 0.55454\n",
      "step: 34765\n",
      "loss: 13.178404808044434\n",
      "steps per second: 0.55258\n",
      "step: 34766\n",
      "loss: 12.7317533493042\n",
      "steps per second: 0.60839\n",
      "step: 34767\n",
      "loss: 12.417067527770996\n",
      "steps per second: 0.55624\n",
      "step: 34768\n",
      "loss: 13.093151092529297\n",
      "steps per second: 0.53900\n",
      "step: 34769\n",
      "loss: 12.681262016296387\n",
      "steps per second: 0.49941\n",
      "step: 34770\n",
      "loss: 13.030169486999512\n",
      "steps per second: 0.55149\n",
      "step: 34771\n",
      "loss: 13.017020225524902\n",
      "steps per second: 0.54241\n",
      "step: 34772\n",
      "loss: 12.432855606079102\n",
      "steps per second: 0.52383\n",
      "step: 34773\n",
      "loss: 12.848634719848633\n",
      "steps per second: 0.56698\n",
      "step: 34774\n",
      "loss: 13.15182113647461\n",
      "steps per second: 0.53437\n",
      "step: 34775\n",
      "loss: 13.317550659179688\n",
      "steps per second: 0.55504\n",
      "step: 34776\n",
      "loss: 13.337355613708496\n",
      "steps per second: 0.52533\n",
      "step: 34777\n",
      "loss: 12.160277366638184\n",
      "steps per second: 0.57880\n",
      "step: 34778\n",
      "loss: 12.372058868408203\n",
      "steps per second: 0.57317\n",
      "step: 34779\n",
      "loss: 12.597139358520508\n",
      "steps per second: 0.55057\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8102788329124451, layer: 11\n",
      "saving at step 34779\n",
      "----------\n",
      "\n",
      "\n",
      "step: 34780\n",
      "loss: 12.823478698730469\n",
      "steps per second: 0.27710\n",
      "step: 34781\n",
      "loss: 12.479094505310059\n",
      "steps per second: 0.51116\n",
      "step: 34782\n",
      "loss: 13.401728630065918\n",
      "steps per second: 0.57398\n",
      "step: 34783\n",
      "loss: 13.714096069335938\n",
      "steps per second: 0.53747\n",
      "step: 34784\n",
      "loss: 12.673469543457031\n",
      "steps per second: 0.57098\n",
      "step: 34785\n",
      "loss: 12.349364280700684\n",
      "steps per second: 0.52256\n",
      "step: 34786\n",
      "loss: 12.815942764282227\n",
      "steps per second: 0.56930\n",
      "step: 34787\n",
      "loss: 12.891433715820312\n",
      "steps per second: 0.52192\n",
      "step: 34788\n",
      "loss: 12.805432319641113\n",
      "steps per second: 0.54248\n",
      "step: 34789\n",
      "loss: 13.3386869430542\n",
      "steps per second: 0.53386\n",
      "step: 34790\n",
      "loss: 12.50655746459961\n",
      "steps per second: 0.60639\n",
      "step: 34791\n",
      "loss: 12.641090393066406\n",
      "steps per second: 0.52829\n",
      "step: 34792\n",
      "loss: 13.105384826660156\n",
      "steps per second: 0.56826\n",
      "step: 34793\n",
      "loss: 12.554471015930176\n",
      "steps per second: 0.53718\n",
      "step: 34794\n",
      "loss: 12.429767608642578\n",
      "steps per second: 0.55700\n",
      "step: 34795\n",
      "loss: 12.985038757324219\n",
      "steps per second: 0.54429\n",
      "step: 34796\n",
      "loss: 13.064156532287598\n",
      "steps per second: 0.52182\n",
      "step: 34797\n",
      "loss: 13.113299369812012\n",
      "steps per second: 0.53589\n",
      "step: 34798\n",
      "loss: 12.852801322937012\n",
      "steps per second: 0.55097\n",
      "step: 34799\n",
      "loss: 12.998852729797363\n",
      "steps per second: 0.55473\n",
      "step: 34800\n",
      "loss: 13.422521591186523\n",
      "steps per second: 0.55245\n",
      "step: 34801\n",
      "loss: 13.11493968963623\n",
      "steps per second: 0.53680\n",
      "step: 34802\n",
      "loss: 13.26097583770752\n",
      "steps per second: 0.53708\n",
      "step: 34803\n",
      "loss: 12.967032432556152\n",
      "steps per second: 0.54189\n",
      "step: 34804\n",
      "loss: 12.724129676818848\n",
      "steps per second: 0.54526\n",
      "step: 34805\n",
      "loss: 13.315908432006836\n",
      "steps per second: 0.53322\n",
      "step: 34806\n",
      "loss: 13.052153587341309\n",
      "steps per second: 0.54141\n",
      "step: 34807\n",
      "loss: 12.309785842895508\n",
      "steps per second: 0.53117\n",
      "step: 34808\n",
      "loss: 12.676236152648926\n",
      "steps per second: 0.54251\n",
      "step: 34809\n",
      "loss: 11.870060920715332\n",
      "steps per second: 0.57717\n",
      "step: 34810\n",
      "loss: 12.752898216247559\n",
      "steps per second: 0.55094\n",
      "step: 34811\n",
      "loss: 12.669410705566406\n",
      "steps per second: 0.55574\n",
      "step: 34812\n",
      "loss: 12.950422286987305\n",
      "steps per second: 0.55648\n",
      "step: 34813\n",
      "loss: 12.71756649017334\n",
      "steps per second: 0.53520\n",
      "step: 34814\n",
      "loss: 12.663993835449219\n",
      "steps per second: 0.57067\n",
      "step: 34815\n",
      "loss: 12.63902473449707\n",
      "steps per second: 0.57838\n",
      "step: 34816\n",
      "loss: 12.301666259765625\n",
      "steps per second: 0.55981\n",
      "step: 34817\n",
      "loss: 12.674332618713379\n",
      "steps per second: 0.48909\n",
      "step: 34818\n",
      "loss: 13.097391128540039\n",
      "steps per second: 0.57322\n",
      "step: 34819\n",
      "loss: 13.1570463180542\n",
      "steps per second: 0.54331\n",
      "step: 34820\n",
      "loss: 12.73013973236084\n",
      "steps per second: 0.54373\n",
      "step: 34821\n",
      "loss: 12.95259952545166\n",
      "steps per second: 0.56946\n",
      "step: 34822\n",
      "loss: 13.260432243347168\n",
      "steps per second: 0.55511\n",
      "step: 34823\n",
      "loss: 12.925217628479004\n",
      "steps per second: 0.53394\n",
      "step: 34824\n",
      "loss: 13.009072303771973\n",
      "steps per second: 0.50061\n",
      "step: 34825\n",
      "loss: 13.028681755065918\n",
      "steps per second: 0.47020\n",
      "step: 34826\n",
      "loss: 12.454863548278809\n",
      "steps per second: 0.56736\n",
      "step: 34827\n",
      "loss: 13.641615867614746\n",
      "steps per second: 0.57229\n",
      "step: 34828\n",
      "loss: 13.041665077209473\n",
      "steps per second: 0.52264\n",
      "step: 34829\n",
      "loss: 13.224270820617676\n",
      "steps per second: 0.52190\n",
      "step: 34830\n",
      "loss: 12.06776237487793\n",
      "steps per second: 0.53614\n",
      "step: 34831\n",
      "loss: 12.91032886505127\n",
      "steps per second: 0.53517\n",
      "step: 34832\n",
      "loss: 12.927239418029785\n",
      "steps per second: 0.55424\n",
      "step: 34833\n",
      "loss: 12.532689094543457\n",
      "steps per second: 0.57037\n",
      "step: 34834\n",
      "loss: 12.378931045532227\n",
      "steps per second: 0.51929\n",
      "step: 34835\n",
      "loss: 12.60354232788086\n",
      "steps per second: 0.53445\n",
      "step: 34836\n",
      "loss: 12.399651527404785\n",
      "steps per second: 0.56937\n",
      "step: 34837\n",
      "loss: 12.591537475585938\n",
      "steps per second: 0.56874\n",
      "step: 34838\n",
      "loss: 12.56761646270752\n",
      "steps per second: 0.53233\n",
      "step: 34839\n",
      "loss: 13.148016929626465\n",
      "steps per second: 0.54941\n",
      "step: 34840\n",
      "loss: 12.694397926330566\n",
      "steps per second: 0.55251\n",
      "step: 34841\n",
      "loss: 12.860941886901855\n",
      "steps per second: 0.57526\n",
      "step: 34842\n",
      "loss: 12.306585311889648\n",
      "steps per second: 0.56711\n",
      "step: 34843\n",
      "loss: 13.076520919799805\n",
      "steps per second: 0.60800\n",
      "step: 34844\n",
      "loss: 12.8030424118042\n",
      "steps per second: 0.53314\n",
      "step: 34845\n",
      "loss: 12.837124824523926\n",
      "steps per second: 0.54068\n",
      "step: 34846\n",
      "loss: 12.832059860229492\n",
      "steps per second: 0.55226\n",
      "step: 34847\n",
      "loss: 12.493896484375\n",
      "steps per second: 0.55443\n",
      "step: 34848\n",
      "loss: 12.974381446838379\n",
      "steps per second: 0.55361\n",
      "step: 34849\n",
      "loss: 13.112646102905273\n",
      "steps per second: 0.54062\n",
      "step: 34850\n",
      "loss: 12.591349601745605\n",
      "steps per second: 0.55595\n",
      "step: 34851\n",
      "loss: 13.060824394226074\n",
      "steps per second: 0.56756\n",
      "step: 34852\n",
      "loss: 12.726424217224121\n",
      "steps per second: 0.56908\n",
      "step: 34853\n",
      "loss: 12.984996795654297\n",
      "steps per second: 0.56925\n",
      "step: 34854\n",
      "loss: 12.885054588317871\n",
      "steps per second: 0.56638\n",
      "step: 34855\n",
      "loss: 12.722853660583496\n",
      "steps per second: 0.56809\n",
      "step: 34856\n",
      "loss: 12.532928466796875\n",
      "steps per second: 0.54094\n",
      "step: 34857\n",
      "loss: 12.961974143981934\n",
      "steps per second: 0.57526\n",
      "step: 34858\n",
      "loss: 12.437110900878906\n",
      "steps per second: 0.57504\n",
      "step: 34859\n",
      "loss: 12.669726371765137\n",
      "steps per second: 0.55286\n",
      "step: 34860\n",
      "loss: 13.296573638916016\n",
      "steps per second: 0.55331\n",
      "step: 34861\n",
      "loss: 12.588327407836914\n",
      "steps per second: 0.55746\n",
      "step: 34862\n",
      "loss: 12.39666748046875\n",
      "steps per second: 0.58008\n",
      "step: 34863\n",
      "loss: 13.09708309173584\n",
      "steps per second: 0.55180\n",
      "step: 34864\n",
      "loss: 13.137449264526367\n",
      "steps per second: 0.55020\n",
      "step: 34865\n",
      "loss: 13.405531883239746\n",
      "steps per second: 0.54919\n",
      "step: 34866\n",
      "loss: 13.127371788024902\n",
      "steps per second: 0.56870\n",
      "step: 34867\n",
      "loss: 12.95399284362793\n",
      "steps per second: 0.53552\n",
      "step: 34868\n",
      "loss: 12.626571655273438\n",
      "steps per second: 0.56572\n",
      "step: 34869\n",
      "loss: 12.912741661071777\n",
      "steps per second: 0.57205\n",
      "step: 34870\n",
      "loss: 13.110879898071289\n",
      "steps per second: 0.52827\n",
      "step: 34871\n",
      "loss: 13.025504112243652\n",
      "steps per second: 0.56643\n",
      "step: 34872\n",
      "loss: 12.98063850402832\n",
      "steps per second: 0.51122\n",
      "step: 34873\n",
      "loss: 12.755413055419922\n",
      "steps per second: 0.52459\n",
      "step: 34874\n",
      "loss: 12.654831886291504\n",
      "steps per second: 0.55422\n",
      "step: 34875\n",
      "loss: 13.137685775756836\n",
      "steps per second: 0.53965\n",
      "step: 34876\n",
      "loss: 12.607269287109375\n",
      "steps per second: 0.51545\n",
      "step: 34877\n",
      "loss: 13.253206253051758\n",
      "steps per second: 0.56545\n",
      "step: 34878\n",
      "loss: 12.658679962158203\n",
      "steps per second: 0.60465\n",
      "step: 34879\n",
      "loss: 12.764912605285645\n",
      "steps per second: 0.55005\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8356611132621765, layer: 11\n",
      "saving at step 34879\n",
      "----------\n",
      "\n",
      "\n",
      "step: 34880\n",
      "loss: 13.588708877563477\n",
      "steps per second: 0.29538\n",
      "step: 34881\n",
      "loss: 12.762290954589844\n",
      "steps per second: 0.55899\n",
      "step: 34882\n",
      "loss: 12.72285270690918\n",
      "steps per second: 0.57345\n",
      "step: 34883\n",
      "loss: 12.68994426727295\n",
      "steps per second: 0.55091\n",
      "step: 34884\n",
      "loss: 13.305863380432129\n",
      "steps per second: 0.52308\n",
      "step: 34885\n",
      "loss: 12.701319694519043\n",
      "steps per second: 0.51674\n",
      "step: 34886\n",
      "loss: 13.02646541595459\n",
      "steps per second: 0.54275\n",
      "step: 34887\n",
      "loss: 12.563263893127441\n",
      "steps per second: 0.57578\n",
      "step: 34888\n",
      "loss: 13.652421951293945\n",
      "steps per second: 0.53437\n",
      "step: 34889\n",
      "loss: 12.74367618560791\n",
      "steps per second: 0.61113\n",
      "step: 34890\n",
      "loss: 12.476655960083008\n",
      "steps per second: 0.51695\n",
      "step: 34891\n",
      "loss: 13.085173606872559\n",
      "steps per second: 0.55713\n",
      "step: 34892\n",
      "loss: 12.441861152648926\n",
      "steps per second: 0.52707\n",
      "step: 34893\n",
      "loss: 12.81082820892334\n",
      "steps per second: 0.56696\n",
      "step: 34894\n",
      "loss: 13.024771690368652\n",
      "steps per second: 0.56479\n",
      "step: 34895\n",
      "loss: 13.658774375915527\n",
      "steps per second: 0.54308\n",
      "step: 34896\n",
      "loss: 12.742118835449219\n",
      "steps per second: 0.54924\n",
      "step: 34897\n",
      "loss: 12.773449897766113\n",
      "steps per second: 0.55600\n",
      "step: 34898\n",
      "loss: 12.436967849731445\n",
      "steps per second: 0.54251\n",
      "step: 34899\n",
      "loss: 12.76386547088623\n",
      "steps per second: 0.56881\n",
      "step: 34900\n",
      "loss: 12.571298599243164\n",
      "steps per second: 0.54989\n",
      "step: 34901\n",
      "loss: 12.17576789855957\n",
      "steps per second: 0.54285\n",
      "step: 34902\n",
      "loss: 12.928083419799805\n",
      "steps per second: 0.56451\n",
      "step: 34903\n",
      "loss: 12.69316291809082\n",
      "steps per second: 0.55651\n",
      "step: 34904\n",
      "loss: 12.964700698852539\n",
      "steps per second: 0.60924\n",
      "step: 34905\n",
      "loss: 12.754209518432617\n",
      "steps per second: 0.53356\n",
      "step: 34906\n",
      "loss: 13.173896789550781\n",
      "steps per second: 0.57624\n",
      "step: 34907\n",
      "loss: 12.634963035583496\n",
      "steps per second: 0.53596\n",
      "step: 34908\n",
      "loss: 12.442279815673828\n",
      "steps per second: 0.53419\n",
      "step: 34909\n",
      "loss: 13.021092414855957\n",
      "steps per second: 0.54247\n",
      "step: 34910\n",
      "loss: 12.457173347473145\n",
      "steps per second: 0.60494\n",
      "step: 34911\n",
      "loss: 13.213544845581055\n",
      "steps per second: 0.55454\n",
      "step: 34912\n",
      "loss: 12.904767036437988\n",
      "steps per second: 0.56709\n",
      "step: 34913\n",
      "loss: 12.94265365600586\n",
      "steps per second: 0.57809\n",
      "step: 34914\n",
      "loss: 13.427413940429688\n",
      "steps per second: 0.55554\n",
      "step: 34915\n",
      "loss: 12.791131973266602\n",
      "steps per second: 0.54322\n",
      "step: 34916\n",
      "loss: 12.460771560668945\n",
      "steps per second: 0.51518\n",
      "step: 34917\n",
      "loss: 12.991214752197266\n",
      "steps per second: 0.56484\n",
      "step: 34918\n",
      "loss: 13.326552391052246\n",
      "steps per second: 0.56171\n",
      "step: 34919\n",
      "loss: 12.511373519897461\n",
      "steps per second: 0.51290\n",
      "step: 34920\n",
      "loss: 12.672316551208496\n",
      "steps per second: 0.56949\n",
      "step: 34921\n",
      "loss: 12.984193801879883\n",
      "steps per second: 0.53521\n",
      "step: 34922\n",
      "loss: 12.60345458984375\n",
      "steps per second: 0.60697\n",
      "step: 34923\n",
      "loss: 12.9567232131958\n",
      "steps per second: 0.54944\n",
      "step: 34924\n",
      "loss: 13.233229637145996\n",
      "steps per second: 0.51594\n",
      "step: 34925\n",
      "loss: 12.257174491882324\n",
      "steps per second: 0.50065\n",
      "step: 34926\n",
      "loss: 12.865913391113281\n",
      "steps per second: 0.50042\n",
      "step: 34927\n",
      "loss: 13.062148094177246\n",
      "steps per second: 0.55514\n",
      "step: 34928\n",
      "loss: 12.741564750671387\n",
      "steps per second: 0.49065\n",
      "step: 34929\n",
      "loss: 12.76909065246582\n",
      "steps per second: 0.55481\n",
      "step: 34930\n",
      "loss: 13.00898265838623\n",
      "steps per second: 0.56989\n",
      "step: 34931\n",
      "loss: 12.3151273727417\n",
      "steps per second: 0.57601\n",
      "step: 34932\n",
      "loss: 12.713908195495605\n",
      "steps per second: 0.54306\n",
      "step: 34933\n",
      "loss: 13.032549858093262\n",
      "steps per second: 0.52088\n",
      "step: 34934\n",
      "loss: 12.151315689086914\n",
      "steps per second: 0.60395\n",
      "step: 34935\n",
      "loss: 12.449792861938477\n",
      "steps per second: 0.55076\n",
      "step: 34936\n",
      "loss: 12.790210723876953\n",
      "steps per second: 0.57792\n",
      "step: 34937\n",
      "loss: 12.460535049438477\n",
      "steps per second: 0.53637\n",
      "step: 34938\n",
      "loss: 12.926076889038086\n",
      "steps per second: 0.54258\n",
      "step: 34939\n",
      "loss: 12.483647346496582\n",
      "steps per second: 0.55280\n",
      "step: 34940\n",
      "loss: 12.588579177856445\n",
      "steps per second: 0.55252\n",
      "step: 34941\n",
      "loss: 13.11586856842041\n",
      "steps per second: 0.56881\n",
      "step: 34942\n",
      "loss: 12.991816520690918\n",
      "steps per second: 0.53366\n",
      "step: 34943\n",
      "loss: 12.856653213500977\n",
      "steps per second: 0.57523\n",
      "step: 34944\n",
      "loss: 13.105046272277832\n",
      "steps per second: 0.53581\n",
      "step: 34945\n",
      "loss: 12.666991233825684\n",
      "steps per second: 0.52109\n",
      "step: 34946\n",
      "loss: 12.357598304748535\n",
      "steps per second: 0.56034\n",
      "step: 34947\n",
      "loss: 12.64818000793457\n",
      "steps per second: 0.54611\n",
      "step: 34948\n",
      "loss: 12.30798625946045\n",
      "steps per second: 0.58253\n",
      "step: 34949\n",
      "loss: 12.788766860961914\n",
      "steps per second: 0.55076\n",
      "step: 34950\n",
      "loss: 12.181015014648438\n",
      "steps per second: 0.53728\n",
      "step: 34951\n",
      "loss: 12.482657432556152\n",
      "steps per second: 0.54889\n",
      "step: 34952\n",
      "loss: 13.149670600891113\n",
      "steps per second: 0.56932\n",
      "step: 34953\n",
      "loss: 12.880791664123535\n",
      "steps per second: 0.57982\n",
      "step: 34954\n",
      "loss: 12.819352149963379\n",
      "steps per second: 0.54656\n",
      "step: 34955\n",
      "loss: 13.055092811584473\n",
      "steps per second: 0.56805\n",
      "step: 34956\n",
      "loss: 12.845544815063477\n",
      "steps per second: 0.54810\n",
      "step: 34957\n",
      "loss: 13.686097145080566\n",
      "steps per second: 0.58623\n",
      "step: 34958\n",
      "loss: 12.40357780456543\n",
      "steps per second: 0.55247\n",
      "step: 34959\n",
      "loss: 12.865191459655762\n",
      "steps per second: 0.56604\n",
      "step: 34960\n",
      "loss: 13.027687072753906\n",
      "steps per second: 0.53481\n",
      "step: 34961\n",
      "loss: 12.499144554138184\n",
      "steps per second: 0.53417\n",
      "step: 34962\n",
      "loss: 12.635289192199707\n",
      "steps per second: 0.52971\n",
      "step: 34963\n",
      "loss: 13.397419929504395\n",
      "steps per second: 0.57041\n",
      "step: 34964\n",
      "loss: 12.792665481567383\n",
      "steps per second: 0.52315\n",
      "step: 34965\n",
      "loss: 12.318833351135254\n",
      "steps per second: 0.55511\n",
      "step: 34966\n",
      "loss: 13.177346229553223\n",
      "steps per second: 0.54018\n",
      "step: 34967\n",
      "loss: 12.771456718444824\n",
      "steps per second: 0.53486\n",
      "step: 34968\n",
      "loss: 12.908125877380371\n",
      "steps per second: 0.57449\n",
      "step: 34969\n",
      "loss: 12.5899658203125\n",
      "steps per second: 0.53204\n",
      "step: 34970\n",
      "loss: 12.818607330322266\n",
      "steps per second: 0.55034\n",
      "step: 34971\n",
      "loss: 12.590177536010742\n",
      "steps per second: 0.49957\n",
      "step: 34972\n",
      "loss: 12.993295669555664\n",
      "steps per second: 0.57666\n",
      "step: 34973\n",
      "loss: 13.404206275939941\n",
      "steps per second: 0.55817\n",
      "step: 34974\n",
      "loss: 12.739900588989258\n",
      "steps per second: 0.57055\n",
      "step: 34975\n",
      "loss: 12.824382781982422\n",
      "steps per second: 0.56849\n",
      "step: 34976\n",
      "loss: 12.719806671142578\n",
      "steps per second: 0.54162\n",
      "step: 34977\n",
      "loss: 13.415999412536621\n",
      "steps per second: 0.55783\n",
      "step: 34978\n",
      "loss: 12.374590873718262\n",
      "steps per second: 0.60979\n",
      "step: 34979\n",
      "loss: 12.526384353637695\n",
      "steps per second: 0.55137\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8075408935546875, layer: 10\n",
      "saving at step 34979\n",
      "----------\n",
      "\n",
      "\n",
      "step: 34980\n",
      "loss: 12.924493789672852\n",
      "steps per second: 0.27330\n",
      "step: 34981\n",
      "loss: 12.291786193847656\n",
      "steps per second: 0.53435\n",
      "step: 34982\n",
      "loss: 12.243002891540527\n",
      "steps per second: 0.55472\n",
      "step: 34983\n",
      "loss: 13.066762924194336\n",
      "steps per second: 0.57766\n",
      "step: 34984\n",
      "loss: 12.916754722595215\n",
      "steps per second: 0.56716\n",
      "step: 34985\n",
      "loss: 12.988580703735352\n",
      "steps per second: 0.59755\n",
      "step: 34986\n",
      "loss: 12.971324920654297\n",
      "steps per second: 0.56717\n",
      "step: 34987\n",
      "loss: 12.737457275390625\n",
      "steps per second: 0.55506\n",
      "step: 34988\n",
      "loss: 12.736539840698242\n",
      "steps per second: 0.57685\n",
      "step: 34989\n",
      "loss: 12.68593692779541\n",
      "steps per second: 0.57842\n",
      "step: 34990\n",
      "loss: 12.997757911682129\n",
      "steps per second: 0.56461\n",
      "step: 34991\n",
      "loss: 12.998594284057617\n",
      "steps per second: 0.55642\n",
      "step: 34992\n",
      "loss: 12.950423240661621\n",
      "steps per second: 0.54968\n",
      "step: 34993\n",
      "loss: 12.551545143127441\n",
      "steps per second: 0.57325\n",
      "step: 34994\n",
      "loss: 12.894944190979004\n",
      "steps per second: 0.54787\n",
      "step: 34995\n",
      "loss: 13.089323997497559\n",
      "steps per second: 0.55669\n",
      "step: 34996\n",
      "loss: 12.995675086975098\n",
      "steps per second: 0.54902\n",
      "step: 34997\n",
      "loss: 12.873202323913574\n",
      "steps per second: 0.53900\n",
      "step: 34998\n",
      "loss: 12.635957717895508\n",
      "steps per second: 0.57539\n",
      "step: 34999\n",
      "loss: 12.504417419433594\n",
      "steps per second: 0.52583\n",
      "step: 35000\n",
      "loss: 12.575559616088867\n",
      "steps per second: 0.58151\n",
      "step: 35001\n",
      "loss: 12.882225036621094\n",
      "steps per second: 0.55875\n",
      "step: 35002\n",
      "loss: 12.919293403625488\n",
      "steps per second: 0.54633\n",
      "step: 35003\n",
      "loss: 12.688831329345703\n",
      "steps per second: 0.51217\n",
      "step: 35004\n",
      "loss: 12.173479080200195\n",
      "steps per second: 0.56891\n",
      "step: 35005\n",
      "loss: 13.050065040588379\n",
      "steps per second: 0.51755\n",
      "step: 35006\n",
      "loss: 12.73244857788086\n",
      "steps per second: 0.56345\n",
      "step: 35007\n",
      "loss: 13.010665893554688\n",
      "steps per second: 0.57165\n",
      "step: 35008\n",
      "loss: 12.42711067199707\n",
      "steps per second: 0.53330\n",
      "step: 35009\n",
      "loss: 12.61176872253418\n",
      "steps per second: 0.51526\n",
      "step: 35010\n",
      "loss: 13.054515838623047\n",
      "steps per second: 0.55245\n",
      "step: 35011\n",
      "loss: 13.025073051452637\n",
      "steps per second: 0.54239\n",
      "step: 35012\n",
      "loss: 12.925379753112793\n",
      "steps per second: 0.52295\n",
      "step: 35013\n",
      "loss: 13.328086853027344\n",
      "steps per second: 0.56904\n",
      "step: 35014\n",
      "loss: 13.03217601776123\n",
      "steps per second: 0.51244\n",
      "step: 35015\n",
      "loss: 12.725995063781738\n",
      "steps per second: 0.56929\n",
      "step: 35016\n",
      "loss: 12.670344352722168\n",
      "steps per second: 0.55613\n",
      "step: 35017\n",
      "loss: 12.896714210510254\n",
      "steps per second: 0.53556\n",
      "step: 35018\n",
      "loss: 13.071793556213379\n",
      "steps per second: 0.61092\n",
      "step: 35019\n",
      "loss: 12.780672073364258\n",
      "steps per second: 0.52327\n",
      "step: 35020\n",
      "loss: 12.424956321716309\n",
      "steps per second: 0.54057\n",
      "step: 35021\n",
      "loss: 13.051892280578613\n",
      "steps per second: 0.52431\n",
      "step: 35022\n",
      "loss: 12.517382621765137\n",
      "steps per second: 0.55055\n",
      "step: 35023\n",
      "loss: 12.32973575592041\n",
      "steps per second: 0.56957\n",
      "step: 35024\n",
      "loss: 12.868003845214844\n",
      "steps per second: 0.54112\n",
      "step: 35025\n",
      "loss: 12.528634071350098\n",
      "steps per second: 0.52794\n",
      "step: 35026\n",
      "loss: 12.48823356628418\n",
      "steps per second: 0.56508\n",
      "step: 35027\n",
      "loss: 12.267020225524902\n",
      "steps per second: 0.57774\n",
      "step: 35028\n",
      "loss: 12.780486106872559\n",
      "steps per second: 0.55985\n",
      "step: 35029\n",
      "loss: 12.546202659606934\n",
      "steps per second: 0.54152\n",
      "step: 35030\n",
      "loss: 13.156551361083984\n",
      "steps per second: 0.55580\n",
      "step: 35031\n",
      "loss: 13.323086738586426\n",
      "steps per second: 0.55458\n",
      "step: 35032\n",
      "loss: 12.583327293395996\n",
      "steps per second: 0.55273\n",
      "step: 35033\n",
      "loss: 13.229920387268066\n",
      "steps per second: 0.49840\n",
      "step: 35034\n",
      "loss: 12.57905101776123\n",
      "steps per second: 0.55819\n",
      "step: 35035\n",
      "loss: 12.683258056640625\n",
      "steps per second: 0.57503\n",
      "step: 35036\n",
      "loss: 12.701974868774414\n",
      "steps per second: 0.52390\n",
      "step: 35037\n",
      "loss: 12.51906967163086\n",
      "steps per second: 0.50948\n",
      "step: 35038\n",
      "loss: 12.589930534362793\n",
      "steps per second: 0.48712\n",
      "step: 35039\n",
      "loss: 13.066311836242676\n",
      "steps per second: 0.55962\n",
      "step: 35040\n",
      "loss: 12.989568710327148\n",
      "steps per second: 0.53962\n",
      "step: 35041\n",
      "loss: 12.553913116455078\n",
      "steps per second: 0.50996\n",
      "step: 35042\n",
      "loss: 12.377257347106934\n",
      "steps per second: 0.53112\n",
      "step: 35043\n",
      "loss: 12.614595413208008\n",
      "steps per second: 0.55836\n",
      "step: 35044\n",
      "loss: 13.040326118469238\n",
      "steps per second: 0.57695\n",
      "step: 35045\n",
      "loss: 13.025449752807617\n",
      "steps per second: 0.60784\n",
      "step: 35046\n",
      "loss: 12.615023612976074\n",
      "steps per second: 0.55413\n",
      "step: 35047\n",
      "loss: 13.468488693237305\n",
      "steps per second: 0.54908\n",
      "step: 35048\n",
      "loss: 12.434415817260742\n",
      "steps per second: 0.57410\n",
      "step: 35049\n",
      "loss: 12.36214542388916\n",
      "steps per second: 0.52642\n",
      "step: 35050\n",
      "loss: 13.017227172851562\n",
      "steps per second: 0.60786\n",
      "step: 35051\n",
      "loss: 13.099933624267578\n",
      "steps per second: 0.55135\n",
      "step: 35052\n",
      "loss: 12.73115062713623\n",
      "steps per second: 0.52121\n",
      "step: 35053\n",
      "loss: 12.719672203063965\n",
      "steps per second: 0.57559\n",
      "step: 35054\n",
      "loss: 12.872224807739258\n",
      "steps per second: 0.56400\n",
      "step: 35055\n",
      "loss: 12.460901260375977\n",
      "steps per second: 0.56772\n",
      "step: 35056\n",
      "loss: 12.331502914428711\n",
      "steps per second: 0.56905\n",
      "step: 35057\n",
      "loss: 13.065011024475098\n",
      "steps per second: 0.54111\n",
      "step: 35058\n",
      "loss: 12.616498947143555\n",
      "steps per second: 0.53336\n",
      "step: 35059\n",
      "loss: 12.871484756469727\n",
      "steps per second: 0.51950\n",
      "step: 35060\n",
      "loss: 12.660785675048828\n",
      "steps per second: 0.57329\n",
      "step: 35061\n",
      "loss: 12.48698616027832\n",
      "steps per second: 0.54975\n",
      "step: 35062\n",
      "loss: 12.661799430847168\n",
      "steps per second: 0.56919\n",
      "step: 35063\n",
      "loss: 12.937896728515625\n",
      "steps per second: 0.52367\n",
      "step: 35064\n",
      "loss: 13.378090858459473\n",
      "steps per second: 0.57461\n",
      "step: 35065\n",
      "loss: 12.796416282653809\n",
      "steps per second: 0.56753\n",
      "step: 35066\n",
      "loss: 12.621402740478516\n",
      "steps per second: 0.53023\n",
      "step: 35067\n",
      "loss: 13.054255485534668\n",
      "steps per second: 0.54729\n",
      "step: 35068\n",
      "loss: 12.377870559692383\n",
      "steps per second: 0.52526\n",
      "step: 35069\n",
      "loss: 12.620418548583984\n",
      "steps per second: 0.55458\n",
      "step: 35070\n",
      "loss: 12.368934631347656\n",
      "steps per second: 0.54404\n",
      "step: 35071\n",
      "loss: 12.217029571533203\n",
      "steps per second: 0.52736\n",
      "step: 35072\n",
      "loss: 12.893866539001465\n",
      "steps per second: 0.53706\n",
      "step: 35073\n",
      "loss: 12.599973678588867\n",
      "steps per second: 0.55477\n",
      "step: 35074\n",
      "loss: 12.606739044189453\n",
      "steps per second: 0.55003\n",
      "step: 35075\n",
      "loss: 13.090682983398438\n",
      "steps per second: 0.47674\n",
      "step: 35076\n",
      "loss: 12.743086814880371\n",
      "steps per second: 0.49754\n",
      "step: 35077\n",
      "loss: 12.496929168701172\n",
      "steps per second: 0.52449\n",
      "step: 35078\n",
      "loss: 12.57259750366211\n",
      "steps per second: 0.55400\n",
      "step: 35079\n",
      "loss: 13.07415771484375\n",
      "steps per second: 0.54324\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8267960548400879, layer: 11\n",
      "saving at step 35079\n",
      "----------\n",
      "\n",
      "\n",
      "step: 35080\n",
      "loss: 12.950640678405762\n",
      "steps per second: 0.27411\n",
      "step: 35081\n",
      "loss: 12.78064250946045\n",
      "steps per second: 0.53780\n",
      "step: 35082\n",
      "loss: 12.752620697021484\n",
      "steps per second: 0.55374\n",
      "step: 35083\n",
      "loss: 13.384493827819824\n",
      "steps per second: 0.55603\n",
      "step: 35084\n",
      "loss: 12.907066345214844\n",
      "steps per second: 0.55633\n",
      "step: 35085\n",
      "loss: 12.473098754882812\n",
      "steps per second: 0.52329\n",
      "step: 35086\n",
      "loss: 12.804777145385742\n",
      "steps per second: 0.51232\n",
      "step: 35087\n",
      "loss: 12.744546890258789\n",
      "steps per second: 0.56106\n",
      "step: 35088\n",
      "loss: 12.797782897949219\n",
      "steps per second: 0.56645\n",
      "step: 35089\n",
      "loss: 13.041492462158203\n",
      "steps per second: 0.53523\n",
      "step: 35090\n",
      "loss: 12.823760032653809\n",
      "steps per second: 0.51580\n",
      "step: 35091\n",
      "loss: 13.163897514343262\n",
      "steps per second: 0.56836\n",
      "step: 35092\n",
      "loss: 12.799568176269531\n",
      "steps per second: 0.55218\n",
      "step: 35093\n",
      "loss: 12.519264221191406\n",
      "steps per second: 0.57633\n",
      "step: 35094\n",
      "loss: 12.683753967285156\n",
      "steps per second: 0.54689\n",
      "step: 35095\n",
      "loss: 12.372249603271484\n",
      "steps per second: 0.52330\n",
      "step: 35096\n",
      "loss: 12.605888366699219\n",
      "steps per second: 0.54858\n",
      "step: 35097\n",
      "loss: 12.913667678833008\n",
      "steps per second: 0.55888\n",
      "step: 35098\n",
      "loss: 12.598341941833496\n",
      "steps per second: 0.55208\n",
      "step: 35099\n",
      "loss: 12.787263870239258\n",
      "steps per second: 0.56610\n",
      "step: 35100\n",
      "loss: 12.380589485168457\n",
      "steps per second: 0.56705\n",
      "step: 35101\n",
      "loss: 12.843889236450195\n",
      "steps per second: 0.51338\n",
      "step: 35102\n",
      "loss: 12.290475845336914\n",
      "steps per second: 0.56752\n",
      "step: 35103\n",
      "loss: 12.980133056640625\n",
      "steps per second: 0.56663\n",
      "step: 35104\n",
      "loss: 12.157435417175293\n",
      "steps per second: 0.53762\n",
      "step: 35105\n",
      "loss: 12.911645889282227\n",
      "steps per second: 0.53432\n",
      "step: 35106\n",
      "loss: 13.057869911193848\n",
      "steps per second: 0.54776\n",
      "step: 35107\n",
      "loss: 12.527745246887207\n",
      "steps per second: 0.52473\n",
      "step: 35108\n",
      "loss: 12.094198226928711\n",
      "steps per second: 0.60326\n",
      "step: 35109\n",
      "loss: 12.692313194274902\n",
      "steps per second: 0.53993\n",
      "step: 35110\n",
      "loss: 12.750384330749512\n",
      "steps per second: 0.54193\n",
      "step: 35111\n",
      "loss: 12.299137115478516\n",
      "steps per second: 0.49204\n",
      "step: 35112\n",
      "loss: 13.49902057647705\n",
      "steps per second: 0.52685\n",
      "step: 35113\n",
      "loss: 12.585794448852539\n",
      "steps per second: 0.54247\n",
      "step: 35114\n",
      "loss: 12.88160228729248\n",
      "steps per second: 0.54311\n",
      "step: 35115\n",
      "loss: 12.796419143676758\n",
      "steps per second: 0.57647\n",
      "step: 35116\n",
      "loss: 12.426339149475098\n",
      "steps per second: 0.54057\n",
      "step: 35117\n",
      "loss: 13.283235549926758\n",
      "steps per second: 0.54872\n",
      "step: 35118\n",
      "loss: 13.232027053833008\n",
      "steps per second: 0.52142\n",
      "step: 35119\n",
      "loss: 13.043052673339844\n",
      "steps per second: 0.52554\n",
      "step: 35120\n",
      "loss: 12.182419776916504\n",
      "steps per second: 0.54946\n",
      "step: 35121\n",
      "loss: 13.212300300598145\n",
      "steps per second: 0.55346\n",
      "step: 35122\n",
      "loss: 12.787779808044434\n",
      "steps per second: 0.54603\n",
      "step: 35123\n",
      "loss: 12.376097679138184\n",
      "steps per second: 0.53194\n",
      "step: 35124\n",
      "loss: 13.557699203491211\n",
      "steps per second: 0.56689\n",
      "step: 35125\n",
      "loss: 12.278271675109863\n",
      "steps per second: 0.56897\n",
      "step: 35126\n",
      "loss: 12.359963417053223\n",
      "steps per second: 0.55783\n",
      "step: 35127\n",
      "loss: 13.006138801574707\n",
      "steps per second: 0.54441\n",
      "step: 35128\n",
      "loss: 12.480753898620605\n",
      "steps per second: 0.54695\n",
      "step: 35129\n",
      "loss: 12.896037101745605\n",
      "steps per second: 0.55647\n",
      "step: 35130\n",
      "loss: 12.659205436706543\n",
      "steps per second: 0.52660\n",
      "step: 35131\n",
      "loss: 12.89301872253418\n",
      "steps per second: 0.55099\n",
      "step: 35132\n",
      "loss: 12.800434112548828\n",
      "steps per second: 0.55614\n",
      "step: 35133\n",
      "loss: 13.517205238342285\n",
      "steps per second: 0.51061\n",
      "step: 35134\n",
      "loss: 12.721153259277344\n",
      "steps per second: 0.57386\n",
      "step: 35135\n",
      "loss: 12.512422561645508\n",
      "steps per second: 0.57921\n",
      "step: 35136\n",
      "loss: 12.963756561279297\n",
      "steps per second: 0.54291\n",
      "step: 35137\n",
      "loss: 12.571803092956543\n",
      "steps per second: 0.52174\n",
      "step: 35138\n",
      "loss: 12.90396785736084\n",
      "steps per second: 0.54442\n",
      "step: 35139\n",
      "loss: 12.830769538879395\n",
      "steps per second: 0.52516\n",
      "step: 35140\n",
      "loss: 12.636778831481934\n",
      "steps per second: 0.53553\n",
      "step: 35141\n",
      "loss: 13.005762100219727\n",
      "steps per second: 0.57532\n",
      "step: 35142\n",
      "loss: 12.394770622253418\n",
      "steps per second: 0.53547\n",
      "step: 35143\n",
      "loss: 12.927512168884277\n",
      "steps per second: 0.54713\n",
      "step: 35144\n",
      "loss: 12.811783790588379\n",
      "steps per second: 0.52746\n",
      "step: 35145\n",
      "loss: 12.341667175292969\n",
      "steps per second: 0.57075\n",
      "step: 35146\n",
      "loss: 12.708842277526855\n",
      "steps per second: 0.52296\n",
      "step: 35147\n",
      "loss: 12.719779968261719\n",
      "steps per second: 0.55192\n",
      "step: 35148\n",
      "loss: 13.09959602355957\n",
      "steps per second: 0.54746\n",
      "step: 35149\n",
      "loss: 12.54647159576416\n",
      "steps per second: 0.51304\n",
      "step: 35150\n",
      "loss: 12.655922889709473\n",
      "steps per second: 0.58479\n",
      "step: 35151\n",
      "loss: 12.865607261657715\n",
      "steps per second: 0.55727\n",
      "step: 35152\n",
      "loss: 12.206558227539062\n",
      "steps per second: 0.53641\n",
      "step: 35153\n",
      "loss: 12.934843063354492\n",
      "steps per second: 0.61028\n",
      "step: 35154\n",
      "loss: 12.674976348876953\n",
      "steps per second: 0.55415\n",
      "step: 35155\n",
      "loss: 12.498100280761719\n",
      "steps per second: 0.54166\n",
      "step: 35156\n",
      "loss: 12.653667449951172\n",
      "steps per second: 0.50956\n",
      "step: 35157\n",
      "loss: 12.67228889465332\n",
      "steps per second: 0.57462\n",
      "step: 35158\n",
      "loss: 12.668391227722168\n",
      "steps per second: 0.53670\n",
      "step: 35159\n",
      "loss: 12.652461051940918\n",
      "steps per second: 0.55427\n",
      "step: 35160\n",
      "loss: 12.764509201049805\n",
      "steps per second: 0.53453\n",
      "step: 35161\n",
      "loss: 12.794571876525879\n",
      "steps per second: 0.53899\n",
      "step: 35162\n",
      "loss: 12.547928810119629\n",
      "steps per second: 0.57753\n",
      "step: 35163\n",
      "loss: 12.963380813598633\n",
      "steps per second: 0.54865\n",
      "step: 35164\n",
      "loss: 13.113382339477539\n",
      "steps per second: 0.57619\n",
      "step: 35165\n",
      "loss: 13.112200736999512\n",
      "steps per second: 0.54034\n",
      "step: 35166\n",
      "loss: 12.70128345489502\n",
      "steps per second: 0.54714\n",
      "step: 35167\n",
      "loss: 12.764554023742676\n",
      "steps per second: 0.54950\n",
      "step: 35168\n",
      "loss: 12.252449989318848\n",
      "steps per second: 0.57576\n",
      "step: 35169\n",
      "loss: 13.752626419067383\n",
      "steps per second: 0.54440\n",
      "step: 35170\n",
      "loss: 12.874951362609863\n",
      "steps per second: 0.54305\n",
      "step: 35171\n",
      "loss: 13.165793418884277\n",
      "steps per second: 0.51808\n",
      "step: 35172\n",
      "loss: 12.791435241699219\n",
      "steps per second: 0.52461\n",
      "step: 35173\n",
      "loss: 13.416991233825684\n",
      "steps per second: 0.57650\n",
      "step: 35174\n",
      "loss: 13.447086334228516\n",
      "steps per second: 0.52757\n",
      "step: 35175\n",
      "loss: 12.498796463012695\n",
      "steps per second: 0.53101\n",
      "step: 35176\n",
      "loss: 13.363922119140625\n",
      "steps per second: 0.55382\n",
      "step: 35177\n",
      "loss: 12.70771598815918\n",
      "steps per second: 0.51220\n",
      "step: 35178\n",
      "loss: 13.000809669494629\n",
      "steps per second: 0.56317\n",
      "step: 35179\n",
      "loss: 12.823902130126953\n",
      "steps per second: 0.59375\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8458136916160583, layer: 10\n",
      "saving at step 35179\n",
      "----------\n",
      "\n",
      "\n",
      "step: 35180\n",
      "loss: 13.189516067504883\n",
      "steps per second: 0.19301\n",
      "step: 35181\n",
      "loss: 12.943733215332031\n",
      "steps per second: 0.55143\n",
      "step: 35182\n",
      "loss: 13.096121788024902\n",
      "steps per second: 0.54463\n",
      "step: 35183\n",
      "loss: 12.619110107421875\n",
      "steps per second: 0.53124\n",
      "step: 35184\n",
      "loss: 12.62745189666748\n",
      "steps per second: 0.53712\n",
      "step: 35185\n",
      "loss: 12.39152717590332\n",
      "steps per second: 0.50788\n",
      "step: 35186\n",
      "loss: 12.834538459777832\n",
      "steps per second: 0.53410\n",
      "step: 35187\n",
      "loss: 13.35617733001709\n",
      "steps per second: 0.52805\n",
      "step: 35188\n",
      "loss: 12.417495727539062\n",
      "steps per second: 0.50015\n",
      "step: 35189\n",
      "loss: 12.795687675476074\n",
      "steps per second: 0.56124\n",
      "step: 35190\n",
      "loss: 12.6268892288208\n",
      "steps per second: 0.60092\n",
      "step: 35191\n",
      "loss: 12.788775444030762\n",
      "steps per second: 0.57413\n",
      "step: 35192\n",
      "loss: 12.361001014709473\n",
      "steps per second: 0.54124\n",
      "step: 35193\n",
      "loss: 12.543303489685059\n",
      "steps per second: 0.57459\n",
      "step: 35194\n",
      "loss: 13.342535018920898\n",
      "steps per second: 0.50942\n",
      "step: 35195\n",
      "loss: 12.5712890625\n",
      "steps per second: 0.54932\n",
      "step: 35196\n",
      "loss: 12.911211967468262\n",
      "steps per second: 0.54669\n",
      "step: 35197\n",
      "loss: 12.684969902038574\n",
      "steps per second: 0.48891\n",
      "step: 35198\n",
      "loss: 12.47651195526123\n",
      "steps per second: 0.52776\n",
      "step: 35199\n",
      "loss: 12.445527076721191\n",
      "steps per second: 0.51968\n",
      "step: 35200\n",
      "loss: 12.622949600219727\n",
      "steps per second: 0.53413\n",
      "step: 35201\n",
      "loss: 13.003877639770508\n",
      "steps per second: 0.53521\n",
      "step: 35202\n",
      "loss: 12.627860069274902\n",
      "steps per second: 0.49863\n",
      "step: 35203\n",
      "loss: 12.5250244140625\n",
      "steps per second: 0.55063\n",
      "step: 35204\n",
      "loss: 12.865058898925781\n",
      "steps per second: 0.59317\n",
      "step: 35205\n",
      "loss: 12.734216690063477\n",
      "steps per second: 0.54360\n",
      "step: 35206\n",
      "loss: 13.40333080291748\n",
      "steps per second: 0.52948\n",
      "step: 35207\n",
      "loss: 13.178498268127441\n",
      "steps per second: 0.55734\n",
      "step: 35208\n",
      "loss: 12.742913246154785\n",
      "steps per second: 0.56808\n",
      "step: 35209\n",
      "loss: 13.195425033569336\n",
      "steps per second: 0.52821\n",
      "step: 35210\n",
      "loss: 13.109294891357422\n",
      "steps per second: 0.50815\n",
      "step: 35211\n",
      "loss: 12.757482528686523\n",
      "steps per second: 0.54281\n",
      "step: 35212\n",
      "loss: 12.37269115447998\n",
      "steps per second: 0.56074\n",
      "step: 35213\n",
      "loss: 12.606643676757812\n",
      "steps per second: 0.54112\n",
      "step: 35214\n",
      "loss: 12.470654487609863\n",
      "steps per second: 0.50015\n",
      "step: 35215\n",
      "loss: 12.591638565063477\n",
      "steps per second: 0.54021\n",
      "step: 35216\n",
      "loss: 12.811861991882324\n",
      "steps per second: 0.56794\n",
      "step: 35217\n",
      "loss: 12.971793174743652\n",
      "steps per second: 0.56810\n",
      "step: 35218\n",
      "loss: 12.867029190063477\n",
      "steps per second: 0.54762\n",
      "step: 35219\n",
      "loss: 12.586356163024902\n",
      "steps per second: 0.53916\n",
      "step: 35220\n",
      "loss: 13.347236633300781\n",
      "steps per second: 0.54150\n",
      "step: 35221\n",
      "loss: 13.263853073120117\n",
      "steps per second: 0.59582\n",
      "step: 35222\n",
      "loss: 12.87234878540039\n",
      "steps per second: 0.52893\n",
      "step: 35223\n",
      "loss: 12.996360778808594\n",
      "steps per second: 0.54175\n",
      "step: 35224\n",
      "loss: 12.947505950927734\n",
      "steps per second: 0.56932\n",
      "step: 35225\n",
      "loss: 12.81983470916748\n",
      "steps per second: 0.59455\n",
      "step: 35226\n",
      "loss: 12.957656860351562\n",
      "steps per second: 0.53709\n",
      "step: 35227\n",
      "loss: 13.047306060791016\n",
      "steps per second: 0.54901\n",
      "step: 35228\n",
      "loss: 12.388922691345215\n",
      "steps per second: 0.53457\n",
      "step: 35229\n",
      "loss: 12.928967475891113\n",
      "steps per second: 0.54670\n",
      "step: 35230\n",
      "loss: 13.223824501037598\n",
      "steps per second: 0.55819\n",
      "step: 35231\n",
      "loss: 13.135095596313477\n",
      "steps per second: 0.54167\n",
      "step: 35232\n",
      "loss: 13.016597747802734\n",
      "steps per second: 0.55345\n",
      "step: 35233\n",
      "loss: 12.5130033493042\n",
      "steps per second: 0.52059\n",
      "step: 35234\n",
      "loss: 12.602482795715332\n",
      "steps per second: 0.55967\n",
      "step: 35235\n",
      "loss: 12.704834938049316\n",
      "steps per second: 0.54815\n",
      "step: 35236\n",
      "loss: 12.363893508911133\n",
      "steps per second: 0.52785\n",
      "step: 35237\n",
      "loss: 12.55817699432373\n",
      "steps per second: 0.50949\n",
      "step: 35238\n",
      "loss: 12.386704444885254\n",
      "steps per second: 0.59910\n",
      "step: 35239\n",
      "loss: 13.453311920166016\n",
      "steps per second: 0.57004\n",
      "step: 35240\n",
      "loss: 13.32317066192627\n",
      "steps per second: 0.59913\n",
      "step: 35241\n",
      "loss: 12.91157341003418\n",
      "steps per second: 0.56218\n",
      "step: 35242\n",
      "loss: 12.732213020324707\n",
      "steps per second: 0.56915\n",
      "step: 35243\n",
      "loss: 13.09981918334961\n",
      "steps per second: 0.51550\n",
      "step: 35244\n",
      "loss: 12.738516807556152\n",
      "steps per second: 0.53579\n",
      "step: 35245\n",
      "loss: 13.580940246582031\n",
      "steps per second: 0.55703\n",
      "step: 35246\n",
      "loss: 12.602164268493652\n",
      "steps per second: 0.54234\n",
      "step: 35247\n",
      "loss: 12.695587158203125\n",
      "steps per second: 0.56781\n",
      "step: 35248\n",
      "loss: 12.166583061218262\n",
      "steps per second: 0.53649\n",
      "step: 35249\n",
      "loss: 12.274359703063965\n",
      "steps per second: 0.50869\n",
      "step: 35250\n",
      "loss: 12.641322135925293\n",
      "steps per second: 0.51639\n",
      "step: 35251\n",
      "loss: 13.172821998596191\n",
      "steps per second: 0.51342\n",
      "step: 35252\n",
      "loss: 13.169219970703125\n",
      "steps per second: 0.52857\n",
      "step: 35253\n",
      "loss: 12.334941864013672\n",
      "steps per second: 0.56106\n",
      "step: 35254\n",
      "loss: 12.458538055419922\n",
      "steps per second: 0.54880\n",
      "step: 35255\n",
      "loss: 12.383145332336426\n",
      "steps per second: 0.56817\n",
      "step: 35256\n",
      "loss: 12.942307472229004\n",
      "steps per second: 0.55508\n",
      "step: 35257\n",
      "loss: 12.967245101928711\n",
      "steps per second: 0.54306\n",
      "step: 35258\n",
      "loss: 12.826263427734375\n",
      "steps per second: 0.51286\n",
      "step: 35259\n",
      "loss: 13.289610862731934\n",
      "steps per second: 0.51628\n",
      "step: 35260\n",
      "loss: 12.875297546386719\n",
      "steps per second: 0.53777\n",
      "step: 35261\n",
      "loss: 12.910778999328613\n",
      "steps per second: 0.56478\n",
      "step: 35262\n",
      "loss: 12.62961483001709\n",
      "steps per second: 0.59926\n",
      "step: 35263\n",
      "loss: 13.340423583984375\n",
      "steps per second: 0.54666\n",
      "step: 35264\n",
      "loss: 12.986135482788086\n",
      "steps per second: 0.54156\n",
      "step: 35265\n",
      "loss: 13.262917518615723\n",
      "steps per second: 0.52955\n",
      "step: 35266\n",
      "loss: 12.777567863464355\n",
      "steps per second: 0.54024\n",
      "step: 35267\n",
      "loss: 13.07318115234375\n",
      "steps per second: 0.55920\n",
      "step: 35268\n",
      "loss: 12.958220481872559\n",
      "steps per second: 0.56570\n",
      "step: 35269\n",
      "loss: 12.61420726776123\n",
      "steps per second: 0.53600\n",
      "step: 35270\n",
      "loss: 12.918410301208496\n",
      "steps per second: 0.53156\n",
      "step: 35271\n",
      "loss: 12.558316230773926\n",
      "steps per second: 0.59391\n",
      "step: 35272\n",
      "loss: 12.718596458435059\n",
      "steps per second: 0.52689\n",
      "step: 35273\n",
      "loss: 12.81283950805664\n",
      "steps per second: 0.56033\n",
      "step: 35274\n",
      "loss: 12.88005256652832\n",
      "steps per second: 0.53552\n",
      "step: 35275\n",
      "loss: 12.488621711730957\n",
      "steps per second: 0.59264\n",
      "step: 35276\n",
      "loss: 12.34990406036377\n",
      "steps per second: 0.54844\n",
      "step: 35277\n",
      "loss: 12.931269645690918\n",
      "steps per second: 0.59709\n",
      "step: 35278\n",
      "loss: 12.612506866455078\n",
      "steps per second: 0.56081\n",
      "step: 35279\n",
      "loss: 12.854226112365723\n",
      "steps per second: 0.56704\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8738278150558472, layer: 11\n",
      "saving at step 35279\n",
      "----------\n",
      "\n",
      "\n",
      "step: 35280\n",
      "loss: 12.790384292602539\n",
      "steps per second: 0.27904\n",
      "step: 35281\n",
      "loss: 13.184364318847656\n",
      "steps per second: 0.53617\n",
      "step: 35282\n",
      "loss: 12.831366539001465\n",
      "steps per second: 0.55892\n",
      "step: 35283\n",
      "loss: 12.256909370422363\n",
      "steps per second: 0.48969\n",
      "step: 35284\n",
      "loss: 12.68860912322998\n",
      "steps per second: 0.59614\n",
      "step: 35285\n",
      "loss: 12.622056007385254\n",
      "steps per second: 0.54708\n",
      "step: 35286\n",
      "loss: 12.20103645324707\n",
      "steps per second: 0.53808\n",
      "step: 35287\n",
      "loss: 12.379738807678223\n",
      "steps per second: 0.57451\n",
      "step: 35288\n",
      "loss: 13.236499786376953\n",
      "steps per second: 0.54866\n",
      "step: 35289\n",
      "loss: 12.718332290649414\n",
      "steps per second: 0.54706\n",
      "step: 35290\n",
      "loss: 13.32565975189209\n",
      "steps per second: 0.51378\n",
      "step: 35291\n",
      "loss: 12.620304107666016\n",
      "steps per second: 0.54205\n",
      "step: 35292\n",
      "loss: 13.25632381439209\n",
      "steps per second: 0.50825\n",
      "step: 35293\n",
      "loss: 12.336804389953613\n",
      "steps per second: 0.56627\n",
      "step: 35294\n",
      "loss: 12.650459289550781\n",
      "steps per second: 0.53985\n",
      "step: 35295\n",
      "loss: 13.001258850097656\n",
      "steps per second: 0.54235\n",
      "step: 35296\n",
      "loss: 12.79890251159668\n",
      "steps per second: 0.59249\n",
      "step: 35297\n",
      "loss: 12.410660743713379\n",
      "steps per second: 0.54612\n",
      "step: 35298\n",
      "loss: 12.5859375\n",
      "steps per second: 0.53771\n",
      "step: 35299\n",
      "loss: 12.962217330932617\n",
      "steps per second: 0.56573\n",
      "step: 35300\n",
      "loss: 11.860971450805664\n",
      "steps per second: 0.51606\n",
      "step: 35301\n",
      "loss: 12.890137672424316\n",
      "steps per second: 0.54653\n",
      "step: 35302\n",
      "loss: 12.817601203918457\n",
      "steps per second: 0.53774\n",
      "step: 35303\n",
      "loss: 12.37467098236084\n",
      "steps per second: 0.56115\n",
      "step: 35304\n",
      "loss: 12.877930641174316\n",
      "steps per second: 0.54268\n",
      "step: 35305\n",
      "loss: 12.472323417663574\n",
      "steps per second: 0.50710\n",
      "step: 35306\n",
      "loss: 13.112356185913086\n",
      "steps per second: 0.52458\n",
      "step: 35307\n",
      "loss: 12.134742736816406\n",
      "steps per second: 0.54946\n",
      "step: 35308\n",
      "loss: 12.290057182312012\n",
      "steps per second: 0.57406\n",
      "step: 35309\n",
      "loss: 12.674470901489258\n",
      "steps per second: 0.56780\n",
      "step: 35310\n",
      "loss: 13.115163803100586\n",
      "steps per second: 0.56033\n",
      "step: 35311\n",
      "loss: 12.955496788024902\n",
      "steps per second: 0.54089\n",
      "step: 35312\n",
      "loss: 12.64504623413086\n",
      "steps per second: 0.52649\n",
      "step: 35313\n",
      "loss: 12.951379776000977\n",
      "steps per second: 0.49143\n",
      "step: 35314\n",
      "loss: 13.273063659667969\n",
      "steps per second: 0.54182\n",
      "step: 35315\n",
      "loss: 13.183639526367188\n",
      "steps per second: 0.54837\n",
      "step: 35316\n",
      "loss: 12.294551849365234\n",
      "steps per second: 0.51533\n",
      "step: 35317\n",
      "loss: 12.982455253601074\n",
      "steps per second: 0.55727\n",
      "step: 35318\n",
      "loss: 12.967071533203125\n",
      "steps per second: 0.54838\n",
      "step: 35319\n",
      "loss: 12.994022369384766\n",
      "steps per second: 0.56579\n",
      "step: 35320\n",
      "loss: 13.167196273803711\n",
      "steps per second: 0.55001\n",
      "step: 35321\n",
      "loss: 12.88858413696289\n",
      "steps per second: 0.54651\n",
      "step: 35322\n",
      "loss: 12.248393058776855\n",
      "steps per second: 0.53375\n",
      "step: 35323\n",
      "loss: 13.039515495300293\n",
      "steps per second: 0.51989\n",
      "step: 35324\n",
      "loss: 12.494880676269531\n",
      "steps per second: 0.53088\n",
      "step: 35325\n",
      "loss: 13.165533065795898\n",
      "steps per second: 0.55708\n",
      "step: 35326\n",
      "loss: 13.62802791595459\n",
      "steps per second: 0.56031\n",
      "step: 35327\n",
      "loss: 12.74241828918457\n",
      "steps per second: 0.53457\n",
      "step: 35328\n",
      "loss: 12.566939353942871\n",
      "steps per second: 0.54859\n",
      "step: 35329\n",
      "loss: 13.21114730834961\n",
      "steps per second: 0.50456\n",
      "step: 35330\n",
      "loss: 13.341497421264648\n",
      "steps per second: 0.53685\n",
      "step: 35331\n",
      "loss: 13.083739280700684\n",
      "steps per second: 0.52828\n",
      "step: 35332\n",
      "loss: 13.118063926696777\n",
      "steps per second: 0.46974\n",
      "step: 35333\n",
      "loss: 12.776731491088867\n",
      "steps per second: 0.47713\n",
      "step: 35334\n",
      "loss: 13.279410362243652\n",
      "steps per second: 0.50369\n",
      "step: 35335\n",
      "loss: 12.629396438598633\n",
      "steps per second: 0.54629\n",
      "step: 35336\n",
      "loss: 13.234187126159668\n",
      "steps per second: 0.53937\n",
      "step: 35337\n",
      "loss: 12.751078605651855\n",
      "steps per second: 0.51746\n",
      "step: 35338\n",
      "loss: 12.488186836242676\n",
      "steps per second: 0.51597\n",
      "step: 35339\n",
      "loss: 12.882866859436035\n",
      "steps per second: 0.54312\n",
      "step: 35340\n",
      "loss: 12.931839942932129\n",
      "steps per second: 0.52894\n",
      "step: 35341\n",
      "loss: 12.643377304077148\n",
      "steps per second: 0.54282\n",
      "step: 35342\n",
      "loss: 12.328808784484863\n",
      "steps per second: 0.54051\n",
      "step: 35343\n",
      "loss: 12.580717086791992\n",
      "steps per second: 0.60002\n",
      "step: 35344\n",
      "loss: 12.75187873840332\n",
      "steps per second: 0.56630\n",
      "step: 35345\n",
      "loss: 12.997747421264648\n",
      "steps per second: 0.55843\n",
      "step: 35346\n",
      "loss: 13.00479793548584\n",
      "steps per second: 0.56166\n",
      "step: 35347\n",
      "loss: 12.349699020385742\n",
      "steps per second: 0.53975\n",
      "step: 35348\n",
      "loss: 12.288407325744629\n",
      "steps per second: 0.55558\n",
      "step: 35349\n",
      "loss: 13.262493133544922\n",
      "steps per second: 0.54000\n",
      "step: 35350\n",
      "loss: 13.302216529846191\n",
      "steps per second: 0.52664\n",
      "step: 35351\n",
      "loss: 12.582101821899414\n",
      "steps per second: 0.54446\n",
      "step: 35352\n",
      "loss: 12.831992149353027\n",
      "steps per second: 0.52772\n",
      "step: 35353\n",
      "loss: 12.31213092803955\n",
      "steps per second: 0.56093\n",
      "step: 35354\n",
      "loss: 12.702279090881348\n",
      "steps per second: 0.52830\n",
      "step: 35355\n",
      "loss: 13.186949729919434\n",
      "steps per second: 0.54399\n",
      "step: 35356\n",
      "loss: 12.816709518432617\n",
      "steps per second: 0.54813\n",
      "step: 35357\n",
      "loss: 13.120275497436523\n",
      "steps per second: 0.51969\n",
      "step: 35358\n",
      "loss: 12.825377464294434\n",
      "steps per second: 0.50642\n",
      "step: 35359\n",
      "loss: 12.571249008178711\n",
      "steps per second: 0.54261\n",
      "step: 35360\n",
      "loss: 13.092666625976562\n",
      "steps per second: 0.51716\n",
      "step: 35361\n",
      "loss: 12.990376472473145\n",
      "steps per second: 0.51809\n",
      "step: 35362\n",
      "loss: 12.89238452911377\n",
      "steps per second: 0.51551\n",
      "step: 35363\n",
      "loss: 12.697813034057617\n",
      "steps per second: 0.56256\n",
      "step: 35364\n",
      "loss: 12.743815422058105\n",
      "steps per second: 0.49736\n",
      "step: 35365\n",
      "loss: 12.716882705688477\n",
      "steps per second: 0.54277\n",
      "step: 35366\n",
      "loss: 12.6588773727417\n",
      "steps per second: 0.56279\n",
      "step: 35367\n",
      "loss: 12.908939361572266\n",
      "steps per second: 0.53022\n",
      "step: 35368\n",
      "loss: 12.414082527160645\n",
      "steps per second: 0.55769\n",
      "step: 35369\n",
      "loss: 12.617107391357422\n",
      "steps per second: 0.55907\n",
      "step: 35370\n",
      "loss: 12.944380760192871\n",
      "steps per second: 0.54450\n",
      "step: 35371\n",
      "loss: 12.814123153686523\n",
      "steps per second: 0.56973\n",
      "step: 35372\n",
      "loss: 13.67160701751709\n",
      "steps per second: 0.55385\n",
      "step: 35373\n",
      "loss: 12.997281074523926\n",
      "steps per second: 0.53518\n",
      "step: 35374\n",
      "loss: 13.273746490478516\n",
      "steps per second: 0.53566\n",
      "step: 35375\n",
      "loss: 12.961822509765625\n",
      "steps per second: 0.55883\n",
      "step: 35376\n",
      "loss: 12.485573768615723\n",
      "steps per second: 0.48939\n",
      "step: 35377\n",
      "loss: 12.882070541381836\n",
      "steps per second: 0.53157\n",
      "step: 35378\n",
      "loss: 12.920708656311035\n",
      "steps per second: 0.49976\n",
      "step: 35379\n",
      "loss: 12.862184524536133\n",
      "steps per second: 0.54328\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8409814238548279, layer: 11\n",
      "saving at step 35379\n",
      "----------\n",
      "\n",
      "\n",
      "step: 35380\n",
      "loss: 12.301315307617188\n",
      "steps per second: 0.27402\n",
      "step: 35381\n",
      "loss: 12.42934799194336\n",
      "steps per second: 0.52386\n",
      "step: 35382\n",
      "loss: 12.760300636291504\n",
      "steps per second: 0.53366\n",
      "step: 35383\n",
      "loss: 13.06081485748291\n",
      "steps per second: 0.53064\n",
      "step: 35384\n",
      "loss: 12.935620307922363\n",
      "steps per second: 0.52991\n",
      "step: 35385\n",
      "loss: 12.035590171813965\n",
      "steps per second: 0.54060\n",
      "step: 35386\n",
      "loss: 12.168436050415039\n",
      "steps per second: 0.49016\n",
      "step: 35387\n",
      "loss: 12.496339797973633\n",
      "steps per second: 0.52867\n",
      "step: 35388\n",
      "loss: 13.106635093688965\n",
      "steps per second: 0.52867\n",
      "step: 35389\n",
      "loss: 12.807394027709961\n",
      "steps per second: 0.54564\n",
      "step: 35390\n",
      "loss: 13.121115684509277\n",
      "steps per second: 0.56270\n",
      "step: 35391\n",
      "loss: 12.442441940307617\n",
      "steps per second: 0.55606\n",
      "step: 35392\n",
      "loss: 12.78945541381836\n",
      "steps per second: 0.53718\n",
      "step: 35393\n",
      "loss: 13.137930870056152\n",
      "steps per second: 0.54955\n",
      "step: 35394\n",
      "loss: 12.989202499389648\n",
      "steps per second: 0.53892\n",
      "step: 35395\n",
      "loss: 13.07691764831543\n",
      "steps per second: 0.53755\n",
      "step: 35396\n",
      "loss: 12.38366985321045\n",
      "steps per second: 0.51676\n",
      "step: 35397\n",
      "loss: 13.060628890991211\n",
      "steps per second: 0.56887\n",
      "step: 35398\n",
      "loss: 13.154118537902832\n",
      "steps per second: 0.53674\n",
      "step: 35399\n",
      "loss: 12.670522689819336\n",
      "steps per second: 0.52742\n",
      "step: 35400\n",
      "loss: 12.564448356628418\n",
      "steps per second: 0.52682\n",
      "step: 35401\n",
      "loss: 12.565800666809082\n",
      "steps per second: 0.54357\n",
      "step: 35402\n",
      "loss: 12.88814926147461\n",
      "steps per second: 0.54586\n",
      "step: 35403\n",
      "loss: 12.693146705627441\n",
      "steps per second: 0.56424\n",
      "step: 35404\n",
      "loss: 12.356237411499023\n",
      "steps per second: 0.56156\n",
      "step: 35405\n",
      "loss: 12.997014999389648\n",
      "steps per second: 0.54652\n",
      "step: 35406\n",
      "loss: 12.945734977722168\n",
      "steps per second: 0.54574\n",
      "step: 35407\n",
      "loss: 12.557700157165527\n",
      "steps per second: 0.53562\n",
      "step: 35408\n",
      "loss: 12.797551155090332\n",
      "steps per second: 0.54566\n",
      "step: 35409\n",
      "loss: 12.610742568969727\n",
      "steps per second: 0.55814\n",
      "step: 35410\n",
      "loss: 12.332815170288086\n",
      "steps per second: 0.54428\n",
      "step: 35411\n",
      "loss: 12.46955680847168\n",
      "steps per second: 0.53913\n",
      "step: 35412\n",
      "loss: 13.236684799194336\n",
      "steps per second: 0.51340\n",
      "step: 35413\n",
      "loss: 12.517032623291016\n",
      "steps per second: 0.53446\n",
      "step: 35414\n",
      "loss: 12.928943634033203\n",
      "steps per second: 0.56542\n",
      "step: 35415\n",
      "loss: 12.989814758300781\n",
      "steps per second: 0.51588\n",
      "step: 35416\n",
      "loss: 12.576897621154785\n",
      "steps per second: 0.51108\n",
      "step: 35417\n",
      "loss: 13.246626853942871\n",
      "steps per second: 0.52445\n",
      "step: 35418\n",
      "loss: 12.46983814239502\n",
      "steps per second: 0.51187\n",
      "step: 35419\n",
      "loss: 12.768315315246582\n",
      "steps per second: 0.55819\n",
      "step: 35420\n",
      "loss: 12.722009658813477\n",
      "steps per second: 0.53248\n",
      "step: 35421\n",
      "loss: 13.022132873535156\n",
      "steps per second: 0.56590\n",
      "step: 35422\n",
      "loss: 13.433120727539062\n",
      "steps per second: 0.52935\n",
      "step: 35423\n",
      "loss: 12.507137298583984\n",
      "steps per second: 0.54478\n",
      "step: 35424\n",
      "loss: 12.850727081298828\n",
      "steps per second: 0.54288\n",
      "step: 35425\n",
      "loss: 12.483367919921875\n",
      "steps per second: 0.54081\n",
      "step: 35426\n",
      "loss: 12.724324226379395\n",
      "steps per second: 0.54303\n",
      "step: 35427\n",
      "loss: 12.900367736816406\n",
      "steps per second: 0.54310\n",
      "step: 35428\n",
      "loss: 12.797465324401855\n",
      "steps per second: 0.52815\n",
      "step: 35429\n",
      "loss: 12.610244750976562\n",
      "steps per second: 0.55527\n",
      "step: 35430\n",
      "loss: 13.352762222290039\n",
      "steps per second: 0.52649\n",
      "step: 35431\n",
      "loss: 12.587015151977539\n",
      "steps per second: 0.54587\n",
      "step: 35432\n",
      "loss: 12.626045227050781\n",
      "steps per second: 0.52629\n",
      "step: 35433\n",
      "loss: 12.893477439880371\n",
      "steps per second: 0.54934\n",
      "step: 35434\n",
      "loss: 13.093855857849121\n",
      "steps per second: 0.56157\n",
      "step: 35435\n",
      "loss: 12.965989112854004\n",
      "steps per second: 0.54658\n",
      "step: 35436\n",
      "loss: 13.679030418395996\n",
      "steps per second: 0.51909\n",
      "step: 35437\n",
      "loss: 12.912899017333984\n",
      "steps per second: 0.52109\n",
      "step: 35438\n",
      "loss: 12.776163101196289\n",
      "steps per second: 0.55415\n",
      "step: 35439\n",
      "loss: 12.581539154052734\n",
      "steps per second: 0.54201\n",
      "step: 35440\n",
      "loss: 12.518887519836426\n",
      "steps per second: 0.52517\n",
      "step: 35441\n",
      "loss: 12.704975128173828\n",
      "steps per second: 0.54851\n",
      "step: 35442\n",
      "loss: 12.874593734741211\n",
      "steps per second: 0.56024\n",
      "step: 35443\n",
      "loss: 13.241103172302246\n",
      "steps per second: 0.54133\n",
      "step: 35444\n",
      "loss: 12.183320999145508\n",
      "steps per second: 0.54573\n",
      "step: 35445\n",
      "loss: 12.39061164855957\n",
      "steps per second: 0.56533\n",
      "step: 35446\n",
      "loss: 13.086724281311035\n",
      "steps per second: 0.55739\n",
      "step: 35447\n",
      "loss: 12.61715316772461\n",
      "steps per second: 0.55747\n",
      "step: 35448\n",
      "loss: 12.419304847717285\n",
      "steps per second: 0.51667\n",
      "step: 35449\n",
      "loss: 12.852228164672852\n",
      "steps per second: 0.54730\n",
      "step: 35450\n",
      "loss: 12.770696640014648\n",
      "steps per second: 0.54734\n",
      "step: 35451\n",
      "loss: 12.803214073181152\n",
      "steps per second: 0.54637\n",
      "step: 35452\n",
      "loss: 13.121196746826172\n",
      "steps per second: 0.54002\n",
      "step: 35453\n",
      "loss: 13.037408828735352\n",
      "steps per second: 0.55897\n",
      "step: 35454\n",
      "loss: 13.203178405761719\n",
      "steps per second: 0.54059\n",
      "step: 35455\n",
      "loss: 12.508315086364746\n",
      "steps per second: 0.52674\n",
      "step: 35456\n",
      "loss: 13.05738353729248\n",
      "steps per second: 0.53773\n",
      "step: 35457\n",
      "loss: 12.162866592407227\n",
      "steps per second: 0.56808\n",
      "step: 35458\n",
      "loss: 12.999558448791504\n",
      "steps per second: 0.54071\n",
      "step: 35459\n",
      "loss: 12.91956615447998\n",
      "steps per second: 0.54339\n",
      "step: 35460\n",
      "loss: 13.250638008117676\n",
      "steps per second: 0.55946\n",
      "step: 35461\n",
      "loss: 12.728520393371582\n",
      "steps per second: 0.53413\n",
      "step: 35462\n",
      "loss: 13.082989692687988\n",
      "steps per second: 0.55763\n",
      "step: 35463\n",
      "loss: 13.004300117492676\n",
      "steps per second: 0.54206\n",
      "step: 35464\n",
      "loss: 13.014491081237793\n",
      "steps per second: 0.52767\n",
      "step: 35465\n",
      "loss: 12.583538055419922\n",
      "steps per second: 0.52534\n",
      "step: 35466\n",
      "loss: 12.809731483459473\n",
      "steps per second: 0.53654\n",
      "step: 35467\n",
      "loss: 12.863935470581055\n",
      "steps per second: 0.53443\n",
      "step: 35468\n",
      "loss: 13.14213752746582\n",
      "steps per second: 0.54039\n",
      "step: 35469\n",
      "loss: 12.940191268920898\n",
      "steps per second: 0.51638\n",
      "step: 35470\n",
      "loss: 12.680716514587402\n",
      "steps per second: 0.53947\n",
      "step: 35471\n",
      "loss: 12.7562255859375\n",
      "steps per second: 0.54504\n",
      "step: 35472\n",
      "loss: 12.627246856689453\n",
      "steps per second: 0.48150\n",
      "step: 35473\n",
      "loss: 12.720808982849121\n",
      "steps per second: 0.51555\n",
      "step: 35474\n",
      "loss: 12.71997356414795\n",
      "steps per second: 0.53602\n",
      "step: 35475\n",
      "loss: 12.784680366516113\n",
      "steps per second: 0.53380\n",
      "step: 35476\n",
      "loss: 13.082500457763672\n",
      "steps per second: 0.51396\n",
      "step: 35477\n",
      "loss: 12.498838424682617\n",
      "steps per second: 0.54630\n",
      "step: 35478\n",
      "loss: 12.953057289123535\n",
      "steps per second: 0.54174\n",
      "step: 35479\n",
      "loss: 12.432351112365723\n",
      "steps per second: 0.51685\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8032810688018799, layer: 11\n",
      "saving at step 35479\n",
      "----------\n",
      "\n",
      "\n",
      "step: 35480\n",
      "loss: 12.27762508392334\n",
      "steps per second: 0.26269\n",
      "step: 35481\n",
      "loss: 12.47396469116211\n",
      "steps per second: 0.54588\n",
      "step: 35482\n",
      "loss: 12.751280784606934\n",
      "steps per second: 0.56752\n",
      "step: 35483\n",
      "loss: 12.563836097717285\n",
      "steps per second: 0.53203\n",
      "step: 35484\n",
      "loss: 13.012439727783203\n",
      "steps per second: 0.53980\n",
      "step: 35485\n",
      "loss: 12.626669883728027\n",
      "steps per second: 0.52565\n",
      "step: 35486\n",
      "loss: 12.775480270385742\n",
      "steps per second: 0.53175\n",
      "step: 35487\n",
      "loss: 13.11107063293457\n",
      "steps per second: 0.54724\n",
      "step: 35488\n",
      "loss: 12.555604934692383\n",
      "steps per second: 0.51909\n",
      "step: 35489\n",
      "loss: 13.165511131286621\n",
      "steps per second: 0.54029\n",
      "step: 35490\n",
      "loss: 12.922294616699219\n",
      "steps per second: 0.54047\n",
      "step: 35491\n",
      "loss: 12.707795143127441\n",
      "steps per second: 0.55910\n",
      "step: 35492\n",
      "loss: 12.700581550598145\n",
      "steps per second: 0.53483\n",
      "step: 35493\n",
      "loss: 12.408509254455566\n",
      "steps per second: 0.53607\n",
      "step: 35494\n",
      "loss: 13.535526275634766\n",
      "steps per second: 0.54744\n",
      "step: 35495\n",
      "loss: 13.130521774291992\n",
      "steps per second: 0.56446\n",
      "step: 35496\n",
      "loss: 12.78111743927002\n",
      "steps per second: 0.52920\n",
      "step: 35497\n",
      "loss: 13.02092170715332\n",
      "steps per second: 0.53293\n",
      "step: 35498\n",
      "loss: 12.812095642089844\n",
      "steps per second: 0.59512\n",
      "step: 35499\n",
      "loss: 12.482990264892578\n",
      "steps per second: 0.54248\n",
      "step: 35500\n",
      "loss: 12.865960121154785\n",
      "steps per second: 0.54628\n",
      "step: 35501\n",
      "loss: 13.18128490447998\n",
      "steps per second: 0.51674\n",
      "step: 35502\n",
      "loss: 12.781280517578125\n",
      "steps per second: 0.52202\n",
      "step: 35503\n",
      "loss: 12.543540000915527\n",
      "steps per second: 0.52036\n",
      "step: 35504\n",
      "loss: 13.000432014465332\n",
      "steps per second: 0.49645\n",
      "step: 35505\n",
      "loss: 13.061742782592773\n",
      "steps per second: 0.51684\n",
      "step: 35506\n",
      "loss: 13.336466789245605\n",
      "steps per second: 0.56498\n",
      "step: 35507\n",
      "loss: 12.300628662109375\n",
      "steps per second: 0.54126\n",
      "step: 35508\n",
      "loss: 12.655891418457031\n",
      "steps per second: 0.55021\n",
      "step: 35509\n",
      "loss: 13.088956832885742\n",
      "steps per second: 0.52880\n",
      "step: 35510\n",
      "loss: 12.43401050567627\n",
      "steps per second: 0.54461\n",
      "step: 35511\n",
      "loss: 13.016190528869629\n",
      "steps per second: 0.54299\n",
      "step: 35512\n",
      "loss: 13.041454315185547\n",
      "steps per second: 0.54746\n",
      "step: 35513\n",
      "loss: 12.503149032592773\n",
      "steps per second: 0.52770\n",
      "step: 35514\n",
      "loss: 13.059861183166504\n",
      "steps per second: 0.53758\n",
      "step: 35515\n",
      "loss: 12.740998268127441\n",
      "steps per second: 0.54023\n",
      "step: 35516\n",
      "loss: 12.604238510131836\n",
      "steps per second: 0.54636\n",
      "step: 35517\n",
      "loss: 12.84625244140625\n",
      "steps per second: 0.56720\n",
      "step: 35518\n",
      "loss: 13.005349159240723\n",
      "steps per second: 0.54821\n",
      "step: 35519\n",
      "loss: 12.409680366516113\n",
      "steps per second: 0.55888\n",
      "step: 35520\n",
      "loss: 13.447925567626953\n",
      "steps per second: 0.49916\n",
      "step: 35521\n",
      "loss: 12.745762825012207\n",
      "steps per second: 0.53915\n",
      "step: 35522\n",
      "loss: 12.936917304992676\n",
      "steps per second: 0.59678\n",
      "step: 35523\n",
      "loss: 12.309107780456543\n",
      "steps per second: 0.51568\n",
      "step: 35524\n",
      "loss: 12.741386413574219\n",
      "steps per second: 0.51429\n",
      "step: 35525\n",
      "loss: 12.507696151733398\n",
      "steps per second: 0.54268\n",
      "step: 35526\n",
      "loss: 12.829977035522461\n",
      "steps per second: 0.59890\n",
      "step: 35527\n",
      "loss: 12.404410362243652\n",
      "steps per second: 0.51500\n",
      "step: 35528\n",
      "loss: 13.383739471435547\n",
      "steps per second: 0.54664\n",
      "step: 35529\n",
      "loss: 12.545893669128418\n",
      "steps per second: 0.51725\n",
      "step: 35530\n",
      "loss: 12.672700881958008\n",
      "steps per second: 0.52847\n",
      "step: 35531\n",
      "loss: 12.60036849975586\n",
      "steps per second: 0.54830\n",
      "step: 35532\n",
      "loss: 12.6737060546875\n",
      "steps per second: 0.57479\n",
      "step: 35533\n",
      "loss: 12.484817504882812\n",
      "steps per second: 0.55957\n",
      "step: 35534\n",
      "loss: 12.336199760437012\n",
      "steps per second: 0.51756\n",
      "step: 35535\n",
      "loss: 12.624743461608887\n",
      "steps per second: 0.54016\n",
      "step: 35536\n",
      "loss: 12.58061408996582\n",
      "steps per second: 0.51687\n",
      "step: 35537\n",
      "loss: 12.887535095214844\n",
      "steps per second: 0.55620\n",
      "step: 35538\n",
      "loss: 12.528538703918457\n",
      "steps per second: 0.56094\n",
      "step: 35539\n",
      "loss: 12.683405876159668\n",
      "steps per second: 0.54402\n",
      "step: 35540\n",
      "loss: 12.829217910766602\n",
      "steps per second: 0.53811\n",
      "step: 35541\n",
      "loss: 12.46622085571289\n",
      "steps per second: 0.53822\n",
      "step: 35542\n",
      "loss: 12.100433349609375\n",
      "steps per second: 0.55828\n",
      "step: 35543\n",
      "loss: 12.167030334472656\n",
      "steps per second: 0.55603\n",
      "step: 35544\n",
      "loss: 12.711363792419434\n",
      "steps per second: 0.57601\n",
      "step: 35545\n",
      "loss: 12.930292129516602\n",
      "steps per second: 0.59232\n",
      "step: 35546\n",
      "loss: 12.813526153564453\n",
      "steps per second: 0.56463\n",
      "step: 35547\n",
      "loss: 12.646662712097168\n",
      "steps per second: 0.52835\n",
      "step: 35548\n",
      "loss: 12.837810516357422\n",
      "steps per second: 0.52042\n",
      "step: 35549\n",
      "loss: 13.162728309631348\n",
      "steps per second: 0.51690\n",
      "step: 35550\n",
      "loss: 12.985547065734863\n",
      "steps per second: 0.56771\n",
      "step: 35551\n",
      "loss: 13.216185569763184\n",
      "steps per second: 0.54904\n",
      "step: 35552\n",
      "loss: 13.180018424987793\n",
      "steps per second: 0.51794\n",
      "step: 35553\n",
      "loss: 12.780790328979492\n",
      "steps per second: 0.54236\n",
      "step: 35554\n",
      "loss: 12.756817817687988\n",
      "steps per second: 0.52707\n",
      "step: 35555\n",
      "loss: 12.702058792114258\n",
      "steps per second: 0.52572\n",
      "step: 35556\n",
      "loss: 12.613844871520996\n",
      "steps per second: 0.59776\n",
      "step: 35557\n",
      "loss: 13.167129516601562\n",
      "steps per second: 0.55785\n",
      "step: 35558\n",
      "loss: 12.591773986816406\n",
      "steps per second: 0.50875\n",
      "step: 35559\n",
      "loss: 12.393482208251953\n",
      "steps per second: 0.51667\n",
      "step: 35560\n",
      "loss: 13.051173210144043\n",
      "steps per second: 0.54850\n",
      "step: 35561\n",
      "loss: 12.705547332763672\n",
      "steps per second: 0.54768\n",
      "step: 35562\n",
      "loss: 13.048171043395996\n",
      "steps per second: 0.51208\n",
      "step: 35563\n",
      "loss: 12.661666870117188\n",
      "steps per second: 0.54160\n",
      "step: 35564\n",
      "loss: 12.498021125793457\n",
      "steps per second: 0.55402\n",
      "step: 35565\n",
      "loss: 13.248414993286133\n",
      "steps per second: 0.51878\n",
      "step: 35566\n",
      "loss: 12.749735832214355\n",
      "steps per second: 0.52647\n",
      "step: 35567\n",
      "loss: 12.810844421386719\n",
      "steps per second: 0.53262\n",
      "step: 35568\n",
      "loss: 12.718086242675781\n",
      "steps per second: 0.55800\n",
      "step: 35569\n",
      "loss: 13.29897403717041\n",
      "steps per second: 0.53429\n",
      "step: 35570\n",
      "loss: 13.020106315612793\n",
      "steps per second: 0.54537\n",
      "step: 35571\n",
      "loss: 12.793097496032715\n",
      "steps per second: 0.48112\n",
      "step: 35572\n",
      "loss: 12.912408828735352\n",
      "steps per second: 0.53345\n",
      "step: 35573\n",
      "loss: 12.761581420898438\n",
      "steps per second: 0.54580\n",
      "step: 35574\n",
      "loss: 13.043741226196289\n",
      "steps per second: 0.56141\n",
      "step: 35575\n",
      "loss: 13.077155113220215\n",
      "steps per second: 0.53419\n",
      "step: 35576\n",
      "loss: 12.009181022644043\n",
      "steps per second: 0.56881\n",
      "step: 35577\n",
      "loss: 12.568408966064453\n",
      "steps per second: 0.51898\n",
      "step: 35578\n",
      "loss: 13.083722114562988\n",
      "steps per second: 0.59291\n",
      "step: 35579\n",
      "loss: 12.552855491638184\n",
      "steps per second: 0.49884\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8005527853965759, layer: 11\n",
      "saving at step 35579\n",
      "----------\n",
      "\n",
      "\n",
      "step: 35580\n",
      "loss: 12.847325325012207\n",
      "steps per second: 0.26024\n",
      "step: 35581\n",
      "loss: 13.200255393981934\n",
      "steps per second: 0.54665\n",
      "step: 35582\n",
      "loss: 12.621641159057617\n",
      "steps per second: 0.51823\n",
      "step: 35583\n",
      "loss: 12.823070526123047\n",
      "steps per second: 0.56738\n",
      "step: 35584\n",
      "loss: 12.704880714416504\n",
      "steps per second: 0.54187\n",
      "step: 35585\n",
      "loss: 12.650406837463379\n",
      "steps per second: 0.51974\n",
      "step: 35586\n",
      "loss: 12.46292781829834\n",
      "steps per second: 0.56196\n",
      "step: 35587\n",
      "loss: 13.052974700927734\n",
      "steps per second: 0.59471\n",
      "step: 35588\n",
      "loss: 13.331511497497559\n",
      "steps per second: 0.55878\n",
      "step: 35589\n",
      "loss: 12.87812328338623\n",
      "steps per second: 0.56705\n",
      "step: 35590\n",
      "loss: 12.662495613098145\n",
      "steps per second: 0.51899\n",
      "step: 35591\n",
      "loss: 12.960519790649414\n",
      "steps per second: 0.60033\n",
      "step: 35592\n",
      "loss: 12.835493087768555\n",
      "steps per second: 0.53740\n",
      "step: 35593\n",
      "loss: 12.956764221191406\n",
      "steps per second: 0.53980\n",
      "step: 35594\n",
      "loss: 12.196792602539062\n",
      "steps per second: 0.53353\n",
      "step: 35595\n",
      "loss: 12.638336181640625\n",
      "steps per second: 0.53559\n",
      "step: 35596\n",
      "loss: 12.85118293762207\n",
      "steps per second: 0.54845\n",
      "step: 35597\n",
      "loss: 12.57571029663086\n",
      "steps per second: 0.56581\n",
      "step: 35598\n",
      "loss: 12.656737327575684\n",
      "steps per second: 0.54721\n",
      "step: 35599\n",
      "loss: 12.84941291809082\n",
      "steps per second: 0.59301\n",
      "step: 35600\n",
      "loss: 12.626287460327148\n",
      "steps per second: 0.54269\n",
      "step: 35601\n",
      "loss: 13.102574348449707\n",
      "steps per second: 0.56031\n",
      "step: 35602\n",
      "loss: 12.901960372924805\n",
      "steps per second: 0.54161\n",
      "step: 35603\n",
      "loss: 12.855793952941895\n",
      "steps per second: 0.55825\n",
      "step: 35604\n",
      "loss: 13.651296615600586\n",
      "steps per second: 0.52359\n",
      "step: 35605\n",
      "loss: 12.923994064331055\n",
      "steps per second: 0.54093\n",
      "step: 35606\n",
      "loss: 12.473824501037598\n",
      "steps per second: 0.53352\n",
      "step: 35607\n",
      "loss: 13.138482093811035\n",
      "steps per second: 0.51413\n",
      "step: 35608\n",
      "loss: 12.737895011901855\n",
      "steps per second: 0.53472\n",
      "step: 35609\n",
      "loss: 13.189759254455566\n",
      "steps per second: 0.54148\n",
      "step: 35610\n",
      "loss: 13.244991302490234\n",
      "steps per second: 0.50450\n",
      "step: 35611\n",
      "loss: 13.045222282409668\n",
      "steps per second: 0.54555\n",
      "step: 35612\n",
      "loss: 12.656621932983398\n",
      "steps per second: 0.54120\n",
      "step: 35613\n",
      "loss: 13.125752449035645\n",
      "steps per second: 0.56564\n",
      "step: 35614\n",
      "loss: 12.918224334716797\n",
      "steps per second: 0.54321\n",
      "step: 35615\n",
      "loss: 12.891666412353516\n",
      "steps per second: 0.52762\n",
      "step: 35616\n",
      "loss: 12.883849143981934\n",
      "steps per second: 0.48834\n",
      "step: 35617\n",
      "loss: 13.245718002319336\n",
      "steps per second: 0.56866\n",
      "step: 35618\n",
      "loss: 13.011292457580566\n",
      "steps per second: 0.59864\n",
      "step: 35619\n",
      "loss: 12.586128234863281\n",
      "steps per second: 0.54554\n",
      "step: 35620\n",
      "loss: 12.403763771057129\n",
      "steps per second: 0.56729\n",
      "step: 35621\n",
      "loss: 12.54857349395752\n",
      "steps per second: 0.55711\n",
      "step: 35622\n",
      "loss: 13.246384620666504\n",
      "steps per second: 0.50896\n",
      "step: 35623\n",
      "loss: 12.885159492492676\n",
      "steps per second: 0.54784\n",
      "step: 35624\n",
      "loss: 12.962732315063477\n",
      "steps per second: 0.53542\n",
      "step: 35625\n",
      "loss: 13.003185272216797\n",
      "steps per second: 0.56873\n",
      "step: 35626\n",
      "loss: 12.698112487792969\n",
      "steps per second: 0.53723\n",
      "step: 35627\n",
      "loss: 12.904425621032715\n",
      "steps per second: 0.51938\n",
      "step: 35628\n",
      "loss: 12.569856643676758\n",
      "steps per second: 0.54559\n",
      "step: 35629\n",
      "loss: 12.8938627243042\n",
      "steps per second: 0.53321\n",
      "step: 35630\n",
      "loss: 13.02681827545166\n",
      "steps per second: 0.50079\n",
      "step: 35631\n",
      "loss: 13.223045349121094\n",
      "steps per second: 0.54497\n",
      "step: 35632\n",
      "loss: 12.124683380126953\n",
      "steps per second: 0.56741\n",
      "step: 35633\n",
      "loss: 12.778103828430176\n",
      "steps per second: 0.53210\n",
      "step: 35634\n",
      "loss: 12.559514999389648\n",
      "steps per second: 0.53556\n",
      "step: 35635\n",
      "loss: 13.0653657913208\n",
      "steps per second: 0.59792\n",
      "step: 35636\n",
      "loss: 13.12979507446289\n",
      "steps per second: 0.56156\n",
      "step: 35637\n",
      "loss: 12.837406158447266\n",
      "steps per second: 0.54050\n",
      "step: 35638\n",
      "loss: 12.372217178344727\n",
      "steps per second: 0.54683\n",
      "step: 35639\n",
      "loss: 12.819523811340332\n",
      "steps per second: 0.53563\n",
      "step: 35640\n",
      "loss: 12.71417236328125\n",
      "steps per second: 0.55966\n",
      "step: 35641\n",
      "loss: 12.089171409606934\n",
      "steps per second: 0.52014\n",
      "step: 35642\n",
      "loss: 12.554689407348633\n",
      "steps per second: 0.56755\n",
      "step: 35643\n",
      "loss: 12.737897872924805\n",
      "steps per second: 0.52327\n",
      "step: 35644\n",
      "loss: 12.959144592285156\n",
      "steps per second: 0.54745\n",
      "step: 35645\n",
      "loss: 12.610189437866211\n",
      "steps per second: 0.52183\n",
      "step: 35646\n",
      "loss: 12.405259132385254\n",
      "steps per second: 0.55885\n",
      "step: 35647\n",
      "loss: 13.006087303161621\n",
      "steps per second: 0.59823\n",
      "step: 35648\n",
      "loss: 13.22607707977295\n",
      "steps per second: 0.55646\n",
      "step: 35649\n",
      "loss: 13.317120552062988\n",
      "steps per second: 0.59551\n",
      "step: 35650\n",
      "loss: 12.73177719116211\n",
      "steps per second: 0.52613\n",
      "step: 35651\n",
      "loss: 13.059257507324219\n",
      "steps per second: 0.59546\n",
      "step: 35652\n",
      "loss: 12.694494247436523\n",
      "steps per second: 0.50938\n",
      "step: 35653\n",
      "loss: 12.82643985748291\n",
      "steps per second: 0.53271\n",
      "step: 35654\n",
      "loss: 12.381113052368164\n",
      "steps per second: 0.54173\n",
      "step: 35655\n",
      "loss: 12.470747947692871\n",
      "steps per second: 0.51896\n",
      "step: 35656\n",
      "loss: 12.710453033447266\n",
      "steps per second: 0.54897\n",
      "step: 35657\n",
      "loss: 12.556330680847168\n",
      "steps per second: 0.54556\n",
      "step: 35658\n",
      "loss: 12.966880798339844\n",
      "steps per second: 0.52739\n",
      "step: 35659\n",
      "loss: 12.617657661437988\n",
      "steps per second: 0.53053\n",
      "step: 35660\n",
      "loss: 12.580720901489258\n",
      "steps per second: 0.48779\n",
      "step: 35661\n",
      "loss: 12.727471351623535\n",
      "steps per second: 0.57449\n",
      "step: 35662\n",
      "loss: 12.800895690917969\n",
      "steps per second: 0.54765\n",
      "step: 35663\n",
      "loss: 12.33457088470459\n",
      "steps per second: 0.54171\n",
      "step: 35664\n",
      "loss: 12.437954902648926\n",
      "steps per second: 0.54234\n",
      "step: 35665\n",
      "loss: 12.767444610595703\n",
      "steps per second: 0.55412\n",
      "step: 35666\n",
      "loss: 13.193994522094727\n",
      "steps per second: 0.51535\n",
      "step: 35667\n",
      "loss: 12.842659950256348\n",
      "steps per second: 0.54408\n",
      "step: 35668\n",
      "loss: 12.8138427734375\n",
      "steps per second: 0.55876\n",
      "step: 35669\n",
      "loss: 12.401386260986328\n",
      "steps per second: 0.53058\n",
      "step: 35670\n",
      "loss: 12.478165626525879\n",
      "steps per second: 0.56145\n",
      "step: 35671\n",
      "loss: 13.225668907165527\n",
      "steps per second: 0.55019\n",
      "step: 35672\n",
      "loss: 12.508156776428223\n",
      "steps per second: 0.53871\n",
      "step: 35673\n",
      "loss: 12.447050094604492\n",
      "steps per second: 0.54247\n",
      "step: 35674\n",
      "loss: 12.912738800048828\n",
      "steps per second: 0.54683\n",
      "step: 35675\n",
      "loss: 12.648923873901367\n",
      "steps per second: 0.55582\n",
      "step: 35676\n",
      "loss: 12.608297348022461\n",
      "steps per second: 0.54108\n",
      "step: 35677\n",
      "loss: 12.779436111450195\n",
      "steps per second: 0.54084\n",
      "step: 35678\n",
      "loss: 13.314542770385742\n",
      "steps per second: 0.52501\n",
      "step: 35679\n",
      "loss: 12.947074890136719\n",
      "steps per second: 0.59608\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.844603955745697, layer: 10\n",
      "saving at step 35679\n",
      "----------\n",
      "\n",
      "\n",
      "step: 35680\n",
      "loss: 12.756545066833496\n",
      "steps per second: 0.28874\n",
      "step: 35681\n",
      "loss: 12.643905639648438\n",
      "steps per second: 0.54977\n",
      "step: 35682\n",
      "loss: 12.986552238464355\n",
      "steps per second: 0.48138\n",
      "step: 35683\n",
      "loss: 13.197164535522461\n",
      "steps per second: 0.53336\n",
      "step: 35684\n",
      "loss: 12.840514183044434\n",
      "steps per second: 0.51778\n",
      "step: 35685\n",
      "loss: 12.837238311767578\n",
      "steps per second: 0.52739\n",
      "step: 35686\n",
      "loss: 13.115250587463379\n",
      "steps per second: 0.54737\n",
      "step: 35687\n",
      "loss: 12.705826759338379\n",
      "steps per second: 0.51594\n",
      "step: 35688\n",
      "loss: 12.657404899597168\n",
      "steps per second: 0.56734\n",
      "step: 35689\n",
      "loss: 12.611734390258789\n",
      "steps per second: 0.53228\n",
      "step: 35690\n",
      "loss: 12.489130020141602\n",
      "steps per second: 0.54852\n",
      "step: 35691\n",
      "loss: 12.863984107971191\n",
      "steps per second: 0.55973\n",
      "step: 35692\n",
      "loss: 12.271218299865723\n",
      "steps per second: 0.59717\n",
      "step: 35693\n",
      "loss: 12.712733268737793\n",
      "steps per second: 0.49741\n",
      "step: 35694\n",
      "loss: 12.369214057922363\n",
      "steps per second: 0.52663\n",
      "step: 35695\n",
      "loss: 12.47131633758545\n",
      "steps per second: 0.53457\n",
      "step: 35696\n",
      "loss: 12.527713775634766\n",
      "steps per second: 0.49129\n",
      "step: 35697\n",
      "loss: 12.615140914916992\n",
      "steps per second: 0.54110\n",
      "step: 35698\n",
      "loss: 12.772003173828125\n",
      "steps per second: 0.51500\n",
      "step: 35699\n",
      "loss: 12.69257926940918\n",
      "steps per second: 0.55320\n",
      "step: 35700\n",
      "loss: 12.70823860168457\n",
      "steps per second: 0.54791\n",
      "step: 35701\n",
      "loss: 13.04075813293457\n",
      "steps per second: 0.50571\n",
      "step: 35702\n",
      "loss: 13.067091941833496\n",
      "steps per second: 0.59831\n",
      "step: 35703\n",
      "loss: 12.952366828918457\n",
      "steps per second: 0.52815\n",
      "step: 35704\n",
      "loss: 12.613637924194336\n",
      "steps per second: 0.56610\n",
      "step: 35705\n",
      "loss: 12.914910316467285\n",
      "steps per second: 0.54797\n",
      "step: 35706\n",
      "loss: 12.688936233520508\n",
      "steps per second: 0.54244\n",
      "step: 35707\n",
      "loss: 12.573079109191895\n",
      "steps per second: 0.55548\n",
      "step: 35708\n",
      "loss: 12.622092247009277\n",
      "steps per second: 0.54117\n",
      "step: 35709\n",
      "loss: 12.787191390991211\n",
      "steps per second: 0.53364\n",
      "step: 35710\n",
      "loss: 12.904985427856445\n",
      "steps per second: 0.51695\n",
      "step: 35711\n",
      "loss: 12.726540565490723\n",
      "steps per second: 0.59454\n",
      "step: 35712\n",
      "loss: 12.877978324890137\n",
      "steps per second: 0.57295\n",
      "step: 35713\n",
      "loss: 12.98569107055664\n",
      "steps per second: 0.54035\n",
      "step: 35714\n",
      "loss: 12.496466636657715\n",
      "steps per second: 0.55625\n",
      "step: 35715\n",
      "loss: 13.496081352233887\n",
      "steps per second: 0.55849\n",
      "step: 35716\n",
      "loss: 12.678414344787598\n",
      "steps per second: 0.56793\n",
      "step: 35717\n",
      "loss: 12.700671195983887\n",
      "steps per second: 0.51476\n",
      "step: 35718\n",
      "loss: 13.10627555847168\n",
      "steps per second: 0.56791\n",
      "step: 35719\n",
      "loss: 12.471400260925293\n",
      "steps per second: 0.52853\n",
      "step: 35720\n",
      "loss: 12.898507118225098\n",
      "steps per second: 0.53319\n",
      "step: 35721\n",
      "loss: 12.987590789794922\n",
      "steps per second: 0.54119\n",
      "step: 35722\n",
      "loss: 12.340086936950684\n",
      "steps per second: 0.52886\n",
      "step: 35723\n",
      "loss: 13.318028450012207\n",
      "steps per second: 0.54127\n",
      "step: 35724\n",
      "loss: 12.81617546081543\n",
      "steps per second: 0.55844\n",
      "step: 35725\n",
      "loss: 12.864493370056152\n",
      "steps per second: 0.54897\n",
      "step: 35726\n",
      "loss: 13.374547958374023\n",
      "steps per second: 0.54171\n",
      "step: 35727\n",
      "loss: 12.564574241638184\n",
      "steps per second: 0.54505\n",
      "step: 35728\n",
      "loss: 12.93371868133545\n",
      "steps per second: 0.51542\n",
      "step: 35729\n",
      "loss: 12.854073524475098\n",
      "steps per second: 0.53493\n",
      "step: 35730\n",
      "loss: 11.880762100219727\n",
      "steps per second: 0.51782\n",
      "step: 35731\n",
      "loss: 12.97103500366211\n",
      "steps per second: 0.54782\n",
      "step: 35732\n",
      "loss: 12.604203224182129\n",
      "steps per second: 0.56006\n",
      "step: 35733\n",
      "loss: 12.543545722961426\n",
      "steps per second: 0.54085\n",
      "step: 35734\n",
      "loss: 13.163287162780762\n",
      "steps per second: 0.53722\n",
      "step: 35735\n",
      "loss: 13.091471672058105\n",
      "steps per second: 0.53397\n",
      "step: 35736\n",
      "loss: 12.830344200134277\n",
      "steps per second: 0.54978\n",
      "step: 35737\n",
      "loss: 12.736037254333496\n",
      "steps per second: 0.56694\n",
      "step: 35738\n",
      "loss: 12.865970611572266\n",
      "steps per second: 0.56034\n",
      "step: 35739\n",
      "loss: 12.96707820892334\n",
      "steps per second: 0.59656\n",
      "step: 35740\n",
      "loss: 12.860616683959961\n",
      "steps per second: 0.55870\n",
      "step: 35741\n",
      "loss: 13.251960754394531\n",
      "steps per second: 0.56662\n",
      "step: 35742\n",
      "loss: 12.546152114868164\n",
      "steps per second: 0.52683\n",
      "step: 35743\n",
      "loss: 12.975690841674805\n",
      "steps per second: 0.52805\n",
      "step: 35744\n",
      "loss: 13.136933326721191\n",
      "steps per second: 0.54739\n",
      "step: 35745\n",
      "loss: 12.465935707092285\n",
      "steps per second: 0.56874\n",
      "step: 35746\n",
      "loss: 12.753328323364258\n",
      "steps per second: 0.54208\n",
      "step: 35747\n",
      "loss: 13.553935050964355\n",
      "steps per second: 0.54908\n",
      "step: 35748\n",
      "loss: 12.400141716003418\n",
      "steps per second: 0.56893\n",
      "step: 35749\n",
      "loss: 12.761127471923828\n",
      "steps per second: 0.53438\n",
      "step: 35750\n",
      "loss: 12.493727684020996\n",
      "steps per second: 0.53245\n",
      "step: 35751\n",
      "loss: 12.976550102233887\n",
      "steps per second: 0.53329\n",
      "step: 35752\n",
      "loss: 12.243907928466797\n",
      "steps per second: 0.52984\n",
      "step: 35753\n",
      "loss: 13.109930038452148\n",
      "steps per second: 0.54027\n",
      "step: 35754\n",
      "loss: 13.215932846069336\n",
      "steps per second: 0.54202\n",
      "step: 35755\n",
      "loss: 12.349159240722656\n",
      "steps per second: 0.53471\n",
      "step: 35756\n",
      "loss: 13.044281959533691\n",
      "steps per second: 0.49988\n",
      "step: 35757\n",
      "loss: 12.910749435424805\n",
      "steps per second: 0.54918\n",
      "step: 35758\n",
      "loss: 12.99962329864502\n",
      "steps per second: 0.54064\n",
      "step: 35759\n",
      "loss: 12.882590293884277\n",
      "steps per second: 0.55579\n",
      "step: 35760\n",
      "loss: 13.032061576843262\n",
      "steps per second: 0.54018\n",
      "step: 35761\n",
      "loss: 12.381284713745117\n",
      "steps per second: 0.52991\n",
      "step: 35762\n",
      "loss: 12.576951026916504\n",
      "steps per second: 0.59693\n",
      "step: 35763\n",
      "loss: 12.952081680297852\n",
      "steps per second: 0.49820\n",
      "step: 35764\n",
      "loss: 12.402200698852539\n",
      "steps per second: 0.51601\n",
      "step: 35765\n",
      "loss: 13.243121147155762\n",
      "steps per second: 0.50059\n",
      "step: 35766\n",
      "loss: 12.960612297058105\n",
      "steps per second: 0.52536\n",
      "step: 35767\n",
      "loss: 12.867746353149414\n",
      "steps per second: 0.53553\n",
      "step: 35768\n",
      "loss: 12.188272476196289\n",
      "steps per second: 0.51768\n",
      "step: 35769\n",
      "loss: 12.963985443115234\n",
      "steps per second: 0.55906\n",
      "step: 35770\n",
      "loss: 13.047771453857422\n",
      "steps per second: 0.53525\n",
      "step: 35771\n",
      "loss: 12.97722339630127\n",
      "steps per second: 0.54447\n",
      "step: 35772\n",
      "loss: 13.259930610656738\n",
      "steps per second: 0.55813\n",
      "step: 35773\n",
      "loss: 13.02901554107666\n",
      "steps per second: 0.49998\n",
      "step: 35774\n",
      "loss: 11.989335060119629\n",
      "steps per second: 0.55855\n",
      "step: 35775\n",
      "loss: 12.98780345916748\n",
      "steps per second: 0.54911\n",
      "step: 35776\n",
      "loss: 12.431694030761719\n",
      "steps per second: 0.54849\n",
      "step: 35777\n",
      "loss: 12.523934364318848\n",
      "steps per second: 0.54623\n",
      "step: 35778\n",
      "loss: 12.790255546569824\n",
      "steps per second: 0.51355\n",
      "step: 35779\n",
      "loss: 12.822829246520996\n",
      "steps per second: 0.53815\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.834509551525116, layer: 11\n",
      "saving at step 35779\n",
      "----------\n",
      "\n",
      "\n",
      "step: 35780\n",
      "loss: 12.201388359069824\n",
      "steps per second: 0.26867\n",
      "step: 35781\n",
      "loss: 12.554787635803223\n",
      "steps per second: 0.50025\n",
      "step: 35782\n",
      "loss: 12.568838119506836\n",
      "steps per second: 0.57445\n",
      "step: 35783\n",
      "loss: 12.489309310913086\n",
      "steps per second: 0.52995\n",
      "step: 35784\n",
      "loss: 12.120935440063477\n",
      "steps per second: 0.50948\n",
      "step: 35785\n",
      "loss: 13.195624351501465\n",
      "steps per second: 0.51722\n",
      "step: 35786\n",
      "loss: 12.675930976867676\n",
      "steps per second: 0.53714\n",
      "step: 35787\n",
      "loss: 12.846529006958008\n",
      "steps per second: 0.56486\n",
      "step: 35788\n",
      "loss: 12.64741325378418\n",
      "steps per second: 0.51296\n",
      "step: 35789\n",
      "loss: 12.264519691467285\n",
      "steps per second: 0.56985\n",
      "step: 35790\n",
      "loss: 13.062567710876465\n",
      "steps per second: 0.51916\n",
      "step: 35791\n",
      "loss: 12.835387229919434\n",
      "steps per second: 0.55804\n",
      "step: 35792\n",
      "loss: 12.704408645629883\n",
      "steps per second: 0.52636\n",
      "step: 35793\n",
      "loss: 12.507071495056152\n",
      "steps per second: 0.52731\n",
      "step: 35794\n",
      "loss: 12.948346138000488\n",
      "steps per second: 0.54918\n",
      "step: 35795\n",
      "loss: 12.40168571472168\n",
      "steps per second: 0.54923\n",
      "step: 35796\n",
      "loss: 12.662007331848145\n",
      "steps per second: 0.50842\n",
      "step: 35797\n",
      "loss: 12.64184284210205\n",
      "steps per second: 0.54179\n",
      "step: 35798\n",
      "loss: 12.785810470581055\n",
      "steps per second: 0.51473\n",
      "step: 35799\n",
      "loss: 13.190048217773438\n",
      "steps per second: 0.54163\n",
      "step: 35800\n",
      "loss: 12.792793273925781\n",
      "steps per second: 0.54635\n",
      "step: 35801\n",
      "loss: 13.198820114135742\n",
      "steps per second: 0.51616\n",
      "step: 35802\n",
      "loss: 13.370819091796875\n",
      "steps per second: 0.53413\n",
      "step: 35803\n",
      "loss: 13.020516395568848\n",
      "steps per second: 0.53128\n",
      "step: 35804\n",
      "loss: 13.054770469665527\n",
      "steps per second: 0.54879\n",
      "step: 35805\n",
      "loss: 12.997057914733887\n",
      "steps per second: 0.54772\n",
      "step: 35806\n",
      "loss: 13.164050102233887\n",
      "steps per second: 0.54149\n",
      "step: 35807\n",
      "loss: 12.594001770019531\n",
      "steps per second: 0.59527\n",
      "step: 35808\n",
      "loss: 13.406193733215332\n",
      "steps per second: 0.57114\n",
      "step: 35809\n",
      "loss: 12.922281265258789\n",
      "steps per second: 0.54264\n",
      "step: 35810\n",
      "loss: 13.095134735107422\n",
      "steps per second: 0.54866\n",
      "step: 35811\n",
      "loss: 13.296586990356445\n",
      "steps per second: 0.53059\n",
      "step: 35812\n",
      "loss: 12.469853401184082\n",
      "steps per second: 0.53486\n",
      "step: 35813\n",
      "loss: 13.097434043884277\n",
      "steps per second: 0.51857\n",
      "step: 35814\n",
      "loss: 13.227591514587402\n",
      "steps per second: 0.51865\n",
      "step: 35815\n",
      "loss: 12.412023544311523\n",
      "steps per second: 0.51694\n",
      "step: 35816\n",
      "loss: 13.297286033630371\n",
      "steps per second: 0.56096\n",
      "step: 35817\n",
      "loss: 12.873366355895996\n",
      "steps per second: 0.55678\n",
      "step: 35818\n",
      "loss: 12.533967018127441\n",
      "steps per second: 0.59243\n",
      "step: 35819\n",
      "loss: 12.725015640258789\n",
      "steps per second: 0.56269\n",
      "step: 35820\n",
      "loss: 12.531694412231445\n",
      "steps per second: 0.54671\n",
      "step: 35821\n",
      "loss: 12.710787773132324\n",
      "steps per second: 0.53350\n",
      "step: 35822\n",
      "loss: 12.291486740112305\n",
      "steps per second: 0.48256\n",
      "step: 35823\n",
      "loss: 12.813919067382812\n",
      "steps per second: 0.47945\n",
      "step: 35824\n",
      "loss: 12.920607566833496\n",
      "steps per second: 0.51130\n",
      "step: 35825\n",
      "loss: 12.865445137023926\n",
      "steps per second: 0.54311\n",
      "step: 35826\n",
      "loss: 12.217256546020508\n",
      "steps per second: 0.53872\n",
      "step: 35827\n",
      "loss: 12.60862922668457\n",
      "steps per second: 0.58930\n",
      "step: 35828\n",
      "loss: 12.546144485473633\n",
      "steps per second: 0.53964\n",
      "step: 35829\n",
      "loss: 12.790502548217773\n",
      "steps per second: 0.55635\n",
      "step: 35830\n",
      "loss: 13.54793643951416\n",
      "steps per second: 0.53392\n",
      "step: 35831\n",
      "loss: 13.084928512573242\n",
      "steps per second: 0.54035\n",
      "step: 35832\n",
      "loss: 13.232479095458984\n",
      "steps per second: 0.52467\n",
      "step: 35833\n",
      "loss: 12.676614761352539\n",
      "steps per second: 0.52666\n",
      "step: 35834\n",
      "loss: 12.759864807128906\n",
      "steps per second: 0.53293\n",
      "step: 35835\n",
      "loss: 12.752476692199707\n",
      "steps per second: 0.53062\n",
      "step: 35836\n",
      "loss: 12.717388153076172\n",
      "steps per second: 0.55443\n",
      "step: 35837\n",
      "loss: 12.04417896270752\n",
      "steps per second: 0.53401\n",
      "step: 35838\n",
      "loss: 13.169524192810059\n",
      "steps per second: 0.51066\n",
      "step: 35839\n",
      "loss: 13.028581619262695\n",
      "steps per second: 0.52388\n",
      "step: 35840\n",
      "loss: 12.902464866638184\n",
      "steps per second: 0.54030\n",
      "step: 35841\n",
      "loss: 12.57291316986084\n",
      "steps per second: 0.49918\n",
      "step: 35842\n",
      "loss: 12.087821960449219\n",
      "steps per second: 0.53901\n",
      "step: 35843\n",
      "loss: 11.976129531860352\n",
      "steps per second: 0.53923\n",
      "step: 35844\n",
      "loss: 12.515054702758789\n",
      "steps per second: 0.59615\n",
      "step: 35845\n",
      "loss: 12.993135452270508\n",
      "steps per second: 0.50702\n",
      "step: 35846\n",
      "loss: 13.349116325378418\n",
      "steps per second: 0.52914\n",
      "step: 35847\n",
      "loss: 13.204537391662598\n",
      "steps per second: 0.55818\n",
      "step: 35848\n",
      "loss: 12.9246826171875\n",
      "steps per second: 0.56775\n",
      "step: 35849\n",
      "loss: 12.856849670410156\n",
      "steps per second: 0.52640\n",
      "step: 35850\n",
      "loss: 13.303803443908691\n",
      "steps per second: 0.54886\n",
      "step: 35851\n",
      "loss: 12.89553451538086\n",
      "steps per second: 0.54415\n",
      "step: 35852\n",
      "loss: 13.078834533691406\n",
      "steps per second: 0.54625\n",
      "step: 35853\n",
      "loss: 12.728466033935547\n",
      "steps per second: 0.54106\n",
      "step: 35854\n",
      "loss: 12.806595802307129\n",
      "steps per second: 0.56571\n",
      "step: 35855\n",
      "loss: 12.908573150634766\n",
      "steps per second: 0.59878\n",
      "step: 35856\n",
      "loss: 12.556143760681152\n",
      "steps per second: 0.51559\n",
      "step: 35857\n",
      "loss: 12.615747451782227\n",
      "steps per second: 0.59824\n",
      "step: 35858\n",
      "loss: 12.613041877746582\n",
      "steps per second: 0.53251\n",
      "step: 35859\n",
      "loss: 13.141663551330566\n",
      "steps per second: 0.53536\n",
      "step: 35860\n",
      "loss: 12.681913375854492\n",
      "steps per second: 0.54259\n",
      "step: 35861\n",
      "loss: 12.709990501403809\n",
      "steps per second: 0.51867\n",
      "step: 35862\n",
      "loss: 12.792766571044922\n",
      "steps per second: 0.55761\n",
      "step: 35863\n",
      "loss: 13.162125587463379\n",
      "steps per second: 0.52848\n",
      "step: 35864\n",
      "loss: 13.239864349365234\n",
      "steps per second: 0.54670\n",
      "step: 35865\n",
      "loss: 12.805612564086914\n",
      "steps per second: 0.54141\n",
      "step: 35866\n",
      "loss: 12.50639820098877\n",
      "steps per second: 0.56109\n",
      "step: 35867\n",
      "loss: 13.260926246643066\n",
      "steps per second: 0.53271\n",
      "step: 35868\n",
      "loss: 12.634156227111816\n",
      "steps per second: 0.55981\n",
      "step: 35869\n",
      "loss: 12.690325736999512\n",
      "steps per second: 0.54888\n",
      "step: 35870\n",
      "loss: 13.1854829788208\n",
      "steps per second: 0.53018\n",
      "step: 35871\n",
      "loss: 12.393771171569824\n",
      "steps per second: 0.54805\n",
      "step: 35872\n",
      "loss: 13.574501991271973\n",
      "steps per second: 0.59557\n",
      "step: 35873\n",
      "loss: 12.685286521911621\n",
      "steps per second: 0.51769\n",
      "step: 35874\n",
      "loss: 12.700063705444336\n",
      "steps per second: 0.56581\n",
      "step: 35875\n",
      "loss: 11.98613166809082\n",
      "steps per second: 0.46739\n",
      "step: 35876\n",
      "loss: 12.624327659606934\n",
      "steps per second: 0.53607\n",
      "step: 35877\n",
      "loss: 12.999598503112793\n",
      "steps per second: 0.54250\n",
      "step: 35878\n",
      "loss: 13.16468620300293\n",
      "steps per second: 0.56037\n",
      "step: 35879\n",
      "loss: 13.108027458190918\n",
      "steps per second: 0.54672\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8696658611297607, layer: 11\n",
      "saving at step 35879\n",
      "----------\n",
      "\n",
      "\n",
      "step: 35880\n",
      "loss: 12.296208381652832\n",
      "steps per second: 0.28298\n",
      "step: 35881\n",
      "loss: 12.725769996643066\n",
      "steps per second: 0.53661\n",
      "step: 35882\n",
      "loss: 12.507411003112793\n",
      "steps per second: 0.52599\n",
      "step: 35883\n",
      "loss: 12.807329177856445\n",
      "steps per second: 0.55624\n",
      "step: 35884\n",
      "loss: 12.54561710357666\n",
      "steps per second: 0.54593\n",
      "step: 35885\n",
      "loss: 12.953455924987793\n",
      "steps per second: 0.53725\n",
      "step: 35886\n",
      "loss: 13.015902519226074\n",
      "steps per second: 0.51579\n",
      "step: 35887\n",
      "loss: 13.13214111328125\n",
      "steps per second: 0.55514\n",
      "step: 35888\n",
      "loss: 13.12519359588623\n",
      "steps per second: 0.56306\n",
      "step: 35889\n",
      "loss: 12.805502891540527\n",
      "steps per second: 0.51649\n",
      "step: 35890\n",
      "loss: 12.909555435180664\n",
      "steps per second: 0.51885\n",
      "step: 35891\n",
      "loss: 12.728791236877441\n",
      "steps per second: 0.52485\n",
      "step: 35892\n",
      "loss: 12.93091869354248\n",
      "steps per second: 0.53701\n",
      "step: 35893\n",
      "loss: 12.780488967895508\n",
      "steps per second: 0.52893\n",
      "step: 35894\n",
      "loss: 13.243112564086914\n",
      "steps per second: 0.51295\n",
      "step: 35895\n",
      "loss: 13.286090850830078\n",
      "steps per second: 0.53973\n",
      "step: 35896\n",
      "loss: 12.547350883483887\n",
      "steps per second: 0.54053\n",
      "step: 35897\n",
      "loss: 12.943089485168457\n",
      "steps per second: 0.54734\n",
      "step: 35898\n",
      "loss: 13.431994438171387\n",
      "steps per second: 0.52758\n",
      "step: 35899\n",
      "loss: 12.064114570617676\n",
      "steps per second: 0.55711\n",
      "step: 35900\n",
      "loss: 13.41743278503418\n",
      "steps per second: 0.53465\n",
      "step: 35901\n",
      "loss: 13.149114608764648\n",
      "steps per second: 0.55907\n",
      "step: 35902\n",
      "loss: 12.578224182128906\n",
      "steps per second: 0.54808\n",
      "step: 35903\n",
      "loss: 12.564579010009766\n",
      "steps per second: 0.54867\n",
      "step: 35904\n",
      "loss: 12.894329071044922\n",
      "steps per second: 0.51661\n",
      "step: 35905\n",
      "loss: 12.765670776367188\n",
      "steps per second: 0.57281\n",
      "step: 35906\n",
      "loss: 12.5412015914917\n",
      "steps per second: 0.52608\n",
      "step: 35907\n",
      "loss: 12.8443603515625\n",
      "steps per second: 0.53710\n",
      "step: 35908\n",
      "loss: 13.022583961486816\n",
      "steps per second: 0.53958\n",
      "step: 35909\n",
      "loss: 12.347250938415527\n",
      "steps per second: 0.55891\n",
      "step: 35910\n",
      "loss: 12.321621894836426\n",
      "steps per second: 0.51749\n",
      "step: 35911\n",
      "loss: 12.329492568969727\n",
      "steps per second: 0.56370\n",
      "step: 35912\n",
      "loss: 12.539567947387695\n",
      "steps per second: 0.52364\n",
      "step: 35913\n",
      "loss: 13.055998802185059\n",
      "steps per second: 0.55926\n",
      "step: 35914\n",
      "loss: 12.486019134521484\n",
      "steps per second: 0.54809\n",
      "step: 35915\n",
      "loss: 12.906123161315918\n",
      "steps per second: 0.50589\n",
      "step: 35916\n",
      "loss: 13.263191223144531\n",
      "steps per second: 0.54033\n",
      "step: 35917\n",
      "loss: 12.893590927124023\n",
      "steps per second: 0.52807\n",
      "step: 35918\n",
      "loss: 12.878334045410156\n",
      "steps per second: 0.59625\n",
      "step: 35919\n",
      "loss: 12.691858291625977\n",
      "steps per second: 0.56028\n",
      "step: 35920\n",
      "loss: 12.550926208496094\n",
      "steps per second: 0.53254\n",
      "step: 35921\n",
      "loss: 12.946868896484375\n",
      "steps per second: 0.54746\n",
      "step: 35922\n",
      "loss: 12.74221134185791\n",
      "steps per second: 0.49884\n",
      "step: 35923\n",
      "loss: 12.444419860839844\n",
      "steps per second: 0.54052\n",
      "step: 35924\n",
      "loss: 13.050262451171875\n",
      "steps per second: 0.51917\n",
      "step: 35925\n",
      "loss: 12.807027816772461\n",
      "steps per second: 0.53843\n",
      "step: 35926\n",
      "loss: 12.594949722290039\n",
      "steps per second: 0.56444\n",
      "step: 35927\n",
      "loss: 12.952000617980957\n",
      "steps per second: 0.54661\n",
      "step: 35928\n",
      "loss: 12.367387771606445\n",
      "steps per second: 0.57417\n",
      "step: 35929\n",
      "loss: 12.410693168640137\n",
      "steps per second: 0.57206\n",
      "step: 35930\n",
      "loss: 12.578137397766113\n",
      "steps per second: 0.54223\n",
      "step: 35931\n",
      "loss: 13.151962280273438\n",
      "steps per second: 0.47796\n",
      "step: 35932\n",
      "loss: 12.28562068939209\n",
      "steps per second: 0.56633\n",
      "step: 35933\n",
      "loss: 12.70771312713623\n",
      "steps per second: 0.55629\n",
      "step: 35934\n",
      "loss: 12.865199089050293\n",
      "steps per second: 0.51738\n",
      "step: 35935\n",
      "loss: 12.310782432556152\n",
      "steps per second: 0.52588\n",
      "step: 35936\n",
      "loss: 12.81827449798584\n",
      "steps per second: 0.57475\n",
      "step: 35937\n",
      "loss: 12.970739364624023\n",
      "steps per second: 0.51235\n",
      "step: 35938\n",
      "loss: 12.64594841003418\n",
      "steps per second: 0.50351\n",
      "step: 35939\n",
      "loss: 12.73622989654541\n",
      "steps per second: 0.56075\n",
      "step: 35940\n",
      "loss: 13.254321098327637\n",
      "steps per second: 0.53940\n",
      "step: 35941\n",
      "loss: 12.77914047241211\n",
      "steps per second: 0.53876\n",
      "step: 35942\n",
      "loss: 12.438835144042969\n",
      "steps per second: 0.55877\n",
      "step: 35943\n",
      "loss: 12.35838508605957\n",
      "steps per second: 0.52761\n",
      "step: 35944\n",
      "loss: 12.962847709655762\n",
      "steps per second: 0.52718\n",
      "step: 35945\n",
      "loss: 12.60633659362793\n",
      "steps per second: 0.53960\n",
      "step: 35946\n",
      "loss: 12.730690002441406\n",
      "steps per second: 0.54709\n",
      "step: 35947\n",
      "loss: 13.003761291503906\n",
      "steps per second: 0.54178\n",
      "step: 35948\n",
      "loss: 12.838845252990723\n",
      "steps per second: 0.51820\n",
      "step: 35949\n",
      "loss: 12.829916000366211\n",
      "steps per second: 0.54522\n",
      "step: 35950\n",
      "loss: 12.58694839477539\n",
      "steps per second: 0.55732\n",
      "step: 35951\n",
      "loss: 12.893661499023438\n",
      "steps per second: 0.56026\n",
      "step: 35952\n",
      "loss: 12.614259719848633\n",
      "steps per second: 0.52118\n",
      "step: 35953\n",
      "loss: 12.50968074798584\n",
      "steps per second: 0.59068\n",
      "step: 35954\n",
      "loss: 12.784439086914062\n",
      "steps per second: 0.56608\n",
      "step: 35955\n",
      "loss: 12.566873550415039\n",
      "steps per second: 0.52744\n",
      "step: 35956\n",
      "loss: 13.317666053771973\n",
      "steps per second: 0.54783\n",
      "step: 35957\n",
      "loss: 12.81318187713623\n",
      "steps per second: 0.51484\n",
      "step: 35958\n",
      "loss: 12.101761817932129\n",
      "steps per second: 0.54798\n",
      "step: 35959\n",
      "loss: 12.532957077026367\n",
      "steps per second: 0.51192\n",
      "step: 35960\n",
      "loss: 12.431646347045898\n",
      "steps per second: 0.52877\n",
      "step: 35961\n",
      "loss: 12.868050575256348\n",
      "steps per second: 0.54497\n",
      "step: 35962\n",
      "loss: 13.08046817779541\n",
      "steps per second: 0.54201\n",
      "step: 35963\n",
      "loss: 12.517704963684082\n",
      "steps per second: 0.51451\n",
      "step: 35964\n",
      "loss: 12.285484313964844\n",
      "steps per second: 0.59799\n",
      "step: 35965\n",
      "loss: 12.528154373168945\n",
      "steps per second: 0.55779\n",
      "step: 35966\n",
      "loss: 12.631047248840332\n",
      "steps per second: 0.55668\n",
      "step: 35967\n",
      "loss: 12.782140731811523\n",
      "steps per second: 0.51798\n",
      "step: 35968\n",
      "loss: 13.047004699707031\n",
      "steps per second: 0.59688\n",
      "step: 35969\n",
      "loss: 12.239665985107422\n",
      "steps per second: 0.55984\n",
      "step: 35970\n",
      "loss: 12.926612854003906\n",
      "steps per second: 0.57450\n",
      "step: 35971\n",
      "loss: 12.340432167053223\n",
      "steps per second: 0.50023\n",
      "step: 35972\n",
      "loss: 12.665558815002441\n",
      "steps per second: 0.56124\n",
      "step: 35973\n",
      "loss: 12.961568832397461\n",
      "steps per second: 0.55891\n",
      "step: 35974\n",
      "loss: 12.536992073059082\n",
      "steps per second: 0.55639\n",
      "step: 35975\n",
      "loss: 12.303058624267578\n",
      "steps per second: 0.56205\n",
      "step: 35976\n",
      "loss: 12.536005973815918\n",
      "steps per second: 0.56879\n",
      "step: 35977\n",
      "loss: 13.107081413269043\n",
      "steps per second: 0.52978\n",
      "step: 35978\n",
      "loss: 12.942410469055176\n",
      "steps per second: 0.52505\n",
      "step: 35979\n",
      "loss: 12.703132629394531\n",
      "steps per second: 0.54683\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8360955715179443, layer: 10\n",
      "saving at step 35979\n",
      "----------\n",
      "\n",
      "\n",
      "step: 35980\n",
      "loss: 12.606285095214844\n",
      "steps per second: 0.27072\n",
      "step: 35981\n",
      "loss: 13.37352180480957\n",
      "steps per second: 0.53974\n",
      "step: 35982\n",
      "loss: 12.473882675170898\n",
      "steps per second: 0.56267\n",
      "step: 35983\n",
      "loss: 13.170683860778809\n",
      "steps per second: 0.54022\n",
      "step: 35984\n",
      "loss: 12.056561470031738\n",
      "steps per second: 0.54094\n",
      "step: 35985\n",
      "loss: 12.74911117553711\n",
      "steps per second: 0.54806\n",
      "step: 35986\n",
      "loss: 12.244122505187988\n",
      "steps per second: 0.56309\n",
      "step: 35987\n",
      "loss: 13.421003341674805\n",
      "steps per second: 0.59822\n",
      "step: 35988\n",
      "loss: 12.757905006408691\n",
      "steps per second: 0.51921\n",
      "step: 35989\n",
      "loss: 12.78376579284668\n",
      "steps per second: 0.54019\n",
      "step: 35990\n",
      "loss: 12.520102500915527\n",
      "steps per second: 0.51548\n",
      "step: 35991\n",
      "loss: 12.899835586547852\n",
      "steps per second: 0.55893\n",
      "step: 35992\n",
      "loss: 12.84156322479248\n",
      "steps per second: 0.52829\n",
      "step: 35993\n",
      "loss: 12.7512788772583\n",
      "steps per second: 0.53320\n",
      "step: 35994\n",
      "loss: 12.435894012451172\n",
      "steps per second: 0.54301\n",
      "step: 35995\n",
      "loss: 13.212124824523926\n",
      "steps per second: 0.53982\n",
      "step: 35996\n",
      "loss: 13.392258644104004\n",
      "steps per second: 0.52728\n",
      "step: 35997\n",
      "loss: 12.537932395935059\n",
      "steps per second: 0.54271\n",
      "step: 35998\n",
      "loss: 12.412415504455566\n",
      "steps per second: 0.59566\n",
      "step: 35999\n",
      "loss: 13.206339836120605\n",
      "steps per second: 0.55991\n",
      "step: 36000\n",
      "loss: 12.397191047668457\n",
      "steps per second: 0.56746\n",
      "step: 36001\n",
      "loss: 13.356731414794922\n",
      "steps per second: 0.53662\n",
      "step: 36002\n",
      "loss: 13.378344535827637\n",
      "steps per second: 0.55958\n",
      "step: 36003\n",
      "loss: 12.633894920349121\n",
      "steps per second: 0.51895\n",
      "step: 36004\n",
      "loss: 12.504861831665039\n",
      "steps per second: 0.49872\n",
      "step: 36005\n",
      "loss: 13.227531433105469\n",
      "steps per second: 0.49115\n",
      "step: 36006\n",
      "loss: 12.387700080871582\n",
      "steps per second: 0.49272\n",
      "step: 36007\n",
      "loss: 12.50447940826416\n",
      "steps per second: 0.55435\n",
      "step: 36008\n",
      "loss: 13.114809036254883\n",
      "steps per second: 0.51920\n",
      "step: 36009\n",
      "loss: 12.955839157104492\n",
      "steps per second: 0.53523\n",
      "step: 36010\n",
      "loss: 12.397137641906738\n",
      "steps per second: 0.51989\n",
      "step: 36011\n",
      "loss: 13.303723335266113\n",
      "steps per second: 0.49805\n",
      "step: 36012\n",
      "loss: 12.748846054077148\n",
      "steps per second: 0.55853\n",
      "step: 36013\n",
      "loss: 12.888568878173828\n",
      "steps per second: 0.52805\n",
      "step: 36014\n",
      "loss: 13.247389793395996\n",
      "steps per second: 0.54503\n",
      "step: 36015\n",
      "loss: 12.241254806518555\n",
      "steps per second: 0.59862\n",
      "step: 36016\n",
      "loss: 12.74677848815918\n",
      "steps per second: 0.51760\n",
      "step: 36017\n",
      "loss: 12.615817070007324\n",
      "steps per second: 0.53879\n",
      "step: 36018\n",
      "loss: 12.708894729614258\n",
      "steps per second: 0.54409\n",
      "step: 36019\n",
      "loss: 13.121387481689453\n",
      "steps per second: 0.56708\n",
      "step: 36020\n",
      "loss: 13.253066062927246\n",
      "steps per second: 0.60097\n",
      "step: 36021\n",
      "loss: 12.796298027038574\n",
      "steps per second: 0.51588\n",
      "step: 36022\n",
      "loss: 12.670184135437012\n",
      "steps per second: 0.55141\n",
      "step: 36023\n",
      "loss: 13.061298370361328\n",
      "steps per second: 0.51391\n",
      "step: 36024\n",
      "loss: 12.905791282653809\n",
      "steps per second: 0.54619\n",
      "step: 36025\n",
      "loss: 13.262179374694824\n",
      "steps per second: 0.55942\n",
      "step: 36026\n",
      "loss: 12.548851013183594\n",
      "steps per second: 0.52501\n",
      "step: 36027\n",
      "loss: 13.087162971496582\n",
      "steps per second: 0.52694\n",
      "step: 36028\n",
      "loss: 12.45130443572998\n",
      "steps per second: 0.53568\n",
      "step: 36029\n",
      "loss: 12.784941673278809\n",
      "steps per second: 0.54990\n",
      "step: 36030\n",
      "loss: 13.207194328308105\n",
      "steps per second: 0.52577\n",
      "step: 36031\n",
      "loss: 12.643731117248535\n",
      "steps per second: 0.54164\n",
      "step: 36032\n",
      "loss: 12.950677871704102\n",
      "steps per second: 0.53660\n",
      "step: 36033\n",
      "loss: 13.308499336242676\n",
      "steps per second: 0.53435\n",
      "step: 36034\n",
      "loss: 12.96628189086914\n",
      "steps per second: 0.59370\n",
      "step: 36035\n",
      "loss: 12.507364273071289\n",
      "steps per second: 0.55021\n",
      "step: 36036\n",
      "loss: 12.851388931274414\n",
      "steps per second: 0.54786\n",
      "step: 36037\n",
      "loss: 12.893074989318848\n",
      "steps per second: 0.49852\n",
      "step: 36038\n",
      "loss: 13.015493392944336\n",
      "steps per second: 0.56418\n",
      "step: 36039\n",
      "loss: 13.480069160461426\n",
      "steps per second: 0.54798\n",
      "step: 36040\n",
      "loss: 13.094408988952637\n",
      "steps per second: 0.54771\n",
      "step: 36041\n",
      "loss: 12.933279991149902\n",
      "steps per second: 0.53329\n",
      "step: 36042\n",
      "loss: 12.522628784179688\n",
      "steps per second: 0.56396\n",
      "step: 36043\n",
      "loss: 12.6836519241333\n",
      "steps per second: 0.54379\n",
      "step: 36044\n",
      "loss: 13.153406143188477\n",
      "steps per second: 0.51662\n",
      "step: 36045\n",
      "loss: 13.109704971313477\n",
      "steps per second: 0.50734\n",
      "step: 36046\n",
      "loss: 13.066990852355957\n",
      "steps per second: 0.53041\n",
      "step: 36047\n",
      "loss: 12.894780158996582\n",
      "steps per second: 0.54325\n",
      "step: 36048\n",
      "loss: 12.63199234008789\n",
      "steps per second: 0.56302\n",
      "step: 36049\n",
      "loss: 11.727487564086914\n",
      "steps per second: 0.49671\n",
      "step: 36050\n",
      "loss: 12.880675315856934\n",
      "steps per second: 0.52784\n",
      "step: 36051\n",
      "loss: 13.457999229431152\n",
      "steps per second: 0.52994\n",
      "step: 36052\n",
      "loss: 13.103626251220703\n",
      "steps per second: 0.56918\n",
      "step: 36053\n",
      "loss: 12.819567680358887\n",
      "steps per second: 0.51373\n",
      "step: 36054\n",
      "loss: 12.683890342712402\n",
      "steps per second: 0.52366\n",
      "step: 36055\n",
      "loss: 12.491266250610352\n",
      "steps per second: 0.54469\n",
      "step: 36056\n",
      "loss: 13.020820617675781\n",
      "steps per second: 0.59088\n",
      "step: 36057\n",
      "loss: 13.247753143310547\n",
      "steps per second: 0.54725\n",
      "step: 36058\n",
      "loss: 12.938760757446289\n",
      "steps per second: 0.53551\n",
      "step: 36059\n",
      "loss: 12.922554016113281\n",
      "steps per second: 0.49406\n",
      "step: 36060\n",
      "loss: 12.765669822692871\n",
      "steps per second: 0.51855\n",
      "step: 36061\n",
      "loss: 12.7330961227417\n",
      "steps per second: 0.51170\n",
      "step: 36062\n",
      "loss: 12.796688079833984\n",
      "steps per second: 0.56629\n",
      "step: 36063\n",
      "loss: 13.226593017578125\n",
      "steps per second: 0.55902\n",
      "step: 36064\n",
      "loss: 12.787177085876465\n",
      "steps per second: 0.52556\n",
      "step: 36065\n",
      "loss: 12.987401008605957\n",
      "steps per second: 0.57562\n",
      "step: 36066\n",
      "loss: 12.639058113098145\n",
      "steps per second: 0.56098\n",
      "step: 36067\n",
      "loss: 12.92159366607666\n",
      "steps per second: 0.53985\n",
      "step: 36068\n",
      "loss: 12.807455062866211\n",
      "steps per second: 0.57851\n",
      "step: 36069\n",
      "loss: 12.565836906433105\n",
      "steps per second: 0.55057\n",
      "step: 36070\n",
      "loss: 12.589361190795898\n",
      "steps per second: 0.57056\n",
      "step: 36071\n",
      "loss: 13.214034080505371\n",
      "steps per second: 0.52687\n",
      "step: 36072\n",
      "loss: 12.513607025146484\n",
      "steps per second: 0.57164\n",
      "step: 36073\n",
      "loss: 13.432626724243164\n",
      "steps per second: 0.56872\n",
      "step: 36074\n",
      "loss: 13.342581748962402\n",
      "steps per second: 0.51706\n",
      "step: 36075\n",
      "loss: 12.86032485961914\n",
      "steps per second: 0.49028\n",
      "step: 36076\n",
      "loss: 12.774463653564453\n",
      "steps per second: 0.52164\n",
      "step: 36077\n",
      "loss: 12.696372032165527\n",
      "steps per second: 0.54256\n",
      "step: 36078\n",
      "loss: 13.146042823791504\n",
      "steps per second: 0.54960\n",
      "step: 36079\n",
      "loss: 12.43410873413086\n",
      "steps per second: 0.60598\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8085413575172424, layer: 12\n",
      "saving at step 36079\n",
      "----------\n",
      "\n",
      "\n",
      "step: 36080\n",
      "loss: 13.454463958740234\n",
      "steps per second: 0.28077\n",
      "step: 36081\n",
      "loss: 11.976545333862305\n",
      "steps per second: 0.53553\n",
      "step: 36082\n",
      "loss: 12.78096866607666\n",
      "steps per second: 0.53127\n",
      "step: 36083\n",
      "loss: 13.162900924682617\n",
      "steps per second: 0.54895\n",
      "step: 36084\n",
      "loss: 13.201708793640137\n",
      "steps per second: 0.52841\n",
      "step: 36085\n",
      "loss: 12.610953330993652\n",
      "steps per second: 0.51796\n",
      "step: 36086\n",
      "loss: 12.784133911132812\n",
      "steps per second: 0.54168\n",
      "step: 36087\n",
      "loss: 12.611385345458984\n",
      "steps per second: 0.52436\n",
      "step: 36088\n",
      "loss: 12.71239948272705\n",
      "steps per second: 0.54720\n",
      "step: 36089\n",
      "loss: 13.266408920288086\n",
      "steps per second: 0.50121\n",
      "step: 36090\n",
      "loss: 13.331552505493164\n",
      "steps per second: 0.57682\n",
      "step: 36091\n",
      "loss: 12.587509155273438\n",
      "steps per second: 0.53233\n",
      "step: 36092\n",
      "loss: 12.700654029846191\n",
      "steps per second: 0.50985\n",
      "step: 36093\n",
      "loss: 13.059653282165527\n",
      "steps per second: 0.54983\n",
      "step: 36094\n",
      "loss: 13.173237800598145\n",
      "steps per second: 0.53865\n",
      "step: 36095\n",
      "loss: 13.153681755065918\n",
      "steps per second: 0.50729\n",
      "step: 36096\n",
      "loss: 12.445812225341797\n",
      "steps per second: 0.60748\n",
      "step: 36097\n",
      "loss: 12.684881210327148\n",
      "steps per second: 0.56021\n",
      "step: 36098\n",
      "loss: 13.247919082641602\n",
      "steps per second: 0.55167\n",
      "step: 36099\n",
      "loss: 12.788835525512695\n",
      "steps per second: 0.53460\n",
      "step: 36100\n",
      "loss: 12.61396312713623\n",
      "steps per second: 0.54803\n",
      "step: 36101\n",
      "loss: 13.113486289978027\n",
      "steps per second: 0.54758\n",
      "step: 36102\n",
      "loss: 12.875743865966797\n",
      "steps per second: 0.54089\n",
      "step: 36103\n",
      "loss: 12.816032409667969\n",
      "steps per second: 0.51521\n",
      "step: 36104\n",
      "loss: 13.266934394836426\n",
      "steps per second: 0.56160\n",
      "step: 36105\n",
      "loss: 12.671744346618652\n",
      "steps per second: 0.54459\n",
      "step: 36106\n",
      "loss: 12.765886306762695\n",
      "steps per second: 0.51694\n",
      "step: 36107\n",
      "loss: 13.015176773071289\n",
      "steps per second: 0.51679\n",
      "step: 36108\n",
      "loss: 12.760708808898926\n",
      "steps per second: 0.52397\n",
      "step: 36109\n",
      "loss: 13.130197525024414\n",
      "steps per second: 0.60844\n",
      "step: 36110\n",
      "loss: 12.53721809387207\n",
      "steps per second: 0.51787\n",
      "step: 36111\n",
      "loss: 13.138935089111328\n",
      "steps per second: 0.54300\n",
      "step: 36112\n",
      "loss: 12.59566879272461\n",
      "steps per second: 0.57611\n",
      "step: 36113\n",
      "loss: 12.590045928955078\n",
      "steps per second: 0.54937\n",
      "step: 36114\n",
      "loss: 12.536322593688965\n",
      "steps per second: 0.51994\n",
      "step: 36115\n",
      "loss: 12.478229522705078\n",
      "steps per second: 0.57368\n",
      "step: 36116\n",
      "loss: 13.109345436096191\n",
      "steps per second: 0.55155\n",
      "step: 36117\n",
      "loss: 12.953136444091797\n",
      "steps per second: 0.60839\n",
      "step: 36118\n",
      "loss: 13.340468406677246\n",
      "steps per second: 0.56131\n",
      "step: 36119\n",
      "loss: 12.985626220703125\n",
      "steps per second: 0.50682\n",
      "step: 36120\n",
      "loss: 12.809545516967773\n",
      "steps per second: 0.57464\n",
      "step: 36121\n",
      "loss: 13.0746488571167\n",
      "steps per second: 0.55793\n",
      "step: 36122\n",
      "loss: 12.67023754119873\n",
      "steps per second: 0.54281\n",
      "step: 36123\n",
      "loss: 12.773137092590332\n",
      "steps per second: 0.53398\n",
      "step: 36124\n",
      "loss: 12.868316650390625\n",
      "steps per second: 0.52698\n",
      "step: 36125\n",
      "loss: 13.147533416748047\n",
      "steps per second: 0.55110\n",
      "step: 36126\n",
      "loss: 12.554718017578125\n",
      "steps per second: 0.53462\n",
      "step: 36127\n",
      "loss: 13.196059226989746\n",
      "steps per second: 0.60669\n",
      "step: 36128\n",
      "loss: 12.824244499206543\n",
      "steps per second: 0.56170\n",
      "step: 36129\n",
      "loss: 13.196249961853027\n",
      "steps per second: 0.57439\n",
      "step: 36130\n",
      "loss: 13.238038063049316\n",
      "steps per second: 0.51954\n",
      "step: 36131\n",
      "loss: 12.870390892028809\n",
      "steps per second: 0.47830\n",
      "step: 36132\n",
      "loss: 12.608565330505371\n",
      "steps per second: 0.53725\n",
      "step: 36133\n",
      "loss: 13.080182075500488\n",
      "steps per second: 0.54485\n",
      "step: 36134\n",
      "loss: 13.020341873168945\n",
      "steps per second: 0.53359\n",
      "step: 36135\n",
      "loss: 12.80422592163086\n",
      "steps per second: 0.56914\n",
      "step: 36136\n",
      "loss: 13.053948402404785\n",
      "steps per second: 0.54882\n",
      "step: 36137\n",
      "loss: 12.804720878601074\n",
      "steps per second: 0.55639\n",
      "step: 36138\n",
      "loss: 12.862022399902344\n",
      "steps per second: 0.54406\n",
      "step: 36139\n",
      "loss: 11.905135154724121\n",
      "steps per second: 0.53508\n",
      "step: 36140\n",
      "loss: 12.04602336883545\n",
      "steps per second: 0.59187\n",
      "step: 36141\n",
      "loss: 12.804057121276855\n",
      "steps per second: 0.55829\n",
      "step: 36142\n",
      "loss: 12.932117462158203\n",
      "steps per second: 0.52864\n",
      "step: 36143\n",
      "loss: 12.375670433044434\n",
      "steps per second: 0.53202\n",
      "step: 36144\n",
      "loss: 12.774492263793945\n",
      "steps per second: 0.53537\n",
      "step: 36145\n",
      "loss: 12.155173301696777\n",
      "steps per second: 0.53413\n",
      "step: 36146\n",
      "loss: 12.84203815460205\n",
      "steps per second: 0.53328\n",
      "step: 36147\n",
      "loss: 12.493059158325195\n",
      "steps per second: 0.51775\n",
      "step: 36148\n",
      "loss: 13.398122787475586\n",
      "steps per second: 0.52557\n",
      "step: 36149\n",
      "loss: 12.719820022583008\n",
      "steps per second: 0.51643\n",
      "step: 36150\n",
      "loss: 12.793432235717773\n",
      "steps per second: 0.57266\n",
      "step: 36151\n",
      "loss: 12.717942237854004\n",
      "steps per second: 0.51777\n",
      "step: 36152\n",
      "loss: 12.875418663024902\n",
      "steps per second: 0.53735\n",
      "step: 36153\n",
      "loss: 12.771284103393555\n",
      "steps per second: 0.54855\n",
      "step: 36154\n",
      "loss: 12.909472465515137\n",
      "steps per second: 0.60033\n",
      "step: 36155\n",
      "loss: 12.620596885681152\n",
      "steps per second: 0.53609\n",
      "step: 36156\n",
      "loss: 12.459819793701172\n",
      "steps per second: 0.54001\n",
      "step: 36157\n",
      "loss: 12.376795768737793\n",
      "steps per second: 0.56173\n",
      "step: 36158\n",
      "loss: 13.306413650512695\n",
      "steps per second: 0.53645\n",
      "step: 36159\n",
      "loss: 12.629865646362305\n",
      "steps per second: 0.46044\n",
      "step: 36160\n",
      "loss: 12.873522758483887\n",
      "steps per second: 0.52527\n",
      "step: 36161\n",
      "loss: 12.764735221862793\n",
      "steps per second: 0.55003\n",
      "step: 36162\n",
      "loss: 13.4405517578125\n",
      "steps per second: 0.57210\n",
      "step: 36163\n",
      "loss: 13.129199028015137\n",
      "steps per second: 0.53729\n",
      "step: 36164\n",
      "loss: 12.939981460571289\n",
      "steps per second: 0.50203\n",
      "step: 36165\n",
      "loss: 12.66021728515625\n",
      "steps per second: 0.54560\n",
      "step: 36166\n",
      "loss: 12.667001724243164\n",
      "steps per second: 0.51519\n",
      "step: 36167\n",
      "loss: 13.449905395507812\n",
      "steps per second: 0.52969\n",
      "step: 36168\n",
      "loss: 12.268248558044434\n",
      "steps per second: 0.55239\n",
      "step: 36169\n",
      "loss: 12.512239456176758\n",
      "steps per second: 0.52395\n",
      "step: 36170\n",
      "loss: 12.587968826293945\n",
      "steps per second: 0.52442\n",
      "step: 36171\n",
      "loss: 12.635363578796387\n",
      "steps per second: 0.50457\n",
      "step: 36172\n",
      "loss: 12.917677879333496\n",
      "steps per second: 0.53630\n",
      "step: 36173\n",
      "loss: 12.73612117767334\n",
      "steps per second: 0.50307\n",
      "step: 36174\n",
      "loss: 12.583148956298828\n",
      "steps per second: 0.51042\n",
      "step: 36175\n",
      "loss: 12.680057525634766\n",
      "steps per second: 0.53575\n",
      "step: 36176\n",
      "loss: 12.239938735961914\n",
      "steps per second: 0.54034\n",
      "step: 36177\n",
      "loss: 12.792007446289062\n",
      "steps per second: 0.52206\n",
      "step: 36178\n",
      "loss: 12.717422485351562\n",
      "steps per second: 0.48680\n",
      "step: 36179\n",
      "loss: 12.51998233795166\n",
      "steps per second: 0.50909\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8079003691673279, layer: 11\n",
      "saving at step 36179\n",
      "----------\n",
      "\n",
      "\n",
      "step: 36180\n",
      "loss: 12.638575553894043\n",
      "steps per second: 0.26256\n",
      "step: 36181\n",
      "loss: 12.663684844970703\n",
      "steps per second: 0.47181\n",
      "step: 36182\n",
      "loss: 12.984065055847168\n",
      "steps per second: 0.52675\n",
      "step: 36183\n",
      "loss: 13.07049560546875\n",
      "steps per second: 0.51227\n",
      "step: 36184\n",
      "loss: 12.832955360412598\n",
      "steps per second: 0.55503\n",
      "step: 36185\n",
      "loss: 12.894818305969238\n",
      "steps per second: 0.53640\n",
      "step: 36186\n",
      "loss: 12.539307594299316\n",
      "steps per second: 0.56487\n",
      "step: 36187\n",
      "loss: 12.6405029296875\n",
      "steps per second: 0.50889\n",
      "step: 36188\n",
      "loss: 13.125306129455566\n",
      "steps per second: 0.53670\n",
      "step: 36189\n",
      "loss: 12.979756355285645\n",
      "steps per second: 0.52738\n",
      "step: 36190\n",
      "loss: 12.712923049926758\n",
      "steps per second: 0.52285\n",
      "step: 36191\n",
      "loss: 12.874943733215332\n",
      "steps per second: 0.53068\n",
      "step: 36192\n",
      "loss: 12.711172103881836\n",
      "steps per second: 0.54799\n",
      "step: 36193\n",
      "loss: 12.574771881103516\n",
      "steps per second: 0.54059\n",
      "step: 36194\n",
      "loss: 12.739053726196289\n",
      "steps per second: 0.57776\n",
      "step: 36195\n",
      "loss: 11.993938446044922\n",
      "steps per second: 0.56594\n",
      "step: 36196\n",
      "loss: 12.558581352233887\n",
      "steps per second: 0.55534\n",
      "step: 36197\n",
      "loss: 11.755948066711426\n",
      "steps per second: 0.56430\n",
      "step: 36198\n",
      "loss: 12.53433895111084\n",
      "steps per second: 0.55041\n",
      "step: 36199\n",
      "loss: 12.57140827178955\n",
      "steps per second: 0.54127\n",
      "step: 36200\n",
      "loss: 13.062278747558594\n",
      "steps per second: 0.55517\n",
      "step: 36201\n",
      "loss: 12.957608222961426\n",
      "steps per second: 0.53310\n",
      "step: 36202\n",
      "loss: 12.274011611938477\n",
      "steps per second: 0.54537\n",
      "step: 36203\n",
      "loss: 12.717082023620605\n",
      "steps per second: 0.56312\n",
      "step: 36204\n",
      "loss: 12.228561401367188\n",
      "steps per second: 0.52861\n",
      "step: 36205\n",
      "loss: 12.290247917175293\n",
      "steps per second: 0.52945\n",
      "step: 36206\n",
      "loss: 12.284829139709473\n",
      "steps per second: 0.60776\n",
      "step: 36207\n",
      "loss: 13.21730899810791\n",
      "steps per second: 0.54519\n",
      "step: 36208\n",
      "loss: 12.928742408752441\n",
      "steps per second: 0.55174\n",
      "step: 36209\n",
      "loss: 12.937725067138672\n",
      "steps per second: 0.54785\n",
      "step: 36210\n",
      "loss: 12.92576789855957\n",
      "steps per second: 0.52821\n",
      "step: 36211\n",
      "loss: 12.99044132232666\n",
      "steps per second: 0.53123\n",
      "step: 36212\n",
      "loss: 12.877171516418457\n",
      "steps per second: 0.54509\n",
      "step: 36213\n",
      "loss: 12.636216163635254\n",
      "steps per second: 0.60860\n",
      "step: 36214\n",
      "loss: 13.09594440460205\n",
      "steps per second: 0.55247\n",
      "step: 36215\n",
      "loss: 12.491820335388184\n",
      "steps per second: 0.53679\n",
      "step: 36216\n",
      "loss: 12.707319259643555\n",
      "steps per second: 0.55281\n",
      "step: 36217\n",
      "loss: 13.011102676391602\n",
      "steps per second: 0.49538\n",
      "step: 36218\n",
      "loss: 12.355497360229492\n",
      "steps per second: 0.55263\n",
      "step: 36219\n",
      "loss: 12.67566204071045\n",
      "steps per second: 0.56478\n",
      "step: 36220\n",
      "loss: 13.434925079345703\n",
      "steps per second: 0.53995\n",
      "step: 36221\n",
      "loss: 12.454896926879883\n",
      "steps per second: 0.53600\n",
      "step: 36222\n",
      "loss: 12.527068138122559\n",
      "steps per second: 0.52652\n",
      "step: 36223\n",
      "loss: 12.255480766296387\n",
      "steps per second: 0.56065\n",
      "step: 36224\n",
      "loss: 13.009572982788086\n",
      "steps per second: 0.54503\n",
      "step: 36225\n",
      "loss: 12.533299446105957\n",
      "steps per second: 0.53620\n",
      "step: 36226\n",
      "loss: 12.642692565917969\n",
      "steps per second: 0.52943\n",
      "step: 36227\n",
      "loss: 13.320623397827148\n",
      "steps per second: 0.57310\n",
      "step: 36228\n",
      "loss: 12.827738761901855\n",
      "steps per second: 0.55234\n",
      "step: 36229\n",
      "loss: 12.777703285217285\n",
      "steps per second: 0.53199\n",
      "step: 36230\n",
      "loss: 12.78178882598877\n",
      "steps per second: 0.51751\n",
      "step: 36231\n",
      "loss: 13.265421867370605\n",
      "steps per second: 0.53644\n",
      "step: 36232\n",
      "loss: 12.627501487731934\n",
      "steps per second: 0.53793\n",
      "step: 36233\n",
      "loss: 13.114239692687988\n",
      "steps per second: 0.48983\n",
      "step: 36234\n",
      "loss: 12.657069206237793\n",
      "steps per second: 0.57727\n",
      "step: 36235\n",
      "loss: 13.050822257995605\n",
      "steps per second: 0.53784\n",
      "step: 36236\n",
      "loss: 12.606423377990723\n",
      "steps per second: 0.51846\n",
      "step: 36237\n",
      "loss: 12.869296073913574\n",
      "steps per second: 0.50501\n",
      "step: 36238\n",
      "loss: 13.053929328918457\n",
      "steps per second: 0.49142\n",
      "step: 36239\n",
      "loss: 13.378630638122559\n",
      "steps per second: 0.52611\n",
      "step: 36240\n",
      "loss: 12.727725982666016\n",
      "steps per second: 0.54948\n",
      "step: 36241\n",
      "loss: 12.778099060058594\n",
      "steps per second: 0.53161\n",
      "step: 36242\n",
      "loss: 12.47479248046875\n",
      "steps per second: 0.53512\n",
      "step: 36243\n",
      "loss: 13.170258522033691\n",
      "steps per second: 0.52378\n",
      "step: 36244\n",
      "loss: 12.951892852783203\n",
      "steps per second: 0.52285\n",
      "step: 36245\n",
      "loss: 13.548154830932617\n",
      "steps per second: 0.57715\n",
      "step: 36246\n",
      "loss: 12.98735237121582\n",
      "steps per second: 0.45808\n",
      "step: 36247\n",
      "loss: 12.774662017822266\n",
      "steps per second: 0.49681\n",
      "step: 36248\n",
      "loss: 13.15771484375\n",
      "steps per second: 0.50430\n",
      "step: 36249\n",
      "loss: 12.89813232421875\n",
      "steps per second: 0.49673\n",
      "step: 36250\n",
      "loss: 12.777711868286133\n",
      "steps per second: 0.51842\n",
      "step: 36251\n",
      "loss: 12.947039604187012\n",
      "steps per second: 0.57302\n",
      "step: 36252\n",
      "loss: 13.05303955078125\n",
      "steps per second: 0.52454\n",
      "step: 36253\n",
      "loss: 12.606779098510742\n",
      "steps per second: 0.54043\n",
      "step: 36254\n",
      "loss: 12.663104057312012\n",
      "steps per second: 0.56218\n",
      "step: 36255\n",
      "loss: 13.199152946472168\n",
      "steps per second: 0.53584\n",
      "step: 36256\n",
      "loss: 12.425618171691895\n",
      "steps per second: 0.53062\n",
      "step: 36257\n",
      "loss: 12.827638626098633\n",
      "steps per second: 0.52214\n",
      "step: 36258\n",
      "loss: 12.698915481567383\n",
      "steps per second: 0.56497\n",
      "step: 36259\n",
      "loss: 13.068184852600098\n",
      "steps per second: 0.48080\n",
      "step: 36260\n",
      "loss: 12.771940231323242\n",
      "steps per second: 0.49103\n",
      "step: 36261\n",
      "loss: 12.821854591369629\n",
      "steps per second: 0.52503\n",
      "step: 36262\n",
      "loss: 12.759702682495117\n",
      "steps per second: 0.53517\n",
      "step: 36263\n",
      "loss: 12.28520679473877\n",
      "steps per second: 0.52540\n",
      "step: 36264\n",
      "loss: 12.781913757324219\n",
      "steps per second: 0.58283\n",
      "step: 36265\n",
      "loss: 13.087790489196777\n",
      "steps per second: 0.55227\n",
      "step: 36266\n",
      "loss: 13.150993347167969\n",
      "steps per second: 0.50138\n",
      "step: 36267\n",
      "loss: 13.067449569702148\n",
      "steps per second: 0.51735\n",
      "step: 36268\n",
      "loss: 12.85960865020752\n",
      "steps per second: 0.58037\n",
      "step: 36269\n",
      "loss: 12.698626518249512\n",
      "steps per second: 0.49477\n",
      "step: 36270\n",
      "loss: 13.02139949798584\n",
      "steps per second: 0.53152\n",
      "step: 36271\n",
      "loss: 12.787202835083008\n",
      "steps per second: 0.55513\n",
      "step: 36272\n",
      "loss: 12.677290916442871\n",
      "steps per second: 0.53036\n",
      "step: 36273\n",
      "loss: 12.495217323303223\n",
      "steps per second: 0.56205\n",
      "step: 36274\n",
      "loss: 12.738377571105957\n",
      "steps per second: 0.52252\n",
      "step: 36275\n",
      "loss: 12.891845703125\n",
      "steps per second: 0.52425\n",
      "step: 36276\n",
      "loss: 12.141651153564453\n",
      "steps per second: 0.47199\n",
      "step: 36277\n",
      "loss: 13.3544282913208\n",
      "steps per second: 0.52275\n",
      "step: 36278\n",
      "loss: 12.977648735046387\n",
      "steps per second: 0.51228\n",
      "step: 36279\n",
      "loss: 11.623950004577637\n",
      "steps per second: 0.54285\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7114349007606506, layer: 11\n",
      "saving at step 36279\n",
      "----------\n",
      "\n",
      "\n",
      "step: 36280\n",
      "loss: 12.72912883758545\n",
      "steps per second: 0.26579\n",
      "step: 36281\n",
      "loss: 13.023776054382324\n",
      "steps per second: 0.58181\n",
      "step: 36282\n",
      "loss: 12.769330024719238\n",
      "steps per second: 0.49816\n",
      "step: 36283\n",
      "loss: 13.21824836730957\n",
      "steps per second: 0.48729\n",
      "step: 36284\n",
      "loss: 12.47430419921875\n",
      "steps per second: 0.51568\n",
      "step: 36285\n",
      "loss: 12.563504219055176\n",
      "steps per second: 0.52587\n",
      "step: 36286\n",
      "loss: 12.925820350646973\n",
      "steps per second: 0.48712\n",
      "step: 36287\n",
      "loss: 13.093862533569336\n",
      "steps per second: 0.52062\n",
      "step: 36288\n",
      "loss: 12.918030738830566\n",
      "steps per second: 0.51290\n",
      "step: 36289\n",
      "loss: 12.783834457397461\n",
      "steps per second: 0.51463\n",
      "step: 36290\n",
      "loss: 13.238849639892578\n",
      "steps per second: 0.51769\n",
      "step: 36291\n",
      "loss: 12.859492301940918\n",
      "steps per second: 0.52810\n",
      "step: 36292\n",
      "loss: 12.723877906799316\n",
      "steps per second: 0.51088\n",
      "step: 36293\n",
      "loss: 12.616219520568848\n",
      "steps per second: 0.48155\n",
      "step: 36294\n",
      "loss: 12.881163597106934\n",
      "steps per second: 0.54826\n",
      "step: 36295\n",
      "loss: 13.01385498046875\n",
      "steps per second: 0.55502\n",
      "step: 36296\n",
      "loss: 12.116052627563477\n",
      "steps per second: 0.54254\n",
      "step: 36297\n",
      "loss: 12.364013671875\n",
      "steps per second: 0.51055\n",
      "step: 36298\n",
      "loss: 12.694957733154297\n",
      "steps per second: 0.49527\n",
      "step: 36299\n",
      "loss: 12.964184761047363\n",
      "steps per second: 0.51293\n",
      "step: 36300\n",
      "loss: 12.565473556518555\n",
      "steps per second: 0.52711\n",
      "step: 36301\n",
      "loss: 12.74207878112793\n",
      "steps per second: 0.51915\n",
      "step: 36302\n",
      "loss: 13.535796165466309\n",
      "steps per second: 0.55063\n",
      "step: 36303\n",
      "loss: 12.610712051391602\n",
      "steps per second: 0.55337\n",
      "step: 36304\n",
      "loss: 12.82125473022461\n",
      "steps per second: 0.50742\n",
      "step: 36305\n",
      "loss: 12.65087890625\n",
      "steps per second: 0.53339\n",
      "step: 36306\n",
      "loss: 12.814230918884277\n",
      "steps per second: 0.54356\n",
      "step: 36307\n",
      "loss: 12.21682357788086\n",
      "steps per second: 0.54887\n",
      "step: 36308\n",
      "loss: 12.830084800720215\n",
      "steps per second: 0.52634\n",
      "step: 36309\n",
      "loss: 12.488889694213867\n",
      "steps per second: 0.49433\n",
      "step: 36310\n",
      "loss: 12.81543254852295\n",
      "steps per second: 0.51235\n",
      "step: 36311\n",
      "loss: 12.732278823852539\n",
      "steps per second: 0.52865\n",
      "step: 36312\n",
      "loss: 12.3577299118042\n",
      "steps per second: 0.51954\n",
      "step: 36313\n",
      "loss: 12.737998008728027\n",
      "steps per second: 0.56332\n",
      "step: 36314\n",
      "loss: 12.643192291259766\n",
      "steps per second: 0.54040\n",
      "step: 36315\n",
      "loss: 13.20371150970459\n",
      "steps per second: 0.57787\n",
      "step: 36316\n",
      "loss: 12.713722229003906\n",
      "steps per second: 0.57500\n",
      "step: 36317\n",
      "loss: 12.895509719848633\n",
      "steps per second: 0.60808\n",
      "step: 36318\n",
      "loss: 12.654080390930176\n",
      "steps per second: 0.56073\n",
      "step: 36319\n",
      "loss: 13.088900566101074\n",
      "steps per second: 0.54904\n",
      "step: 36320\n",
      "loss: 12.781533241271973\n",
      "steps per second: 0.60894\n",
      "step: 36321\n",
      "loss: 12.73957347869873\n",
      "steps per second: 0.55182\n",
      "step: 36322\n",
      "loss: 12.374093055725098\n",
      "steps per second: 0.56176\n",
      "step: 36323\n",
      "loss: 13.107121467590332\n",
      "steps per second: 0.51801\n",
      "step: 36324\n",
      "loss: 12.381205558776855\n",
      "steps per second: 0.52780\n",
      "step: 36325\n",
      "loss: 12.459835052490234\n",
      "steps per second: 0.54423\n",
      "step: 36326\n",
      "loss: 12.927000999450684\n",
      "steps per second: 0.53754\n",
      "step: 36327\n",
      "loss: 13.364248275756836\n",
      "steps per second: 0.52112\n",
      "step: 36328\n",
      "loss: 13.143757820129395\n",
      "steps per second: 0.49760\n",
      "step: 36329\n",
      "loss: 12.4761381149292\n",
      "steps per second: 0.54495\n",
      "step: 36330\n",
      "loss: 13.056467056274414\n",
      "steps per second: 0.56340\n",
      "step: 36331\n",
      "loss: 12.958846092224121\n",
      "steps per second: 0.53013\n",
      "step: 36332\n",
      "loss: 12.96646785736084\n",
      "steps per second: 0.51923\n",
      "step: 36333\n",
      "loss: 12.703340530395508\n",
      "steps per second: 0.56356\n",
      "step: 36334\n",
      "loss: 12.786561965942383\n",
      "steps per second: 0.54257\n",
      "step: 36335\n",
      "loss: 12.625001907348633\n",
      "steps per second: 0.52854\n",
      "step: 36336\n",
      "loss: 12.963719367980957\n",
      "steps per second: 0.52900\n",
      "step: 36337\n",
      "loss: 12.796089172363281\n",
      "steps per second: 0.54848\n",
      "step: 36338\n",
      "loss: 12.739418029785156\n",
      "steps per second: 0.53076\n",
      "step: 36339\n",
      "loss: 13.10929012298584\n",
      "steps per second: 0.51814\n",
      "step: 36340\n",
      "loss: 12.832123756408691\n",
      "steps per second: 0.56027\n",
      "step: 36341\n",
      "loss: 13.00938606262207\n",
      "steps per second: 0.55996\n",
      "step: 36342\n",
      "loss: 12.6579008102417\n",
      "steps per second: 0.54407\n",
      "step: 36343\n",
      "loss: 12.891888618469238\n",
      "steps per second: 0.54958\n",
      "step: 36344\n",
      "loss: 13.326818466186523\n",
      "steps per second: 0.60474\n",
      "step: 36345\n",
      "loss: 13.237031936645508\n",
      "steps per second: 0.54978\n",
      "step: 36346\n",
      "loss: 13.1279296875\n",
      "steps per second: 0.57975\n",
      "step: 36347\n",
      "loss: 13.044325828552246\n",
      "steps per second: 0.60467\n",
      "step: 36348\n",
      "loss: 12.923480987548828\n",
      "steps per second: 0.56164\n",
      "step: 36349\n",
      "loss: 12.730871200561523\n",
      "steps per second: 0.56326\n",
      "step: 36350\n",
      "loss: 13.512269020080566\n",
      "steps per second: 0.56175\n",
      "step: 36351\n",
      "loss: 12.179966926574707\n",
      "steps per second: 0.60937\n",
      "step: 36352\n",
      "loss: 12.39926815032959\n",
      "steps per second: 0.52704\n",
      "step: 36353\n",
      "loss: 12.728025436401367\n",
      "steps per second: 0.54298\n",
      "step: 36354\n",
      "loss: 13.210335731506348\n",
      "steps per second: 0.54834\n",
      "step: 36355\n",
      "loss: 12.867195129394531\n",
      "steps per second: 0.53711\n",
      "step: 36356\n",
      "loss: 12.166979789733887\n",
      "steps per second: 0.56256\n",
      "step: 36357\n",
      "loss: 12.809136390686035\n",
      "steps per second: 0.57502\n",
      "step: 36358\n",
      "loss: 13.062070846557617\n",
      "steps per second: 0.51058\n",
      "step: 36359\n",
      "loss: 13.363738059997559\n",
      "steps per second: 0.54280\n",
      "step: 36360\n",
      "loss: 12.576888084411621\n",
      "steps per second: 0.54648\n",
      "step: 36361\n",
      "loss: 12.92198371887207\n",
      "steps per second: 0.53006\n",
      "step: 36362\n",
      "loss: 12.69070816040039\n",
      "steps per second: 0.57821\n",
      "step: 36363\n",
      "loss: 12.906881332397461\n",
      "steps per second: 0.56152\n",
      "step: 36364\n",
      "loss: 13.543728828430176\n",
      "steps per second: 0.56850\n",
      "step: 36365\n",
      "loss: 12.889676094055176\n",
      "steps per second: 0.56222\n",
      "step: 36366\n",
      "loss: 13.233832359313965\n",
      "steps per second: 0.54222\n",
      "step: 36367\n",
      "loss: 13.207226753234863\n",
      "steps per second: 0.53546\n",
      "step: 36368\n",
      "loss: 12.67080307006836\n",
      "steps per second: 0.53821\n",
      "step: 36369\n",
      "loss: 13.35185432434082\n",
      "steps per second: 0.53662\n",
      "step: 36370\n",
      "loss: 13.262592315673828\n",
      "steps per second: 0.53533\n",
      "step: 36371\n",
      "loss: 12.782876968383789\n",
      "steps per second: 0.51060\n",
      "step: 36372\n",
      "loss: 12.459426879882812\n",
      "steps per second: 0.54060\n",
      "step: 36373\n",
      "loss: 13.033926010131836\n",
      "steps per second: 0.53831\n",
      "step: 36374\n",
      "loss: 12.496737480163574\n",
      "steps per second: 0.54934\n",
      "step: 36375\n",
      "loss: 12.475959777832031\n",
      "steps per second: 0.56011\n",
      "step: 36376\n",
      "loss: 13.147226333618164\n",
      "steps per second: 0.52216\n",
      "step: 36377\n",
      "loss: 12.526511192321777\n",
      "steps per second: 0.55281\n",
      "step: 36378\n",
      "loss: 12.706693649291992\n",
      "steps per second: 0.54316\n",
      "step: 36379\n",
      "loss: 12.738358497619629\n",
      "steps per second: 0.53795\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8466012477874756, layer: 12\n",
      "saving at step 36379\n",
      "----------\n",
      "\n",
      "\n",
      "step: 36380\n",
      "loss: 13.266622543334961\n",
      "steps per second: 0.26815\n",
      "step: 36381\n",
      "loss: 12.663384437561035\n",
      "steps per second: 0.54492\n",
      "step: 36382\n",
      "loss: 13.070027351379395\n",
      "steps per second: 0.54239\n",
      "step: 36383\n",
      "loss: 13.031804084777832\n",
      "steps per second: 0.49148\n",
      "step: 36384\n",
      "loss: 12.504880905151367\n",
      "steps per second: 0.54347\n",
      "step: 36385\n",
      "loss: 12.851712226867676\n",
      "steps per second: 0.54337\n",
      "step: 36386\n",
      "loss: 13.172101020812988\n",
      "steps per second: 0.54473\n",
      "step: 36387\n",
      "loss: 13.130674362182617\n",
      "steps per second: 0.55925\n",
      "step: 36388\n",
      "loss: 13.706212997436523\n",
      "steps per second: 0.53539\n",
      "step: 36389\n",
      "loss: 13.052384376525879\n",
      "steps per second: 0.50217\n",
      "step: 36390\n",
      "loss: 12.928552627563477\n",
      "steps per second: 0.55614\n",
      "step: 36391\n",
      "loss: 12.812664031982422\n",
      "steps per second: 0.50802\n",
      "step: 36392\n",
      "loss: 12.923276901245117\n",
      "steps per second: 0.52271\n",
      "step: 36393\n",
      "loss: 11.93472671508789\n",
      "steps per second: 0.51159\n",
      "step: 36394\n",
      "loss: 13.207249641418457\n",
      "steps per second: 0.48913\n",
      "step: 36395\n",
      "loss: 12.688169479370117\n",
      "steps per second: 0.51071\n",
      "step: 36396\n",
      "loss: 13.069533348083496\n",
      "steps per second: 0.56088\n",
      "step: 36397\n",
      "loss: 13.111075401306152\n",
      "steps per second: 0.55208\n",
      "step: 36398\n",
      "loss: 13.131196975708008\n",
      "steps per second: 0.57530\n",
      "step: 36399\n",
      "loss: 13.262022972106934\n",
      "steps per second: 0.51994\n",
      "step: 36400\n",
      "loss: 12.471709251403809\n",
      "steps per second: 0.52660\n",
      "step: 36401\n",
      "loss: 13.152393341064453\n",
      "steps per second: 0.54481\n",
      "step: 36402\n",
      "loss: 12.520379066467285\n",
      "steps per second: 0.53733\n",
      "step: 36403\n",
      "loss: 12.379711151123047\n",
      "steps per second: 0.57297\n",
      "step: 36404\n",
      "loss: 12.339591979980469\n",
      "steps per second: 0.54423\n",
      "step: 36405\n",
      "loss: 12.341184616088867\n",
      "steps per second: 0.51740\n",
      "step: 36406\n",
      "loss: 12.881294250488281\n",
      "steps per second: 0.53661\n",
      "step: 36407\n",
      "loss: 12.472949981689453\n",
      "steps per second: 0.55948\n",
      "step: 36408\n",
      "loss: 13.050511360168457\n",
      "steps per second: 0.55408\n",
      "step: 36409\n",
      "loss: 12.911602020263672\n",
      "steps per second: 0.53790\n",
      "step: 36410\n",
      "loss: 12.669037818908691\n",
      "steps per second: 0.54360\n",
      "step: 36411\n",
      "loss: 13.085667610168457\n",
      "steps per second: 0.53585\n",
      "step: 36412\n",
      "loss: 13.263252258300781\n",
      "steps per second: 0.56367\n",
      "step: 36413\n",
      "loss: 12.876991271972656\n",
      "steps per second: 0.53442\n",
      "step: 36414\n",
      "loss: 13.338994979858398\n",
      "steps per second: 0.54110\n",
      "step: 36415\n",
      "loss: 13.027482032775879\n",
      "steps per second: 0.54416\n",
      "step: 36416\n",
      "loss: 13.000566482543945\n",
      "steps per second: 0.54421\n",
      "step: 36417\n",
      "loss: 12.978163719177246\n",
      "steps per second: 0.52962\n",
      "step: 36418\n",
      "loss: 12.508493423461914\n",
      "steps per second: 0.56241\n",
      "step: 36419\n",
      "loss: 12.946352005004883\n",
      "steps per second: 0.54603\n",
      "step: 36420\n",
      "loss: 13.416109085083008\n",
      "steps per second: 0.56217\n",
      "step: 36421\n",
      "loss: 13.042284965515137\n",
      "steps per second: 0.54989\n",
      "step: 36422\n",
      "loss: 12.292191505432129\n",
      "steps per second: 0.54356\n",
      "step: 36423\n",
      "loss: 12.573067665100098\n",
      "steps per second: 0.54674\n",
      "step: 36424\n",
      "loss: 12.774702072143555\n",
      "steps per second: 0.56029\n",
      "step: 36425\n",
      "loss: 12.765986442565918\n",
      "steps per second: 0.55829\n",
      "step: 36426\n",
      "loss: 13.46235466003418\n",
      "steps per second: 0.55087\n",
      "step: 36427\n",
      "loss: 12.971858024597168\n",
      "steps per second: 0.52753\n",
      "step: 36428\n",
      "loss: 13.354117393493652\n",
      "steps per second: 0.55769\n",
      "step: 36429\n",
      "loss: 12.624872207641602\n",
      "steps per second: 0.51288\n",
      "step: 36430\n",
      "loss: 12.632227897644043\n",
      "steps per second: 0.57578\n",
      "step: 36431\n",
      "loss: 12.506484031677246\n",
      "steps per second: 0.55136\n",
      "step: 36432\n",
      "loss: 12.708837509155273\n",
      "steps per second: 0.52776\n",
      "step: 36433\n",
      "loss: 12.404983520507812\n",
      "steps per second: 0.54421\n",
      "step: 36434\n",
      "loss: 12.991329193115234\n",
      "steps per second: 0.56004\n",
      "step: 36435\n",
      "loss: 12.633926391601562\n",
      "steps per second: 0.54195\n",
      "step: 36436\n",
      "loss: 13.0239896774292\n",
      "steps per second: 0.55897\n",
      "step: 36437\n",
      "loss: 12.684247970581055\n",
      "steps per second: 0.54891\n",
      "step: 36438\n",
      "loss: 12.781693458557129\n",
      "steps per second: 0.52326\n",
      "step: 36439\n",
      "loss: 13.064616203308105\n",
      "steps per second: 0.55721\n",
      "step: 36440\n",
      "loss: 12.512657165527344\n",
      "steps per second: 0.53405\n",
      "step: 36441\n",
      "loss: 13.126908302307129\n",
      "steps per second: 0.56158\n",
      "step: 36442\n",
      "loss: 12.798377990722656\n",
      "steps per second: 0.55727\n",
      "step: 36443\n",
      "loss: 13.035174369812012\n",
      "steps per second: 0.55825\n",
      "step: 36444\n",
      "loss: 13.332427978515625\n",
      "steps per second: 0.60208\n",
      "step: 36445\n",
      "loss: 12.752081871032715\n",
      "steps per second: 0.54923\n",
      "step: 36446\n",
      "loss: 12.411356925964355\n",
      "steps per second: 0.54283\n",
      "step: 36447\n",
      "loss: 13.040654182434082\n",
      "steps per second: 0.54051\n",
      "step: 36448\n",
      "loss: 12.874322891235352\n",
      "steps per second: 0.54771\n",
      "step: 36449\n",
      "loss: 12.205467224121094\n",
      "steps per second: 0.52576\n",
      "step: 36450\n",
      "loss: 12.056854248046875\n",
      "steps per second: 0.57601\n",
      "step: 36451\n",
      "loss: 12.414241790771484\n",
      "steps per second: 0.55745\n",
      "step: 36452\n",
      "loss: 12.691853523254395\n",
      "steps per second: 0.57341\n",
      "step: 36453\n",
      "loss: 12.851139068603516\n",
      "steps per second: 0.56200\n",
      "step: 36454\n",
      "loss: 12.081862449645996\n",
      "steps per second: 0.54030\n",
      "step: 36455\n",
      "loss: 12.884181022644043\n",
      "steps per second: 0.54690\n",
      "step: 36456\n",
      "loss: 13.248690605163574\n",
      "steps per second: 0.54818\n",
      "step: 36457\n",
      "loss: 12.655495643615723\n",
      "steps per second: 0.56009\n",
      "step: 36458\n",
      "loss: 13.332509994506836\n",
      "steps per second: 0.53565\n",
      "step: 36459\n",
      "loss: 13.1712007522583\n",
      "steps per second: 0.49967\n",
      "step: 36460\n",
      "loss: 12.99059772491455\n",
      "steps per second: 0.53781\n",
      "step: 36461\n",
      "loss: 12.815084457397461\n",
      "steps per second: 0.53835\n",
      "step: 36462\n",
      "loss: 12.825583457946777\n",
      "steps per second: 0.56071\n",
      "step: 36463\n",
      "loss: 12.779802322387695\n",
      "steps per second: 0.60465\n",
      "step: 36464\n",
      "loss: 12.615283012390137\n",
      "steps per second: 0.49699\n",
      "step: 36465\n",
      "loss: 12.928833961486816\n",
      "steps per second: 0.60145\n",
      "step: 36466\n",
      "loss: 12.73579216003418\n",
      "steps per second: 0.58341\n",
      "step: 36467\n",
      "loss: 12.47469711303711\n",
      "steps per second: 0.52181\n",
      "step: 36468\n",
      "loss: 12.541285514831543\n",
      "steps per second: 0.55012\n",
      "step: 36469\n",
      "loss: 12.72477912902832\n",
      "steps per second: 0.53558\n",
      "step: 36470\n",
      "loss: 12.879138946533203\n",
      "steps per second: 0.53667\n",
      "step: 36471\n",
      "loss: 12.30247974395752\n",
      "steps per second: 0.49864\n",
      "step: 36472\n",
      "loss: 12.901235580444336\n",
      "steps per second: 0.52377\n",
      "step: 36473\n",
      "loss: 12.592996597290039\n",
      "steps per second: 0.52946\n",
      "step: 36474\n",
      "loss: 12.719035148620605\n",
      "steps per second: 0.57638\n",
      "step: 36475\n",
      "loss: 12.768601417541504\n",
      "steps per second: 0.51949\n",
      "step: 36476\n",
      "loss: 12.889737129211426\n",
      "steps per second: 0.57678\n",
      "step: 36477\n",
      "loss: 12.55709457397461\n",
      "steps per second: 0.51739\n",
      "step: 36478\n",
      "loss: 12.956843376159668\n",
      "steps per second: 0.54016\n",
      "step: 36479\n",
      "loss: 13.294825553894043\n",
      "steps per second: 0.54582\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8557742834091187, layer: 11\n",
      "saving at step 36479\n",
      "----------\n",
      "\n",
      "\n",
      "step: 36480\n",
      "loss: 12.363525390625\n",
      "steps per second: 0.29215\n",
      "step: 36481\n",
      "loss: 12.98904037475586\n",
      "steps per second: 0.54973\n",
      "step: 36482\n",
      "loss: 12.673733711242676\n",
      "steps per second: 0.53839\n",
      "step: 36483\n",
      "loss: 12.560279846191406\n",
      "steps per second: 0.53603\n",
      "step: 36484\n",
      "loss: 12.325183868408203\n",
      "steps per second: 0.52806\n",
      "step: 36485\n",
      "loss: 13.172986030578613\n",
      "steps per second: 0.54276\n",
      "step: 36486\n",
      "loss: 12.938488960266113\n",
      "steps per second: 0.51525\n",
      "step: 36487\n",
      "loss: 12.778210639953613\n",
      "steps per second: 0.47834\n",
      "step: 36488\n",
      "loss: 12.924400329589844\n",
      "steps per second: 0.57485\n",
      "step: 36489\n",
      "loss: 12.753079414367676\n",
      "steps per second: 0.54280\n",
      "step: 36490\n",
      "loss: 12.661290168762207\n",
      "steps per second: 0.54086\n",
      "step: 36491\n",
      "loss: 12.468167304992676\n",
      "steps per second: 0.54033\n",
      "step: 36492\n",
      "loss: 12.78064250946045\n",
      "steps per second: 0.50717\n",
      "step: 36493\n",
      "loss: 13.17287540435791\n",
      "steps per second: 0.54374\n",
      "step: 36494\n",
      "loss: 12.40573501586914\n",
      "steps per second: 0.53916\n",
      "step: 36495\n",
      "loss: 12.01593017578125\n",
      "steps per second: 0.54120\n",
      "step: 36496\n",
      "loss: 12.832937240600586\n",
      "steps per second: 0.50500\n",
      "step: 36497\n",
      "loss: 12.806539535522461\n",
      "steps per second: 0.54985\n",
      "step: 36498\n",
      "loss: 12.83126449584961\n",
      "steps per second: 0.60561\n",
      "step: 36499\n",
      "loss: 12.448675155639648\n",
      "steps per second: 0.57329\n",
      "step: 36500\n",
      "loss: 12.708833694458008\n",
      "steps per second: 0.50021\n",
      "step: 36501\n",
      "loss: 12.530437469482422\n",
      "steps per second: 0.57368\n",
      "step: 36502\n",
      "loss: 13.041213035583496\n",
      "steps per second: 0.57101\n",
      "step: 36503\n",
      "loss: 12.375850677490234\n",
      "steps per second: 0.55917\n",
      "step: 36504\n",
      "loss: 12.778633117675781\n",
      "steps per second: 0.54227\n",
      "step: 36505\n",
      "loss: 12.79875373840332\n",
      "steps per second: 0.50749\n",
      "step: 36506\n",
      "loss: 13.134990692138672\n",
      "steps per second: 0.54090\n",
      "step: 36507\n",
      "loss: 12.742198944091797\n",
      "steps per second: 0.53637\n",
      "step: 36508\n",
      "loss: 12.702202796936035\n",
      "steps per second: 0.54026\n",
      "step: 36509\n",
      "loss: 13.004870414733887\n",
      "steps per second: 0.51449\n",
      "step: 36510\n",
      "loss: 12.715574264526367\n",
      "steps per second: 0.54930\n",
      "step: 36511\n",
      "loss: 12.86540412902832\n",
      "steps per second: 0.52686\n",
      "step: 36512\n",
      "loss: 12.311816215515137\n",
      "steps per second: 0.54689\n",
      "step: 36513\n",
      "loss: 12.487537384033203\n",
      "steps per second: 0.54542\n",
      "step: 36514\n",
      "loss: 13.393778800964355\n",
      "steps per second: 0.54013\n",
      "step: 36515\n",
      "loss: 13.025094032287598\n",
      "steps per second: 0.56049\n",
      "step: 36516\n",
      "loss: 12.330782890319824\n",
      "steps per second: 0.53462\n",
      "step: 36517\n",
      "loss: 12.504033088684082\n",
      "steps per second: 0.53448\n",
      "step: 36518\n",
      "loss: 13.030949592590332\n",
      "steps per second: 0.59586\n",
      "step: 36519\n",
      "loss: 12.372567176818848\n",
      "steps per second: 0.50547\n",
      "step: 36520\n",
      "loss: 12.703863143920898\n",
      "steps per second: 0.59957\n",
      "step: 36521\n",
      "loss: 12.803771018981934\n",
      "steps per second: 0.54238\n",
      "step: 36522\n",
      "loss: 13.080700874328613\n",
      "steps per second: 0.51613\n",
      "step: 36523\n",
      "loss: 13.163592338562012\n",
      "steps per second: 0.53465\n",
      "step: 36524\n",
      "loss: 12.857687950134277\n",
      "steps per second: 0.54745\n",
      "step: 36525\n",
      "loss: 12.906497955322266\n",
      "steps per second: 0.57036\n",
      "step: 36526\n",
      "loss: 12.829930305480957\n",
      "steps per second: 0.53057\n",
      "step: 36527\n",
      "loss: 12.893527030944824\n",
      "steps per second: 0.60441\n",
      "step: 36528\n",
      "loss: 12.640835762023926\n",
      "steps per second: 0.60451\n",
      "step: 36529\n",
      "loss: 12.519700050354004\n",
      "steps per second: 0.55902\n",
      "step: 36530\n",
      "loss: 12.869245529174805\n",
      "steps per second: 0.54681\n",
      "step: 36531\n",
      "loss: 13.011163711547852\n",
      "steps per second: 0.52465\n",
      "step: 36532\n",
      "loss: 12.864262580871582\n",
      "steps per second: 0.54047\n",
      "step: 36533\n",
      "loss: 12.456128120422363\n",
      "steps per second: 0.56224\n",
      "step: 36534\n",
      "loss: 12.918667793273926\n",
      "steps per second: 0.51202\n",
      "step: 36535\n",
      "loss: 13.226737022399902\n",
      "steps per second: 0.49708\n",
      "step: 36536\n",
      "loss: 12.491427421569824\n",
      "steps per second: 0.54580\n",
      "step: 36537\n",
      "loss: 12.627195358276367\n",
      "steps per second: 0.55900\n",
      "step: 36538\n",
      "loss: 13.189873695373535\n",
      "steps per second: 0.52830\n",
      "step: 36539\n",
      "loss: 12.535438537597656\n",
      "steps per second: 0.54119\n",
      "step: 36540\n",
      "loss: 12.66252326965332\n",
      "steps per second: 0.55819\n",
      "step: 36541\n",
      "loss: 12.957917213439941\n",
      "steps per second: 0.55731\n",
      "step: 36542\n",
      "loss: 13.02807903289795\n",
      "steps per second: 0.51708\n",
      "step: 36543\n",
      "loss: 13.224177360534668\n",
      "steps per second: 0.53439\n",
      "step: 36544\n",
      "loss: 12.343621253967285\n",
      "steps per second: 0.57518\n",
      "step: 36545\n",
      "loss: 12.922284126281738\n",
      "steps per second: 0.51686\n",
      "step: 36546\n",
      "loss: 12.469576835632324\n",
      "steps per second: 0.54551\n",
      "step: 36547\n",
      "loss: 12.28031063079834\n",
      "steps per second: 0.60489\n",
      "step: 36548\n",
      "loss: 12.975113868713379\n",
      "steps per second: 0.53911\n",
      "step: 36549\n",
      "loss: 12.807744026184082\n",
      "steps per second: 0.54867\n",
      "step: 36550\n",
      "loss: 12.832723617553711\n",
      "steps per second: 0.56162\n",
      "step: 36551\n",
      "loss: 12.441055297851562\n",
      "steps per second: 0.52420\n",
      "step: 36552\n",
      "loss: 12.71233081817627\n",
      "steps per second: 0.54061\n",
      "step: 36553\n",
      "loss: 12.725851058959961\n",
      "steps per second: 0.54239\n",
      "step: 36554\n",
      "loss: 12.951462745666504\n",
      "steps per second: 0.55964\n",
      "step: 36555\n",
      "loss: 13.249638557434082\n",
      "steps per second: 0.53329\n",
      "step: 36556\n",
      "loss: 13.042448997497559\n",
      "steps per second: 0.52893\n",
      "step: 36557\n",
      "loss: 13.477654457092285\n",
      "steps per second: 0.55829\n",
      "step: 36558\n",
      "loss: 12.91906452178955\n",
      "steps per second: 0.54217\n",
      "step: 36559\n",
      "loss: 12.633658409118652\n",
      "steps per second: 0.54054\n",
      "step: 36560\n",
      "loss: 12.723323822021484\n",
      "steps per second: 0.58217\n",
      "step: 36561\n",
      "loss: 12.766761779785156\n",
      "steps per second: 0.54850\n",
      "step: 36562\n",
      "loss: 12.920047760009766\n",
      "steps per second: 0.55860\n",
      "step: 36563\n",
      "loss: 13.087661743164062\n",
      "steps per second: 0.56667\n",
      "step: 36564\n",
      "loss: 12.19017219543457\n",
      "steps per second: 0.53438\n",
      "step: 36565\n",
      "loss: 12.712445259094238\n",
      "steps per second: 0.51197\n",
      "step: 36566\n",
      "loss: 12.399677276611328\n",
      "steps per second: 0.54847\n",
      "step: 36567\n",
      "loss: 12.51569938659668\n",
      "steps per second: 0.53887\n",
      "step: 36568\n",
      "loss: 12.983010292053223\n",
      "steps per second: 0.53262\n",
      "step: 36569\n",
      "loss: 12.659056663513184\n",
      "steps per second: 0.53899\n",
      "step: 36570\n",
      "loss: 12.043197631835938\n",
      "steps per second: 0.55960\n",
      "step: 36571\n",
      "loss: 13.162544250488281\n",
      "steps per second: 0.51552\n",
      "step: 36572\n",
      "loss: 12.59438705444336\n",
      "steps per second: 0.50600\n",
      "step: 36573\n",
      "loss: 12.62830924987793\n",
      "steps per second: 0.54000\n",
      "step: 36574\n",
      "loss: 13.021773338317871\n",
      "steps per second: 0.57011\n",
      "step: 36575\n",
      "loss: 12.915342330932617\n",
      "steps per second: 0.54680\n",
      "step: 36576\n",
      "loss: 12.887195587158203\n",
      "steps per second: 0.54114\n",
      "step: 36577\n",
      "loss: 12.864994049072266\n",
      "steps per second: 0.51668\n",
      "step: 36578\n",
      "loss: 12.68702507019043\n",
      "steps per second: 0.54200\n",
      "step: 36579\n",
      "loss: 12.57285213470459\n",
      "steps per second: 0.51590\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8385257124900818, layer: 11\n",
      "saving at step 36579\n",
      "----------\n",
      "\n",
      "\n",
      "step: 36580\n",
      "loss: 12.77569580078125\n",
      "steps per second: 0.26188\n",
      "step: 36581\n",
      "loss: 13.2348051071167\n",
      "steps per second: 0.51867\n",
      "step: 36582\n",
      "loss: 12.700809478759766\n",
      "steps per second: 0.54123\n",
      "step: 36583\n",
      "loss: 12.36569595336914\n",
      "steps per second: 0.54247\n",
      "step: 36584\n",
      "loss: 12.790277481079102\n",
      "steps per second: 0.55916\n",
      "step: 36585\n",
      "loss: 12.829642295837402\n",
      "steps per second: 0.54107\n",
      "step: 36586\n",
      "loss: 12.583537101745605\n",
      "steps per second: 0.54090\n",
      "step: 36587\n",
      "loss: 12.907448768615723\n",
      "steps per second: 0.53519\n",
      "step: 36588\n",
      "loss: 12.395841598510742\n",
      "steps per second: 0.55951\n",
      "step: 36589\n",
      "loss: 12.47800350189209\n",
      "steps per second: 0.54445\n",
      "step: 36590\n",
      "loss: 12.792234420776367\n",
      "steps per second: 0.54453\n",
      "step: 36591\n",
      "loss: 13.261549949645996\n",
      "steps per second: 0.55970\n",
      "step: 36592\n",
      "loss: 13.078351020812988\n",
      "steps per second: 0.51861\n",
      "step: 36593\n",
      "loss: 13.13759708404541\n",
      "steps per second: 0.53714\n",
      "step: 36594\n",
      "loss: 13.097758293151855\n",
      "steps per second: 0.52813\n",
      "step: 36595\n",
      "loss: 12.785021781921387\n",
      "steps per second: 0.54174\n",
      "step: 36596\n",
      "loss: 12.790688514709473\n",
      "steps per second: 0.54859\n",
      "step: 36597\n",
      "loss: 12.64588451385498\n",
      "steps per second: 0.54645\n",
      "step: 36598\n",
      "loss: 12.553160667419434\n",
      "steps per second: 0.51365\n",
      "step: 36599\n",
      "loss: 13.1635160446167\n",
      "steps per second: 0.51060\n",
      "step: 36600\n",
      "loss: 12.653827667236328\n",
      "steps per second: 0.54253\n",
      "step: 36601\n",
      "loss: 12.869747161865234\n",
      "steps per second: 0.55544\n",
      "step: 36602\n",
      "loss: 13.136752128601074\n",
      "steps per second: 0.53492\n",
      "step: 36603\n",
      "loss: 12.764245986938477\n",
      "steps per second: 0.53535\n",
      "step: 36604\n",
      "loss: 12.641966819763184\n",
      "steps per second: 0.54176\n",
      "step: 36605\n",
      "loss: 12.862706184387207\n",
      "steps per second: 0.54081\n",
      "step: 36606\n",
      "loss: 12.742262840270996\n",
      "steps per second: 0.52891\n",
      "step: 36607\n",
      "loss: 13.0341796875\n",
      "steps per second: 0.53683\n",
      "step: 36608\n",
      "loss: 13.41341495513916\n",
      "steps per second: 0.54907\n",
      "step: 36609\n",
      "loss: 13.310637474060059\n",
      "steps per second: 0.60984\n",
      "step: 36610\n",
      "loss: 11.998035430908203\n",
      "steps per second: 0.54322\n",
      "step: 36611\n",
      "loss: 12.47264575958252\n",
      "steps per second: 0.51938\n",
      "step: 36612\n",
      "loss: 13.470938682556152\n",
      "steps per second: 0.53655\n",
      "step: 36613\n",
      "loss: 13.11128044128418\n",
      "steps per second: 0.54231\n",
      "step: 36614\n",
      "loss: 12.644387245178223\n",
      "steps per second: 0.49603\n",
      "step: 36615\n",
      "loss: 13.396559715270996\n",
      "steps per second: 0.54714\n",
      "step: 36616\n",
      "loss: 13.209413528442383\n",
      "steps per second: 0.51816\n",
      "step: 36617\n",
      "loss: 13.30889892578125\n",
      "steps per second: 0.50881\n",
      "step: 36618\n",
      "loss: 12.835957527160645\n",
      "steps per second: 0.54177\n",
      "step: 36619\n",
      "loss: 12.465628623962402\n",
      "steps per second: 0.55524\n",
      "step: 36620\n",
      "loss: 13.052132606506348\n",
      "steps per second: 0.53531\n",
      "step: 36621\n",
      "loss: 13.02665901184082\n",
      "steps per second: 0.51574\n",
      "step: 36622\n",
      "loss: 13.508039474487305\n",
      "steps per second: 0.56365\n",
      "step: 36623\n",
      "loss: 13.329980850219727\n",
      "steps per second: 0.54027\n",
      "step: 36624\n",
      "loss: 12.834920883178711\n",
      "steps per second: 0.60748\n",
      "step: 36625\n",
      "loss: 12.65855598449707\n",
      "steps per second: 0.54707\n",
      "step: 36626\n",
      "loss: 12.791231155395508\n",
      "steps per second: 0.51640\n",
      "step: 36627\n",
      "loss: 12.699164390563965\n",
      "steps per second: 0.56091\n",
      "step: 36628\n",
      "loss: 13.003506660461426\n",
      "steps per second: 0.51578\n",
      "step: 36629\n",
      "loss: 13.448162078857422\n",
      "steps per second: 0.54189\n",
      "step: 36630\n",
      "loss: 12.739095687866211\n",
      "steps per second: 0.53589\n",
      "step: 36631\n",
      "loss: 13.143350601196289\n",
      "steps per second: 0.51595\n",
      "step: 36632\n",
      "loss: 12.878570556640625\n",
      "steps per second: 0.53691\n",
      "step: 36633\n",
      "loss: 12.785968780517578\n",
      "steps per second: 0.57371\n",
      "step: 36634\n",
      "loss: 12.919522285461426\n",
      "steps per second: 0.54319\n",
      "step: 36635\n",
      "loss: 12.451282501220703\n",
      "steps per second: 0.50821\n",
      "step: 36636\n",
      "loss: 13.165482521057129\n",
      "steps per second: 0.53589\n",
      "step: 36637\n",
      "loss: 12.541108131408691\n",
      "steps per second: 0.53391\n",
      "step: 36638\n",
      "loss: 12.707358360290527\n",
      "steps per second: 0.53479\n",
      "step: 36639\n",
      "loss: 12.4205961227417\n",
      "steps per second: 0.56009\n",
      "step: 36640\n",
      "loss: 12.258162498474121\n",
      "steps per second: 0.56108\n",
      "step: 36641\n",
      "loss: 12.922507286071777\n",
      "steps per second: 0.54941\n",
      "step: 36642\n",
      "loss: 12.691041946411133\n",
      "steps per second: 0.54305\n",
      "step: 36643\n",
      "loss: 12.99708080291748\n",
      "steps per second: 0.54148\n",
      "step: 36644\n",
      "loss: 12.859831809997559\n",
      "steps per second: 0.54462\n",
      "step: 36645\n",
      "loss: 12.703786849975586\n",
      "steps per second: 0.56190\n",
      "step: 36646\n",
      "loss: 12.893686294555664\n",
      "steps per second: 0.51817\n",
      "step: 36647\n",
      "loss: 12.704301834106445\n",
      "steps per second: 0.55862\n",
      "step: 36648\n",
      "loss: 12.812444686889648\n",
      "steps per second: 0.55935\n",
      "step: 36649\n",
      "loss: 12.937339782714844\n",
      "steps per second: 0.52775\n",
      "step: 36650\n",
      "loss: 13.32576847076416\n",
      "steps per second: 0.57723\n",
      "step: 36651\n",
      "loss: 13.124102592468262\n",
      "steps per second: 0.53796\n",
      "step: 36652\n",
      "loss: 12.996347427368164\n",
      "steps per second: 0.54927\n",
      "step: 36653\n",
      "loss: 12.816872596740723\n",
      "steps per second: 0.57646\n",
      "step: 36654\n",
      "loss: 13.052033424377441\n",
      "steps per second: 0.58376\n",
      "step: 36655\n",
      "loss: 13.253495216369629\n",
      "steps per second: 0.58533\n",
      "step: 36656\n",
      "loss: 12.031803131103516\n",
      "steps per second: 0.55878\n",
      "step: 36657\n",
      "loss: 12.679731369018555\n",
      "steps per second: 0.54919\n",
      "step: 36658\n",
      "loss: 12.152386665344238\n",
      "steps per second: 0.54874\n",
      "step: 36659\n",
      "loss: 12.37169361114502\n",
      "steps per second: 0.57133\n",
      "step: 36660\n",
      "loss: 12.463113784790039\n",
      "steps per second: 0.53319\n",
      "step: 36661\n",
      "loss: 13.306536674499512\n",
      "steps per second: 0.54814\n",
      "step: 36662\n",
      "loss: 12.533474922180176\n",
      "steps per second: 0.53655\n",
      "step: 36663\n",
      "loss: 13.207552909851074\n",
      "steps per second: 0.52769\n",
      "step: 36664\n",
      "loss: 13.16865348815918\n",
      "steps per second: 0.57147\n",
      "step: 36665\n",
      "loss: 12.84943675994873\n",
      "steps per second: 0.49400\n",
      "step: 36666\n",
      "loss: 12.378608703613281\n",
      "steps per second: 0.54774\n",
      "step: 36667\n",
      "loss: 12.873157501220703\n",
      "steps per second: 0.53074\n",
      "step: 36668\n",
      "loss: 12.672313690185547\n",
      "steps per second: 0.60638\n",
      "step: 36669\n",
      "loss: 12.611875534057617\n",
      "steps per second: 0.52752\n",
      "step: 36670\n",
      "loss: 13.050698280334473\n",
      "steps per second: 0.54582\n",
      "step: 36671\n",
      "loss: 12.686211585998535\n",
      "steps per second: 0.55792\n",
      "step: 36672\n",
      "loss: 13.12100887298584\n",
      "steps per second: 0.57677\n",
      "step: 36673\n",
      "loss: 13.018180847167969\n",
      "steps per second: 0.54742\n",
      "step: 36674\n",
      "loss: 12.749480247497559\n",
      "steps per second: 0.54253\n",
      "step: 36675\n",
      "loss: 12.711697578430176\n",
      "steps per second: 0.57262\n",
      "step: 36676\n",
      "loss: 13.391864776611328\n",
      "steps per second: 0.53573\n",
      "step: 36677\n",
      "loss: 12.687182426452637\n",
      "steps per second: 0.52847\n",
      "step: 36678\n",
      "loss: 12.5038423538208\n",
      "steps per second: 0.51826\n",
      "step: 36679\n",
      "loss: 12.742498397827148\n",
      "steps per second: 0.53364\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8263695240020752, layer: 11\n",
      "saving at step 36679\n",
      "----------\n",
      "\n",
      "\n",
      "step: 36680\n",
      "loss: 12.99030590057373\n",
      "steps per second: 0.27159\n",
      "step: 36681\n",
      "loss: 12.938536643981934\n",
      "steps per second: 0.57590\n",
      "step: 36682\n",
      "loss: 12.845681190490723\n",
      "steps per second: 0.49509\n",
      "step: 36683\n",
      "loss: 13.164459228515625\n",
      "steps per second: 0.52069\n",
      "step: 36684\n",
      "loss: 12.458139419555664\n",
      "steps per second: 0.53486\n",
      "step: 36685\n",
      "loss: 13.36001205444336\n",
      "steps per second: 0.52667\n",
      "step: 36686\n",
      "loss: 12.519250869750977\n",
      "steps per second: 0.54637\n",
      "step: 36687\n",
      "loss: 12.407882690429688\n",
      "steps per second: 0.53382\n",
      "step: 36688\n",
      "loss: 12.821695327758789\n",
      "steps per second: 0.51986\n",
      "step: 36689\n",
      "loss: 12.616363525390625\n",
      "steps per second: 0.54548\n",
      "step: 36690\n",
      "loss: 13.152645111083984\n",
      "steps per second: 0.53231\n",
      "step: 36691\n",
      "loss: 12.48041820526123\n",
      "steps per second: 0.53585\n",
      "step: 36692\n",
      "loss: 12.663007736206055\n",
      "steps per second: 0.51285\n",
      "step: 36693\n",
      "loss: 12.747581481933594\n",
      "steps per second: 0.50996\n",
      "step: 36694\n",
      "loss: 12.709477424621582\n",
      "steps per second: 0.52514\n",
      "step: 36695\n",
      "loss: 13.127674102783203\n",
      "steps per second: 0.55075\n",
      "step: 36696\n",
      "loss: 12.831116676330566\n",
      "steps per second: 0.53458\n",
      "step: 36697\n",
      "loss: 12.240699768066406\n",
      "steps per second: 0.55443\n",
      "step: 36698\n",
      "loss: 12.891205787658691\n",
      "steps per second: 0.52736\n",
      "step: 36699\n",
      "loss: 12.239368438720703\n",
      "steps per second: 0.54388\n",
      "step: 36700\n",
      "loss: 12.738365173339844\n",
      "steps per second: 0.52995\n",
      "step: 36701\n",
      "loss: 12.747568130493164\n",
      "steps per second: 0.52651\n",
      "step: 36702\n",
      "loss: 11.985733032226562\n",
      "steps per second: 0.60976\n",
      "step: 36703\n",
      "loss: 13.235625267028809\n",
      "steps per second: 0.54789\n",
      "step: 36704\n",
      "loss: 12.402739524841309\n",
      "steps per second: 0.54426\n",
      "step: 36705\n",
      "loss: 12.476773262023926\n",
      "steps per second: 0.49865\n",
      "step: 36706\n",
      "loss: 12.538432121276855\n",
      "steps per second: 0.58329\n",
      "step: 36707\n",
      "loss: 13.10527515411377\n",
      "steps per second: 0.49883\n",
      "step: 36708\n",
      "loss: 12.857979774475098\n",
      "steps per second: 0.61038\n",
      "step: 36709\n",
      "loss: 12.579833984375\n",
      "steps per second: 0.52134\n",
      "step: 36710\n",
      "loss: 13.225784301757812\n",
      "steps per second: 0.54966\n",
      "step: 36711\n",
      "loss: 13.145554542541504\n",
      "steps per second: 0.55751\n",
      "step: 36712\n",
      "loss: 12.542061805725098\n",
      "steps per second: 0.60346\n",
      "step: 36713\n",
      "loss: 13.100991249084473\n",
      "steps per second: 0.57541\n",
      "step: 36714\n",
      "loss: 12.904683113098145\n",
      "steps per second: 0.51347\n",
      "step: 36715\n",
      "loss: 12.971197128295898\n",
      "steps per second: 0.53459\n",
      "step: 36716\n",
      "loss: 13.316519737243652\n",
      "steps per second: 0.51986\n",
      "step: 36717\n",
      "loss: 11.832503318786621\n",
      "steps per second: 0.52951\n",
      "step: 36718\n",
      "loss: 12.554293632507324\n",
      "steps per second: 0.52625\n",
      "step: 36719\n",
      "loss: 12.741137504577637\n",
      "steps per second: 0.55933\n",
      "step: 36720\n",
      "loss: 12.91441535949707\n",
      "steps per second: 0.56247\n",
      "step: 36721\n",
      "loss: 12.17503833770752\n",
      "steps per second: 0.56205\n",
      "step: 36722\n",
      "loss: 12.357885360717773\n",
      "steps per second: 0.53236\n",
      "step: 36723\n",
      "loss: 12.37293529510498\n",
      "steps per second: 0.52624\n",
      "step: 36724\n",
      "loss: 13.02737808227539\n",
      "steps per second: 0.51645\n",
      "step: 36725\n",
      "loss: 12.265076637268066\n",
      "steps per second: 0.53425\n",
      "step: 36726\n",
      "loss: 12.712038040161133\n",
      "steps per second: 0.53329\n",
      "step: 36727\n",
      "loss: 12.395219802856445\n",
      "steps per second: 0.56029\n",
      "step: 36728\n",
      "loss: 13.204734802246094\n",
      "steps per second: 0.60477\n",
      "step: 36729\n",
      "loss: 12.327109336853027\n",
      "steps per second: 0.43625\n",
      "step: 36730\n",
      "loss: 12.686543464660645\n",
      "steps per second: 0.53250\n",
      "step: 36731\n",
      "loss: 12.948468208312988\n",
      "steps per second: 0.53729\n",
      "step: 36732\n",
      "loss: 12.496942520141602\n",
      "steps per second: 0.53616\n",
      "step: 36733\n",
      "loss: 12.927128791809082\n",
      "steps per second: 0.50108\n",
      "step: 36734\n",
      "loss: 12.418147087097168\n",
      "steps per second: 0.53351\n",
      "step: 36735\n",
      "loss: 12.682229995727539\n",
      "steps per second: 0.49812\n",
      "step: 36736\n",
      "loss: 12.441609382629395\n",
      "steps per second: 0.51757\n",
      "step: 36737\n",
      "loss: 12.766672134399414\n",
      "steps per second: 0.60744\n",
      "step: 36738\n",
      "loss: 12.829452514648438\n",
      "steps per second: 0.51012\n",
      "step: 36739\n",
      "loss: 12.918201446533203\n",
      "steps per second: 0.54083\n",
      "step: 36740\n",
      "loss: 12.872161865234375\n",
      "steps per second: 0.53014\n",
      "step: 36741\n",
      "loss: 12.855977058410645\n",
      "steps per second: 0.54491\n",
      "step: 36742\n",
      "loss: 12.738127708435059\n",
      "steps per second: 0.54319\n",
      "step: 36743\n",
      "loss: 12.49577808380127\n",
      "steps per second: 0.57109\n",
      "step: 36744\n",
      "loss: 12.440235137939453\n",
      "steps per second: 0.52902\n",
      "step: 36745\n",
      "loss: 13.404809951782227\n",
      "steps per second: 0.54273\n",
      "step: 36746\n",
      "loss: 12.034689903259277\n",
      "steps per second: 0.54494\n",
      "step: 36747\n",
      "loss: 12.745346069335938\n",
      "steps per second: 0.51643\n",
      "step: 36748\n",
      "loss: 13.52734088897705\n",
      "steps per second: 0.53427\n",
      "step: 36749\n",
      "loss: 13.474698066711426\n",
      "steps per second: 0.50087\n",
      "step: 36750\n",
      "loss: 12.93716049194336\n",
      "steps per second: 0.53112\n",
      "step: 36751\n",
      "loss: 12.701813697814941\n",
      "steps per second: 0.53476\n",
      "step: 36752\n",
      "loss: 12.57564640045166\n",
      "steps per second: 0.60600\n",
      "step: 36753\n",
      "loss: 12.747208595275879\n",
      "steps per second: 0.54326\n",
      "step: 36754\n",
      "loss: 12.764451026916504\n",
      "steps per second: 0.54195\n",
      "step: 36755\n",
      "loss: 12.697351455688477\n",
      "steps per second: 0.57376\n",
      "step: 36756\n",
      "loss: 12.698801040649414\n",
      "steps per second: 0.53299\n",
      "step: 36757\n",
      "loss: 12.116741180419922\n",
      "steps per second: 0.56306\n",
      "step: 36758\n",
      "loss: 12.722838401794434\n",
      "steps per second: 0.52876\n",
      "step: 36759\n",
      "loss: 12.541531562805176\n",
      "steps per second: 0.55878\n",
      "step: 36760\n",
      "loss: 12.423258781433105\n",
      "steps per second: 0.57485\n",
      "step: 36761\n",
      "loss: 12.308934211730957\n",
      "steps per second: 0.52866\n",
      "step: 36762\n",
      "loss: 12.347223281860352\n",
      "steps per second: 0.52356\n",
      "step: 36763\n",
      "loss: 12.876336097717285\n",
      "steps per second: 0.57300\n",
      "step: 36764\n",
      "loss: 12.743573188781738\n",
      "steps per second: 0.58268\n",
      "step: 36765\n",
      "loss: 12.661944389343262\n",
      "steps per second: 0.53427\n",
      "step: 36766\n",
      "loss: 13.331636428833008\n",
      "steps per second: 0.53390\n",
      "step: 36767\n",
      "loss: 12.509513854980469\n",
      "steps per second: 0.53123\n",
      "step: 36768\n",
      "loss: 12.387179374694824\n",
      "steps per second: 0.53545\n",
      "step: 36769\n",
      "loss: 12.264541625976562\n",
      "steps per second: 0.54183\n",
      "step: 36770\n",
      "loss: 12.749351501464844\n",
      "steps per second: 0.53426\n",
      "step: 36771\n",
      "loss: 13.184656143188477\n",
      "steps per second: 0.50911\n",
      "step: 36772\n",
      "loss: 13.26867961883545\n",
      "steps per second: 0.50071\n",
      "step: 36773\n",
      "loss: 12.848780632019043\n",
      "steps per second: 0.50899\n",
      "step: 36774\n",
      "loss: 12.396479606628418\n",
      "steps per second: 0.54142\n",
      "step: 36775\n",
      "loss: 12.724199295043945\n",
      "steps per second: 0.57461\n",
      "step: 36776\n",
      "loss: 12.496070861816406\n",
      "steps per second: 0.52612\n",
      "step: 36777\n",
      "loss: 12.431815147399902\n",
      "steps per second: 0.51744\n",
      "step: 36778\n",
      "loss: 12.947002410888672\n",
      "steps per second: 0.55091\n",
      "step: 36779\n",
      "loss: 12.758391380310059\n",
      "steps per second: 0.55861\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8128523826599121, layer: 12\n",
      "saving at step 36779\n",
      "----------\n",
      "\n",
      "\n",
      "step: 36780\n",
      "loss: 13.121110916137695\n",
      "steps per second: 0.26670\n",
      "step: 36781\n",
      "loss: 12.997322082519531\n",
      "steps per second: 0.50818\n",
      "step: 36782\n",
      "loss: 13.242938995361328\n",
      "steps per second: 0.53947\n",
      "step: 36783\n",
      "loss: 12.965335845947266\n",
      "steps per second: 0.53460\n",
      "step: 36784\n",
      "loss: 13.052202224731445\n",
      "steps per second: 0.56143\n",
      "step: 36785\n",
      "loss: 12.634231567382812\n",
      "steps per second: 0.56947\n",
      "step: 36786\n",
      "loss: 13.162765502929688\n",
      "steps per second: 0.52677\n",
      "step: 36787\n",
      "loss: 12.810477256774902\n",
      "steps per second: 0.49539\n",
      "step: 36788\n",
      "loss: 13.307165145874023\n",
      "steps per second: 0.54117\n",
      "step: 36789\n",
      "loss: 12.480487823486328\n",
      "steps per second: 0.54330\n",
      "step: 36790\n",
      "loss: 13.272115707397461\n",
      "steps per second: 0.54568\n",
      "step: 36791\n",
      "loss: 12.705769538879395\n",
      "steps per second: 0.55474\n",
      "step: 36792\n",
      "loss: 12.642317771911621\n",
      "steps per second: 0.52591\n",
      "step: 36793\n",
      "loss: 13.232720375061035\n",
      "steps per second: 0.55986\n",
      "step: 36794\n",
      "loss: 12.372318267822266\n",
      "steps per second: 0.53727\n",
      "step: 36795\n",
      "loss: 12.440417289733887\n",
      "steps per second: 0.53461\n",
      "step: 36796\n",
      "loss: 12.679261207580566\n",
      "steps per second: 0.52173\n",
      "step: 36797\n",
      "loss: 12.889339447021484\n",
      "steps per second: 0.54668\n",
      "step: 36798\n",
      "loss: 12.53805923461914\n",
      "steps per second: 0.57131\n",
      "step: 36799\n",
      "loss: 12.926856994628906\n",
      "steps per second: 0.52993\n",
      "step: 36800\n",
      "loss: 12.698101043701172\n",
      "steps per second: 0.50714\n",
      "step: 36801\n",
      "loss: 12.857629776000977\n",
      "steps per second: 0.48394\n",
      "step: 36802\n",
      "loss: 12.429086685180664\n",
      "steps per second: 0.55393\n",
      "step: 36803\n",
      "loss: 12.223066329956055\n",
      "steps per second: 0.51709\n",
      "step: 36804\n",
      "loss: 13.076513290405273\n",
      "steps per second: 0.52806\n",
      "step: 36805\n",
      "loss: 13.25872802734375\n",
      "steps per second: 0.54078\n",
      "step: 36806\n",
      "loss: 12.897300720214844\n",
      "steps per second: 0.51225\n",
      "step: 36807\n",
      "loss: 13.092551231384277\n",
      "steps per second: 0.53352\n",
      "step: 36808\n",
      "loss: 12.792132377624512\n",
      "steps per second: 0.52778\n",
      "step: 36809\n",
      "loss: 13.102005958557129\n",
      "steps per second: 0.56154\n",
      "step: 36810\n",
      "loss: 12.470247268676758\n",
      "steps per second: 0.49252\n",
      "step: 36811\n",
      "loss: 12.495898246765137\n",
      "steps per second: 0.57428\n",
      "step: 36812\n",
      "loss: 13.525875091552734\n",
      "steps per second: 0.54931\n",
      "step: 36813\n",
      "loss: 12.92338752746582\n",
      "steps per second: 0.60474\n",
      "step: 36814\n",
      "loss: 12.41005802154541\n",
      "steps per second: 0.53919\n",
      "step: 36815\n",
      "loss: 12.85333251953125\n",
      "steps per second: 0.56106\n",
      "step: 36816\n",
      "loss: 12.279743194580078\n",
      "steps per second: 0.51967\n",
      "step: 36817\n",
      "loss: 12.694671630859375\n",
      "steps per second: 0.54366\n",
      "step: 36818\n",
      "loss: 12.952569961547852\n",
      "steps per second: 0.51639\n",
      "step: 36819\n",
      "loss: 12.630517959594727\n",
      "steps per second: 0.54329\n",
      "step: 36820\n",
      "loss: 12.879611015319824\n",
      "steps per second: 0.52797\n",
      "step: 36821\n",
      "loss: 12.95565128326416\n",
      "steps per second: 0.51534\n",
      "step: 36822\n",
      "loss: 12.93528938293457\n",
      "steps per second: 0.52882\n",
      "step: 36823\n",
      "loss: 12.919048309326172\n",
      "steps per second: 0.53563\n",
      "step: 36824\n",
      "loss: 12.731133460998535\n",
      "steps per second: 0.53941\n",
      "step: 36825\n",
      "loss: 12.470255851745605\n",
      "steps per second: 0.54495\n",
      "step: 36826\n",
      "loss: 12.204760551452637\n",
      "steps per second: 0.55153\n",
      "step: 36827\n",
      "loss: 12.79517650604248\n",
      "steps per second: 0.53243\n",
      "step: 36828\n",
      "loss: 12.81663990020752\n",
      "steps per second: 0.60692\n",
      "step: 36829\n",
      "loss: 13.396512031555176\n",
      "steps per second: 0.51568\n",
      "step: 36830\n",
      "loss: 13.036314964294434\n",
      "steps per second: 0.48284\n",
      "step: 36831\n",
      "loss: 13.064798355102539\n",
      "steps per second: 0.49942\n",
      "step: 36832\n",
      "loss: 12.353534698486328\n",
      "steps per second: 0.51520\n",
      "step: 36833\n",
      "loss: 12.474818229675293\n",
      "steps per second: 0.53752\n",
      "step: 36834\n",
      "loss: 12.758647918701172\n",
      "steps per second: 0.53625\n",
      "step: 36835\n",
      "loss: 12.954010009765625\n",
      "steps per second: 0.51243\n",
      "step: 36836\n",
      "loss: 12.463767051696777\n",
      "steps per second: 0.55471\n",
      "step: 36837\n",
      "loss: 12.596671104431152\n",
      "steps per second: 0.56896\n",
      "step: 36838\n",
      "loss: 13.064906120300293\n",
      "steps per second: 0.55861\n",
      "step: 36839\n",
      "loss: 12.932159423828125\n",
      "steps per second: 0.51274\n",
      "step: 36840\n",
      "loss: 13.349530220031738\n",
      "steps per second: 0.53447\n",
      "step: 36841\n",
      "loss: 12.487787246704102\n",
      "steps per second: 0.53911\n",
      "step: 36842\n",
      "loss: 12.935264587402344\n",
      "steps per second: 0.53995\n",
      "step: 36843\n",
      "loss: 12.738282203674316\n",
      "steps per second: 0.52360\n",
      "step: 36844\n",
      "loss: 13.028592109680176\n",
      "steps per second: 0.55386\n",
      "step: 36845\n",
      "loss: 13.401887893676758\n",
      "steps per second: 0.57078\n",
      "step: 36846\n",
      "loss: 12.727469444274902\n",
      "steps per second: 0.54477\n",
      "step: 36847\n",
      "loss: 13.23393726348877\n",
      "steps per second: 0.55593\n",
      "step: 36848\n",
      "loss: 12.863338470458984\n",
      "steps per second: 0.54187\n",
      "step: 36849\n",
      "loss: 12.946599960327148\n",
      "steps per second: 0.54526\n",
      "step: 36850\n",
      "loss: 13.00593090057373\n",
      "steps per second: 0.49343\n",
      "step: 36851\n",
      "loss: 12.3569974899292\n",
      "steps per second: 0.57360\n",
      "step: 36852\n",
      "loss: 13.051937103271484\n",
      "steps per second: 0.57047\n",
      "step: 36853\n",
      "loss: 12.84814453125\n",
      "steps per second: 0.49622\n",
      "step: 36854\n",
      "loss: 12.705209732055664\n",
      "steps per second: 0.52982\n",
      "step: 36855\n",
      "loss: 13.103790283203125\n",
      "steps per second: 0.49182\n",
      "step: 36856\n",
      "loss: 13.244830131530762\n",
      "steps per second: 0.53962\n",
      "step: 36857\n",
      "loss: 13.348501205444336\n",
      "steps per second: 0.44811\n",
      "step: 36858\n",
      "loss: 13.248647689819336\n",
      "steps per second: 0.51005\n",
      "step: 36859\n",
      "loss: 12.750194549560547\n",
      "steps per second: 0.54703\n",
      "step: 36860\n",
      "loss: 13.182337760925293\n",
      "steps per second: 0.51139\n",
      "step: 36861\n",
      "loss: 13.116667747497559\n",
      "steps per second: 0.47106\n",
      "step: 36862\n",
      "loss: 12.54375171661377\n",
      "steps per second: 0.49154\n",
      "step: 36863\n",
      "loss: 13.457292556762695\n",
      "steps per second: 0.51761\n",
      "step: 36864\n",
      "loss: 12.800342559814453\n",
      "steps per second: 0.53934\n",
      "step: 36865\n",
      "loss: 12.963936805725098\n",
      "steps per second: 0.56164\n",
      "step: 36866\n",
      "loss: 13.103233337402344\n",
      "steps per second: 0.53519\n",
      "step: 36867\n",
      "loss: 12.711417198181152\n",
      "steps per second: 0.55994\n",
      "step: 36868\n",
      "loss: 12.935568809509277\n",
      "steps per second: 0.54423\n",
      "step: 36869\n",
      "loss: 12.70042610168457\n",
      "steps per second: 0.52345\n",
      "step: 36870\n",
      "loss: 12.70497989654541\n",
      "steps per second: 0.57905\n",
      "step: 36871\n",
      "loss: 12.885385513305664\n",
      "steps per second: 0.56670\n",
      "step: 36872\n",
      "loss: 13.305039405822754\n",
      "steps per second: 0.56116\n",
      "step: 36873\n",
      "loss: 12.491059303283691\n",
      "steps per second: 0.52562\n",
      "step: 36874\n",
      "loss: 12.931205749511719\n",
      "steps per second: 0.58743\n",
      "step: 36875\n",
      "loss: 12.56844425201416\n",
      "steps per second: 0.55502\n",
      "step: 36876\n",
      "loss: 12.848501205444336\n",
      "steps per second: 0.58358\n",
      "step: 36877\n",
      "loss: 12.329551696777344\n",
      "steps per second: 0.57060\n",
      "step: 36878\n",
      "loss: 12.816365242004395\n",
      "steps per second: 0.52222\n",
      "step: 36879\n",
      "loss: 12.474238395690918\n",
      "steps per second: 0.53736\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8180602788925171, layer: 10\n",
      "saving at step 36879\n",
      "----------\n",
      "\n",
      "\n",
      "step: 36880\n",
      "loss: 12.721535682678223\n",
      "steps per second: 0.28043\n",
      "step: 36881\n",
      "loss: 12.964479446411133\n",
      "steps per second: 0.54166\n",
      "step: 36882\n",
      "loss: 13.04642105102539\n",
      "steps per second: 0.52976\n",
      "step: 36883\n",
      "loss: 12.516820907592773\n",
      "steps per second: 0.55359\n",
      "step: 36884\n",
      "loss: 12.637906074523926\n",
      "steps per second: 0.56094\n",
      "step: 36885\n",
      "loss: 13.014731407165527\n",
      "steps per second: 0.52896\n",
      "step: 36886\n",
      "loss: 12.432344436645508\n",
      "steps per second: 0.55609\n",
      "step: 36887\n",
      "loss: 12.275413513183594\n",
      "steps per second: 0.55023\n",
      "step: 36888\n",
      "loss: 12.67971134185791\n",
      "steps per second: 0.57040\n",
      "step: 36889\n",
      "loss: 12.85218334197998\n",
      "steps per second: 0.53729\n",
      "step: 36890\n",
      "loss: 12.045669555664062\n",
      "steps per second: 0.49682\n",
      "step: 36891\n",
      "loss: 12.844799995422363\n",
      "steps per second: 0.54441\n",
      "step: 36892\n",
      "loss: 12.93593692779541\n",
      "steps per second: 0.56980\n",
      "step: 36893\n",
      "loss: 12.87501335144043\n",
      "steps per second: 0.61741\n",
      "step: 36894\n",
      "loss: 12.379498481750488\n",
      "steps per second: 0.49402\n",
      "step: 36895\n",
      "loss: 12.808110237121582\n",
      "steps per second: 0.55130\n",
      "step: 36896\n",
      "loss: 12.591516494750977\n",
      "steps per second: 0.54941\n",
      "step: 36897\n",
      "loss: 13.234590530395508\n",
      "steps per second: 0.54696\n",
      "step: 36898\n",
      "loss: 13.095582962036133\n",
      "steps per second: 0.49246\n",
      "step: 36899\n",
      "loss: 12.926861763000488\n",
      "steps per second: 0.55221\n",
      "step: 36900\n",
      "loss: 12.581012725830078\n",
      "steps per second: 0.52013\n",
      "step: 36901\n",
      "loss: 13.221453666687012\n",
      "steps per second: 0.51941\n",
      "step: 36902\n",
      "loss: 13.299548149108887\n",
      "steps per second: 0.54575\n",
      "step: 36903\n",
      "loss: 12.775700569152832\n",
      "steps per second: 0.52525\n",
      "step: 36904\n",
      "loss: 12.861939430236816\n",
      "steps per second: 0.57504\n",
      "step: 36905\n",
      "loss: 12.538739204406738\n",
      "steps per second: 0.52728\n",
      "step: 36906\n",
      "loss: 12.826095581054688\n",
      "steps per second: 0.50061\n",
      "step: 36907\n",
      "loss: 12.970755577087402\n",
      "steps per second: 0.59423\n",
      "step: 36908\n",
      "loss: 12.948344230651855\n",
      "steps per second: 0.52765\n",
      "step: 36909\n",
      "loss: 13.126396179199219\n",
      "steps per second: 0.49147\n",
      "step: 36910\n",
      "loss: 12.95843505859375\n",
      "steps per second: 0.51106\n",
      "step: 36911\n",
      "loss: 13.085668563842773\n",
      "steps per second: 0.52142\n",
      "step: 36912\n",
      "loss: 12.222809791564941\n",
      "steps per second: 0.51833\n",
      "step: 36913\n",
      "loss: 12.88305377960205\n",
      "steps per second: 0.54714\n",
      "step: 36914\n",
      "loss: 12.672163963317871\n",
      "steps per second: 0.50361\n",
      "step: 36915\n",
      "loss: 13.06221866607666\n",
      "steps per second: 0.49631\n",
      "step: 36916\n",
      "loss: 12.509749412536621\n",
      "steps per second: 0.51450\n",
      "step: 36917\n",
      "loss: 12.536465644836426\n",
      "steps per second: 0.52495\n",
      "step: 36918\n",
      "loss: 12.582294464111328\n",
      "steps per second: 0.55736\n",
      "step: 36919\n",
      "loss: 12.66771125793457\n",
      "steps per second: 0.54748\n",
      "step: 36920\n",
      "loss: 13.353802680969238\n",
      "steps per second: 0.51066\n",
      "step: 36921\n",
      "loss: 12.86499309539795\n",
      "steps per second: 0.51570\n",
      "step: 36922\n",
      "loss: 12.751701354980469\n",
      "steps per second: 0.55768\n",
      "step: 36923\n",
      "loss: 12.493313789367676\n",
      "steps per second: 0.52583\n",
      "step: 36924\n",
      "loss: 12.334336280822754\n",
      "steps per second: 0.54470\n",
      "step: 36925\n",
      "loss: 12.925028800964355\n",
      "steps per second: 0.56547\n",
      "step: 36926\n",
      "loss: 13.64144229888916\n",
      "steps per second: 0.49890\n",
      "step: 36927\n",
      "loss: 12.604875564575195\n",
      "steps per second: 0.60599\n",
      "step: 36928\n",
      "loss: 12.731042861938477\n",
      "steps per second: 0.51210\n",
      "step: 36929\n",
      "loss: 12.961047172546387\n",
      "steps per second: 0.57546\n",
      "step: 36930\n",
      "loss: 12.31139850616455\n",
      "steps per second: 0.53920\n",
      "step: 36931\n",
      "loss: 12.478743553161621\n",
      "steps per second: 0.51403\n",
      "step: 36932\n",
      "loss: 12.195944786071777\n",
      "steps per second: 0.54310\n",
      "step: 36933\n",
      "loss: 12.753485679626465\n",
      "steps per second: 0.52017\n",
      "step: 36934\n",
      "loss: 12.748275756835938\n",
      "steps per second: 0.54963\n",
      "step: 36935\n",
      "loss: 12.666336059570312\n",
      "steps per second: 0.55774\n",
      "step: 36936\n",
      "loss: 12.837157249450684\n",
      "steps per second: 0.52189\n",
      "step: 36937\n",
      "loss: 12.99367618560791\n",
      "steps per second: 0.55246\n",
      "step: 36938\n",
      "loss: 12.56735610961914\n",
      "steps per second: 0.54658\n",
      "step: 36939\n",
      "loss: 13.022844314575195\n",
      "steps per second: 0.54411\n",
      "step: 36940\n",
      "loss: 11.985289573669434\n",
      "steps per second: 0.53585\n",
      "step: 36941\n",
      "loss: 12.71580982208252\n",
      "steps per second: 0.49627\n",
      "step: 36942\n",
      "loss: 12.953186988830566\n",
      "steps per second: 0.51749\n",
      "step: 36943\n",
      "loss: 12.566520690917969\n",
      "steps per second: 0.51840\n",
      "step: 36944\n",
      "loss: 12.972765922546387\n",
      "steps per second: 0.52721\n",
      "step: 36945\n",
      "loss: 12.836899757385254\n",
      "steps per second: 0.55389\n",
      "step: 36946\n",
      "loss: 12.80064582824707\n",
      "steps per second: 0.55106\n",
      "step: 36947\n",
      "loss: 12.77991771697998\n",
      "steps per second: 0.48277\n",
      "step: 36948\n",
      "loss: 13.00635051727295\n",
      "steps per second: 0.49913\n",
      "step: 36949\n",
      "loss: 12.739187240600586\n",
      "steps per second: 0.52275\n",
      "step: 36950\n",
      "loss: 12.714962005615234\n",
      "steps per second: 0.52096\n",
      "step: 36951\n",
      "loss: 11.995649337768555\n",
      "steps per second: 0.50095\n",
      "step: 36952\n",
      "loss: 12.247681617736816\n",
      "steps per second: 0.51183\n",
      "step: 36953\n",
      "loss: 12.619889259338379\n",
      "steps per second: 0.50522\n",
      "step: 36954\n",
      "loss: 12.647618293762207\n",
      "steps per second: 0.57801\n",
      "step: 36955\n",
      "loss: 12.602484703063965\n",
      "steps per second: 0.46785\n",
      "step: 36956\n",
      "loss: 12.26433277130127\n",
      "steps per second: 0.50053\n",
      "step: 36957\n",
      "loss: 13.263707160949707\n",
      "steps per second: 0.53664\n",
      "step: 36958\n",
      "loss: 13.096857070922852\n",
      "steps per second: 0.51559\n",
      "step: 36959\n",
      "loss: 12.281689643859863\n",
      "steps per second: 0.52641\n",
      "step: 36960\n",
      "loss: 12.873841285705566\n",
      "steps per second: 0.54570\n",
      "step: 36961\n",
      "loss: 13.497223854064941\n",
      "steps per second: 0.53392\n",
      "step: 36962\n",
      "loss: 12.945833206176758\n",
      "steps per second: 0.53418\n",
      "step: 36963\n",
      "loss: 12.871389389038086\n",
      "steps per second: 0.54254\n",
      "step: 36964\n",
      "loss: 12.764474868774414\n",
      "steps per second: 0.55252\n",
      "step: 36965\n",
      "loss: 12.903802871704102\n",
      "steps per second: 0.58669\n",
      "step: 36966\n",
      "loss: 12.347831726074219\n",
      "steps per second: 0.53294\n",
      "step: 36967\n",
      "loss: 12.652186393737793\n",
      "steps per second: 0.48801\n",
      "step: 36968\n",
      "loss: 12.904647827148438\n",
      "steps per second: 0.53570\n",
      "step: 36969\n",
      "loss: 12.680343627929688\n",
      "steps per second: 0.52570\n",
      "step: 36970\n",
      "loss: 12.856770515441895\n",
      "steps per second: 0.52822\n",
      "step: 36971\n",
      "loss: 12.763283729553223\n",
      "steps per second: 0.48146\n",
      "step: 36972\n",
      "loss: 12.69161605834961\n",
      "steps per second: 0.49970\n",
      "step: 36973\n",
      "loss: 12.397038459777832\n",
      "steps per second: 0.49997\n",
      "step: 36974\n",
      "loss: 12.612837791442871\n",
      "steps per second: 0.49115\n",
      "step: 36975\n",
      "loss: 13.5325288772583\n",
      "steps per second: 0.51464\n",
      "step: 36976\n",
      "loss: 12.945939064025879\n",
      "steps per second: 0.53328\n",
      "step: 36977\n",
      "loss: 12.444907188415527\n",
      "steps per second: 0.50298\n",
      "step: 36978\n",
      "loss: 12.838274955749512\n",
      "steps per second: 0.54618\n",
      "step: 36979\n",
      "loss: 13.160955429077148\n",
      "steps per second: 0.51230\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8412790298461914, layer: 12\n",
      "saving at step 36979\n",
      "----------\n",
      "\n",
      "\n",
      "step: 36980\n",
      "loss: 13.056118965148926\n",
      "steps per second: 0.26259\n",
      "step: 36981\n",
      "loss: 12.325885772705078\n",
      "steps per second: 0.53019\n",
      "step: 36982\n",
      "loss: 13.070722579956055\n",
      "steps per second: 0.50590\n",
      "step: 36983\n",
      "loss: 12.853416442871094\n",
      "steps per second: 0.52998\n",
      "step: 36984\n",
      "loss: 12.12939739227295\n",
      "steps per second: 0.54159\n",
      "step: 36985\n",
      "loss: 12.589812278747559\n",
      "steps per second: 0.52751\n",
      "step: 36986\n",
      "loss: 12.891023635864258\n",
      "steps per second: 0.51016\n",
      "step: 36987\n",
      "loss: 12.777575492858887\n",
      "steps per second: 0.50388\n",
      "step: 36988\n",
      "loss: 13.20002269744873\n",
      "steps per second: 0.52325\n",
      "step: 36989\n",
      "loss: 12.556049346923828\n",
      "steps per second: 0.52492\n",
      "step: 36990\n",
      "loss: 12.461485862731934\n",
      "steps per second: 0.55306\n",
      "step: 36991\n",
      "loss: 12.842517852783203\n",
      "steps per second: 0.52584\n",
      "step: 36992\n",
      "loss: 12.741477966308594\n",
      "steps per second: 0.52286\n",
      "step: 36993\n",
      "loss: 12.741055488586426\n",
      "steps per second: 0.51937\n",
      "step: 36994\n",
      "loss: 12.704290390014648\n",
      "steps per second: 0.52849\n",
      "step: 36995\n",
      "loss: 12.658504486083984\n",
      "steps per second: 0.54528\n",
      "step: 36996\n",
      "loss: 13.040005683898926\n",
      "steps per second: 0.55649\n",
      "step: 36997\n",
      "loss: 12.72365665435791\n",
      "steps per second: 0.53599\n",
      "step: 36998\n",
      "loss: 12.1925687789917\n",
      "steps per second: 0.51573\n",
      "step: 36999\n",
      "loss: 12.261021614074707\n",
      "steps per second: 0.50447\n",
      "step: 37000\n",
      "loss: 12.285881042480469\n",
      "steps per second: 0.53669\n",
      "step: 37001\n",
      "loss: 12.447693824768066\n",
      "steps per second: 0.58619\n",
      "step: 37002\n",
      "loss: 12.55888557434082\n",
      "steps per second: 0.55114\n",
      "step: 37003\n",
      "loss: 12.544112205505371\n",
      "steps per second: 0.51992\n",
      "step: 37004\n",
      "loss: 12.834339141845703\n",
      "steps per second: 0.58468\n",
      "step: 37005\n",
      "loss: 13.069916725158691\n",
      "steps per second: 0.53260\n",
      "step: 37006\n",
      "loss: 12.552947998046875\n",
      "steps per second: 0.51778\n",
      "step: 37007\n",
      "loss: 13.23471736907959\n",
      "steps per second: 0.55477\n",
      "step: 37008\n",
      "loss: 13.306124687194824\n",
      "steps per second: 0.56699\n",
      "step: 37009\n",
      "loss: 12.57163143157959\n",
      "steps per second: 0.52442\n",
      "step: 37010\n",
      "loss: 12.365551948547363\n",
      "steps per second: 0.49281\n",
      "step: 37011\n",
      "loss: 12.188420295715332\n",
      "steps per second: 0.52186\n",
      "step: 37012\n",
      "loss: 12.958113670349121\n",
      "steps per second: 0.56477\n",
      "step: 37013\n",
      "loss: 12.293675422668457\n",
      "steps per second: 0.57450\n",
      "step: 37014\n",
      "loss: 12.601743698120117\n",
      "steps per second: 0.51411\n",
      "step: 37015\n",
      "loss: 12.95755672454834\n",
      "steps per second: 0.51042\n",
      "step: 37016\n",
      "loss: 12.770184516906738\n",
      "steps per second: 0.49739\n",
      "step: 37017\n",
      "loss: 12.655116081237793\n",
      "steps per second: 0.52614\n",
      "step: 37018\n",
      "loss: 12.299878120422363\n",
      "steps per second: 0.52722\n",
      "step: 37019\n",
      "loss: 12.962088584899902\n",
      "steps per second: 0.51825\n",
      "step: 37020\n",
      "loss: 13.082032203674316\n",
      "steps per second: 0.49103\n",
      "step: 37021\n",
      "loss: 12.766554832458496\n",
      "steps per second: 0.51761\n",
      "step: 37022\n",
      "loss: 12.60397720336914\n",
      "steps per second: 0.51495\n",
      "step: 37023\n",
      "loss: 13.043120384216309\n",
      "steps per second: 0.55368\n",
      "step: 37024\n",
      "loss: 12.476278305053711\n",
      "steps per second: 0.54827\n",
      "step: 37025\n",
      "loss: 12.23140811920166\n",
      "steps per second: 0.48855\n",
      "step: 37026\n",
      "loss: 12.607234954833984\n",
      "steps per second: 0.47878\n",
      "step: 37027\n",
      "loss: 13.284865379333496\n",
      "steps per second: 0.48563\n",
      "step: 37028\n",
      "loss: 12.717771530151367\n",
      "steps per second: 0.51424\n",
      "step: 37029\n",
      "loss: 12.632380485534668\n",
      "steps per second: 0.52015\n",
      "step: 37030\n",
      "loss: 13.501947402954102\n",
      "steps per second: 0.54284\n",
      "step: 37031\n",
      "loss: 12.928047180175781\n",
      "steps per second: 0.46492\n",
      "step: 37032\n",
      "loss: 12.691073417663574\n",
      "steps per second: 0.49708\n",
      "step: 37033\n",
      "loss: 12.898213386535645\n",
      "steps per second: 0.53923\n",
      "step: 37034\n",
      "loss: 12.446388244628906\n",
      "steps per second: 0.54427\n",
      "step: 37035\n",
      "loss: 12.788206100463867\n",
      "steps per second: 0.53203\n",
      "step: 37036\n",
      "loss: 13.03065299987793\n",
      "steps per second: 0.47416\n",
      "step: 37037\n",
      "loss: 12.70470905303955\n",
      "steps per second: 0.55748\n",
      "step: 37038\n",
      "loss: 13.18018627166748\n",
      "steps per second: 0.54639\n",
      "step: 37039\n",
      "loss: 12.697491645812988\n",
      "steps per second: 0.52172\n",
      "step: 37040\n",
      "loss: 12.683602333068848\n",
      "steps per second: 0.54872\n",
      "step: 37041\n",
      "loss: 12.966374397277832\n",
      "steps per second: 0.50681\n",
      "step: 37042\n",
      "loss: 13.104606628417969\n",
      "steps per second: 0.53398\n",
      "step: 37043\n",
      "loss: 12.396432876586914\n",
      "steps per second: 0.55268\n",
      "step: 37044\n",
      "loss: 12.937047958374023\n",
      "steps per second: 0.56483\n",
      "step: 37045\n",
      "loss: 12.783339500427246\n",
      "steps per second: 0.53589\n",
      "step: 37046\n",
      "loss: 12.780536651611328\n",
      "steps per second: 0.53304\n",
      "step: 37047\n",
      "loss: 12.66927719116211\n",
      "steps per second: 0.47925\n",
      "step: 37048\n",
      "loss: 12.915057182312012\n",
      "steps per second: 0.54583\n",
      "step: 37049\n",
      "loss: 13.205757141113281\n",
      "steps per second: 0.54567\n",
      "step: 37050\n",
      "loss: 12.976974487304688\n",
      "steps per second: 0.54194\n",
      "step: 37051\n",
      "loss: 12.78170394897461\n",
      "steps per second: 0.51056\n",
      "step: 37052\n",
      "loss: 13.363115310668945\n",
      "steps per second: 0.53790\n",
      "step: 37053\n",
      "loss: 13.267940521240234\n",
      "steps per second: 0.51431\n",
      "step: 37054\n",
      "loss: 12.778718948364258\n",
      "steps per second: 0.55185\n",
      "step: 37055\n",
      "loss: 12.813045501708984\n",
      "steps per second: 0.57853\n",
      "step: 37056\n",
      "loss: 12.665914535522461\n",
      "steps per second: 0.52550\n",
      "step: 37057\n",
      "loss: 13.184443473815918\n",
      "steps per second: 0.51039\n",
      "step: 37058\n",
      "loss: 12.953902244567871\n",
      "steps per second: 0.50511\n",
      "step: 37059\n",
      "loss: 12.926551818847656\n",
      "steps per second: 0.57305\n",
      "step: 37060\n",
      "loss: 12.74467945098877\n",
      "steps per second: 0.54852\n",
      "step: 37061\n",
      "loss: 13.008841514587402\n",
      "steps per second: 0.53932\n",
      "step: 37062\n",
      "loss: 13.039656639099121\n",
      "steps per second: 0.55805\n",
      "step: 37063\n",
      "loss: 12.802529335021973\n",
      "steps per second: 0.50056\n",
      "step: 37064\n",
      "loss: 13.183568954467773\n",
      "steps per second: 0.51283\n",
      "step: 37065\n",
      "loss: 13.155482292175293\n",
      "steps per second: 0.51645\n",
      "step: 37066\n",
      "loss: 12.874164581298828\n",
      "steps per second: 0.57084\n",
      "step: 37067\n",
      "loss: 12.801712036132812\n",
      "steps per second: 0.55355\n",
      "step: 37068\n",
      "loss: 12.59804916381836\n",
      "steps per second: 0.53287\n",
      "step: 37069\n",
      "loss: 13.191624641418457\n",
      "steps per second: 0.51750\n",
      "step: 37070\n",
      "loss: 12.43015193939209\n",
      "steps per second: 0.54045\n",
      "step: 37071\n",
      "loss: 12.839150428771973\n",
      "steps per second: 0.55679\n",
      "step: 37072\n",
      "loss: 12.525776863098145\n",
      "steps per second: 0.53234\n",
      "step: 37073\n",
      "loss: 12.272658348083496\n",
      "steps per second: 0.53317\n",
      "step: 37074\n",
      "loss: 12.585331916809082\n",
      "steps per second: 0.50070\n",
      "step: 37075\n",
      "loss: 12.875003814697266\n",
      "steps per second: 0.54925\n",
      "step: 37076\n",
      "loss: 13.023679733276367\n",
      "steps per second: 0.52394\n",
      "step: 37077\n",
      "loss: 13.073053359985352\n",
      "steps per second: 0.51220\n",
      "step: 37078\n",
      "loss: 12.4629487991333\n",
      "steps per second: 0.55126\n",
      "step: 37079\n",
      "loss: 12.274916648864746\n",
      "steps per second: 0.54920\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7777695655822754, layer: 10\n",
      "saving at step 37079\n",
      "----------\n",
      "\n",
      "\n",
      "step: 37080\n",
      "loss: 12.803689002990723\n",
      "steps per second: 0.28111\n",
      "step: 37081\n",
      "loss: 12.806262016296387\n",
      "steps per second: 0.60557\n",
      "step: 37082\n",
      "loss: 13.014546394348145\n",
      "steps per second: 0.50458\n",
      "step: 37083\n",
      "loss: 12.904847145080566\n",
      "steps per second: 0.51602\n",
      "step: 37084\n",
      "loss: 13.175369262695312\n",
      "steps per second: 0.52920\n",
      "step: 37085\n",
      "loss: 12.932585716247559\n",
      "steps per second: 0.57034\n",
      "step: 37086\n",
      "loss: 12.9088773727417\n",
      "steps per second: 0.53598\n",
      "step: 37087\n",
      "loss: 12.838397979736328\n",
      "steps per second: 0.54255\n",
      "step: 37088\n",
      "loss: 12.661017417907715\n",
      "steps per second: 0.48051\n",
      "step: 37089\n",
      "loss: 12.270702362060547\n",
      "steps per second: 0.54757\n",
      "step: 37090\n",
      "loss: 13.616499900817871\n",
      "steps per second: 0.55086\n",
      "step: 37091\n",
      "loss: 12.429553031921387\n",
      "steps per second: 0.53993\n",
      "step: 37092\n",
      "loss: 13.052263259887695\n",
      "steps per second: 0.54228\n",
      "step: 37093\n",
      "loss: 13.323233604431152\n",
      "steps per second: 0.57324\n",
      "step: 37094\n",
      "loss: 12.68069076538086\n",
      "steps per second: 0.52896\n",
      "step: 37095\n",
      "loss: 12.763773918151855\n",
      "steps per second: 0.54679\n",
      "step: 37096\n",
      "loss: 12.598834991455078\n",
      "steps per second: 0.54133\n",
      "step: 37097\n",
      "loss: 12.698677062988281\n",
      "steps per second: 0.51084\n",
      "step: 37098\n",
      "loss: 12.96764087677002\n",
      "steps per second: 0.58317\n",
      "step: 37099\n",
      "loss: 12.660228729248047\n",
      "steps per second: 0.54444\n",
      "step: 37100\n",
      "loss: 12.618966102600098\n",
      "steps per second: 0.55172\n",
      "step: 37101\n",
      "loss: 13.463841438293457\n",
      "steps per second: 0.52180\n",
      "step: 37102\n",
      "loss: 13.050226211547852\n",
      "steps per second: 0.57358\n",
      "step: 37103\n",
      "loss: 12.197101593017578\n",
      "steps per second: 0.53912\n",
      "step: 37104\n",
      "loss: 13.05511474609375\n",
      "steps per second: 0.54386\n",
      "step: 37105\n",
      "loss: 12.420441627502441\n",
      "steps per second: 0.53175\n",
      "step: 37106\n",
      "loss: 12.51834487915039\n",
      "steps per second: 0.53049\n",
      "step: 37107\n",
      "loss: 12.780014038085938\n",
      "steps per second: 0.57143\n",
      "step: 37108\n",
      "loss: 12.777865409851074\n",
      "steps per second: 0.54988\n",
      "step: 37109\n",
      "loss: 12.363082885742188\n",
      "steps per second: 0.57989\n",
      "step: 37110\n",
      "loss: 12.629837989807129\n",
      "steps per second: 0.60858\n",
      "step: 37111\n",
      "loss: 12.412561416625977\n",
      "steps per second: 0.54648\n",
      "step: 37112\n",
      "loss: 13.000234603881836\n",
      "steps per second: 0.56615\n",
      "step: 37113\n",
      "loss: 12.53831672668457\n",
      "steps per second: 0.55018\n",
      "step: 37114\n",
      "loss: 12.68270492553711\n",
      "steps per second: 0.53342\n",
      "step: 37115\n",
      "loss: 12.747747421264648\n",
      "steps per second: 0.55893\n",
      "step: 37116\n",
      "loss: 13.039107322692871\n",
      "steps per second: 0.57429\n",
      "step: 37117\n",
      "loss: 12.716476440429688\n",
      "steps per second: 0.55667\n",
      "step: 37118\n",
      "loss: 13.411497116088867\n",
      "steps per second: 0.58414\n",
      "step: 37119\n",
      "loss: 12.35584545135498\n",
      "steps per second: 0.55785\n",
      "step: 37120\n",
      "loss: 12.976546287536621\n",
      "steps per second: 0.55816\n",
      "step: 37121\n",
      "loss: 12.379501342773438\n",
      "steps per second: 0.55300\n",
      "step: 37122\n",
      "loss: 12.850411415100098\n",
      "steps per second: 0.55397\n",
      "step: 37123\n",
      "loss: 12.885113716125488\n",
      "steps per second: 0.52542\n",
      "step: 37124\n",
      "loss: 12.020088195800781\n",
      "steps per second: 0.55456\n",
      "step: 37125\n",
      "loss: 12.981592178344727\n",
      "steps per second: 0.55469\n",
      "step: 37126\n",
      "loss: 13.308466911315918\n",
      "steps per second: 0.57116\n",
      "step: 37127\n",
      "loss: 12.390027046203613\n",
      "steps per second: 0.54918\n",
      "step: 37128\n",
      "loss: 12.776576042175293\n",
      "steps per second: 0.61645\n",
      "step: 37129\n",
      "loss: 12.71142864227295\n",
      "steps per second: 0.55754\n",
      "step: 37130\n",
      "loss: 13.249317169189453\n",
      "steps per second: 0.53992\n",
      "step: 37131\n",
      "loss: 12.519930839538574\n",
      "steps per second: 0.55399\n",
      "step: 37132\n",
      "loss: 12.264370918273926\n",
      "steps per second: 0.56965\n",
      "step: 37133\n",
      "loss: 12.855326652526855\n",
      "steps per second: 0.54484\n",
      "step: 37134\n",
      "loss: 12.709787368774414\n",
      "steps per second: 0.60750\n",
      "step: 37135\n",
      "loss: 13.112616539001465\n",
      "steps per second: 0.57242\n",
      "step: 37136\n",
      "loss: 12.417129516601562\n",
      "steps per second: 0.54057\n",
      "step: 37137\n",
      "loss: 12.680181503295898\n",
      "steps per second: 0.50895\n",
      "step: 37138\n",
      "loss: 12.649285316467285\n",
      "steps per second: 0.54849\n",
      "step: 37139\n",
      "loss: 12.564871788024902\n",
      "steps per second: 0.51822\n",
      "step: 37140\n",
      "loss: 12.575759887695312\n",
      "steps per second: 0.53891\n",
      "step: 37141\n",
      "loss: 13.06887149810791\n",
      "steps per second: 0.51733\n",
      "step: 37142\n",
      "loss: 12.254144668579102\n",
      "steps per second: 0.54851\n",
      "step: 37143\n",
      "loss: 12.766691207885742\n",
      "steps per second: 0.54631\n",
      "step: 37144\n",
      "loss: 12.35424518585205\n",
      "steps per second: 0.55690\n",
      "step: 37145\n",
      "loss: 12.612371444702148\n",
      "steps per second: 0.55027\n",
      "step: 37146\n",
      "loss: 12.592292785644531\n",
      "steps per second: 0.52406\n",
      "step: 37147\n",
      "loss: 12.701251029968262\n",
      "steps per second: 0.56498\n",
      "step: 37148\n",
      "loss: 12.992636680603027\n",
      "steps per second: 0.58152\n",
      "step: 37149\n",
      "loss: 12.407234191894531\n",
      "steps per second: 0.59027\n",
      "step: 37150\n",
      "loss: 12.877100944519043\n",
      "steps per second: 0.50542\n",
      "step: 37151\n",
      "loss: 12.63330364227295\n",
      "steps per second: 0.54953\n",
      "step: 37152\n",
      "loss: 12.725817680358887\n",
      "steps per second: 0.52240\n",
      "step: 37153\n",
      "loss: 13.217187881469727\n",
      "steps per second: 0.50415\n",
      "step: 37154\n",
      "loss: 12.830028533935547\n",
      "steps per second: 0.53417\n",
      "step: 37155\n",
      "loss: 12.322257041931152\n",
      "steps per second: 0.55234\n",
      "step: 37156\n",
      "loss: 13.450013160705566\n",
      "steps per second: 0.55346\n",
      "step: 37157\n",
      "loss: 12.9325590133667\n",
      "steps per second: 0.55255\n",
      "step: 37158\n",
      "loss: 13.003151893615723\n",
      "steps per second: 0.53233\n",
      "step: 37159\n",
      "loss: 13.12592601776123\n",
      "steps per second: 0.55610\n",
      "step: 37160\n",
      "loss: 13.384882926940918\n",
      "steps per second: 0.55677\n",
      "step: 37161\n",
      "loss: 13.011639595031738\n",
      "steps per second: 0.49622\n",
      "step: 37162\n",
      "loss: 12.732450485229492\n",
      "steps per second: 0.57163\n",
      "step: 37163\n",
      "loss: 13.169039726257324\n",
      "steps per second: 0.54294\n",
      "step: 37164\n",
      "loss: 12.846850395202637\n",
      "steps per second: 0.55037\n",
      "step: 37165\n",
      "loss: 12.970203399658203\n",
      "steps per second: 0.52953\n",
      "step: 37166\n",
      "loss: 12.092733383178711\n",
      "steps per second: 0.53988\n",
      "step: 37167\n",
      "loss: 13.165007591247559\n",
      "steps per second: 0.54002\n",
      "step: 37168\n",
      "loss: 13.039447784423828\n",
      "steps per second: 0.53677\n",
      "step: 37169\n",
      "loss: 13.039656639099121\n",
      "steps per second: 0.52884\n",
      "step: 37170\n",
      "loss: 12.582660675048828\n",
      "steps per second: 0.56579\n",
      "step: 37171\n",
      "loss: 12.606261253356934\n",
      "steps per second: 0.55961\n",
      "step: 37172\n",
      "loss: 12.903556823730469\n",
      "steps per second: 0.53339\n",
      "step: 37173\n",
      "loss: 13.012012481689453\n",
      "steps per second: 0.58272\n",
      "step: 37174\n",
      "loss: 13.17236328125\n",
      "steps per second: 0.54304\n",
      "step: 37175\n",
      "loss: 12.858564376831055\n",
      "steps per second: 0.56182\n",
      "step: 37176\n",
      "loss: 12.810742378234863\n",
      "steps per second: 0.56201\n",
      "step: 37177\n",
      "loss: 12.655847549438477\n",
      "steps per second: 0.54870\n",
      "step: 37178\n",
      "loss: 12.28311538696289\n",
      "steps per second: 0.57052\n",
      "step: 37179\n",
      "loss: 12.845481872558594\n",
      "steps per second: 0.58236\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8356170058250427, layer: 12\n",
      "saving at step 37179\n",
      "----------\n",
      "\n",
      "\n",
      "step: 37180\n",
      "loss: 13.398759841918945\n",
      "steps per second: 0.22918\n",
      "step: 37181\n",
      "loss: 13.130105972290039\n",
      "steps per second: 0.48645\n",
      "step: 37182\n",
      "loss: 13.0331392288208\n",
      "steps per second: 0.50426\n",
      "step: 37183\n",
      "loss: 12.577249526977539\n",
      "steps per second: 0.50153\n",
      "step: 37184\n",
      "loss: 13.193267822265625\n",
      "steps per second: 0.55531\n",
      "step: 37185\n",
      "loss: 12.918635368347168\n",
      "steps per second: 0.47213\n",
      "step: 37186\n",
      "loss: 12.68027114868164\n",
      "steps per second: 0.43242\n",
      "step: 37187\n",
      "loss: 12.830140113830566\n",
      "steps per second: 0.55699\n",
      "step: 37188\n",
      "loss: 13.151548385620117\n",
      "steps per second: 0.56066\n",
      "step: 37189\n",
      "loss: 12.627549171447754\n",
      "steps per second: 0.58764\n",
      "step: 37190\n",
      "loss: 12.641646385192871\n",
      "steps per second: 0.52811\n",
      "step: 37191\n",
      "loss: 12.542281150817871\n",
      "steps per second: 0.47202\n",
      "step: 37192\n",
      "loss: 13.050600051879883\n",
      "steps per second: 0.50076\n",
      "step: 37193\n",
      "loss: 12.933509826660156\n",
      "steps per second: 0.51995\n",
      "step: 37194\n",
      "loss: 12.398510932922363\n",
      "steps per second: 0.55561\n",
      "step: 37195\n",
      "loss: 12.440112113952637\n",
      "steps per second: 0.56093\n",
      "step: 37196\n",
      "loss: 12.346670150756836\n",
      "steps per second: 0.53003\n",
      "step: 37197\n",
      "loss: 13.430095672607422\n",
      "steps per second: 0.59987\n",
      "step: 37198\n",
      "loss: 12.392570495605469\n",
      "steps per second: 0.51548\n",
      "step: 37199\n",
      "loss: 12.879029273986816\n",
      "steps per second: 0.56285\n",
      "step: 37200\n",
      "loss: 12.819908142089844\n",
      "steps per second: 0.54618\n",
      "step: 37201\n",
      "loss: 12.742898941040039\n",
      "steps per second: 0.56515\n",
      "step: 37202\n",
      "loss: 13.182854652404785\n",
      "steps per second: 0.52976\n",
      "step: 37203\n",
      "loss: 13.112591743469238\n",
      "steps per second: 0.56085\n",
      "step: 37204\n",
      "loss: 12.577072143554688\n",
      "steps per second: 0.55985\n",
      "step: 37205\n",
      "loss: 12.052756309509277\n",
      "steps per second: 0.52947\n",
      "step: 37206\n",
      "loss: 13.187732696533203\n",
      "steps per second: 0.56462\n",
      "step: 37207\n",
      "loss: 13.582074165344238\n",
      "steps per second: 0.56574\n",
      "step: 37208\n",
      "loss: 12.782708168029785\n",
      "steps per second: 0.52188\n",
      "step: 37209\n",
      "loss: 12.831642150878906\n",
      "steps per second: 0.49644\n",
      "step: 37210\n",
      "loss: 12.225688934326172\n",
      "steps per second: 0.54152\n",
      "step: 37211\n",
      "loss: 12.988805770874023\n",
      "steps per second: 0.57187\n",
      "step: 37212\n",
      "loss: 12.713361740112305\n",
      "steps per second: 0.56580\n",
      "step: 37213\n",
      "loss: 12.941880226135254\n",
      "steps per second: 0.54626\n",
      "step: 37214\n",
      "loss: 12.382604598999023\n",
      "steps per second: 0.54944\n",
      "step: 37215\n",
      "loss: 12.241584777832031\n",
      "steps per second: 0.57106\n",
      "step: 37216\n",
      "loss: 12.78376579284668\n",
      "steps per second: 0.53980\n",
      "step: 37217\n",
      "loss: 13.20345687866211\n",
      "steps per second: 0.52886\n",
      "step: 37218\n",
      "loss: 13.262479782104492\n",
      "steps per second: 0.57498\n",
      "step: 37219\n",
      "loss: 12.79384994506836\n",
      "steps per second: 0.57492\n",
      "step: 37220\n",
      "loss: 12.795297622680664\n",
      "steps per second: 0.52527\n",
      "step: 37221\n",
      "loss: 13.231002807617188\n",
      "steps per second: 0.54534\n",
      "step: 37222\n",
      "loss: 12.865538597106934\n",
      "steps per second: 0.51922\n",
      "step: 37223\n",
      "loss: 12.915571212768555\n",
      "steps per second: 0.51692\n",
      "step: 37224\n",
      "loss: 12.896865844726562\n",
      "steps per second: 0.56471\n",
      "step: 37225\n",
      "loss: 12.812539100646973\n",
      "steps per second: 0.56396\n",
      "step: 37226\n",
      "loss: 12.804338455200195\n",
      "steps per second: 0.51285\n",
      "step: 37227\n",
      "loss: 12.774724006652832\n",
      "steps per second: 0.51400\n",
      "step: 37228\n",
      "loss: 12.761484146118164\n",
      "steps per second: 0.60033\n",
      "step: 37229\n",
      "loss: 12.93820571899414\n",
      "steps per second: 0.56561\n",
      "step: 37230\n",
      "loss: 12.792400360107422\n",
      "steps per second: 0.53208\n",
      "step: 37231\n",
      "loss: 12.679776191711426\n",
      "steps per second: 0.52346\n",
      "step: 37232\n",
      "loss: 13.010242462158203\n",
      "steps per second: 0.55158\n",
      "step: 37233\n",
      "loss: 12.714021682739258\n",
      "steps per second: 0.53352\n",
      "step: 37234\n",
      "loss: 12.294886589050293\n",
      "steps per second: 0.54639\n",
      "step: 37235\n",
      "loss: 12.67795181274414\n",
      "steps per second: 0.55064\n",
      "step: 37236\n",
      "loss: 12.524165153503418\n",
      "steps per second: 0.53450\n",
      "step: 37237\n",
      "loss: 12.556657791137695\n",
      "steps per second: 0.56004\n",
      "step: 37238\n",
      "loss: 12.732473373413086\n",
      "steps per second: 0.57014\n",
      "step: 37239\n",
      "loss: 12.610427856445312\n",
      "steps per second: 0.51844\n",
      "step: 37240\n",
      "loss: 12.905558586120605\n",
      "steps per second: 0.55462\n",
      "step: 37241\n",
      "loss: 12.884932518005371\n",
      "steps per second: 0.46265\n",
      "step: 37242\n",
      "loss: 12.597696304321289\n",
      "steps per second: 0.51468\n",
      "step: 37243\n",
      "loss: 12.899452209472656\n",
      "steps per second: 0.54166\n",
      "step: 37244\n",
      "loss: 12.794382095336914\n",
      "steps per second: 0.53943\n",
      "step: 37245\n",
      "loss: 12.654206275939941\n",
      "steps per second: 0.57001\n",
      "step: 37246\n",
      "loss: 13.219953536987305\n",
      "steps per second: 0.56461\n",
      "step: 37247\n",
      "loss: 13.171544075012207\n",
      "steps per second: 0.57461\n",
      "step: 37248\n",
      "loss: 12.517768859863281\n",
      "steps per second: 0.53121\n",
      "step: 37249\n",
      "loss: 12.789149284362793\n",
      "steps per second: 0.54180\n",
      "step: 37250\n",
      "loss: 12.834250450134277\n",
      "steps per second: 0.51606\n",
      "step: 37251\n",
      "loss: 12.573864936828613\n",
      "steps per second: 0.52427\n",
      "step: 37252\n",
      "loss: 12.911079406738281\n",
      "steps per second: 0.54924\n",
      "step: 37253\n",
      "loss: 12.614447593688965\n",
      "steps per second: 0.55631\n",
      "step: 37254\n",
      "loss: 12.281109809875488\n",
      "steps per second: 0.60429\n",
      "step: 37255\n",
      "loss: 12.580073356628418\n",
      "steps per second: 0.48447\n",
      "step: 37256\n",
      "loss: 12.831725120544434\n",
      "steps per second: 0.57158\n",
      "step: 37257\n",
      "loss: 12.686615943908691\n",
      "steps per second: 0.53439\n",
      "step: 37258\n",
      "loss: 12.347077369689941\n",
      "steps per second: 0.57313\n",
      "step: 37259\n",
      "loss: 12.564007759094238\n",
      "steps per second: 0.54049\n",
      "step: 37260\n",
      "loss: 12.579703330993652\n",
      "steps per second: 0.53471\n",
      "step: 37261\n",
      "loss: 12.31875228881836\n",
      "steps per second: 0.51693\n",
      "step: 37262\n",
      "loss: 12.427877426147461\n",
      "steps per second: 0.52198\n",
      "step: 37263\n",
      "loss: 12.640283584594727\n",
      "steps per second: 0.47204\n",
      "step: 37264\n",
      "loss: 13.163840293884277\n",
      "steps per second: 0.56246\n",
      "step: 37265\n",
      "loss: 12.47609806060791\n",
      "steps per second: 0.53195\n",
      "step: 37266\n",
      "loss: 13.081071853637695\n",
      "steps per second: 0.51009\n",
      "step: 37267\n",
      "loss: 13.059183120727539\n",
      "steps per second: 0.56278\n",
      "step: 37268\n",
      "loss: 12.684898376464844\n",
      "steps per second: 0.52363\n",
      "step: 37269\n",
      "loss: 12.907286643981934\n",
      "steps per second: 0.54843\n",
      "step: 37270\n",
      "loss: 12.161551475524902\n",
      "steps per second: 0.60509\n",
      "step: 37271\n",
      "loss: 12.649300575256348\n",
      "steps per second: 0.56205\n",
      "step: 37272\n",
      "loss: 13.103960037231445\n",
      "steps per second: 0.53302\n",
      "step: 37273\n",
      "loss: 12.633739471435547\n",
      "steps per second: 0.56990\n",
      "step: 37274\n",
      "loss: 13.071612358093262\n",
      "steps per second: 0.52625\n",
      "step: 37275\n",
      "loss: 13.16854190826416\n",
      "steps per second: 0.60675\n",
      "step: 37276\n",
      "loss: 12.709023475646973\n",
      "steps per second: 0.54631\n",
      "step: 37277\n",
      "loss: 13.074618339538574\n",
      "steps per second: 0.54645\n",
      "step: 37278\n",
      "loss: 13.032340049743652\n",
      "steps per second: 0.53867\n",
      "step: 37279\n",
      "loss: 13.151556015014648\n",
      "steps per second: 0.53740\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8729922771453857, layer: 10\n",
      "saving at step 37279\n",
      "----------\n",
      "\n",
      "\n",
      "step: 37280\n",
      "loss: 12.617175102233887\n",
      "steps per second: 0.26093\n",
      "step: 37281\n",
      "loss: 12.933384895324707\n",
      "steps per second: 0.48158\n",
      "step: 37282\n",
      "loss: 13.314497947692871\n",
      "steps per second: 0.53979\n",
      "step: 37283\n",
      "loss: 12.445829391479492\n",
      "steps per second: 0.55341\n",
      "step: 37284\n",
      "loss: 12.923474311828613\n",
      "steps per second: 0.56899\n",
      "step: 37285\n",
      "loss: 12.241471290588379\n",
      "steps per second: 0.54191\n",
      "step: 37286\n",
      "loss: 12.782135963439941\n",
      "steps per second: 0.55211\n",
      "step: 37287\n",
      "loss: 13.537612915039062\n",
      "steps per second: 0.56477\n",
      "step: 37288\n",
      "loss: 13.020663261413574\n",
      "steps per second: 0.53026\n",
      "step: 37289\n",
      "loss: 12.595937728881836\n",
      "steps per second: 0.54896\n",
      "step: 37290\n",
      "loss: 12.599149703979492\n",
      "steps per second: 0.53167\n",
      "step: 37291\n",
      "loss: 12.387389183044434\n",
      "steps per second: 0.48601\n",
      "step: 37292\n",
      "loss: 12.941619873046875\n",
      "steps per second: 0.56257\n",
      "step: 37293\n",
      "loss: 13.311662673950195\n",
      "steps per second: 0.51723\n",
      "step: 37294\n",
      "loss: 12.279144287109375\n",
      "steps per second: 0.54836\n",
      "step: 37295\n",
      "loss: 12.58012580871582\n",
      "steps per second: 0.51620\n",
      "step: 37296\n",
      "loss: 12.620519638061523\n",
      "steps per second: 0.56470\n",
      "step: 37297\n",
      "loss: 13.321499824523926\n",
      "steps per second: 0.53215\n",
      "step: 37298\n",
      "loss: 12.964472770690918\n",
      "steps per second: 0.57406\n",
      "step: 37299\n",
      "loss: 12.320207595825195\n",
      "steps per second: 0.53695\n",
      "step: 37300\n",
      "loss: 12.770740509033203\n",
      "steps per second: 0.60573\n",
      "step: 37301\n",
      "loss: 12.541214942932129\n",
      "steps per second: 0.54486\n",
      "step: 37302\n",
      "loss: 12.871866226196289\n",
      "steps per second: 0.61623\n",
      "step: 37303\n",
      "loss: 12.693098068237305\n",
      "steps per second: 0.55563\n",
      "step: 37304\n",
      "loss: 12.543693542480469\n",
      "steps per second: 0.52566\n",
      "step: 37305\n",
      "loss: 12.821338653564453\n",
      "steps per second: 0.54056\n",
      "step: 37306\n",
      "loss: 12.88526725769043\n",
      "steps per second: 0.52852\n",
      "step: 37307\n",
      "loss: 12.96024227142334\n",
      "steps per second: 0.55129\n",
      "step: 37308\n",
      "loss: 12.981585502624512\n",
      "steps per second: 0.57564\n",
      "step: 37309\n",
      "loss: 12.818262100219727\n",
      "steps per second: 0.52783\n",
      "step: 37310\n",
      "loss: 12.40673828125\n",
      "steps per second: 0.53719\n",
      "step: 37311\n",
      "loss: 12.489727020263672\n",
      "steps per second: 0.54163\n",
      "step: 37312\n",
      "loss: 13.335553169250488\n",
      "steps per second: 0.56577\n",
      "step: 37313\n",
      "loss: 12.601807594299316\n",
      "steps per second: 0.53838\n",
      "step: 37314\n",
      "loss: 12.718040466308594\n",
      "steps per second: 0.52689\n",
      "step: 37315\n",
      "loss: 12.161164283752441\n",
      "steps per second: 0.55118\n",
      "step: 37316\n",
      "loss: 12.86523723602295\n",
      "steps per second: 0.57772\n",
      "step: 37317\n",
      "loss: 12.689269065856934\n",
      "steps per second: 0.56959\n",
      "step: 37318\n",
      "loss: 12.654850959777832\n",
      "steps per second: 0.55366\n",
      "step: 37319\n",
      "loss: 13.291462898254395\n",
      "steps per second: 0.53614\n",
      "step: 37320\n",
      "loss: 12.922818183898926\n",
      "steps per second: 0.54990\n",
      "step: 37321\n",
      "loss: 12.817280769348145\n",
      "steps per second: 0.56661\n",
      "step: 37322\n",
      "loss: 13.161377906799316\n",
      "steps per second: 0.55189\n",
      "step: 37323\n",
      "loss: 12.679861068725586\n",
      "steps per second: 0.57950\n",
      "step: 37324\n",
      "loss: 12.94323444366455\n",
      "steps per second: 0.56049\n",
      "step: 37325\n",
      "loss: 12.821097373962402\n",
      "steps per second: 0.54427\n",
      "step: 37326\n",
      "loss: 12.054450988769531\n",
      "steps per second: 0.58600\n",
      "step: 37327\n",
      "loss: 13.076021194458008\n",
      "steps per second: 0.54986\n",
      "step: 37328\n",
      "loss: 12.440410614013672\n",
      "steps per second: 0.52692\n",
      "step: 37329\n",
      "loss: 13.022300720214844\n",
      "steps per second: 0.61289\n",
      "step: 37330\n",
      "loss: 12.581195831298828\n",
      "steps per second: 0.57201\n",
      "step: 37331\n",
      "loss: 13.226781845092773\n",
      "steps per second: 0.52446\n",
      "step: 37332\n",
      "loss: 12.950584411621094\n",
      "steps per second: 0.56486\n",
      "step: 37333\n",
      "loss: 12.428618431091309\n",
      "steps per second: 0.54930\n",
      "step: 37334\n",
      "loss: 12.755786895751953\n",
      "steps per second: 0.55646\n",
      "step: 37335\n",
      "loss: 12.463871002197266\n",
      "steps per second: 0.57477\n",
      "step: 37336\n",
      "loss: 12.834941864013672\n",
      "steps per second: 0.50554\n",
      "step: 37337\n",
      "loss: 13.053388595581055\n",
      "steps per second: 0.54251\n",
      "step: 37338\n",
      "loss: 12.578679084777832\n",
      "steps per second: 0.55292\n",
      "step: 37339\n",
      "loss: 12.832767486572266\n",
      "steps per second: 0.55343\n",
      "step: 37340\n",
      "loss: 12.432137489318848\n",
      "steps per second: 0.61670\n",
      "step: 37341\n",
      "loss: 13.261316299438477\n",
      "steps per second: 0.55716\n",
      "step: 37342\n",
      "loss: 13.175289154052734\n",
      "steps per second: 0.57740\n",
      "step: 37343\n",
      "loss: 12.680682182312012\n",
      "steps per second: 0.57577\n",
      "step: 37344\n",
      "loss: 12.082396507263184\n",
      "steps per second: 0.56128\n",
      "step: 37345\n",
      "loss: 12.815560340881348\n",
      "steps per second: 0.57938\n",
      "step: 37346\n",
      "loss: 12.7349214553833\n",
      "steps per second: 0.60052\n",
      "step: 37347\n",
      "loss: 12.856119155883789\n",
      "steps per second: 0.58639\n",
      "step: 37348\n",
      "loss: 13.624004364013672\n",
      "steps per second: 0.56244\n",
      "step: 37349\n",
      "loss: 12.859782218933105\n",
      "steps per second: 0.57158\n",
      "step: 37350\n",
      "loss: 12.834343910217285\n",
      "steps per second: 0.55909\n",
      "step: 37351\n",
      "loss: 12.926102638244629\n",
      "steps per second: 0.60855\n",
      "step: 37352\n",
      "loss: 12.923644065856934\n",
      "steps per second: 0.54132\n",
      "step: 37353\n",
      "loss: 13.003532409667969\n",
      "steps per second: 0.55368\n",
      "step: 37354\n",
      "loss: 12.78760051727295\n",
      "steps per second: 0.55078\n",
      "step: 37355\n",
      "loss: 12.761590003967285\n",
      "steps per second: 0.57795\n",
      "step: 37356\n",
      "loss: 12.653511047363281\n",
      "steps per second: 0.54913\n",
      "step: 37357\n",
      "loss: 12.551366806030273\n",
      "steps per second: 0.52831\n",
      "step: 37358\n",
      "loss: 13.13054370880127\n",
      "steps per second: 0.57722\n",
      "step: 37359\n",
      "loss: 12.309138298034668\n",
      "steps per second: 0.56462\n",
      "step: 37360\n",
      "loss: 12.191642761230469\n",
      "steps per second: 0.56187\n",
      "step: 37361\n",
      "loss: 12.341176986694336\n",
      "steps per second: 0.53456\n",
      "step: 37362\n",
      "loss: 12.902189254760742\n",
      "steps per second: 0.53554\n",
      "step: 37363\n",
      "loss: 13.129109382629395\n",
      "steps per second: 0.52150\n",
      "step: 37364\n",
      "loss: 12.286409378051758\n",
      "steps per second: 0.55852\n",
      "step: 37365\n",
      "loss: 13.778410911560059\n",
      "steps per second: 0.52059\n",
      "step: 37366\n",
      "loss: 12.456501960754395\n",
      "steps per second: 0.60478\n",
      "step: 37367\n",
      "loss: 12.669486045837402\n",
      "steps per second: 0.55274\n",
      "step: 37368\n",
      "loss: 12.81688117980957\n",
      "steps per second: 0.54007\n",
      "step: 37369\n",
      "loss: 12.500633239746094\n",
      "steps per second: 0.56832\n",
      "step: 37370\n",
      "loss: 13.110946655273438\n",
      "steps per second: 0.58148\n",
      "step: 37371\n",
      "loss: 12.964655876159668\n",
      "steps per second: 0.53283\n",
      "step: 37372\n",
      "loss: 13.112984657287598\n",
      "steps per second: 0.56589\n",
      "step: 37373\n",
      "loss: 13.186027526855469\n",
      "steps per second: 0.56453\n",
      "step: 37374\n",
      "loss: 12.385998725891113\n",
      "steps per second: 0.54211\n",
      "step: 37375\n",
      "loss: 13.192138671875\n",
      "steps per second: 0.49045\n",
      "step: 37376\n",
      "loss: 12.677921295166016\n",
      "steps per second: 0.54727\n",
      "step: 37377\n",
      "loss: 13.312346458435059\n",
      "steps per second: 0.55145\n",
      "step: 37378\n",
      "loss: 12.60960578918457\n",
      "steps per second: 0.52939\n",
      "step: 37379\n",
      "loss: 12.833348274230957\n",
      "steps per second: 0.54568\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8140577077865601, layer: 11\n",
      "saving at step 37379\n",
      "----------\n",
      "\n",
      "\n",
      "step: 37380\n",
      "loss: 12.766607284545898\n",
      "steps per second: 0.28338\n",
      "step: 37381\n",
      "loss: 12.967231750488281\n",
      "steps per second: 0.53872\n",
      "step: 37382\n",
      "loss: 12.84502124786377\n",
      "steps per second: 0.53149\n",
      "step: 37383\n",
      "loss: 13.63593864440918\n",
      "steps per second: 0.52936\n",
      "step: 37384\n",
      "loss: 12.594017028808594\n",
      "steps per second: 0.57462\n",
      "step: 37385\n",
      "loss: 13.273656845092773\n",
      "steps per second: 0.52626\n",
      "step: 37386\n",
      "loss: 13.174581527709961\n",
      "steps per second: 0.52469\n",
      "step: 37387\n",
      "loss: 12.665711402893066\n",
      "steps per second: 0.58368\n",
      "step: 37388\n",
      "loss: 12.417044639587402\n",
      "steps per second: 0.58871\n",
      "step: 37389\n",
      "loss: 13.042261123657227\n",
      "steps per second: 0.55354\n",
      "step: 37390\n",
      "loss: 12.667654991149902\n",
      "steps per second: 0.56057\n",
      "step: 37391\n",
      "loss: 12.633763313293457\n",
      "steps per second: 0.57610\n",
      "step: 37392\n",
      "loss: 13.191008567810059\n",
      "steps per second: 0.58410\n",
      "step: 37393\n",
      "loss: 12.989733695983887\n",
      "steps per second: 0.58578\n",
      "step: 37394\n",
      "loss: 13.473191261291504\n",
      "steps per second: 0.51307\n",
      "step: 37395\n",
      "loss: 12.637216567993164\n",
      "steps per second: 0.55915\n",
      "step: 37396\n",
      "loss: 13.00633430480957\n",
      "steps per second: 0.57710\n",
      "step: 37397\n",
      "loss: 12.93429183959961\n",
      "steps per second: 0.55760\n",
      "step: 37398\n",
      "loss: 12.52647590637207\n",
      "steps per second: 0.58171\n",
      "step: 37399\n",
      "loss: 12.74608325958252\n",
      "steps per second: 0.51994\n",
      "step: 37400\n",
      "loss: 13.081788063049316\n",
      "steps per second: 0.56489\n",
      "step: 37401\n",
      "loss: 12.797394752502441\n",
      "steps per second: 0.54338\n",
      "step: 37402\n",
      "loss: 13.04772663116455\n",
      "steps per second: 0.55581\n",
      "step: 37403\n",
      "loss: 12.588882446289062\n",
      "steps per second: 0.58005\n",
      "step: 37404\n",
      "loss: 13.076955795288086\n",
      "steps per second: 0.57247\n",
      "step: 37405\n",
      "loss: 12.82984447479248\n",
      "steps per second: 0.53926\n",
      "step: 37406\n",
      "loss: 12.617554664611816\n",
      "steps per second: 0.55764\n",
      "step: 37407\n",
      "loss: 12.167787551879883\n",
      "steps per second: 0.61112\n",
      "step: 37408\n",
      "loss: 13.052091598510742\n",
      "steps per second: 0.57561\n",
      "step: 37409\n",
      "loss: 12.537919998168945\n",
      "steps per second: 0.55495\n",
      "step: 37410\n",
      "loss: 13.300653457641602\n",
      "steps per second: 0.54037\n",
      "step: 37411\n",
      "loss: 13.712767601013184\n",
      "steps per second: 0.53219\n",
      "step: 37412\n",
      "loss: 12.680072784423828\n",
      "steps per second: 0.61708\n",
      "step: 37413\n",
      "loss: 12.784706115722656\n",
      "steps per second: 0.55196\n",
      "step: 37414\n",
      "loss: 12.517838478088379\n",
      "steps per second: 0.55711\n",
      "step: 37415\n",
      "loss: 12.776625633239746\n",
      "steps per second: 0.56685\n",
      "step: 37416\n",
      "loss: 12.498040199279785\n",
      "steps per second: 0.52206\n",
      "step: 37417\n",
      "loss: 12.649463653564453\n",
      "steps per second: 0.58050\n",
      "step: 37418\n",
      "loss: 13.084610939025879\n",
      "steps per second: 0.54993\n",
      "step: 37419\n",
      "loss: 12.785780906677246\n",
      "steps per second: 0.58248\n",
      "step: 37420\n",
      "loss: 12.881538391113281\n",
      "steps per second: 0.55725\n",
      "step: 37421\n",
      "loss: 12.563169479370117\n",
      "steps per second: 0.52615\n",
      "step: 37422\n",
      "loss: 12.315011024475098\n",
      "steps per second: 0.54326\n",
      "step: 37423\n",
      "loss: 12.932608604431152\n",
      "steps per second: 0.58646\n",
      "step: 37424\n",
      "loss: 13.234074592590332\n",
      "steps per second: 0.58592\n",
      "step: 37425\n",
      "loss: 12.612960815429688\n",
      "steps per second: 0.53219\n",
      "step: 37426\n",
      "loss: 12.52515983581543\n",
      "steps per second: 0.55311\n",
      "step: 37427\n",
      "loss: 12.704377174377441\n",
      "steps per second: 0.57657\n",
      "step: 37428\n",
      "loss: 13.326297760009766\n",
      "steps per second: 0.56141\n",
      "step: 37429\n",
      "loss: 12.43486213684082\n",
      "steps per second: 0.57811\n",
      "step: 37430\n",
      "loss: 12.883711814880371\n",
      "steps per second: 0.54563\n",
      "step: 37431\n",
      "loss: 12.880435943603516\n",
      "steps per second: 0.54315\n",
      "step: 37432\n",
      "loss: 12.731334686279297\n",
      "steps per second: 0.51057\n",
      "step: 37433\n",
      "loss: 12.808026313781738\n",
      "steps per second: 0.53553\n",
      "step: 37434\n",
      "loss: 12.642630577087402\n",
      "steps per second: 0.55521\n",
      "step: 37435\n",
      "loss: 12.309103012084961\n",
      "steps per second: 0.52194\n",
      "step: 37436\n",
      "loss: 12.668745994567871\n",
      "steps per second: 0.56228\n",
      "step: 37437\n",
      "loss: 13.195077896118164\n",
      "steps per second: 0.56456\n",
      "step: 37438\n",
      "loss: 12.727974891662598\n",
      "steps per second: 0.55818\n",
      "step: 37439\n",
      "loss: 12.552572250366211\n",
      "steps per second: 0.51774\n",
      "step: 37440\n",
      "loss: 12.796841621398926\n",
      "steps per second: 0.54281\n",
      "step: 37441\n",
      "loss: 13.222870826721191\n",
      "steps per second: 0.54082\n",
      "step: 37442\n",
      "loss: 12.790145874023438\n",
      "steps per second: 0.56261\n",
      "step: 37443\n",
      "loss: 12.085890769958496\n",
      "steps per second: 0.54633\n",
      "step: 37444\n",
      "loss: 12.543546676635742\n",
      "steps per second: 0.61443\n",
      "step: 37445\n",
      "loss: 12.886168479919434\n",
      "steps per second: 0.55990\n",
      "step: 37446\n",
      "loss: 12.739836692810059\n",
      "steps per second: 0.52096\n",
      "step: 37447\n",
      "loss: 13.620372772216797\n",
      "steps per second: 0.54278\n",
      "step: 37448\n",
      "loss: 12.36458969116211\n",
      "steps per second: 0.50862\n",
      "step: 37449\n",
      "loss: 13.022525787353516\n",
      "steps per second: 0.56678\n",
      "step: 37450\n",
      "loss: 12.629814147949219\n",
      "steps per second: 0.58543\n",
      "step: 37451\n",
      "loss: 12.676796913146973\n",
      "steps per second: 0.56711\n",
      "step: 37452\n",
      "loss: 12.30688190460205\n",
      "steps per second: 0.55840\n",
      "step: 37453\n",
      "loss: 12.713345527648926\n",
      "steps per second: 0.52887\n",
      "step: 37454\n",
      "loss: 12.839568138122559\n",
      "steps per second: 0.57158\n",
      "step: 37455\n",
      "loss: 12.73137092590332\n",
      "steps per second: 0.58137\n",
      "step: 37456\n",
      "loss: 12.8269624710083\n",
      "steps per second: 0.50076\n",
      "step: 37457\n",
      "loss: 13.016343116760254\n",
      "steps per second: 0.57078\n",
      "step: 37458\n",
      "loss: 12.853216171264648\n",
      "steps per second: 0.61781\n",
      "step: 37459\n",
      "loss: 13.163052558898926\n",
      "steps per second: 0.56243\n",
      "step: 37460\n",
      "loss: 13.392406463623047\n",
      "steps per second: 0.54228\n",
      "step: 37461\n",
      "loss: 12.892492294311523\n",
      "steps per second: 0.54676\n",
      "step: 37462\n",
      "loss: 12.695072174072266\n",
      "steps per second: 0.58574\n",
      "step: 37463\n",
      "loss: 12.927470207214355\n",
      "steps per second: 0.57168\n",
      "step: 37464\n",
      "loss: 12.84427261352539\n",
      "steps per second: 0.56385\n",
      "step: 37465\n",
      "loss: 12.894643783569336\n",
      "steps per second: 0.57408\n",
      "step: 37466\n",
      "loss: 13.064108848571777\n",
      "steps per second: 0.54445\n",
      "step: 37467\n",
      "loss: 12.607953071594238\n",
      "steps per second: 0.57743\n",
      "step: 37468\n",
      "loss: 12.610522270202637\n",
      "steps per second: 0.61893\n",
      "step: 37469\n",
      "loss: 12.744800567626953\n",
      "steps per second: 0.59001\n",
      "step: 37470\n",
      "loss: 12.784722328186035\n",
      "steps per second: 0.52929\n",
      "step: 37471\n",
      "loss: 13.041116714477539\n",
      "steps per second: 0.54324\n",
      "step: 37472\n",
      "loss: 12.473735809326172\n",
      "steps per second: 0.55029\n",
      "step: 37473\n",
      "loss: 12.744675636291504\n",
      "steps per second: 0.55922\n",
      "step: 37474\n",
      "loss: 12.722753524780273\n",
      "steps per second: 0.53041\n",
      "step: 37475\n",
      "loss: 12.541011810302734\n",
      "steps per second: 0.53431\n",
      "step: 37476\n",
      "loss: 12.479461669921875\n",
      "steps per second: 0.54800\n",
      "step: 37477\n",
      "loss: 12.842961311340332\n",
      "steps per second: 0.56347\n",
      "step: 37478\n",
      "loss: 12.8722505569458\n",
      "steps per second: 0.55708\n",
      "step: 37479\n",
      "loss: 12.856213569641113\n",
      "steps per second: 0.58037\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8188827633857727, layer: 10\n",
      "saving at step 37479\n",
      "----------\n",
      "\n",
      "\n",
      "step: 37480\n",
      "loss: 12.935097694396973\n",
      "steps per second: 0.28860\n",
      "step: 37481\n",
      "loss: 13.146210670471191\n",
      "steps per second: 0.56756\n",
      "step: 37482\n",
      "loss: 13.437017440795898\n",
      "steps per second: 0.55811\n",
      "step: 37483\n",
      "loss: 13.06505012512207\n",
      "steps per second: 0.55843\n",
      "step: 37484\n",
      "loss: 12.589489936828613\n",
      "steps per second: 0.56159\n",
      "step: 37485\n",
      "loss: 12.75102424621582\n",
      "steps per second: 0.57672\n",
      "step: 37486\n",
      "loss: 12.679967880249023\n",
      "steps per second: 0.57958\n",
      "step: 37487\n",
      "loss: 13.065391540527344\n",
      "steps per second: 0.55564\n",
      "step: 37488\n",
      "loss: 13.215163230895996\n",
      "steps per second: 0.58888\n",
      "step: 37489\n",
      "loss: 12.380247116088867\n",
      "steps per second: 0.58867\n",
      "step: 37490\n",
      "loss: 12.577044486999512\n",
      "steps per second: 0.56536\n",
      "step: 37491\n",
      "loss: 12.930502891540527\n",
      "steps per second: 0.56273\n",
      "step: 37492\n",
      "loss: 12.293539047241211\n",
      "steps per second: 0.56916\n",
      "step: 37493\n",
      "loss: 12.669166564941406\n",
      "steps per second: 0.57835\n",
      "step: 37494\n",
      "loss: 12.49483871459961\n",
      "steps per second: 0.54414\n",
      "step: 37495\n",
      "loss: 12.867953300476074\n",
      "steps per second: 0.55538\n",
      "step: 37496\n",
      "loss: 12.294218063354492\n",
      "steps per second: 0.53175\n",
      "step: 37497\n",
      "loss: 12.787821769714355\n",
      "steps per second: 0.55222\n",
      "step: 37498\n",
      "loss: 12.649404525756836\n",
      "steps per second: 0.62090\n",
      "step: 37499\n",
      "loss: 12.676785469055176\n",
      "steps per second: 0.57977\n",
      "step: 37500\n",
      "loss: 13.163320541381836\n",
      "steps per second: 0.56429\n",
      "step: 37501\n",
      "loss: 12.893584251403809\n",
      "steps per second: 0.50216\n",
      "step: 37502\n",
      "loss: 13.106114387512207\n",
      "steps per second: 0.55492\n",
      "step: 37503\n",
      "loss: 13.045289993286133\n",
      "steps per second: 0.50849\n",
      "step: 37504\n",
      "loss: 12.615813255310059\n",
      "steps per second: 0.56857\n",
      "step: 37505\n",
      "loss: 12.340056419372559\n",
      "steps per second: 0.56572\n",
      "step: 37506\n",
      "loss: 13.24936580657959\n",
      "steps per second: 0.54797\n",
      "step: 37507\n",
      "loss: 12.706573486328125\n",
      "steps per second: 0.52309\n",
      "step: 37508\n",
      "loss: 12.24432373046875\n",
      "steps per second: 0.53283\n",
      "step: 37509\n",
      "loss: 13.201003074645996\n",
      "steps per second: 0.54867\n",
      "step: 37510\n",
      "loss: 12.72110366821289\n",
      "steps per second: 0.53106\n",
      "step: 37511\n",
      "loss: 12.774004936218262\n",
      "steps per second: 0.51190\n",
      "step: 37512\n",
      "loss: 13.597742080688477\n",
      "steps per second: 0.57742\n",
      "step: 37513\n",
      "loss: 12.331852912902832\n",
      "steps per second: 0.51741\n",
      "step: 37514\n",
      "loss: 13.370465278625488\n",
      "steps per second: 0.53089\n",
      "step: 37515\n",
      "loss: 12.802000045776367\n",
      "steps per second: 0.59094\n",
      "step: 37516\n",
      "loss: 12.47114372253418\n",
      "steps per second: 0.54583\n",
      "step: 37517\n",
      "loss: 12.97073745727539\n",
      "steps per second: 0.57667\n",
      "step: 37518\n",
      "loss: 12.967538833618164\n",
      "steps per second: 0.56060\n",
      "step: 37519\n",
      "loss: 12.27122688293457\n",
      "steps per second: 0.48889\n",
      "step: 37520\n",
      "loss: 12.808419227600098\n",
      "steps per second: 0.55571\n",
      "step: 37521\n",
      "loss: 12.98357105255127\n",
      "steps per second: 0.59589\n",
      "step: 37522\n",
      "loss: 13.313628196716309\n",
      "steps per second: 0.61065\n",
      "step: 37523\n",
      "loss: 12.788891792297363\n",
      "steps per second: 0.55911\n",
      "step: 37524\n",
      "loss: 12.782127380371094\n",
      "steps per second: 0.51731\n",
      "step: 37525\n",
      "loss: 12.207698822021484\n",
      "steps per second: 0.53642\n",
      "step: 37526\n",
      "loss: 13.348097801208496\n",
      "steps per second: 0.53259\n",
      "step: 37527\n",
      "loss: 12.710007667541504\n",
      "steps per second: 0.51406\n",
      "step: 37528\n",
      "loss: 12.807544708251953\n",
      "steps per second: 0.55466\n",
      "step: 37529\n",
      "loss: 12.871650695800781\n",
      "steps per second: 0.54551\n",
      "step: 37530\n",
      "loss: 12.468570709228516\n",
      "steps per second: 0.56903\n",
      "step: 37531\n",
      "loss: 12.53257942199707\n",
      "steps per second: 0.54532\n",
      "step: 37532\n",
      "loss: 12.546110153198242\n",
      "steps per second: 0.57239\n",
      "step: 37533\n",
      "loss: 12.966845512390137\n",
      "steps per second: 0.56270\n",
      "step: 37534\n",
      "loss: 13.04061508178711\n",
      "steps per second: 0.52213\n",
      "step: 37535\n",
      "loss: 12.579512596130371\n",
      "steps per second: 0.57359\n",
      "step: 37536\n",
      "loss: 13.038378715515137\n",
      "steps per second: 0.55645\n",
      "step: 37537\n",
      "loss: 13.01779556274414\n",
      "steps per second: 0.60694\n",
      "step: 37538\n",
      "loss: 12.298843383789062\n",
      "steps per second: 0.55562\n",
      "step: 37539\n",
      "loss: 12.941617965698242\n",
      "steps per second: 0.56434\n",
      "step: 37540\n",
      "loss: 13.109198570251465\n",
      "steps per second: 0.61533\n",
      "step: 37541\n",
      "loss: 12.927772521972656\n",
      "steps per second: 0.55788\n",
      "step: 37542\n",
      "loss: 12.765275001525879\n",
      "steps per second: 0.62309\n",
      "step: 37543\n",
      "loss: 12.494246482849121\n",
      "steps per second: 0.55852\n",
      "step: 37544\n",
      "loss: 13.516305923461914\n",
      "steps per second: 0.59332\n",
      "step: 37545\n",
      "loss: 12.63593864440918\n",
      "steps per second: 0.54394\n",
      "step: 37546\n",
      "loss: 12.782083511352539\n",
      "steps per second: 0.57563\n",
      "step: 37547\n",
      "loss: 12.519099235534668\n",
      "steps per second: 0.46410\n",
      "step: 37548\n",
      "loss: 12.677885055541992\n",
      "steps per second: 0.56049\n",
      "step: 37549\n",
      "loss: 12.751504898071289\n",
      "steps per second: 0.52873\n",
      "step: 37550\n",
      "loss: 12.549842834472656\n",
      "steps per second: 0.56852\n",
      "step: 37551\n",
      "loss: 13.387323379516602\n",
      "steps per second: 0.51208\n",
      "step: 37552\n",
      "loss: 12.683640480041504\n",
      "steps per second: 0.56485\n",
      "step: 37553\n",
      "loss: 12.799202919006348\n",
      "steps per second: 0.58902\n",
      "step: 37554\n",
      "loss: 12.902759552001953\n",
      "steps per second: 0.59305\n",
      "step: 37555\n",
      "loss: 13.336102485656738\n",
      "steps per second: 0.54859\n",
      "step: 37556\n",
      "loss: 12.53896427154541\n",
      "steps per second: 0.62178\n",
      "step: 37557\n",
      "loss: 13.07807731628418\n",
      "steps per second: 0.54617\n",
      "step: 37558\n",
      "loss: 13.007251739501953\n",
      "steps per second: 0.61322\n",
      "step: 37559\n",
      "loss: 13.245760917663574\n",
      "steps per second: 0.53178\n",
      "step: 37560\n",
      "loss: 13.12000560760498\n",
      "steps per second: 0.57414\n",
      "step: 37561\n",
      "loss: 12.7954683303833\n",
      "steps per second: 0.55931\n",
      "step: 37562\n",
      "loss: 12.44532585144043\n",
      "steps per second: 0.61441\n",
      "step: 37563\n",
      "loss: 12.285627365112305\n",
      "steps per second: 0.56820\n",
      "step: 37564\n",
      "loss: 12.390094757080078\n",
      "steps per second: 0.53036\n",
      "step: 37565\n",
      "loss: 12.948285102844238\n",
      "steps per second: 0.52459\n",
      "step: 37566\n",
      "loss: 12.967580795288086\n",
      "steps per second: 0.56157\n",
      "step: 37567\n",
      "loss: 12.963610649108887\n",
      "steps per second: 0.55644\n",
      "step: 37568\n",
      "loss: 12.6021089553833\n",
      "steps per second: 0.53169\n",
      "step: 37569\n",
      "loss: 12.318510055541992\n",
      "steps per second: 0.54851\n",
      "step: 37570\n",
      "loss: 13.552754402160645\n",
      "steps per second: 0.53069\n",
      "step: 37571\n",
      "loss: 13.178723335266113\n",
      "steps per second: 0.57387\n",
      "step: 37572\n",
      "loss: 12.944977760314941\n",
      "steps per second: 0.56662\n",
      "step: 37573\n",
      "loss: 12.883770942687988\n",
      "steps per second: 0.54251\n",
      "step: 37574\n",
      "loss: 12.797760009765625\n",
      "steps per second: 0.55034\n",
      "step: 37575\n",
      "loss: 12.402447700500488\n",
      "steps per second: 0.55641\n",
      "step: 37576\n",
      "loss: 12.909618377685547\n",
      "steps per second: 0.50656\n",
      "step: 37577\n",
      "loss: 12.505687713623047\n",
      "steps per second: 0.56480\n",
      "step: 37578\n",
      "loss: 13.378482818603516\n",
      "steps per second: 0.62477\n",
      "step: 37579\n",
      "loss: 12.720671653747559\n",
      "steps per second: 0.56194\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7953980565071106, layer: 10\n",
      "saving at step 37579\n",
      "----------\n",
      "\n",
      "\n",
      "step: 37580\n",
      "loss: 12.573362350463867\n",
      "steps per second: 0.26366\n",
      "step: 37581\n",
      "loss: 12.707648277282715\n",
      "steps per second: 0.57488\n",
      "step: 37582\n",
      "loss: 12.303272247314453\n",
      "steps per second: 0.52572\n",
      "step: 37583\n",
      "loss: 12.579813003540039\n",
      "steps per second: 0.55481\n",
      "step: 37584\n",
      "loss: 12.992794036865234\n",
      "steps per second: 0.54171\n",
      "step: 37585\n",
      "loss: 13.828155517578125\n",
      "steps per second: 0.57446\n",
      "step: 37586\n",
      "loss: 12.86105728149414\n",
      "steps per second: 0.59275\n",
      "step: 37587\n",
      "loss: 12.66756820678711\n",
      "steps per second: 0.51200\n",
      "step: 37588\n",
      "loss: 13.231030464172363\n",
      "steps per second: 0.58783\n",
      "step: 37589\n",
      "loss: 13.330622673034668\n",
      "steps per second: 0.52766\n",
      "step: 37590\n",
      "loss: 12.58151912689209\n",
      "steps per second: 0.56242\n",
      "step: 37591\n",
      "loss: 12.994397163391113\n",
      "steps per second: 0.54382\n",
      "step: 37592\n",
      "loss: 12.714890480041504\n",
      "steps per second: 0.53506\n",
      "step: 37593\n",
      "loss: 12.868645668029785\n",
      "steps per second: 0.55597\n",
      "step: 37594\n",
      "loss: 12.498583793640137\n",
      "steps per second: 0.52768\n",
      "step: 37595\n",
      "loss: 12.805071830749512\n",
      "steps per second: 0.57651\n",
      "step: 37596\n",
      "loss: 13.15137767791748\n",
      "steps per second: 0.55306\n",
      "step: 37597\n",
      "loss: 12.564065933227539\n",
      "steps per second: 0.58452\n",
      "step: 37598\n",
      "loss: 12.936850547790527\n",
      "steps per second: 0.55651\n",
      "step: 37599\n",
      "loss: 12.8963623046875\n",
      "steps per second: 0.58298\n",
      "step: 37600\n",
      "loss: 12.643574714660645\n",
      "steps per second: 0.54477\n",
      "step: 37601\n",
      "loss: 11.954469680786133\n",
      "steps per second: 0.51718\n",
      "step: 37602\n",
      "loss: 12.777569770812988\n",
      "steps per second: 0.54556\n",
      "step: 37603\n",
      "loss: 12.855952262878418\n",
      "steps per second: 0.49505\n",
      "step: 37604\n",
      "loss: 12.9177827835083\n",
      "steps per second: 0.50639\n",
      "step: 37605\n",
      "loss: 12.625298500061035\n",
      "steps per second: 0.56130\n",
      "step: 37606\n",
      "loss: 12.626575469970703\n",
      "steps per second: 0.59674\n",
      "step: 37607\n",
      "loss: 13.10305118560791\n",
      "steps per second: 0.57378\n",
      "step: 37608\n",
      "loss: 12.8402738571167\n",
      "steps per second: 0.50538\n",
      "step: 37609\n",
      "loss: 12.766337394714355\n",
      "steps per second: 0.52082\n",
      "step: 37610\n",
      "loss: 13.165166854858398\n",
      "steps per second: 0.55325\n",
      "step: 37611\n",
      "loss: 13.386069297790527\n",
      "steps per second: 0.55065\n",
      "step: 37612\n",
      "loss: 13.06110954284668\n",
      "steps per second: 0.53094\n",
      "step: 37613\n",
      "loss: 12.81933879852295\n",
      "steps per second: 0.54218\n",
      "step: 37614\n",
      "loss: 12.952550888061523\n",
      "steps per second: 0.56592\n",
      "step: 37615\n",
      "loss: 12.33409309387207\n",
      "steps per second: 0.43246\n",
      "step: 37616\n",
      "loss: 12.921162605285645\n",
      "steps per second: 0.48564\n",
      "step: 37617\n",
      "loss: 12.910938262939453\n",
      "steps per second: 0.55512\n",
      "step: 37618\n",
      "loss: 12.154817581176758\n",
      "steps per second: 0.57195\n",
      "step: 37619\n",
      "loss: 12.828900337219238\n",
      "steps per second: 0.52569\n",
      "step: 37620\n",
      "loss: 12.728166580200195\n",
      "steps per second: 0.46345\n",
      "step: 37621\n",
      "loss: 12.567773818969727\n",
      "steps per second: 0.41795\n",
      "step: 37622\n",
      "loss: 12.818035125732422\n",
      "steps per second: 0.52022\n",
      "step: 37623\n",
      "loss: 12.127098083496094\n",
      "steps per second: 0.61947\n",
      "step: 37624\n",
      "loss: 12.475146293640137\n",
      "steps per second: 0.55893\n",
      "step: 37625\n",
      "loss: 12.939037322998047\n",
      "steps per second: 0.56084\n",
      "step: 37626\n",
      "loss: 13.162546157836914\n",
      "steps per second: 0.60509\n",
      "step: 37627\n",
      "loss: 12.756021499633789\n",
      "steps per second: 0.54219\n",
      "step: 37628\n",
      "loss: 12.823169708251953\n",
      "steps per second: 0.55858\n",
      "step: 37629\n",
      "loss: 12.698736190795898\n",
      "steps per second: 0.56384\n",
      "step: 37630\n",
      "loss: 12.63492488861084\n",
      "steps per second: 0.57000\n",
      "step: 37631\n",
      "loss: 13.082319259643555\n",
      "steps per second: 0.55351\n",
      "step: 37632\n",
      "loss: 12.359471321105957\n",
      "steps per second: 0.54189\n",
      "step: 37633\n",
      "loss: 12.94731616973877\n",
      "steps per second: 0.54760\n",
      "step: 37634\n",
      "loss: 13.114625930786133\n",
      "steps per second: 0.51515\n",
      "step: 37635\n",
      "loss: 12.67529582977295\n",
      "steps per second: 0.56280\n",
      "step: 37636\n",
      "loss: 13.050493240356445\n",
      "steps per second: 0.58065\n",
      "step: 37637\n",
      "loss: 12.788703918457031\n",
      "steps per second: 0.56302\n",
      "step: 37638\n",
      "loss: 12.813590049743652\n",
      "steps per second: 0.56442\n",
      "step: 37639\n",
      "loss: 12.55205249786377\n",
      "steps per second: 0.54354\n",
      "step: 37640\n",
      "loss: 12.630910873413086\n",
      "steps per second: 0.55460\n",
      "step: 37641\n",
      "loss: 13.199259757995605\n",
      "steps per second: 0.49461\n",
      "step: 37642\n",
      "loss: 13.00314998626709\n",
      "steps per second: 0.45613\n",
      "step: 37643\n",
      "loss: 13.546860694885254\n",
      "steps per second: 0.49524\n",
      "step: 37644\n",
      "loss: 12.184017181396484\n",
      "steps per second: 0.52209\n",
      "step: 37645\n",
      "loss: 13.221155166625977\n",
      "steps per second: 0.53953\n",
      "step: 37646\n",
      "loss: 13.62426471710205\n",
      "steps per second: 0.53211\n",
      "step: 37647\n",
      "loss: 12.590961456298828\n",
      "steps per second: 0.49557\n",
      "step: 37648\n",
      "loss: 12.763901710510254\n",
      "steps per second: 0.52926\n",
      "step: 37649\n",
      "loss: 13.317306518554688\n",
      "steps per second: 0.50688\n",
      "step: 37650\n",
      "loss: 12.656070709228516\n",
      "steps per second: 0.47733\n",
      "step: 37651\n",
      "loss: 12.96406364440918\n",
      "steps per second: 0.48528\n",
      "step: 37652\n",
      "loss: 13.250619888305664\n",
      "steps per second: 0.47625\n",
      "step: 37653\n",
      "loss: 12.742276191711426\n",
      "steps per second: 0.53436\n",
      "step: 37654\n",
      "loss: 12.508330345153809\n",
      "steps per second: 0.46612\n",
      "step: 37655\n",
      "loss: 13.148673057556152\n",
      "steps per second: 0.48502\n",
      "step: 37656\n",
      "loss: 12.903026580810547\n",
      "steps per second: 0.55162\n",
      "step: 37657\n",
      "loss: 12.14190673828125\n",
      "steps per second: 0.51548\n",
      "step: 37658\n",
      "loss: 13.039223670959473\n",
      "steps per second: 0.51955\n",
      "step: 37659\n",
      "loss: 13.06362533569336\n",
      "steps per second: 0.51422\n",
      "step: 37660\n",
      "loss: 12.690251350402832\n",
      "steps per second: 0.49008\n",
      "step: 37661\n",
      "loss: 12.83654499053955\n",
      "steps per second: 0.42660\n",
      "step: 37662\n",
      "loss: 12.898438453674316\n",
      "steps per second: 0.49394\n",
      "step: 37663\n",
      "loss: 12.880393981933594\n",
      "steps per second: 0.51894\n",
      "step: 37664\n",
      "loss: 12.706262588500977\n",
      "steps per second: 0.53891\n",
      "step: 37665\n",
      "loss: 12.962764739990234\n",
      "steps per second: 0.54560\n",
      "step: 37666\n",
      "loss: 12.726371765136719\n",
      "steps per second: 0.51540\n",
      "step: 37667\n",
      "loss: 12.734698295593262\n",
      "steps per second: 0.57201\n",
      "step: 37668\n",
      "loss: 12.837373733520508\n",
      "steps per second: 0.49052\n",
      "step: 37669\n",
      "loss: 12.639939308166504\n",
      "steps per second: 0.49236\n",
      "step: 37670\n",
      "loss: 13.390347480773926\n",
      "steps per second: 0.52168\n",
      "step: 37671\n",
      "loss: 12.874351501464844\n",
      "steps per second: 0.54596\n",
      "step: 37672\n",
      "loss: 13.010927200317383\n",
      "steps per second: 0.53628\n",
      "step: 37673\n",
      "loss: 13.151833534240723\n",
      "steps per second: 0.49564\n",
      "step: 37674\n",
      "loss: 12.683329582214355\n",
      "steps per second: 0.57731\n",
      "step: 37675\n",
      "loss: 13.314400672912598\n",
      "steps per second: 0.59652\n",
      "step: 37676\n",
      "loss: 12.482976913452148\n",
      "steps per second: 0.54030\n",
      "step: 37677\n",
      "loss: 13.151694297790527\n",
      "steps per second: 0.51105\n",
      "step: 37678\n",
      "loss: 12.800863265991211\n",
      "steps per second: 0.54796\n",
      "step: 37679\n",
      "loss: 12.510311126708984\n",
      "steps per second: 0.57075\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7940390706062317, layer: 11\n",
      "saving at step 37679\n",
      "----------\n",
      "\n",
      "\n",
      "step: 37680\n",
      "loss: 12.368043899536133\n",
      "steps per second: 0.26477\n",
      "step: 37681\n",
      "loss: 12.823424339294434\n",
      "steps per second: 0.47024\n",
      "step: 37682\n",
      "loss: 12.734699249267578\n",
      "steps per second: 0.49846\n",
      "step: 37683\n",
      "loss: 12.83194637298584\n",
      "steps per second: 0.48289\n",
      "step: 37684\n",
      "loss: 13.328837394714355\n",
      "steps per second: 0.43069\n",
      "step: 37685\n",
      "loss: 13.079426765441895\n",
      "steps per second: 0.47263\n",
      "step: 37686\n",
      "loss: 12.811652183532715\n",
      "steps per second: 0.38220\n",
      "step: 37687\n",
      "loss: 12.131876945495605\n",
      "steps per second: 0.37828\n",
      "step: 37688\n",
      "loss: 12.22791862487793\n",
      "steps per second: 0.50576\n",
      "step: 37689\n",
      "loss: 12.72895622253418\n",
      "steps per second: 0.48029\n",
      "step: 37690\n",
      "loss: 12.884178161621094\n",
      "steps per second: 0.50464\n",
      "step: 37691\n",
      "loss: 12.657160758972168\n",
      "steps per second: 0.49745\n",
      "step: 37692\n",
      "loss: 13.000992774963379\n",
      "steps per second: 0.51253\n",
      "step: 37693\n",
      "loss: 12.513715744018555\n",
      "steps per second: 0.51874\n",
      "step: 37694\n",
      "loss: 13.24735164642334\n",
      "steps per second: 0.49168\n",
      "step: 37695\n",
      "loss: 12.685713768005371\n",
      "steps per second: 0.46519\n",
      "step: 37696\n",
      "loss: 12.848697662353516\n",
      "steps per second: 0.45173\n",
      "step: 37697\n",
      "loss: 12.690864562988281\n",
      "steps per second: 0.47711\n",
      "step: 37698\n",
      "loss: 12.796289443969727\n",
      "steps per second: 0.53919\n",
      "step: 37699\n",
      "loss: 12.517526626586914\n",
      "steps per second: 0.52684\n",
      "step: 37700\n",
      "loss: 12.589995384216309\n",
      "steps per second: 0.49350\n",
      "step: 37701\n",
      "loss: 12.44704818725586\n",
      "steps per second: 0.50449\n",
      "step: 37702\n",
      "loss: 13.126337051391602\n",
      "steps per second: 0.50829\n",
      "step: 37703\n",
      "loss: 13.424958229064941\n",
      "steps per second: 0.49809\n",
      "step: 37704\n",
      "loss: 13.108428001403809\n",
      "steps per second: 0.51347\n",
      "step: 37705\n",
      "loss: 13.070301055908203\n",
      "steps per second: 0.46800\n",
      "step: 37706\n",
      "loss: 12.773843765258789\n",
      "steps per second: 0.43048\n",
      "step: 37707\n",
      "loss: 12.515149116516113\n",
      "steps per second: 0.49896\n",
      "step: 37708\n",
      "loss: 12.573416709899902\n",
      "steps per second: 0.51707\n",
      "step: 37709\n",
      "loss: 13.22238826751709\n",
      "steps per second: 0.50161\n",
      "step: 37710\n",
      "loss: 12.379509925842285\n",
      "steps per second: 0.49677\n",
      "step: 37711\n",
      "loss: 12.83620834350586\n",
      "steps per second: 0.48768\n",
      "step: 37712\n",
      "loss: 12.458118438720703\n",
      "steps per second: 0.47759\n",
      "step: 37713\n",
      "loss: 12.794153213500977\n",
      "steps per second: 0.50799\n",
      "step: 37714\n",
      "loss: 12.40046501159668\n",
      "steps per second: 0.53006\n",
      "step: 37715\n",
      "loss: 12.933571815490723\n",
      "steps per second: 0.55158\n",
      "step: 37716\n",
      "loss: 12.708438873291016\n",
      "steps per second: 0.54452\n",
      "step: 37717\n",
      "loss: 13.449509620666504\n",
      "steps per second: 0.52452\n",
      "step: 37718\n",
      "loss: 12.806844711303711\n",
      "steps per second: 0.58355\n",
      "step: 37719\n",
      "loss: 12.302053451538086\n",
      "steps per second: 0.54170\n",
      "step: 37720\n",
      "loss: 12.664202690124512\n",
      "steps per second: 0.55419\n",
      "step: 37721\n",
      "loss: 13.018597602844238\n",
      "steps per second: 0.52982\n",
      "step: 37722\n",
      "loss: 12.37949275970459\n",
      "steps per second: 0.55242\n",
      "step: 37723\n",
      "loss: 12.826515197753906\n",
      "steps per second: 0.51418\n",
      "step: 37724\n",
      "loss: 13.221665382385254\n",
      "steps per second: 0.56857\n",
      "step: 37725\n",
      "loss: 12.809803009033203\n",
      "steps per second: 0.57373\n",
      "step: 37726\n",
      "loss: 12.275872230529785\n",
      "steps per second: 0.55363\n",
      "step: 37727\n",
      "loss: 12.819249153137207\n",
      "steps per second: 0.55205\n",
      "step: 37728\n",
      "loss: 13.164194107055664\n",
      "steps per second: 0.54043\n",
      "step: 37729\n",
      "loss: 12.75407600402832\n",
      "steps per second: 0.53864\n",
      "step: 37730\n",
      "loss: 12.642671585083008\n",
      "steps per second: 0.55426\n",
      "step: 37731\n",
      "loss: 12.851404190063477\n",
      "steps per second: 0.54070\n",
      "step: 37732\n",
      "loss: 12.59375\n",
      "steps per second: 0.54886\n",
      "step: 37733\n",
      "loss: 12.471534729003906\n",
      "steps per second: 0.55340\n",
      "step: 37734\n",
      "loss: 12.920268058776855\n",
      "steps per second: 0.54655\n",
      "step: 37735\n",
      "loss: 12.917829513549805\n",
      "steps per second: 0.57095\n",
      "step: 37736\n",
      "loss: 12.572524070739746\n",
      "steps per second: 0.57048\n",
      "step: 37737\n",
      "loss: 12.57208251953125\n",
      "steps per second: 0.57686\n",
      "step: 37738\n",
      "loss: 13.513833999633789\n",
      "steps per second: 0.51783\n",
      "step: 37739\n",
      "loss: 13.101910591125488\n",
      "steps per second: 0.61294\n",
      "step: 37740\n",
      "loss: 13.023371696472168\n",
      "steps per second: 0.50124\n",
      "step: 37741\n",
      "loss: 12.95802116394043\n",
      "steps per second: 0.57269\n",
      "step: 37742\n",
      "loss: 13.322617530822754\n",
      "steps per second: 0.57753\n",
      "step: 37743\n",
      "loss: 12.567280769348145\n",
      "steps per second: 0.55252\n",
      "step: 37744\n",
      "loss: 12.725141525268555\n",
      "steps per second: 0.61311\n",
      "step: 37745\n",
      "loss: 12.130440711975098\n",
      "steps per second: 0.52344\n",
      "step: 37746\n",
      "loss: 13.171711921691895\n",
      "steps per second: 0.55642\n",
      "step: 37747\n",
      "loss: 12.661417961120605\n",
      "steps per second: 0.60783\n",
      "step: 37748\n",
      "loss: 12.89996337890625\n",
      "steps per second: 0.54965\n",
      "step: 37749\n",
      "loss: 12.761850357055664\n",
      "steps per second: 0.61354\n",
      "step: 37750\n",
      "loss: 12.395539283752441\n",
      "steps per second: 0.56064\n",
      "step: 37751\n",
      "loss: 12.80659294128418\n",
      "steps per second: 0.52894\n",
      "step: 37752\n",
      "loss: 12.340313911437988\n",
      "steps per second: 0.52133\n",
      "step: 37753\n",
      "loss: 13.121039390563965\n",
      "steps per second: 0.57267\n",
      "step: 37754\n",
      "loss: 12.55669116973877\n",
      "steps per second: 0.57083\n",
      "step: 37755\n",
      "loss: 13.261209487915039\n",
      "steps per second: 0.50348\n",
      "step: 37756\n",
      "loss: 12.98239803314209\n",
      "steps per second: 0.53774\n",
      "step: 37757\n",
      "loss: 12.751957893371582\n",
      "steps per second: 0.55795\n",
      "step: 37758\n",
      "loss: 12.865516662597656\n",
      "steps per second: 0.49393\n",
      "step: 37759\n",
      "loss: 13.153127670288086\n",
      "steps per second: 0.42874\n",
      "step: 37760\n",
      "loss: 12.611087799072266\n",
      "steps per second: 0.45729\n",
      "step: 37761\n",
      "loss: 12.662932395935059\n",
      "steps per second: 0.48262\n",
      "step: 37762\n",
      "loss: 12.64605712890625\n",
      "steps per second: 0.51022\n",
      "step: 37763\n",
      "loss: 13.39784049987793\n",
      "steps per second: 0.51264\n",
      "step: 37764\n",
      "loss: 12.835451126098633\n",
      "steps per second: 0.54738\n",
      "step: 37765\n",
      "loss: 13.048171997070312\n",
      "steps per second: 0.51849\n",
      "step: 37766\n",
      "loss: 12.814096450805664\n",
      "steps per second: 0.54685\n",
      "step: 37767\n",
      "loss: 12.864471435546875\n",
      "steps per second: 0.48903\n",
      "step: 37768\n",
      "loss: 12.940751075744629\n",
      "steps per second: 0.48933\n",
      "step: 37769\n",
      "loss: 13.476680755615234\n",
      "steps per second: 0.47016\n",
      "step: 37770\n",
      "loss: 13.046457290649414\n",
      "steps per second: 0.44774\n",
      "step: 37771\n",
      "loss: 12.803318977355957\n",
      "steps per second: 0.52005\n",
      "step: 37772\n",
      "loss: 12.425874710083008\n",
      "steps per second: 0.50560\n",
      "step: 37773\n",
      "loss: 12.75845718383789\n",
      "steps per second: 0.53373\n",
      "step: 37774\n",
      "loss: 12.367578506469727\n",
      "steps per second: 0.51797\n",
      "step: 37775\n",
      "loss: 12.770750045776367\n",
      "steps per second: 0.50564\n",
      "step: 37776\n",
      "loss: 12.839715957641602\n",
      "steps per second: 0.50190\n",
      "step: 37777\n",
      "loss: 12.515451431274414\n",
      "steps per second: 0.51804\n",
      "step: 37778\n",
      "loss: 12.704874992370605\n",
      "steps per second: 0.47140\n",
      "step: 37779\n",
      "loss: 12.19985294342041\n",
      "steps per second: 0.50401\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7873188257217407, layer: 10\n",
      "saving at step 37779\n",
      "----------\n",
      "\n",
      "\n",
      "step: 37780\n",
      "loss: 12.854844093322754\n",
      "steps per second: 0.26844\n",
      "step: 37781\n",
      "loss: 12.831854820251465\n",
      "steps per second: 0.53252\n",
      "step: 37782\n",
      "loss: 12.44669246673584\n",
      "steps per second: 0.52863\n",
      "step: 37783\n",
      "loss: 12.373369216918945\n",
      "steps per second: 0.54551\n",
      "step: 37784\n",
      "loss: 12.575871467590332\n",
      "steps per second: 0.49075\n",
      "step: 37785\n",
      "loss: 12.703319549560547\n",
      "steps per second: 0.52011\n",
      "step: 37786\n",
      "loss: 13.028871536254883\n",
      "steps per second: 0.49010\n",
      "step: 37787\n",
      "loss: 12.550021171569824\n",
      "steps per second: 0.53851\n",
      "step: 37788\n",
      "loss: 13.031590461730957\n",
      "steps per second: 0.57454\n",
      "step: 37789\n",
      "loss: 12.831064224243164\n",
      "steps per second: 0.53915\n",
      "step: 37790\n",
      "loss: 12.718835830688477\n",
      "steps per second: 0.58696\n",
      "step: 37791\n",
      "loss: 13.158408164978027\n",
      "steps per second: 0.57799\n",
      "step: 37792\n",
      "loss: 12.451348304748535\n",
      "steps per second: 0.58190\n",
      "step: 37793\n",
      "loss: 12.655759811401367\n",
      "steps per second: 0.55220\n",
      "step: 37794\n",
      "loss: 13.1195068359375\n",
      "steps per second: 0.54960\n",
      "step: 37795\n",
      "loss: 12.504729270935059\n",
      "steps per second: 0.54549\n",
      "step: 37796\n",
      "loss: 13.682394027709961\n",
      "steps per second: 0.54471\n",
      "step: 37797\n",
      "loss: 12.845843315124512\n",
      "steps per second: 0.51946\n",
      "step: 37798\n",
      "loss: 12.846193313598633\n",
      "steps per second: 0.51647\n",
      "step: 37799\n",
      "loss: 12.516670227050781\n",
      "steps per second: 0.54330\n",
      "step: 37800\n",
      "loss: 12.843944549560547\n",
      "steps per second: 0.57959\n",
      "step: 37801\n",
      "loss: 13.086944580078125\n",
      "steps per second: 0.56688\n",
      "step: 37802\n",
      "loss: 12.212201118469238\n",
      "steps per second: 0.55723\n",
      "step: 37803\n",
      "loss: 12.59100341796875\n",
      "steps per second: 0.51691\n",
      "step: 37804\n",
      "loss: 13.147960662841797\n",
      "steps per second: 0.52630\n",
      "step: 37805\n",
      "loss: 13.405134201049805\n",
      "steps per second: 0.46519\n",
      "step: 37806\n",
      "loss: 13.106420516967773\n",
      "steps per second: 0.47499\n",
      "step: 37807\n",
      "loss: 12.507867813110352\n",
      "steps per second: 0.51434\n",
      "step: 37808\n",
      "loss: 12.220100402832031\n",
      "steps per second: 0.51756\n",
      "step: 37809\n",
      "loss: 12.680156707763672\n",
      "steps per second: 0.48187\n",
      "step: 37810\n",
      "loss: 13.174561500549316\n",
      "steps per second: 0.51115\n",
      "step: 37811\n",
      "loss: 12.914179801940918\n",
      "steps per second: 0.53562\n",
      "step: 37812\n",
      "loss: 12.808809280395508\n",
      "steps per second: 0.53254\n",
      "step: 37813\n",
      "loss: 13.118257522583008\n",
      "steps per second: 0.51657\n",
      "step: 37814\n",
      "loss: 12.861044883728027\n",
      "steps per second: 0.44745\n",
      "step: 37815\n",
      "loss: 12.952369689941406\n",
      "steps per second: 0.44903\n",
      "step: 37816\n",
      "loss: 13.027359008789062\n",
      "steps per second: 0.47631\n",
      "step: 37817\n",
      "loss: 12.714981079101562\n",
      "steps per second: 0.51218\n",
      "step: 37818\n",
      "loss: 12.79858684539795\n",
      "steps per second: 0.53664\n",
      "step: 37819\n",
      "loss: 12.807367324829102\n",
      "steps per second: 0.50203\n",
      "step: 37820\n",
      "loss: 12.806015014648438\n",
      "steps per second: 0.56709\n",
      "step: 37821\n",
      "loss: 12.574338912963867\n",
      "steps per second: 0.52131\n",
      "step: 37822\n",
      "loss: 13.902256965637207\n",
      "steps per second: 0.51801\n",
      "step: 37823\n",
      "loss: 12.894617080688477\n",
      "steps per second: 0.47465\n",
      "step: 37824\n",
      "loss: 12.731700897216797\n",
      "steps per second: 0.43692\n",
      "step: 37825\n",
      "loss: 12.61715316772461\n",
      "steps per second: 0.49725\n",
      "step: 37826\n",
      "loss: 12.447400093078613\n",
      "steps per second: 0.48876\n",
      "step: 37827\n",
      "loss: 13.157393455505371\n",
      "steps per second: 0.48564\n",
      "step: 37828\n",
      "loss: 13.116004943847656\n",
      "steps per second: 0.51600\n",
      "step: 37829\n",
      "loss: 12.952986717224121\n",
      "steps per second: 0.49105\n",
      "step: 37830\n",
      "loss: 12.96922492980957\n",
      "steps per second: 0.54719\n",
      "step: 37831\n",
      "loss: 12.997749328613281\n",
      "steps per second: 0.53597\n",
      "step: 37832\n",
      "loss: 12.605107307434082\n",
      "steps per second: 0.51367\n",
      "step: 37833\n",
      "loss: 12.827900886535645\n",
      "steps per second: 0.53648\n",
      "step: 37834\n",
      "loss: 13.045815467834473\n",
      "steps per second: 0.57814\n",
      "step: 37835\n",
      "loss: 12.455856323242188\n",
      "steps per second: 0.53552\n",
      "step: 37836\n",
      "loss: 12.766342163085938\n",
      "steps per second: 0.49443\n",
      "step: 37837\n",
      "loss: 12.513969421386719\n",
      "steps per second: 0.61944\n",
      "step: 37838\n",
      "loss: 13.07226276397705\n",
      "steps per second: 0.61717\n",
      "step: 37839\n",
      "loss: 13.478874206542969\n",
      "steps per second: 0.53602\n",
      "step: 37840\n",
      "loss: 12.593978881835938\n",
      "steps per second: 0.55652\n",
      "step: 37841\n",
      "loss: 12.719284057617188\n",
      "steps per second: 0.54397\n",
      "step: 37842\n",
      "loss: 12.664965629577637\n",
      "steps per second: 0.55843\n",
      "step: 37843\n",
      "loss: 12.651328086853027\n",
      "steps per second: 0.54000\n",
      "step: 37844\n",
      "loss: 12.74573802947998\n",
      "steps per second: 0.58875\n",
      "step: 37845\n",
      "loss: 13.23071575164795\n",
      "steps per second: 0.57808\n",
      "step: 37846\n",
      "loss: 12.948541641235352\n",
      "steps per second: 0.58139\n",
      "step: 37847\n",
      "loss: 13.161569595336914\n",
      "steps per second: 0.54352\n",
      "step: 37848\n",
      "loss: 13.189849853515625\n",
      "steps per second: 0.53704\n",
      "step: 37849\n",
      "loss: 12.978425025939941\n",
      "steps per second: 0.54967\n",
      "step: 37850\n",
      "loss: 12.78549575805664\n",
      "steps per second: 0.51578\n",
      "step: 37851\n",
      "loss: 12.655556678771973\n",
      "steps per second: 0.55209\n",
      "step: 37852\n",
      "loss: 12.606630325317383\n",
      "steps per second: 0.53115\n",
      "step: 37853\n",
      "loss: 12.768280982971191\n",
      "steps per second: 0.55832\n",
      "step: 37854\n",
      "loss: 12.805667877197266\n",
      "steps per second: 0.58467\n",
      "step: 37855\n",
      "loss: 12.610239028930664\n",
      "steps per second: 0.53612\n",
      "step: 37856\n",
      "loss: 12.637045860290527\n",
      "steps per second: 0.51648\n",
      "step: 37857\n",
      "loss: 12.30400276184082\n",
      "steps per second: 0.56933\n",
      "step: 37858\n",
      "loss: 12.603901863098145\n",
      "steps per second: 0.59397\n",
      "step: 37859\n",
      "loss: 12.920307159423828\n",
      "steps per second: 0.55889\n",
      "step: 37860\n",
      "loss: 12.438323020935059\n",
      "steps per second: 0.55351\n",
      "step: 37861\n",
      "loss: 12.576022148132324\n",
      "steps per second: 0.57884\n",
      "step: 37862\n",
      "loss: 13.542912483215332\n",
      "steps per second: 0.52793\n",
      "step: 37863\n",
      "loss: 13.006193161010742\n",
      "steps per second: 0.55756\n",
      "step: 37864\n",
      "loss: 12.941250801086426\n",
      "steps per second: 0.57797\n",
      "step: 37865\n",
      "loss: 13.462831497192383\n",
      "steps per second: 0.54490\n",
      "step: 37866\n",
      "loss: 13.061816215515137\n",
      "steps per second: 0.54651\n",
      "step: 37867\n",
      "loss: 12.388450622558594\n",
      "steps per second: 0.60824\n",
      "step: 37868\n",
      "loss: 13.016444206237793\n",
      "steps per second: 0.52329\n",
      "step: 37869\n",
      "loss: 13.58318042755127\n",
      "steps per second: 0.58562\n",
      "step: 37870\n",
      "loss: 12.636419296264648\n",
      "steps per second: 0.55236\n",
      "step: 37871\n",
      "loss: 13.229881286621094\n",
      "steps per second: 0.55794\n",
      "step: 37872\n",
      "loss: 13.43079948425293\n",
      "steps per second: 0.57443\n",
      "step: 37873\n",
      "loss: 12.826567649841309\n",
      "steps per second: 0.54128\n",
      "step: 37874\n",
      "loss: 12.55626106262207\n",
      "steps per second: 0.61824\n",
      "step: 37875\n",
      "loss: 13.073366165161133\n",
      "steps per second: 0.52453\n",
      "step: 37876\n",
      "loss: 12.889235496520996\n",
      "steps per second: 0.54100\n",
      "step: 37877\n",
      "loss: 13.076855659484863\n",
      "steps per second: 0.51215\n",
      "step: 37878\n",
      "loss: 12.200206756591797\n",
      "steps per second: 0.55208\n",
      "step: 37879\n",
      "loss: 12.57767105102539\n",
      "steps per second: 0.53050\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8411287665367126, layer: 10\n",
      "saving at step 37879\n",
      "----------\n",
      "\n",
      "\n",
      "step: 37880\n",
      "loss: 12.770829200744629\n",
      "steps per second: 0.26834\n",
      "step: 37881\n",
      "loss: 12.488117218017578\n",
      "steps per second: 0.52734\n",
      "step: 37882\n",
      "loss: 12.706172943115234\n",
      "steps per second: 0.61593\n",
      "step: 37883\n",
      "loss: 12.584198951721191\n",
      "steps per second: 0.55125\n",
      "step: 37884\n",
      "loss: 12.723549842834473\n",
      "steps per second: 0.55485\n",
      "step: 37885\n",
      "loss: 13.425643920898438\n",
      "steps per second: 0.54450\n",
      "step: 37886\n",
      "loss: 12.652517318725586\n",
      "steps per second: 0.54915\n",
      "step: 37887\n",
      "loss: 13.27272891998291\n",
      "steps per second: 0.55661\n",
      "step: 37888\n",
      "loss: 12.617108345031738\n",
      "steps per second: 0.52093\n",
      "step: 37889\n",
      "loss: 12.938403129577637\n",
      "steps per second: 0.57247\n",
      "step: 37890\n",
      "loss: 12.859034538269043\n",
      "steps per second: 0.55484\n",
      "step: 37891\n",
      "loss: 12.636449813842773\n",
      "steps per second: 0.61705\n",
      "step: 37892\n",
      "loss: 12.465706825256348\n",
      "steps per second: 0.54640\n",
      "step: 37893\n",
      "loss: 12.620407104492188\n",
      "steps per second: 0.54024\n",
      "step: 37894\n",
      "loss: 13.076639175415039\n",
      "steps per second: 0.55514\n",
      "step: 37895\n",
      "loss: 12.341997146606445\n",
      "steps per second: 0.55335\n",
      "step: 37896\n",
      "loss: 12.241753578186035\n",
      "steps per second: 0.55290\n",
      "step: 37897\n",
      "loss: 12.81514835357666\n",
      "steps per second: 0.50381\n",
      "step: 37898\n",
      "loss: 12.478788375854492\n",
      "steps per second: 0.57711\n",
      "step: 37899\n",
      "loss: 13.526015281677246\n",
      "steps per second: 0.58295\n",
      "step: 37900\n",
      "loss: 13.107394218444824\n",
      "steps per second: 0.55630\n",
      "step: 37901\n",
      "loss: 12.662030220031738\n",
      "steps per second: 0.54317\n",
      "step: 37902\n",
      "loss: 12.527353286743164\n",
      "steps per second: 0.54225\n",
      "step: 37903\n",
      "loss: 12.347765922546387\n",
      "steps per second: 0.57618\n",
      "step: 37904\n",
      "loss: 12.858514785766602\n",
      "steps per second: 0.62139\n",
      "step: 37905\n",
      "loss: 12.665104866027832\n",
      "steps per second: 0.58001\n",
      "step: 37906\n",
      "loss: 12.785232543945312\n",
      "steps per second: 0.57591\n",
      "step: 37907\n",
      "loss: 12.537898063659668\n",
      "steps per second: 0.57431\n",
      "step: 37908\n",
      "loss: 12.7116060256958\n",
      "steps per second: 0.55756\n",
      "step: 37909\n",
      "loss: 13.456379890441895\n",
      "steps per second: 0.53934\n",
      "step: 37910\n",
      "loss: 12.5570650100708\n",
      "steps per second: 0.48964\n",
      "step: 37911\n",
      "loss: 12.967167854309082\n",
      "steps per second: 0.52818\n",
      "step: 37912\n",
      "loss: 12.575737953186035\n",
      "steps per second: 0.57878\n",
      "step: 37913\n",
      "loss: 13.056306838989258\n",
      "steps per second: 0.50249\n",
      "step: 37914\n",
      "loss: 12.73853874206543\n",
      "steps per second: 0.55731\n",
      "step: 37915\n",
      "loss: 13.030125617980957\n",
      "steps per second: 0.53452\n",
      "step: 37916\n",
      "loss: 12.971983909606934\n",
      "steps per second: 0.54917\n",
      "step: 37917\n",
      "loss: 12.92525577545166\n",
      "steps per second: 0.58020\n",
      "step: 37918\n",
      "loss: 12.48784351348877\n",
      "steps per second: 0.54494\n",
      "step: 37919\n",
      "loss: 12.856846809387207\n",
      "steps per second: 0.58217\n",
      "step: 37920\n",
      "loss: 12.581330299377441\n",
      "steps per second: 0.52244\n",
      "step: 37921\n",
      "loss: 12.25413990020752\n",
      "steps per second: 0.55779\n",
      "step: 37922\n",
      "loss: 12.380518913269043\n",
      "steps per second: 0.57179\n",
      "step: 37923\n",
      "loss: 12.830939292907715\n",
      "steps per second: 0.58081\n",
      "step: 37924\n",
      "loss: 12.509522438049316\n",
      "steps per second: 0.51879\n",
      "step: 37925\n",
      "loss: 13.082189559936523\n",
      "steps per second: 0.53912\n",
      "step: 37926\n",
      "loss: 13.407304763793945\n",
      "steps per second: 0.57872\n",
      "step: 37927\n",
      "loss: 12.572786331176758\n",
      "steps per second: 0.58275\n",
      "step: 37928\n",
      "loss: 12.450979232788086\n",
      "steps per second: 0.54588\n",
      "step: 37929\n",
      "loss: 13.352944374084473\n",
      "steps per second: 0.55648\n",
      "step: 37930\n",
      "loss: 12.992219924926758\n",
      "steps per second: 0.56689\n",
      "step: 37931\n",
      "loss: 12.920104026794434\n",
      "steps per second: 0.52511\n",
      "step: 37932\n",
      "loss: 13.6116304397583\n",
      "steps per second: 0.55191\n",
      "step: 37933\n",
      "loss: 12.725024223327637\n",
      "steps per second: 0.50322\n",
      "step: 37934\n",
      "loss: 12.245607376098633\n",
      "steps per second: 0.57591\n",
      "step: 37935\n",
      "loss: 12.698393821716309\n",
      "steps per second: 0.51745\n",
      "step: 37936\n",
      "loss: 12.718302726745605\n",
      "steps per second: 0.54537\n",
      "step: 37937\n",
      "loss: 12.375853538513184\n",
      "steps per second: 0.54970\n",
      "step: 37938\n",
      "loss: 13.11758804321289\n",
      "steps per second: 0.55112\n",
      "step: 37939\n",
      "loss: 13.038909912109375\n",
      "steps per second: 0.53355\n",
      "step: 37940\n",
      "loss: 12.47517204284668\n",
      "steps per second: 0.55724\n",
      "step: 37941\n",
      "loss: 12.729521751403809\n",
      "steps per second: 0.50034\n",
      "step: 37942\n",
      "loss: 12.795570373535156\n",
      "steps per second: 0.55765\n",
      "step: 37943\n",
      "loss: 13.429950714111328\n",
      "steps per second: 0.51111\n",
      "step: 37944\n",
      "loss: 12.775006294250488\n",
      "steps per second: 0.49695\n",
      "step: 37945\n",
      "loss: 12.919920921325684\n",
      "steps per second: 0.60395\n",
      "step: 37946\n",
      "loss: 12.784981727600098\n",
      "steps per second: 0.52865\n",
      "step: 37947\n",
      "loss: 12.793097496032715\n",
      "steps per second: 0.55567\n",
      "step: 37948\n",
      "loss: 12.853742599487305\n",
      "steps per second: 0.55220\n",
      "step: 37949\n",
      "loss: 12.798181533813477\n",
      "steps per second: 0.55635\n",
      "step: 37950\n",
      "loss: 12.572081565856934\n",
      "steps per second: 0.55964\n",
      "step: 37951\n",
      "loss: 12.549175262451172\n",
      "steps per second: 0.53688\n",
      "step: 37952\n",
      "loss: 12.43403434753418\n",
      "steps per second: 0.58220\n",
      "step: 37953\n",
      "loss: 12.898127555847168\n",
      "steps per second: 0.53245\n",
      "step: 37954\n",
      "loss: 12.953346252441406\n",
      "steps per second: 0.57334\n",
      "step: 37955\n",
      "loss: 13.03001594543457\n",
      "steps per second: 0.54350\n",
      "step: 37956\n",
      "loss: 12.79178237915039\n",
      "steps per second: 0.54270\n",
      "step: 37957\n",
      "loss: 13.039523124694824\n",
      "steps per second: 0.49753\n",
      "step: 37958\n",
      "loss: 12.955717086791992\n",
      "steps per second: 0.54252\n",
      "step: 37959\n",
      "loss: 13.040811538696289\n",
      "steps per second: 0.55607\n",
      "step: 37960\n",
      "loss: 12.928956985473633\n",
      "steps per second: 0.61881\n",
      "step: 37961\n",
      "loss: 13.021397590637207\n",
      "steps per second: 0.58369\n",
      "step: 37962\n",
      "loss: 12.97156047821045\n",
      "steps per second: 0.55296\n",
      "step: 37963\n",
      "loss: 13.034270286560059\n",
      "steps per second: 0.57974\n",
      "step: 37964\n",
      "loss: 12.868471145629883\n",
      "steps per second: 0.54308\n",
      "step: 37965\n",
      "loss: 12.148812294006348\n",
      "steps per second: 0.60905\n",
      "step: 37966\n",
      "loss: 12.735321998596191\n",
      "steps per second: 0.54012\n",
      "step: 37967\n",
      "loss: 13.018895149230957\n",
      "steps per second: 0.55593\n",
      "step: 37968\n",
      "loss: 13.292219161987305\n",
      "steps per second: 0.57147\n",
      "step: 37969\n",
      "loss: 12.814545631408691\n",
      "steps per second: 0.57157\n",
      "step: 37970\n",
      "loss: 12.75558853149414\n",
      "steps per second: 0.55317\n",
      "step: 37971\n",
      "loss: 13.153995513916016\n",
      "steps per second: 0.57343\n",
      "step: 37972\n",
      "loss: 13.186225891113281\n",
      "steps per second: 0.54297\n",
      "step: 37973\n",
      "loss: 13.06163215637207\n",
      "steps per second: 0.53638\n",
      "step: 37974\n",
      "loss: 12.702971458435059\n",
      "steps per second: 0.55406\n",
      "step: 37975\n",
      "loss: 13.149235725402832\n",
      "steps per second: 0.50730\n",
      "step: 37976\n",
      "loss: 12.83495044708252\n",
      "steps per second: 0.57812\n",
      "step: 37977\n",
      "loss: 12.361364364624023\n",
      "steps per second: 0.58101\n",
      "step: 37978\n",
      "loss: 12.68264102935791\n",
      "steps per second: 0.54517\n",
      "step: 37979\n",
      "loss: 13.308975219726562\n",
      "steps per second: 0.55599\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8846448659896851, layer: 12\n",
      "saving at step 37979\n",
      "----------\n",
      "\n",
      "\n",
      "step: 37980\n",
      "loss: 12.625901222229004\n",
      "steps per second: 0.27661\n",
      "step: 37981\n",
      "loss: 13.765351295471191\n",
      "steps per second: 0.53983\n",
      "step: 37982\n",
      "loss: 12.176753044128418\n",
      "steps per second: 0.59383\n",
      "step: 37983\n",
      "loss: 13.029536247253418\n",
      "steps per second: 0.57834\n",
      "step: 37984\n",
      "loss: 13.101227760314941\n",
      "steps per second: 0.54443\n",
      "step: 37985\n",
      "loss: 12.837050437927246\n",
      "steps per second: 0.52243\n",
      "step: 37986\n",
      "loss: 13.331284523010254\n",
      "steps per second: 0.55203\n",
      "step: 37987\n",
      "loss: 12.585975646972656\n",
      "steps per second: 0.57846\n",
      "step: 37988\n",
      "loss: 13.0746488571167\n",
      "steps per second: 0.56073\n",
      "step: 37989\n",
      "loss: 12.738546371459961\n",
      "steps per second: 0.50468\n",
      "step: 37990\n",
      "loss: 12.892452239990234\n",
      "steps per second: 0.61633\n",
      "step: 37991\n",
      "loss: 12.453312873840332\n",
      "steps per second: 0.54882\n",
      "step: 37992\n",
      "loss: 12.713869094848633\n",
      "steps per second: 0.49811\n",
      "step: 37993\n",
      "loss: 13.10469913482666\n",
      "steps per second: 0.53128\n",
      "step: 37994\n",
      "loss: 13.140374183654785\n",
      "steps per second: 0.58537\n",
      "step: 37995\n",
      "loss: 13.468355178833008\n",
      "steps per second: 0.55725\n",
      "step: 37996\n",
      "loss: 13.05003547668457\n",
      "steps per second: 0.59245\n",
      "step: 37997\n",
      "loss: 12.655388832092285\n",
      "steps per second: 0.49766\n",
      "step: 37998\n",
      "loss: 13.104832649230957\n",
      "steps per second: 0.52233\n",
      "step: 37999\n",
      "loss: 12.382355690002441\n",
      "steps per second: 0.54789\n",
      "step: 38000\n",
      "loss: 12.691922187805176\n",
      "steps per second: 0.55572\n",
      "step: 38001\n",
      "loss: 12.402419090270996\n",
      "steps per second: 0.54878\n",
      "step: 38002\n",
      "loss: 13.266111373901367\n",
      "steps per second: 0.62057\n",
      "step: 38003\n",
      "loss: 13.001382827758789\n",
      "steps per second: 0.55331\n",
      "step: 38004\n",
      "loss: 12.811503410339355\n",
      "steps per second: 0.57040\n",
      "step: 38005\n",
      "loss: 12.820711135864258\n",
      "steps per second: 0.53578\n",
      "step: 38006\n",
      "loss: 13.008820533752441\n",
      "steps per second: 0.53970\n",
      "step: 38007\n",
      "loss: 12.322782516479492\n",
      "steps per second: 0.62262\n",
      "step: 38008\n",
      "loss: 13.00636100769043\n",
      "steps per second: 0.55571\n",
      "step: 38009\n",
      "loss: 12.077535629272461\n",
      "steps per second: 0.57267\n",
      "step: 38010\n",
      "loss: 12.511054992675781\n",
      "steps per second: 0.55183\n",
      "step: 38011\n",
      "loss: 12.86578369140625\n",
      "steps per second: 0.54671\n",
      "step: 38012\n",
      "loss: 12.761507987976074\n",
      "steps per second: 0.57751\n",
      "step: 38013\n",
      "loss: 12.820758819580078\n",
      "steps per second: 0.57667\n",
      "step: 38014\n",
      "loss: 12.486800193786621\n",
      "steps per second: 0.57152\n",
      "step: 38015\n",
      "loss: 12.731931686401367\n",
      "steps per second: 0.52740\n",
      "step: 38016\n",
      "loss: 12.879936218261719\n",
      "steps per second: 0.55668\n",
      "step: 38017\n",
      "loss: 12.547500610351562\n",
      "steps per second: 0.51369\n",
      "step: 38018\n",
      "loss: 13.150650024414062\n",
      "steps per second: 0.55132\n",
      "step: 38019\n",
      "loss: 13.424912452697754\n",
      "steps per second: 0.55733\n",
      "step: 38020\n",
      "loss: 13.10213851928711\n",
      "steps per second: 0.52628\n",
      "step: 38021\n",
      "loss: 12.917530059814453\n",
      "steps per second: 0.49685\n",
      "step: 38022\n",
      "loss: 12.51335620880127\n",
      "steps per second: 0.53014\n",
      "step: 38023\n",
      "loss: 12.460050582885742\n",
      "steps per second: 0.54320\n",
      "step: 38024\n",
      "loss: 13.284320831298828\n",
      "steps per second: 0.54444\n",
      "step: 38025\n",
      "loss: 12.491201400756836\n",
      "steps per second: 0.61482\n",
      "step: 38026\n",
      "loss: 12.676862716674805\n",
      "steps per second: 0.57146\n",
      "step: 38027\n",
      "loss: 12.655218124389648\n",
      "steps per second: 0.53479\n",
      "step: 38028\n",
      "loss: 12.751385688781738\n",
      "steps per second: 0.57667\n",
      "step: 38029\n",
      "loss: 13.021900177001953\n",
      "steps per second: 0.57800\n",
      "step: 38030\n",
      "loss: 12.719962120056152\n",
      "steps per second: 0.55449\n",
      "step: 38031\n",
      "loss: 12.551900863647461\n",
      "steps per second: 0.54985\n",
      "step: 38032\n",
      "loss: 12.747262954711914\n",
      "steps per second: 0.55348\n",
      "step: 38033\n",
      "loss: 12.471921920776367\n",
      "steps per second: 0.55541\n",
      "step: 38034\n",
      "loss: 12.249807357788086\n",
      "steps per second: 0.57533\n",
      "step: 38035\n",
      "loss: 12.720335960388184\n",
      "steps per second: 0.55297\n",
      "step: 38036\n",
      "loss: 12.801335334777832\n",
      "steps per second: 0.56917\n",
      "step: 38037\n",
      "loss: 12.669014930725098\n",
      "steps per second: 0.53153\n",
      "step: 38038\n",
      "loss: 12.425735473632812\n",
      "steps per second: 0.60726\n",
      "step: 38039\n",
      "loss: 12.14486312866211\n",
      "steps per second: 0.61279\n",
      "step: 38040\n",
      "loss: 12.487273216247559\n",
      "steps per second: 0.61595\n",
      "step: 38041\n",
      "loss: 12.726531028747559\n",
      "steps per second: 0.57785\n",
      "step: 38042\n",
      "loss: 13.077285766601562\n",
      "steps per second: 0.56007\n",
      "step: 38043\n",
      "loss: 12.954707145690918\n",
      "steps per second: 0.49938\n",
      "step: 38044\n",
      "loss: 12.963652610778809\n",
      "steps per second: 0.56706\n",
      "step: 38045\n",
      "loss: 13.190770149230957\n",
      "steps per second: 0.54290\n",
      "step: 38046\n",
      "loss: 12.963546752929688\n",
      "steps per second: 0.57861\n",
      "step: 38047\n",
      "loss: 12.914093017578125\n",
      "steps per second: 0.52628\n",
      "step: 38048\n",
      "loss: 12.646174430847168\n",
      "steps per second: 0.61228\n",
      "step: 38049\n",
      "loss: 11.917129516601562\n",
      "steps per second: 0.57772\n",
      "step: 38050\n",
      "loss: 13.04293155670166\n",
      "steps per second: 0.57134\n",
      "step: 38051\n",
      "loss: 12.41614055633545\n",
      "steps per second: 0.58173\n",
      "step: 38052\n",
      "loss: 12.760746002197266\n",
      "steps per second: 0.52994\n",
      "step: 38053\n",
      "loss: 12.976481437683105\n",
      "steps per second: 0.55357\n",
      "step: 38054\n",
      "loss: 12.418580055236816\n",
      "steps per second: 0.53084\n",
      "step: 38055\n",
      "loss: 12.90260124206543\n",
      "steps per second: 0.55502\n",
      "step: 38056\n",
      "loss: 13.04692268371582\n",
      "steps per second: 0.57589\n",
      "step: 38057\n",
      "loss: 12.298138618469238\n",
      "steps per second: 0.55621\n",
      "step: 38058\n",
      "loss: 13.460127830505371\n",
      "steps per second: 0.55663\n",
      "step: 38059\n",
      "loss: 12.176046371459961\n",
      "steps per second: 0.50140\n",
      "step: 38060\n",
      "loss: 12.974226951599121\n",
      "steps per second: 0.55174\n",
      "step: 38061\n",
      "loss: 12.893166542053223\n",
      "steps per second: 0.61530\n",
      "step: 38062\n",
      "loss: 12.583020210266113\n",
      "steps per second: 0.56000\n",
      "step: 38063\n",
      "loss: 12.938820838928223\n",
      "steps per second: 0.53086\n",
      "step: 38064\n",
      "loss: 13.238134384155273\n",
      "steps per second: 0.54663\n",
      "step: 38065\n",
      "loss: 12.949000358581543\n",
      "steps per second: 0.51630\n",
      "step: 38066\n",
      "loss: 13.22488784790039\n",
      "steps per second: 0.54568\n",
      "step: 38067\n",
      "loss: 12.815840721130371\n",
      "steps per second: 0.53522\n",
      "step: 38068\n",
      "loss: 12.693697929382324\n",
      "steps per second: 0.53024\n",
      "step: 38069\n",
      "loss: 12.598957061767578\n",
      "steps per second: 0.57846\n",
      "step: 38070\n",
      "loss: 13.007832527160645\n",
      "steps per second: 0.53478\n",
      "step: 38071\n",
      "loss: 12.667284965515137\n",
      "steps per second: 0.57153\n",
      "step: 38072\n",
      "loss: 12.872675895690918\n",
      "steps per second: 0.55314\n",
      "step: 38073\n",
      "loss: 12.170019149780273\n",
      "steps per second: 0.53594\n",
      "step: 38074\n",
      "loss: 13.036544799804688\n",
      "steps per second: 0.59198\n",
      "step: 38075\n",
      "loss: 13.153487205505371\n",
      "steps per second: 0.55028\n",
      "step: 38076\n",
      "loss: 12.137988090515137\n",
      "steps per second: 0.58184\n",
      "step: 38077\n",
      "loss: 12.384354591369629\n",
      "steps per second: 0.55095\n",
      "step: 38078\n",
      "loss: 12.89957332611084\n",
      "steps per second: 0.54670\n",
      "step: 38079\n",
      "loss: 12.76024341583252\n",
      "steps per second: 0.54002\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8362854719161987, layer: 11\n",
      "saving at step 38079\n",
      "----------\n",
      "\n",
      "\n",
      "step: 38080\n",
      "loss: 12.517915725708008\n",
      "steps per second: 0.27033\n",
      "step: 38081\n",
      "loss: 13.048680305480957\n",
      "steps per second: 0.52785\n",
      "step: 38082\n",
      "loss: 13.105230331420898\n",
      "steps per second: 0.57799\n",
      "step: 38083\n",
      "loss: 12.302989959716797\n",
      "steps per second: 0.53440\n",
      "step: 38084\n",
      "loss: 13.090310096740723\n",
      "steps per second: 0.55679\n",
      "step: 38085\n",
      "loss: 12.839369773864746\n",
      "steps per second: 0.54332\n",
      "step: 38086\n",
      "loss: 12.301700592041016\n",
      "steps per second: 0.55966\n",
      "step: 38087\n",
      "loss: 12.656012535095215\n",
      "steps per second: 0.50017\n",
      "step: 38088\n",
      "loss: 13.05669116973877\n",
      "steps per second: 0.53809\n",
      "step: 38089\n",
      "loss: 12.976054191589355\n",
      "steps per second: 0.52464\n",
      "step: 38090\n",
      "loss: 12.645788192749023\n",
      "steps per second: 0.52447\n",
      "step: 38091\n",
      "loss: 13.03797721862793\n",
      "steps per second: 0.52859\n",
      "step: 38092\n",
      "loss: 12.84682559967041\n",
      "steps per second: 0.56989\n",
      "step: 38093\n",
      "loss: 12.999897956848145\n",
      "steps per second: 0.51965\n",
      "step: 38094\n",
      "loss: 13.070070266723633\n",
      "steps per second: 0.52992\n",
      "step: 38095\n",
      "loss: 13.029745101928711\n",
      "steps per second: 0.54853\n",
      "step: 38096\n",
      "loss: 13.10659408569336\n",
      "steps per second: 0.51593\n",
      "step: 38097\n",
      "loss: 12.703354835510254\n",
      "steps per second: 0.51040\n",
      "step: 38098\n",
      "loss: 13.081586837768555\n",
      "steps per second: 0.53291\n",
      "step: 38099\n",
      "loss: 12.568828582763672\n",
      "steps per second: 0.59113\n",
      "step: 38100\n",
      "loss: 12.724257469177246\n",
      "steps per second: 0.56068\n",
      "step: 38101\n",
      "loss: 12.81356143951416\n",
      "steps per second: 0.52160\n",
      "step: 38102\n",
      "loss: 12.626713752746582\n",
      "steps per second: 0.60279\n",
      "step: 38103\n",
      "loss: 13.01014232635498\n",
      "steps per second: 0.51865\n",
      "step: 38104\n",
      "loss: 13.009041786193848\n",
      "steps per second: 0.54741\n",
      "step: 38105\n",
      "loss: 11.948084831237793\n",
      "steps per second: 0.53004\n",
      "step: 38106\n",
      "loss: 13.07924747467041\n",
      "steps per second: 0.48768\n",
      "step: 38107\n",
      "loss: 12.592110633850098\n",
      "steps per second: 0.50349\n",
      "step: 38108\n",
      "loss: 12.847845077514648\n",
      "steps per second: 0.52552\n",
      "step: 38109\n",
      "loss: 12.65082836151123\n",
      "steps per second: 0.51197\n",
      "step: 38110\n",
      "loss: 13.456445693969727\n",
      "steps per second: 0.45664\n",
      "step: 38111\n",
      "loss: 12.662447929382324\n",
      "steps per second: 0.46617\n",
      "step: 38112\n",
      "loss: 12.716302871704102\n",
      "steps per second: 0.51102\n",
      "step: 38113\n",
      "loss: 13.025134086608887\n",
      "steps per second: 0.51763\n",
      "step: 38114\n",
      "loss: 12.313678741455078\n",
      "steps per second: 0.53286\n",
      "step: 38115\n",
      "loss: 12.45156478881836\n",
      "steps per second: 0.48845\n",
      "step: 38116\n",
      "loss: 13.14738941192627\n",
      "steps per second: 0.50936\n",
      "step: 38117\n",
      "loss: 12.94991397857666\n",
      "steps per second: 0.52984\n",
      "step: 38118\n",
      "loss: 12.60801887512207\n",
      "steps per second: 0.54670\n",
      "step: 38119\n",
      "loss: 12.403718948364258\n",
      "steps per second: 0.54314\n",
      "step: 38120\n",
      "loss: 12.264775276184082\n",
      "steps per second: 0.60023\n",
      "step: 38121\n",
      "loss: 12.28407096862793\n",
      "steps per second: 0.54513\n",
      "step: 38122\n",
      "loss: 12.587103843688965\n",
      "steps per second: 0.53186\n",
      "step: 38123\n",
      "loss: 12.553013801574707\n",
      "steps per second: 0.54262\n",
      "step: 38124\n",
      "loss: 12.195412635803223\n",
      "steps per second: 0.53455\n",
      "step: 38125\n",
      "loss: 12.539144515991211\n",
      "steps per second: 0.54832\n",
      "step: 38126\n",
      "loss: 12.654458999633789\n",
      "steps per second: 0.55661\n",
      "step: 38127\n",
      "loss: 12.8715238571167\n",
      "steps per second: 0.61820\n",
      "step: 38128\n",
      "loss: 12.532337188720703\n",
      "steps per second: 0.56009\n",
      "step: 38129\n",
      "loss: 12.746891975402832\n",
      "steps per second: 0.57620\n",
      "step: 38130\n",
      "loss: 13.082910537719727\n",
      "steps per second: 0.58614\n",
      "step: 38131\n",
      "loss: 13.036865234375\n",
      "steps per second: 0.55566\n",
      "step: 38132\n",
      "loss: 12.403854370117188\n",
      "steps per second: 0.57635\n",
      "step: 38133\n",
      "loss: 12.81216049194336\n",
      "steps per second: 0.55749\n",
      "step: 38134\n",
      "loss: 12.574714660644531\n",
      "steps per second: 0.56466\n",
      "step: 38135\n",
      "loss: 12.719199180603027\n",
      "steps per second: 0.50027\n",
      "step: 38136\n",
      "loss: 12.609084129333496\n",
      "steps per second: 0.53150\n",
      "step: 38137\n",
      "loss: 12.758426666259766\n",
      "steps per second: 0.56222\n",
      "step: 38138\n",
      "loss: 12.403860092163086\n",
      "steps per second: 0.53699\n",
      "step: 38139\n",
      "loss: 12.534225463867188\n",
      "steps per second: 0.51912\n",
      "step: 38140\n",
      "loss: 12.590744018554688\n",
      "steps per second: 0.53764\n",
      "step: 38141\n",
      "loss: 12.318514823913574\n",
      "steps per second: 0.51957\n",
      "step: 38142\n",
      "loss: 13.104700088500977\n",
      "steps per second: 0.59573\n",
      "step: 38143\n",
      "loss: 12.72486400604248\n",
      "steps per second: 0.55579\n",
      "step: 38144\n",
      "loss: 12.163283348083496\n",
      "steps per second: 0.55667\n",
      "step: 38145\n",
      "loss: 12.712987899780273\n",
      "steps per second: 0.50407\n",
      "step: 38146\n",
      "loss: 12.953536033630371\n",
      "steps per second: 0.52134\n",
      "step: 38147\n",
      "loss: 12.53815746307373\n",
      "steps per second: 0.53481\n",
      "step: 38148\n",
      "loss: 12.33652114868164\n",
      "steps per second: 0.61803\n",
      "step: 38149\n",
      "loss: 13.211286544799805\n",
      "steps per second: 0.55633\n",
      "step: 38150\n",
      "loss: 12.520098686218262\n",
      "steps per second: 0.53297\n",
      "step: 38151\n",
      "loss: 12.831785202026367\n",
      "steps per second: 0.51971\n",
      "step: 38152\n",
      "loss: 12.398920059204102\n",
      "steps per second: 0.55028\n",
      "step: 38153\n",
      "loss: 12.779611587524414\n",
      "steps per second: 0.53108\n",
      "step: 38154\n",
      "loss: 12.59936237335205\n",
      "steps per second: 0.58621\n",
      "step: 38155\n",
      "loss: 12.499722480773926\n",
      "steps per second: 0.51695\n",
      "step: 38156\n",
      "loss: 13.091719627380371\n",
      "steps per second: 0.57293\n",
      "step: 38157\n",
      "loss: 12.808703422546387\n",
      "steps per second: 0.53831\n",
      "step: 38158\n",
      "loss: 12.976040840148926\n",
      "steps per second: 0.48009\n",
      "step: 38159\n",
      "loss: 12.484623908996582\n",
      "steps per second: 0.40683\n",
      "step: 38160\n",
      "loss: 12.97372817993164\n",
      "steps per second: 0.44935\n",
      "step: 38161\n",
      "loss: 12.952566146850586\n",
      "steps per second: 0.43606\n",
      "step: 38162\n",
      "loss: 12.97945785522461\n",
      "steps per second: 0.47368\n",
      "step: 38163\n",
      "loss: 13.124350547790527\n",
      "steps per second: 0.47393\n",
      "step: 38164\n",
      "loss: 13.63969612121582\n",
      "steps per second: 0.49364\n",
      "step: 38165\n",
      "loss: 13.20725154876709\n",
      "steps per second: 0.45010\n",
      "step: 38166\n",
      "loss: 13.049126625061035\n",
      "steps per second: 0.43329\n",
      "step: 38167\n",
      "loss: 13.071168899536133\n",
      "steps per second: 0.48578\n",
      "step: 38168\n",
      "loss: 12.512537002563477\n",
      "steps per second: 0.53633\n",
      "step: 38169\n",
      "loss: 13.3191499710083\n",
      "steps per second: 0.55980\n",
      "step: 38170\n",
      "loss: 12.959290504455566\n",
      "steps per second: 0.56176\n",
      "step: 38171\n",
      "loss: 12.601308822631836\n",
      "steps per second: 0.54154\n",
      "step: 38172\n",
      "loss: 12.573258399963379\n",
      "steps per second: 0.54609\n",
      "step: 38173\n",
      "loss: 12.705466270446777\n",
      "steps per second: 0.54271\n",
      "step: 38174\n",
      "loss: 12.726439476013184\n",
      "steps per second: 0.57445\n",
      "step: 38175\n",
      "loss: 12.817179679870605\n",
      "steps per second: 0.57526\n",
      "step: 38176\n",
      "loss: 12.393527030944824\n",
      "steps per second: 0.55769\n",
      "step: 38177\n",
      "loss: 13.241462707519531\n",
      "steps per second: 0.57447\n",
      "step: 38178\n",
      "loss: 12.794511795043945\n",
      "steps per second: 0.52607\n",
      "step: 38179\n",
      "loss: 12.323875427246094\n",
      "steps per second: 0.55855\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7962338924407959, layer: 11\n",
      "saving at step 38179\n",
      "----------\n",
      "\n",
      "\n",
      "step: 38180\n",
      "loss: 12.681719779968262\n",
      "steps per second: 0.27467\n",
      "step: 38181\n",
      "loss: 12.890003204345703\n",
      "steps per second: 0.57283\n",
      "step: 38182\n",
      "loss: 13.097902297973633\n",
      "steps per second: 0.56326\n",
      "step: 38183\n",
      "loss: 12.683287620544434\n",
      "steps per second: 0.54468\n",
      "step: 38184\n",
      "loss: 12.878378868103027\n",
      "steps per second: 0.56725\n",
      "step: 38185\n",
      "loss: 12.656722068786621\n",
      "steps per second: 0.49802\n",
      "step: 38186\n",
      "loss: 12.852544784545898\n",
      "steps per second: 0.51412\n",
      "step: 38187\n",
      "loss: 12.401211738586426\n",
      "steps per second: 0.51334\n",
      "step: 38188\n",
      "loss: 12.618882179260254\n",
      "steps per second: 0.52067\n",
      "step: 38189\n",
      "loss: 13.047720909118652\n",
      "steps per second: 0.55006\n",
      "step: 38190\n",
      "loss: 12.740569114685059\n",
      "steps per second: 0.56679\n",
      "step: 38191\n",
      "loss: 13.215431213378906\n",
      "steps per second: 0.57815\n",
      "step: 38192\n",
      "loss: 13.259636878967285\n",
      "steps per second: 0.55093\n",
      "step: 38193\n",
      "loss: 12.807084083557129\n",
      "steps per second: 0.60128\n",
      "step: 38194\n",
      "loss: 12.67339038848877\n",
      "steps per second: 0.55206\n",
      "step: 38195\n",
      "loss: 12.870336532592773\n",
      "steps per second: 0.51555\n",
      "step: 38196\n",
      "loss: 12.826119422912598\n",
      "steps per second: 0.57311\n",
      "step: 38197\n",
      "loss: 12.46061897277832\n",
      "steps per second: 0.57222\n",
      "step: 38198\n",
      "loss: 13.001513481140137\n",
      "steps per second: 0.55294\n",
      "step: 38199\n",
      "loss: 12.226662635803223\n",
      "steps per second: 0.51704\n",
      "step: 38200\n",
      "loss: 12.420256614685059\n",
      "steps per second: 0.55002\n",
      "step: 38201\n",
      "loss: 12.834869384765625\n",
      "steps per second: 0.52757\n",
      "step: 38202\n",
      "loss: 13.101323127746582\n",
      "steps per second: 0.56946\n",
      "step: 38203\n",
      "loss: 12.75642204284668\n",
      "steps per second: 0.51892\n",
      "step: 38204\n",
      "loss: 12.803420066833496\n",
      "steps per second: 0.55429\n",
      "step: 38205\n",
      "loss: 12.73455810546875\n",
      "steps per second: 0.56495\n",
      "step: 38206\n",
      "loss: 12.50328540802002\n",
      "steps per second: 0.49212\n",
      "step: 38207\n",
      "loss: 12.791431427001953\n",
      "steps per second: 0.57951\n",
      "step: 38208\n",
      "loss: 12.849910736083984\n",
      "steps per second: 0.55167\n",
      "step: 38209\n",
      "loss: 12.840710639953613\n",
      "steps per second: 0.56042\n",
      "step: 38210\n",
      "loss: 12.676870346069336\n",
      "steps per second: 0.57978\n",
      "step: 38211\n",
      "loss: 13.047932624816895\n",
      "steps per second: 0.55816\n",
      "step: 38212\n",
      "loss: 12.4356107711792\n",
      "steps per second: 0.55105\n",
      "step: 38213\n",
      "loss: 12.902008056640625\n",
      "steps per second: 0.51487\n",
      "step: 38214\n",
      "loss: 13.447741508483887\n",
      "steps per second: 0.55125\n",
      "step: 38215\n",
      "loss: 12.727339744567871\n",
      "steps per second: 0.52760\n",
      "step: 38216\n",
      "loss: 12.358890533447266\n",
      "steps per second: 0.57249\n",
      "step: 38217\n",
      "loss: 12.938272476196289\n",
      "steps per second: 0.54556\n",
      "step: 38218\n",
      "loss: 12.4100980758667\n",
      "steps per second: 0.52882\n",
      "step: 38219\n",
      "loss: 12.955183029174805\n",
      "steps per second: 0.57731\n",
      "step: 38220\n",
      "loss: 13.682241439819336\n",
      "steps per second: 0.57753\n",
      "step: 38221\n",
      "loss: 12.823969841003418\n",
      "steps per second: 0.61910\n",
      "step: 38222\n",
      "loss: 12.805546760559082\n",
      "steps per second: 0.55728\n",
      "step: 38223\n",
      "loss: 12.272194862365723\n",
      "steps per second: 0.55643\n",
      "step: 38224\n",
      "loss: 13.126633644104004\n",
      "steps per second: 0.51829\n",
      "step: 38225\n",
      "loss: 12.98432445526123\n",
      "steps per second: 0.57600\n",
      "step: 38226\n",
      "loss: 12.959092140197754\n",
      "steps per second: 0.52242\n",
      "step: 38227\n",
      "loss: 13.399433135986328\n",
      "steps per second: 0.55845\n",
      "step: 38228\n",
      "loss: 12.962156295776367\n",
      "steps per second: 0.55972\n",
      "step: 38229\n",
      "loss: 13.086713790893555\n",
      "steps per second: 0.51490\n",
      "step: 38230\n",
      "loss: 12.65778923034668\n",
      "steps per second: 0.57455\n",
      "step: 38231\n",
      "loss: 12.871681213378906\n",
      "steps per second: 0.57686\n",
      "step: 38232\n",
      "loss: 12.848281860351562\n",
      "steps per second: 0.54698\n",
      "step: 38233\n",
      "loss: 12.528341293334961\n",
      "steps per second: 0.58404\n",
      "step: 38234\n",
      "loss: 12.02677059173584\n",
      "steps per second: 0.50883\n",
      "step: 38235\n",
      "loss: 12.664240837097168\n",
      "steps per second: 0.53678\n",
      "step: 38236\n",
      "loss: 13.00733470916748\n",
      "steps per second: 0.58485\n",
      "step: 38237\n",
      "loss: 12.994598388671875\n",
      "steps per second: 0.56023\n",
      "step: 38238\n",
      "loss: 12.847149848937988\n",
      "steps per second: 0.50973\n",
      "step: 38239\n",
      "loss: 12.4181489944458\n",
      "steps per second: 0.57127\n",
      "step: 38240\n",
      "loss: 12.686365127563477\n",
      "steps per second: 0.55796\n",
      "step: 38241\n",
      "loss: 13.433999061584473\n",
      "steps per second: 0.58591\n",
      "step: 38242\n",
      "loss: 12.465432167053223\n",
      "steps per second: 0.50300\n",
      "step: 38243\n",
      "loss: 12.271026611328125\n",
      "steps per second: 0.52319\n",
      "step: 38244\n",
      "loss: 12.815059661865234\n",
      "steps per second: 0.61683\n",
      "step: 38245\n",
      "loss: 12.449799537658691\n",
      "steps per second: 0.51679\n",
      "step: 38246\n",
      "loss: 12.573526382446289\n",
      "steps per second: 0.50228\n",
      "step: 38247\n",
      "loss: 13.44005298614502\n",
      "steps per second: 0.61689\n",
      "step: 38248\n",
      "loss: 12.32504940032959\n",
      "steps per second: 0.61681\n",
      "step: 38249\n",
      "loss: 12.321908950805664\n",
      "steps per second: 0.56137\n",
      "step: 38250\n",
      "loss: 12.565727233886719\n",
      "steps per second: 0.62068\n",
      "step: 38251\n",
      "loss: 13.162993431091309\n",
      "steps per second: 0.52716\n",
      "step: 38252\n",
      "loss: 12.662245750427246\n",
      "steps per second: 0.58541\n",
      "step: 38253\n",
      "loss: 13.109617233276367\n",
      "steps per second: 0.56345\n",
      "step: 38254\n",
      "loss: 12.851619720458984\n",
      "steps per second: 0.52268\n",
      "step: 38255\n",
      "loss: 12.855599403381348\n",
      "steps per second: 0.52607\n",
      "step: 38256\n",
      "loss: 12.636802673339844\n",
      "steps per second: 0.56413\n",
      "step: 38257\n",
      "loss: 12.813139915466309\n",
      "steps per second: 0.55345\n",
      "step: 38258\n",
      "loss: 12.863912582397461\n",
      "steps per second: 0.50748\n",
      "step: 38259\n",
      "loss: 12.691255569458008\n",
      "steps per second: 0.55297\n",
      "step: 38260\n",
      "loss: 12.221012115478516\n",
      "steps per second: 0.56209\n",
      "step: 38261\n",
      "loss: 12.621349334716797\n",
      "steps per second: 0.54921\n",
      "step: 38262\n",
      "loss: 12.961199760437012\n",
      "steps per second: 0.56814\n",
      "step: 38263\n",
      "loss: 12.70691967010498\n",
      "steps per second: 0.58213\n",
      "step: 38264\n",
      "loss: 13.036469459533691\n",
      "steps per second: 0.55169\n",
      "step: 38265\n",
      "loss: 12.303736686706543\n",
      "steps per second: 0.56256\n",
      "step: 38266\n",
      "loss: 13.033110618591309\n",
      "steps per second: 0.55274\n",
      "step: 38267\n",
      "loss: 12.900062561035156\n",
      "steps per second: 0.57487\n",
      "step: 38268\n",
      "loss: 12.601635932922363\n",
      "steps per second: 0.59452\n",
      "step: 38269\n",
      "loss: 13.219158172607422\n",
      "steps per second: 0.53873\n",
      "step: 38270\n",
      "loss: 12.451461791992188\n",
      "steps per second: 0.52263\n",
      "step: 38271\n",
      "loss: 12.453655242919922\n",
      "steps per second: 0.52232\n",
      "step: 38272\n",
      "loss: 12.621602058410645\n",
      "steps per second: 0.56745\n",
      "step: 38273\n",
      "loss: 12.542069435119629\n",
      "steps per second: 0.56035\n",
      "step: 38274\n",
      "loss: 13.18418025970459\n",
      "steps per second: 0.56316\n",
      "step: 38275\n",
      "loss: 12.874990463256836\n",
      "steps per second: 0.58068\n",
      "step: 38276\n",
      "loss: 12.68639087677002\n",
      "steps per second: 0.51612\n",
      "step: 38277\n",
      "loss: 12.709914207458496\n",
      "steps per second: 0.56057\n",
      "step: 38278\n",
      "loss: 12.700085639953613\n",
      "steps per second: 0.55878\n",
      "step: 38279\n",
      "loss: 13.242022514343262\n",
      "steps per second: 0.50459\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8608484268188477, layer: 11\n",
      "saving at step 38279\n",
      "----------\n",
      "\n",
      "\n",
      "step: 38280\n",
      "loss: 12.91585922241211\n",
      "steps per second: 0.25766\n",
      "step: 38281\n",
      "loss: 12.449480056762695\n",
      "steps per second: 0.53958\n",
      "step: 38282\n",
      "loss: 13.217429161071777\n",
      "steps per second: 0.52747\n",
      "step: 38283\n",
      "loss: 12.65071964263916\n",
      "steps per second: 0.55470\n",
      "step: 38284\n",
      "loss: 13.207172393798828\n",
      "steps per second: 0.49994\n",
      "step: 38285\n",
      "loss: 12.904034614562988\n",
      "steps per second: 0.52547\n",
      "step: 38286\n",
      "loss: 12.287018775939941\n",
      "steps per second: 0.49249\n",
      "step: 38287\n",
      "loss: 12.795465469360352\n",
      "steps per second: 0.48323\n",
      "step: 38288\n",
      "loss: 13.050895690917969\n",
      "steps per second: 0.52672\n",
      "step: 38289\n",
      "loss: 12.937894821166992\n",
      "steps per second: 0.53004\n",
      "step: 38290\n",
      "loss: 12.304430961608887\n",
      "steps per second: 0.49242\n",
      "step: 38291\n",
      "loss: 12.647165298461914\n",
      "steps per second: 0.53396\n",
      "step: 38292\n",
      "loss: 12.686239242553711\n",
      "steps per second: 0.52256\n",
      "step: 38293\n",
      "loss: 12.902443885803223\n",
      "steps per second: 0.53569\n",
      "step: 38294\n",
      "loss: 12.816472053527832\n",
      "steps per second: 0.51723\n",
      "step: 38295\n",
      "loss: 12.964127540588379\n",
      "steps per second: 0.59942\n",
      "step: 38296\n",
      "loss: 12.599553108215332\n",
      "steps per second: 0.56593\n",
      "step: 38297\n",
      "loss: 13.309212684631348\n",
      "steps per second: 0.54241\n",
      "step: 38298\n",
      "loss: 12.535602569580078\n",
      "steps per second: 0.52994\n",
      "step: 38299\n",
      "loss: 12.457012176513672\n",
      "steps per second: 0.56625\n",
      "step: 38300\n",
      "loss: 12.855210304260254\n",
      "steps per second: 0.54525\n",
      "step: 38301\n",
      "loss: 13.238668441772461\n",
      "steps per second: 0.51658\n",
      "step: 38302\n",
      "loss: 13.131924629211426\n",
      "steps per second: 0.53508\n",
      "step: 38303\n",
      "loss: 12.432174682617188\n",
      "steps per second: 0.55918\n",
      "step: 38304\n",
      "loss: 13.004141807556152\n",
      "steps per second: 0.52204\n",
      "step: 38305\n",
      "loss: 13.642984390258789\n",
      "steps per second: 0.54941\n",
      "step: 38306\n",
      "loss: 13.161001205444336\n",
      "steps per second: 0.53292\n",
      "step: 38307\n",
      "loss: 12.470025062561035\n",
      "steps per second: 0.49721\n",
      "step: 38308\n",
      "loss: 13.165179252624512\n",
      "steps per second: 0.55402\n",
      "step: 38309\n",
      "loss: 12.890558242797852\n",
      "steps per second: 0.54831\n",
      "step: 38310\n",
      "loss: 12.766101837158203\n",
      "steps per second: 0.52691\n",
      "step: 38311\n",
      "loss: 13.13143539428711\n",
      "steps per second: 0.50921\n",
      "step: 38312\n",
      "loss: 12.502543449401855\n",
      "steps per second: 0.56364\n",
      "step: 38313\n",
      "loss: 12.865851402282715\n",
      "steps per second: 0.52119\n",
      "step: 38314\n",
      "loss: 13.01137924194336\n",
      "steps per second: 0.53975\n",
      "step: 38315\n",
      "loss: 13.128658294677734\n",
      "steps per second: 0.53272\n",
      "step: 38316\n",
      "loss: 12.640351295471191\n",
      "steps per second: 0.55670\n",
      "step: 38317\n",
      "loss: 12.515908241271973\n",
      "steps per second: 0.54068\n",
      "step: 38318\n",
      "loss: 12.63817024230957\n",
      "steps per second: 0.54250\n",
      "step: 38319\n",
      "loss: 12.407369613647461\n",
      "steps per second: 0.53679\n",
      "step: 38320\n",
      "loss: 13.108457565307617\n",
      "steps per second: 0.50973\n",
      "step: 38321\n",
      "loss: 12.641639709472656\n",
      "steps per second: 0.51476\n",
      "step: 38322\n",
      "loss: 12.979172706604004\n",
      "steps per second: 0.53996\n",
      "step: 38323\n",
      "loss: 13.264923095703125\n",
      "steps per second: 0.53841\n",
      "step: 38324\n",
      "loss: 12.505614280700684\n",
      "steps per second: 0.49490\n",
      "step: 38325\n",
      "loss: 12.59849739074707\n",
      "steps per second: 0.55289\n",
      "step: 38326\n",
      "loss: 12.822293281555176\n",
      "steps per second: 0.53464\n",
      "step: 38327\n",
      "loss: 12.164989471435547\n",
      "steps per second: 0.51541\n",
      "step: 38328\n",
      "loss: 12.839393615722656\n",
      "steps per second: 0.56085\n",
      "step: 38329\n",
      "loss: 12.823966979980469\n",
      "steps per second: 0.51923\n",
      "step: 38330\n",
      "loss: 12.293896675109863\n",
      "steps per second: 0.50858\n",
      "step: 38331\n",
      "loss: 12.372701644897461\n",
      "steps per second: 0.52051\n",
      "step: 38332\n",
      "loss: 12.775757789611816\n",
      "steps per second: 0.52346\n",
      "step: 38333\n",
      "loss: 13.220816612243652\n",
      "steps per second: 0.53756\n",
      "step: 38334\n",
      "loss: 12.60193920135498\n",
      "steps per second: 0.54046\n",
      "step: 38335\n",
      "loss: 12.816100120544434\n",
      "steps per second: 0.52674\n",
      "step: 38336\n",
      "loss: 12.431705474853516\n",
      "steps per second: 0.53116\n",
      "step: 38337\n",
      "loss: 12.543452262878418\n",
      "steps per second: 0.57359\n",
      "step: 38338\n",
      "loss: 13.039021492004395\n",
      "steps per second: 0.55665\n",
      "step: 38339\n",
      "loss: 13.412393569946289\n",
      "steps per second: 0.52734\n",
      "step: 38340\n",
      "loss: 13.166608810424805\n",
      "steps per second: 0.56607\n",
      "step: 38341\n",
      "loss: 12.458921432495117\n",
      "steps per second: 0.53514\n",
      "step: 38342\n",
      "loss: 12.596817970275879\n",
      "steps per second: 0.57035\n",
      "step: 38343\n",
      "loss: 12.991106033325195\n",
      "steps per second: 0.57988\n",
      "step: 38344\n",
      "loss: 12.948450088500977\n",
      "steps per second: 0.56277\n",
      "step: 38345\n",
      "loss: 13.05775260925293\n",
      "steps per second: 0.55156\n",
      "step: 38346\n",
      "loss: 13.026801109313965\n",
      "steps per second: 0.55020\n",
      "step: 38347\n",
      "loss: 12.941871643066406\n",
      "steps per second: 0.56357\n",
      "step: 38348\n",
      "loss: 12.795951843261719\n",
      "steps per second: 0.54912\n",
      "step: 38349\n",
      "loss: 12.722175598144531\n",
      "steps per second: 0.56285\n",
      "step: 38350\n",
      "loss: 12.53316593170166\n",
      "steps per second: 0.52322\n",
      "step: 38351\n",
      "loss: 12.69965648651123\n",
      "steps per second: 0.54288\n",
      "step: 38352\n",
      "loss: 12.422821044921875\n",
      "steps per second: 0.54292\n",
      "step: 38353\n",
      "loss: 12.085674285888672\n",
      "steps per second: 0.58584\n",
      "step: 38354\n",
      "loss: 12.82841968536377\n",
      "steps per second: 0.55134\n",
      "step: 38355\n",
      "loss: 13.122042655944824\n",
      "steps per second: 0.55902\n",
      "step: 38356\n",
      "loss: 12.519477844238281\n",
      "steps per second: 0.55172\n",
      "step: 38357\n",
      "loss: 13.326570510864258\n",
      "steps per second: 0.52968\n",
      "step: 38358\n",
      "loss: 12.769193649291992\n",
      "steps per second: 0.59177\n",
      "step: 38359\n",
      "loss: 12.620359420776367\n",
      "steps per second: 0.62058\n",
      "step: 38360\n",
      "loss: 13.38736629486084\n",
      "steps per second: 0.56067\n",
      "step: 38361\n",
      "loss: 13.038256645202637\n",
      "steps per second: 0.54511\n",
      "step: 38362\n",
      "loss: 13.184117317199707\n",
      "steps per second: 0.55615\n",
      "step: 38363\n",
      "loss: 12.299602508544922\n",
      "steps per second: 0.62052\n",
      "step: 38364\n",
      "loss: 13.321307182312012\n",
      "steps per second: 0.49934\n",
      "step: 38365\n",
      "loss: 12.976716995239258\n",
      "steps per second: 0.58760\n",
      "step: 38366\n",
      "loss: 13.254133224487305\n",
      "steps per second: 0.55938\n",
      "step: 38367\n",
      "loss: 12.79725456237793\n",
      "steps per second: 0.53249\n",
      "step: 38368\n",
      "loss: 12.389589309692383\n",
      "steps per second: 0.56775\n",
      "step: 38369\n",
      "loss: 12.927488327026367\n",
      "steps per second: 0.55246\n",
      "step: 38370\n",
      "loss: 12.610477447509766\n",
      "steps per second: 0.58423\n",
      "step: 38371\n",
      "loss: 12.673620223999023\n",
      "steps per second: 0.49264\n",
      "step: 38372\n",
      "loss: 13.063612937927246\n",
      "steps per second: 0.56335\n",
      "step: 38373\n",
      "loss: 13.041897773742676\n",
      "steps per second: 0.55927\n",
      "step: 38374\n",
      "loss: 12.81417179107666\n",
      "steps per second: 0.58657\n",
      "step: 38375\n",
      "loss: 12.574898719787598\n",
      "steps per second: 0.52641\n",
      "step: 38376\n",
      "loss: 12.7518310546875\n",
      "steps per second: 0.52879\n",
      "step: 38377\n",
      "loss: 12.712796211242676\n",
      "steps per second: 0.54266\n",
      "step: 38378\n",
      "loss: 13.034732818603516\n",
      "steps per second: 0.56535\n",
      "step: 38379\n",
      "loss: 13.166316032409668\n",
      "steps per second: 0.55314\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8515479564666748, layer: 11\n",
      "saving at step 38379\n",
      "----------\n",
      "\n",
      "\n",
      "step: 38380\n",
      "loss: 12.70755386352539\n",
      "steps per second: 0.27898\n",
      "step: 38381\n",
      "loss: 13.082415580749512\n",
      "steps per second: 0.55657\n",
      "step: 38382\n",
      "loss: 12.94665241241455\n",
      "steps per second: 0.55163\n",
      "step: 38383\n",
      "loss: 13.138911247253418\n",
      "steps per second: 0.55548\n",
      "step: 38384\n",
      "loss: 12.739594459533691\n",
      "steps per second: 0.56233\n",
      "step: 38385\n",
      "loss: 12.780599594116211\n",
      "steps per second: 0.55851\n",
      "step: 38386\n",
      "loss: 12.725285530090332\n",
      "steps per second: 0.53908\n",
      "step: 38387\n",
      "loss: 12.739252090454102\n",
      "steps per second: 0.53710\n",
      "step: 38388\n",
      "loss: 12.861098289489746\n",
      "steps per second: 0.55628\n",
      "step: 38389\n",
      "loss: 12.59823226928711\n",
      "steps per second: 0.56933\n",
      "step: 38390\n",
      "loss: 12.572712898254395\n",
      "steps per second: 0.57441\n",
      "step: 38391\n",
      "loss: 13.11841106414795\n",
      "steps per second: 0.56093\n",
      "step: 38392\n",
      "loss: 12.642229080200195\n",
      "steps per second: 0.54806\n",
      "step: 38393\n",
      "loss: 12.692178726196289\n",
      "steps per second: 0.54048\n",
      "step: 38394\n",
      "loss: 12.807880401611328\n",
      "steps per second: 0.53844\n",
      "step: 38395\n",
      "loss: 13.20952033996582\n",
      "steps per second: 0.54346\n",
      "step: 38396\n",
      "loss: 13.0089111328125\n",
      "steps per second: 0.53869\n",
      "step: 38397\n",
      "loss: 13.504905700683594\n",
      "steps per second: 0.52937\n",
      "step: 38398\n",
      "loss: 12.61856746673584\n",
      "steps per second: 0.49508\n",
      "step: 38399\n",
      "loss: 12.518651008605957\n",
      "steps per second: 0.48956\n",
      "step: 38400\n",
      "loss: 12.557787895202637\n",
      "steps per second: 0.54433\n",
      "step: 38401\n",
      "loss: 12.62807559967041\n",
      "steps per second: 0.51474\n",
      "step: 38402\n",
      "loss: 12.774212837219238\n",
      "steps per second: 0.50866\n",
      "step: 38403\n",
      "loss: 12.539907455444336\n",
      "steps per second: 0.54259\n",
      "step: 38404\n",
      "loss: 12.854987144470215\n",
      "steps per second: 0.49762\n",
      "step: 38405\n",
      "loss: 13.014063835144043\n",
      "steps per second: 0.49328\n",
      "step: 38406\n",
      "loss: 12.85426139831543\n",
      "steps per second: 0.51182\n",
      "step: 38407\n",
      "loss: 12.858222961425781\n",
      "steps per second: 0.54207\n",
      "step: 38408\n",
      "loss: 12.897965431213379\n",
      "steps per second: 0.53675\n",
      "step: 38409\n",
      "loss: 12.687801361083984\n",
      "steps per second: 0.55425\n",
      "step: 38410\n",
      "loss: 12.481494903564453\n",
      "steps per second: 0.61421\n",
      "step: 38411\n",
      "loss: 12.682234764099121\n",
      "steps per second: 0.50917\n",
      "step: 38412\n",
      "loss: 13.096718788146973\n",
      "steps per second: 0.52513\n",
      "step: 38413\n",
      "loss: 13.16460132598877\n",
      "steps per second: 0.55960\n",
      "step: 38414\n",
      "loss: 12.783005714416504\n",
      "steps per second: 0.53451\n",
      "step: 38415\n",
      "loss: 12.94046688079834\n",
      "steps per second: 0.56807\n",
      "step: 38416\n",
      "loss: 12.937265396118164\n",
      "steps per second: 0.58271\n",
      "step: 38417\n",
      "loss: 12.222623825073242\n",
      "steps per second: 0.52304\n",
      "step: 38418\n",
      "loss: 12.729049682617188\n",
      "steps per second: 0.55019\n",
      "step: 38419\n",
      "loss: 12.682014465332031\n",
      "steps per second: 0.57375\n",
      "step: 38420\n",
      "loss: 12.665181159973145\n",
      "steps per second: 0.55514\n",
      "step: 38421\n",
      "loss: 12.975845336914062\n",
      "steps per second: 0.54326\n",
      "step: 38422\n",
      "loss: 12.443896293640137\n",
      "steps per second: 0.55592\n",
      "step: 38423\n",
      "loss: 12.964276313781738\n",
      "steps per second: 0.50553\n",
      "step: 38424\n",
      "loss: 12.44186782836914\n",
      "steps per second: 0.53802\n",
      "step: 38425\n",
      "loss: 12.8677339553833\n",
      "steps per second: 0.54649\n",
      "step: 38426\n",
      "loss: 13.451216697692871\n",
      "steps per second: 0.55205\n",
      "step: 38427\n",
      "loss: 13.175201416015625\n",
      "steps per second: 0.55852\n",
      "step: 38428\n",
      "loss: 12.934419631958008\n",
      "steps per second: 0.52701\n",
      "step: 38429\n",
      "loss: 13.07433032989502\n",
      "steps per second: 0.50943\n",
      "step: 38430\n",
      "loss: 12.731937408447266\n",
      "steps per second: 0.54946\n",
      "step: 38431\n",
      "loss: 12.677201271057129\n",
      "steps per second: 0.53748\n",
      "step: 38432\n",
      "loss: 12.301939964294434\n",
      "steps per second: 0.50977\n",
      "step: 38433\n",
      "loss: 13.009618759155273\n",
      "steps per second: 0.61594\n",
      "step: 38434\n",
      "loss: 12.883854866027832\n",
      "steps per second: 0.56180\n",
      "step: 38435\n",
      "loss: 12.65613842010498\n",
      "steps per second: 0.58470\n",
      "step: 38436\n",
      "loss: 12.59463882446289\n",
      "steps per second: 0.54774\n",
      "step: 38437\n",
      "loss: 12.604283332824707\n",
      "steps per second: 0.57254\n",
      "step: 38438\n",
      "loss: 12.909048080444336\n",
      "steps per second: 0.53091\n",
      "step: 38439\n",
      "loss: 12.824676513671875\n",
      "steps per second: 0.50396\n",
      "step: 38440\n",
      "loss: 12.255820274353027\n",
      "steps per second: 0.54715\n",
      "step: 38441\n",
      "loss: 13.027387619018555\n",
      "steps per second: 0.58645\n",
      "step: 38442\n",
      "loss: 12.69190502166748\n",
      "steps per second: 0.58122\n",
      "step: 38443\n",
      "loss: 13.325060844421387\n",
      "steps per second: 0.59366\n",
      "step: 38444\n",
      "loss: 12.805610656738281\n",
      "steps per second: 0.53998\n",
      "step: 38445\n",
      "loss: 12.944334983825684\n",
      "steps per second: 0.51075\n",
      "step: 38446\n",
      "loss: 12.467660903930664\n",
      "steps per second: 0.55434\n",
      "step: 38447\n",
      "loss: 12.766939163208008\n",
      "steps per second: 0.59061\n",
      "step: 38448\n",
      "loss: 12.210832595825195\n",
      "steps per second: 0.51828\n",
      "step: 38449\n",
      "loss: 12.725790023803711\n",
      "steps per second: 0.61793\n",
      "step: 38450\n",
      "loss: 12.784360885620117\n",
      "steps per second: 0.50605\n",
      "step: 38451\n",
      "loss: 13.193943977355957\n",
      "steps per second: 0.55823\n",
      "step: 38452\n",
      "loss: 12.727018356323242\n",
      "steps per second: 0.46847\n",
      "step: 38453\n",
      "loss: 13.170001983642578\n",
      "steps per second: 0.54454\n",
      "step: 38454\n",
      "loss: 12.62893295288086\n",
      "steps per second: 0.56636\n",
      "step: 38455\n",
      "loss: 12.736917495727539\n",
      "steps per second: 0.60414\n",
      "step: 38456\n",
      "loss: 13.121018409729004\n",
      "steps per second: 0.50890\n",
      "step: 38457\n",
      "loss: 12.90947151184082\n",
      "steps per second: 0.60527\n",
      "step: 38458\n",
      "loss: 13.570335388183594\n",
      "steps per second: 0.49489\n",
      "step: 38459\n",
      "loss: 12.786059379577637\n",
      "steps per second: 0.56913\n",
      "step: 38460\n",
      "loss: 13.544206619262695\n",
      "steps per second: 0.54427\n",
      "step: 38461\n",
      "loss: 12.99212646484375\n",
      "steps per second: 0.52416\n",
      "step: 38462\n",
      "loss: 13.246770858764648\n",
      "steps per second: 0.56788\n",
      "step: 38463\n",
      "loss: 12.838930130004883\n",
      "steps per second: 0.55199\n",
      "step: 38464\n",
      "loss: 12.735193252563477\n",
      "steps per second: 0.55830\n",
      "step: 38465\n",
      "loss: 12.874490737915039\n",
      "steps per second: 0.52010\n",
      "step: 38466\n",
      "loss: 12.588292121887207\n",
      "steps per second: 0.58585\n",
      "step: 38467\n",
      "loss: 13.32409381866455\n",
      "steps per second: 0.56565\n",
      "step: 38468\n",
      "loss: 12.900690078735352\n",
      "steps per second: 0.58768\n",
      "step: 38469\n",
      "loss: 12.822382926940918\n",
      "steps per second: 0.56697\n",
      "step: 38470\n",
      "loss: 12.886478424072266\n",
      "steps per second: 0.53556\n",
      "step: 38471\n",
      "loss: 13.066672325134277\n",
      "steps per second: 0.56104\n",
      "step: 38472\n",
      "loss: 12.834229469299316\n",
      "steps per second: 0.56352\n",
      "step: 38473\n",
      "loss: 12.516709327697754\n",
      "steps per second: 0.55152\n",
      "step: 38474\n",
      "loss: 12.83252239227295\n",
      "steps per second: 0.55728\n",
      "step: 38475\n",
      "loss: 12.563812255859375\n",
      "steps per second: 0.50418\n",
      "step: 38476\n",
      "loss: 12.177209854125977\n",
      "steps per second: 0.57423\n",
      "step: 38477\n",
      "loss: 12.831136703491211\n",
      "steps per second: 0.54383\n",
      "step: 38478\n",
      "loss: 12.193995475769043\n",
      "steps per second: 0.54878\n",
      "step: 38479\n",
      "loss: 12.334099769592285\n",
      "steps per second: 0.52962\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.795954167842865, layer: 11\n",
      "saving at step 38479\n",
      "----------\n",
      "\n",
      "\n",
      "step: 38480\n",
      "loss: 13.094680786132812\n",
      "steps per second: 0.26503\n",
      "step: 38481\n",
      "loss: 12.384017944335938\n",
      "steps per second: 0.58118\n",
      "step: 38482\n",
      "loss: 12.204712867736816\n",
      "steps per second: 0.54434\n",
      "step: 38483\n",
      "loss: 12.641691207885742\n",
      "steps per second: 0.54809\n",
      "step: 38484\n",
      "loss: 12.647212028503418\n",
      "steps per second: 0.55473\n",
      "step: 38485\n",
      "loss: 13.026105880737305\n",
      "steps per second: 0.51964\n",
      "step: 38486\n",
      "loss: 12.212469100952148\n",
      "steps per second: 0.53671\n",
      "step: 38487\n",
      "loss: 12.518793106079102\n",
      "steps per second: 0.56727\n",
      "step: 38488\n",
      "loss: 13.001233100891113\n",
      "steps per second: 0.55016\n",
      "step: 38489\n",
      "loss: 12.821155548095703\n",
      "steps per second: 0.51712\n",
      "step: 38490\n",
      "loss: 13.208351135253906\n",
      "steps per second: 0.55451\n",
      "step: 38491\n",
      "loss: 12.764240264892578\n",
      "steps per second: 0.54007\n",
      "step: 38492\n",
      "loss: 12.853408813476562\n",
      "steps per second: 0.56380\n",
      "step: 38493\n",
      "loss: 12.221824645996094\n",
      "steps per second: 0.54590\n",
      "step: 38494\n",
      "loss: 12.434979438781738\n",
      "steps per second: 0.55116\n",
      "step: 38495\n",
      "loss: 12.368613243103027\n",
      "steps per second: 0.51859\n",
      "step: 38496\n",
      "loss: 13.338675498962402\n",
      "steps per second: 0.54963\n",
      "step: 38497\n",
      "loss: 12.945625305175781\n",
      "steps per second: 0.53717\n",
      "step: 38498\n",
      "loss: 12.800382614135742\n",
      "steps per second: 0.55581\n",
      "step: 38499\n",
      "loss: 12.37024974822998\n",
      "steps per second: 0.49837\n",
      "step: 38500\n",
      "loss: 12.73798656463623\n",
      "steps per second: 0.55035\n",
      "step: 38501\n",
      "loss: 12.519282341003418\n",
      "steps per second: 0.56844\n",
      "step: 38502\n",
      "loss: 12.455737113952637\n",
      "steps per second: 0.56883\n",
      "step: 38503\n",
      "loss: 12.567910194396973\n",
      "steps per second: 0.56511\n",
      "step: 38504\n",
      "loss: 12.509481430053711\n",
      "steps per second: 0.52820\n",
      "step: 38505\n",
      "loss: 12.964448928833008\n",
      "steps per second: 0.52119\n",
      "step: 38506\n",
      "loss: 12.886244773864746\n",
      "steps per second: 0.48981\n",
      "step: 38507\n",
      "loss: 13.115477561950684\n",
      "steps per second: 0.48056\n",
      "step: 38508\n",
      "loss: 12.42017936706543\n",
      "steps per second: 0.51310\n",
      "step: 38509\n",
      "loss: 12.802718162536621\n",
      "steps per second: 0.60120\n",
      "step: 38510\n",
      "loss: 12.844566345214844\n",
      "steps per second: 0.56491\n",
      "step: 38511\n",
      "loss: 12.873127937316895\n",
      "steps per second: 0.55797\n",
      "step: 38512\n",
      "loss: 12.567766189575195\n",
      "steps per second: 0.58013\n",
      "step: 38513\n",
      "loss: 12.781999588012695\n",
      "steps per second: 0.58666\n",
      "step: 38514\n",
      "loss: 12.688543319702148\n",
      "steps per second: 0.53931\n",
      "step: 38515\n",
      "loss: 13.022106170654297\n",
      "steps per second: 0.58195\n",
      "step: 38516\n",
      "loss: 13.276679992675781\n",
      "steps per second: 0.53092\n",
      "step: 38517\n",
      "loss: 12.66423225402832\n",
      "steps per second: 0.55954\n",
      "step: 38518\n",
      "loss: 13.149393081665039\n",
      "steps per second: 0.55749\n",
      "step: 38519\n",
      "loss: 12.73803997039795\n",
      "steps per second: 0.56147\n",
      "step: 38520\n",
      "loss: 12.298408508300781\n",
      "steps per second: 0.57914\n",
      "step: 38521\n",
      "loss: 12.664163589477539\n",
      "steps per second: 0.61662\n",
      "step: 38522\n",
      "loss: 13.340526580810547\n",
      "steps per second: 0.55995\n",
      "step: 38523\n",
      "loss: 12.51512622833252\n",
      "steps per second: 0.55904\n",
      "step: 38524\n",
      "loss: 12.731117248535156\n",
      "steps per second: 0.50476\n",
      "step: 38525\n",
      "loss: 13.458085060119629\n",
      "steps per second: 0.54112\n",
      "step: 38526\n",
      "loss: 13.111961364746094\n",
      "steps per second: 0.53108\n",
      "step: 38527\n",
      "loss: 12.853315353393555\n",
      "steps per second: 0.55737\n",
      "step: 38528\n",
      "loss: 12.572582244873047\n",
      "steps per second: 0.58240\n",
      "step: 38529\n",
      "loss: 12.887527465820312\n",
      "steps per second: 0.58095\n",
      "step: 38530\n",
      "loss: 12.644277572631836\n",
      "steps per second: 0.57842\n",
      "step: 38531\n",
      "loss: 12.183880805969238\n",
      "steps per second: 0.52734\n",
      "step: 38532\n",
      "loss: 12.739477157592773\n",
      "steps per second: 0.52628\n",
      "step: 38533\n",
      "loss: 13.606263160705566\n",
      "steps per second: 0.51875\n",
      "step: 38534\n",
      "loss: 11.642532348632812\n",
      "steps per second: 0.57805\n",
      "step: 38535\n",
      "loss: 12.734189987182617\n",
      "steps per second: 0.56715\n",
      "step: 38536\n",
      "loss: 12.804931640625\n",
      "steps per second: 0.56146\n",
      "step: 38537\n",
      "loss: 12.146283149719238\n",
      "steps per second: 0.55928\n",
      "step: 38538\n",
      "loss: 12.458639144897461\n",
      "steps per second: 0.57453\n",
      "step: 38539\n",
      "loss: 12.88351821899414\n",
      "steps per second: 0.57659\n",
      "step: 38540\n",
      "loss: 13.685318946838379\n",
      "steps per second: 0.55356\n",
      "step: 38541\n",
      "loss: 12.484040260314941\n",
      "steps per second: 0.55182\n",
      "step: 38542\n",
      "loss: 12.69297981262207\n",
      "steps per second: 0.55378\n",
      "step: 38543\n",
      "loss: 12.648528099060059\n",
      "steps per second: 0.56252\n",
      "step: 38544\n",
      "loss: 12.829360008239746\n",
      "steps per second: 0.55655\n",
      "step: 38545\n",
      "loss: 12.166211128234863\n",
      "steps per second: 0.58240\n",
      "step: 38546\n",
      "loss: 12.351706504821777\n",
      "steps per second: 0.57729\n",
      "step: 38547\n",
      "loss: 13.216547966003418\n",
      "steps per second: 0.58700\n",
      "step: 38548\n",
      "loss: 13.180534362792969\n",
      "steps per second: 0.52577\n",
      "step: 38549\n",
      "loss: 12.703474998474121\n",
      "steps per second: 0.55354\n",
      "step: 38550\n",
      "loss: 12.879677772521973\n",
      "steps per second: 0.53931\n",
      "step: 38551\n",
      "loss: 13.059893608093262\n",
      "steps per second: 0.53802\n",
      "step: 38552\n",
      "loss: 12.674891471862793\n",
      "steps per second: 0.48050\n",
      "step: 38553\n",
      "loss: 12.921365737915039\n",
      "steps per second: 0.58794\n",
      "step: 38554\n",
      "loss: 12.714214324951172\n",
      "steps per second: 0.51795\n",
      "step: 38555\n",
      "loss: 13.090652465820312\n",
      "steps per second: 0.58154\n",
      "step: 38556\n",
      "loss: 13.07922077178955\n",
      "steps per second: 0.55956\n",
      "step: 38557\n",
      "loss: 12.742656707763672\n",
      "steps per second: 0.61908\n",
      "step: 38558\n",
      "loss: 12.709442138671875\n",
      "steps per second: 0.53017\n",
      "step: 38559\n",
      "loss: 12.415207862854004\n",
      "steps per second: 0.57523\n",
      "step: 38560\n",
      "loss: 12.977723121643066\n",
      "steps per second: 0.58539\n",
      "step: 38561\n",
      "loss: 12.715449333190918\n",
      "steps per second: 0.55087\n",
      "step: 38562\n",
      "loss: 13.148691177368164\n",
      "steps per second: 0.53879\n",
      "step: 38563\n",
      "loss: 13.226004600524902\n",
      "steps per second: 0.55140\n",
      "step: 38564\n",
      "loss: 13.066350936889648\n",
      "steps per second: 0.58882\n",
      "step: 38565\n",
      "loss: 12.611886024475098\n",
      "steps per second: 0.57810\n",
      "step: 38566\n",
      "loss: 13.177038192749023\n",
      "steps per second: 0.56525\n",
      "step: 38567\n",
      "loss: 12.094858169555664\n",
      "steps per second: 0.58088\n",
      "step: 38568\n",
      "loss: 12.421112060546875\n",
      "steps per second: 0.55705\n",
      "step: 38569\n",
      "loss: 12.881413459777832\n",
      "steps per second: 0.56621\n",
      "step: 38570\n",
      "loss: 13.344196319580078\n",
      "steps per second: 0.55206\n",
      "step: 38571\n",
      "loss: 13.347616195678711\n",
      "steps per second: 0.55829\n",
      "step: 38572\n",
      "loss: 12.667978286743164\n",
      "steps per second: 0.55830\n",
      "step: 38573\n",
      "loss: 13.124192237854004\n",
      "steps per second: 0.51068\n",
      "step: 38574\n",
      "loss: 12.89054012298584\n",
      "steps per second: 0.53962\n",
      "step: 38575\n",
      "loss: 12.827787399291992\n",
      "steps per second: 0.51607\n",
      "step: 38576\n",
      "loss: 12.660093307495117\n",
      "steps per second: 0.52808\n",
      "step: 38577\n",
      "loss: 12.60831069946289\n",
      "steps per second: 0.47829\n",
      "step: 38578\n",
      "loss: 12.687965393066406\n",
      "steps per second: 0.49805\n",
      "step: 38579\n",
      "loss: 12.668142318725586\n",
      "steps per second: 0.54268\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8408583402633667, layer: 12\n",
      "saving at step 38579\n",
      "----------\n",
      "\n",
      "\n",
      "step: 38580\n",
      "loss: 12.722681999206543\n",
      "steps per second: 0.26536\n",
      "step: 38581\n",
      "loss: 12.447237014770508\n",
      "steps per second: 0.54672\n",
      "step: 38582\n",
      "loss: 12.531332015991211\n",
      "steps per second: 0.50489\n",
      "step: 38583\n",
      "loss: 12.553098678588867\n",
      "steps per second: 0.52448\n",
      "step: 38584\n",
      "loss: 12.22092056274414\n",
      "steps per second: 0.54975\n",
      "step: 38585\n",
      "loss: 13.165393829345703\n",
      "steps per second: 0.53131\n",
      "step: 38586\n",
      "loss: 12.745811462402344\n",
      "steps per second: 0.55587\n",
      "step: 38587\n",
      "loss: 13.083100318908691\n",
      "steps per second: 0.52114\n",
      "step: 38588\n",
      "loss: 12.713276863098145\n",
      "steps per second: 0.56489\n",
      "step: 38589\n",
      "loss: 12.31053638458252\n",
      "steps per second: 0.54882\n",
      "step: 38590\n",
      "loss: 12.606776237487793\n",
      "steps per second: 0.54245\n",
      "step: 38591\n",
      "loss: 12.41811752319336\n",
      "steps per second: 0.55612\n",
      "step: 38592\n",
      "loss: 13.02805233001709\n",
      "steps per second: 0.54654\n",
      "step: 38593\n",
      "loss: 12.806986808776855\n",
      "steps per second: 0.53383\n",
      "step: 38594\n",
      "loss: 12.88429069519043\n",
      "steps per second: 0.51207\n",
      "step: 38595\n",
      "loss: 12.752260208129883\n",
      "steps per second: 0.50581\n",
      "step: 38596\n",
      "loss: 12.355926513671875\n",
      "steps per second: 0.53636\n",
      "step: 38597\n",
      "loss: 13.343436241149902\n",
      "steps per second: 0.53757\n",
      "step: 38598\n",
      "loss: 12.462560653686523\n",
      "steps per second: 0.52815\n",
      "step: 38599\n",
      "loss: 13.047672271728516\n",
      "steps per second: 0.49873\n",
      "step: 38600\n",
      "loss: 12.862058639526367\n",
      "steps per second: 0.50431\n",
      "step: 38601\n",
      "loss: 12.830273628234863\n",
      "steps per second: 0.53576\n",
      "step: 38602\n",
      "loss: 13.294710159301758\n",
      "steps per second: 0.51970\n",
      "step: 38603\n",
      "loss: 13.0029935836792\n",
      "steps per second: 0.52663\n",
      "step: 38604\n",
      "loss: 12.527326583862305\n",
      "steps per second: 0.60699\n",
      "step: 38605\n",
      "loss: 13.327263832092285\n",
      "steps per second: 0.56474\n",
      "step: 38606\n",
      "loss: 13.178874015808105\n",
      "steps per second: 0.52606\n",
      "step: 38607\n",
      "loss: 12.647910118103027\n",
      "steps per second: 0.56657\n",
      "step: 38608\n",
      "loss: 12.595600128173828\n",
      "steps per second: 0.57633\n",
      "step: 38609\n",
      "loss: 12.771831512451172\n",
      "steps per second: 0.55376\n",
      "step: 38610\n",
      "loss: 12.721336364746094\n",
      "steps per second: 0.52546\n",
      "step: 38611\n",
      "loss: 12.667475700378418\n",
      "steps per second: 0.57674\n",
      "step: 38612\n",
      "loss: 12.77109146118164\n",
      "steps per second: 0.53191\n",
      "step: 38613\n",
      "loss: 12.926107406616211\n",
      "steps per second: 0.53265\n",
      "step: 38614\n",
      "loss: 12.600423812866211\n",
      "steps per second: 0.61668\n",
      "step: 38615\n",
      "loss: 12.964987754821777\n",
      "steps per second: 0.58479\n",
      "step: 38616\n",
      "loss: 12.335107803344727\n",
      "steps per second: 0.55507\n",
      "step: 38617\n",
      "loss: 12.697507858276367\n",
      "steps per second: 0.55124\n",
      "step: 38618\n",
      "loss: 12.410972595214844\n",
      "steps per second: 0.57148\n",
      "step: 38619\n",
      "loss: 12.843757629394531\n",
      "steps per second: 0.55333\n",
      "step: 38620\n",
      "loss: 12.342394828796387\n",
      "steps per second: 0.53725\n",
      "step: 38621\n",
      "loss: 12.30277156829834\n",
      "steps per second: 0.56510\n",
      "step: 38622\n",
      "loss: 12.7445650100708\n",
      "steps per second: 0.56686\n",
      "step: 38623\n",
      "loss: 12.469029426574707\n",
      "steps per second: 0.54156\n",
      "step: 38624\n",
      "loss: 12.910536766052246\n",
      "steps per second: 0.55105\n",
      "step: 38625\n",
      "loss: 13.147268295288086\n",
      "steps per second: 0.52824\n",
      "step: 38626\n",
      "loss: 13.008794784545898\n",
      "steps per second: 0.55866\n",
      "step: 38627\n",
      "loss: 12.787007331848145\n",
      "steps per second: 0.58013\n",
      "step: 38628\n",
      "loss: 13.040084838867188\n",
      "steps per second: 0.58657\n",
      "step: 38629\n",
      "loss: 12.996294975280762\n",
      "steps per second: 0.55326\n",
      "step: 38630\n",
      "loss: 12.986656188964844\n",
      "steps per second: 0.56143\n",
      "step: 38631\n",
      "loss: 12.730168342590332\n",
      "steps per second: 0.55054\n",
      "step: 38632\n",
      "loss: 12.998007774353027\n",
      "steps per second: 0.56700\n",
      "step: 38633\n",
      "loss: 13.058263778686523\n",
      "steps per second: 0.61438\n",
      "step: 38634\n",
      "loss: 12.528264999389648\n",
      "steps per second: 0.58081\n",
      "step: 38635\n",
      "loss: 12.434874534606934\n",
      "steps per second: 0.55965\n",
      "step: 38636\n",
      "loss: 12.602322578430176\n",
      "steps per second: 0.49998\n",
      "step: 38637\n",
      "loss: 12.91026496887207\n",
      "steps per second: 0.56121\n",
      "step: 38638\n",
      "loss: 13.298234939575195\n",
      "steps per second: 0.57782\n",
      "step: 38639\n",
      "loss: 12.088537216186523\n",
      "steps per second: 0.56129\n",
      "step: 38640\n",
      "loss: 12.638949394226074\n",
      "steps per second: 0.54369\n",
      "step: 38641\n",
      "loss: 12.8404541015625\n",
      "steps per second: 0.58123\n",
      "step: 38642\n",
      "loss: 12.754565238952637\n",
      "steps per second: 0.57559\n",
      "step: 38643\n",
      "loss: 12.663357734680176\n",
      "steps per second: 0.57450\n",
      "step: 38644\n",
      "loss: 12.82811164855957\n",
      "steps per second: 0.55055\n",
      "step: 38645\n",
      "loss: 12.633389472961426\n",
      "steps per second: 0.53798\n",
      "step: 38646\n",
      "loss: 12.16862678527832\n",
      "steps per second: 0.52008\n",
      "step: 38647\n",
      "loss: 13.049773216247559\n",
      "steps per second: 0.54584\n",
      "step: 38648\n",
      "loss: 12.419561386108398\n",
      "steps per second: 0.54004\n",
      "step: 38649\n",
      "loss: 12.699358940124512\n",
      "steps per second: 0.47626\n",
      "step: 38650\n",
      "loss: 14.1338529586792\n",
      "steps per second: 0.54631\n",
      "step: 38651\n",
      "loss: 13.029562950134277\n",
      "steps per second: 0.52871\n",
      "step: 38652\n",
      "loss: 12.881784439086914\n",
      "steps per second: 0.51038\n",
      "step: 38653\n",
      "loss: 12.97790813446045\n",
      "steps per second: 0.49712\n",
      "step: 38654\n",
      "loss: 12.498064041137695\n",
      "steps per second: 0.53555\n",
      "step: 38655\n",
      "loss: 12.448541641235352\n",
      "steps per second: 0.53689\n",
      "step: 38656\n",
      "loss: 12.97725772857666\n",
      "steps per second: 0.50079\n",
      "step: 38657\n",
      "loss: 12.254081726074219\n",
      "steps per second: 0.53149\n",
      "step: 38658\n",
      "loss: 12.640298843383789\n",
      "steps per second: 0.55290\n",
      "step: 38659\n",
      "loss: 12.281623840332031\n",
      "steps per second: 0.51122\n",
      "step: 38660\n",
      "loss: 12.578855514526367\n",
      "steps per second: 0.54908\n",
      "step: 38661\n",
      "loss: 12.640486717224121\n",
      "steps per second: 0.47442\n",
      "step: 38662\n",
      "loss: 13.268938064575195\n",
      "steps per second: 0.54114\n",
      "step: 38663\n",
      "loss: 12.726655960083008\n",
      "steps per second: 0.49055\n",
      "step: 38664\n",
      "loss: 12.442846298217773\n",
      "steps per second: 0.56538\n",
      "step: 38665\n",
      "loss: 12.400664329528809\n",
      "steps per second: 0.51269\n",
      "step: 38666\n",
      "loss: 12.574119567871094\n",
      "steps per second: 0.53169\n",
      "step: 38667\n",
      "loss: 12.77763557434082\n",
      "steps per second: 0.55079\n",
      "step: 38668\n",
      "loss: 12.574054718017578\n",
      "steps per second: 0.50426\n",
      "step: 38669\n",
      "loss: 12.859671592712402\n",
      "steps per second: 0.49377\n",
      "step: 38670\n",
      "loss: 12.800848007202148\n",
      "steps per second: 0.51533\n",
      "step: 38671\n",
      "loss: 12.648826599121094\n",
      "steps per second: 0.51525\n",
      "step: 38672\n",
      "loss: 12.912044525146484\n",
      "steps per second: 0.50743\n",
      "step: 38673\n",
      "loss: 12.666668891906738\n",
      "steps per second: 0.56079\n",
      "step: 38674\n",
      "loss: 12.70181655883789\n",
      "steps per second: 0.55063\n",
      "step: 38675\n",
      "loss: 12.51381778717041\n",
      "steps per second: 0.56665\n",
      "step: 38676\n",
      "loss: 12.599364280700684\n",
      "steps per second: 0.54544\n",
      "step: 38677\n",
      "loss: 12.672836303710938\n",
      "steps per second: 0.61500\n",
      "step: 38678\n",
      "loss: 12.407999038696289\n",
      "steps per second: 0.56637\n",
      "step: 38679\n",
      "loss: 12.375165939331055\n",
      "steps per second: 0.58701\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7988370060920715, layer: 11\n",
      "saving at step 38679\n",
      "----------\n",
      "\n",
      "\n",
      "step: 38680\n",
      "loss: 12.717780113220215\n",
      "steps per second: 0.28900\n",
      "step: 38681\n",
      "loss: 12.703329086303711\n",
      "steps per second: 0.56634\n",
      "step: 38682\n",
      "loss: 13.13699722290039\n",
      "steps per second: 0.56933\n",
      "step: 38683\n",
      "loss: 12.661962509155273\n",
      "steps per second: 0.52438\n",
      "step: 38684\n",
      "loss: 12.917717933654785\n",
      "steps per second: 0.53236\n",
      "step: 38685\n",
      "loss: 12.554141998291016\n",
      "steps per second: 0.51166\n",
      "step: 38686\n",
      "loss: 12.84360122680664\n",
      "steps per second: 0.54649\n",
      "step: 38687\n",
      "loss: 12.895842552185059\n",
      "steps per second: 0.56438\n",
      "step: 38688\n",
      "loss: 12.530030250549316\n",
      "steps per second: 0.53487\n",
      "step: 38689\n",
      "loss: 12.391301155090332\n",
      "steps per second: 0.53172\n",
      "step: 38690\n",
      "loss: 12.923625946044922\n",
      "steps per second: 0.45760\n",
      "step: 38691\n",
      "loss: 12.966790199279785\n",
      "steps per second: 0.55317\n",
      "step: 38692\n",
      "loss: 12.538422584533691\n",
      "steps per second: 0.49826\n",
      "step: 38693\n",
      "loss: 12.60634994506836\n",
      "steps per second: 0.52063\n",
      "step: 38694\n",
      "loss: 12.55453872680664\n",
      "steps per second: 0.53666\n",
      "step: 38695\n",
      "loss: 13.416180610656738\n",
      "steps per second: 0.60554\n",
      "step: 38696\n",
      "loss: 12.49302864074707\n",
      "steps per second: 0.54010\n",
      "step: 38697\n",
      "loss: 12.562285423278809\n",
      "steps per second: 0.55148\n",
      "step: 38698\n",
      "loss: 12.715059280395508\n",
      "steps per second: 0.54732\n",
      "step: 38699\n",
      "loss: 12.584725379943848\n",
      "steps per second: 0.55181\n",
      "step: 38700\n",
      "loss: 13.29806137084961\n",
      "steps per second: 0.54347\n",
      "step: 38701\n",
      "loss: 12.310900688171387\n",
      "steps per second: 0.53323\n",
      "step: 38702\n",
      "loss: 12.392401695251465\n",
      "steps per second: 0.52458\n",
      "step: 38703\n",
      "loss: 12.550617218017578\n",
      "steps per second: 0.53060\n",
      "step: 38704\n",
      "loss: 12.945104598999023\n",
      "steps per second: 0.57711\n",
      "step: 38705\n",
      "loss: 12.587403297424316\n",
      "steps per second: 0.56820\n",
      "step: 38706\n",
      "loss: 12.672816276550293\n",
      "steps per second: 0.56495\n",
      "step: 38707\n",
      "loss: 12.399334907531738\n",
      "steps per second: 0.57281\n",
      "step: 38708\n",
      "loss: 12.091238021850586\n",
      "steps per second: 0.52525\n",
      "step: 38709\n",
      "loss: 12.731941223144531\n",
      "steps per second: 0.55248\n",
      "step: 38710\n",
      "loss: 12.64519214630127\n",
      "steps per second: 0.55157\n",
      "step: 38711\n",
      "loss: 12.66015911102295\n",
      "steps per second: 0.57217\n",
      "step: 38712\n",
      "loss: 12.954574584960938\n",
      "steps per second: 0.55522\n",
      "step: 38713\n",
      "loss: 13.09703540802002\n",
      "steps per second: 0.55359\n",
      "step: 38714\n",
      "loss: 12.17130184173584\n",
      "steps per second: 0.54774\n",
      "step: 38715\n",
      "loss: 12.7137451171875\n",
      "steps per second: 0.53223\n",
      "step: 38716\n",
      "loss: 12.863540649414062\n",
      "steps per second: 0.50776\n",
      "step: 38717\n",
      "loss: 12.608182907104492\n",
      "steps per second: 0.55687\n",
      "step: 38718\n",
      "loss: 12.771849632263184\n",
      "steps per second: 0.56175\n",
      "step: 38719\n",
      "loss: 12.377535820007324\n",
      "steps per second: 0.53854\n",
      "step: 38720\n",
      "loss: 12.379209518432617\n",
      "steps per second: 0.53153\n",
      "step: 38721\n",
      "loss: 13.158736228942871\n",
      "steps per second: 0.49815\n",
      "step: 38722\n",
      "loss: 12.864564895629883\n",
      "steps per second: 0.54854\n",
      "step: 38723\n",
      "loss: 12.962851524353027\n",
      "steps per second: 0.55069\n",
      "step: 38724\n",
      "loss: 13.080083847045898\n",
      "steps per second: 0.58438\n",
      "step: 38725\n",
      "loss: 12.819018363952637\n",
      "steps per second: 0.52727\n",
      "step: 38726\n",
      "loss: 12.466349601745605\n",
      "steps per second: 0.54682\n",
      "step: 38727\n",
      "loss: 12.999446868896484\n",
      "steps per second: 0.56557\n",
      "step: 38728\n",
      "loss: 12.691965103149414\n",
      "steps per second: 0.51753\n",
      "step: 38729\n",
      "loss: 12.7473783493042\n",
      "steps per second: 0.55898\n",
      "step: 38730\n",
      "loss: 12.717094421386719\n",
      "steps per second: 0.53312\n",
      "step: 38731\n",
      "loss: 13.015639305114746\n",
      "steps per second: 0.56171\n",
      "step: 38732\n",
      "loss: 13.164055824279785\n",
      "steps per second: 0.55533\n",
      "step: 38733\n",
      "loss: 12.56607723236084\n",
      "steps per second: 0.56222\n",
      "step: 38734\n",
      "loss: 12.989285469055176\n",
      "steps per second: 0.55645\n",
      "step: 38735\n",
      "loss: 12.956992149353027\n",
      "steps per second: 0.58284\n",
      "step: 38736\n",
      "loss: 13.069106101989746\n",
      "steps per second: 0.53089\n",
      "step: 38737\n",
      "loss: 12.201105117797852\n",
      "steps per second: 0.58921\n",
      "step: 38738\n",
      "loss: 12.818578720092773\n",
      "steps per second: 0.54218\n",
      "step: 38739\n",
      "loss: 12.230599403381348\n",
      "steps per second: 0.55873\n",
      "step: 38740\n",
      "loss: 12.418389320373535\n",
      "steps per second: 0.58168\n",
      "step: 38741\n",
      "loss: 13.050664901733398\n",
      "steps per second: 0.56088\n",
      "step: 38742\n",
      "loss: 12.603232383728027\n",
      "steps per second: 0.53908\n",
      "step: 38743\n",
      "loss: 12.604523658752441\n",
      "steps per second: 0.58890\n",
      "step: 38744\n",
      "loss: 12.375280380249023\n",
      "steps per second: 0.62395\n",
      "step: 38745\n",
      "loss: 12.801633834838867\n",
      "steps per second: 0.55315\n",
      "step: 38746\n",
      "loss: 13.116678237915039\n",
      "steps per second: 0.56073\n",
      "step: 38747\n",
      "loss: 13.141677856445312\n",
      "steps per second: 0.58395\n",
      "step: 38748\n",
      "loss: 12.599359512329102\n",
      "steps per second: 0.50828\n",
      "step: 38749\n",
      "loss: 12.687232971191406\n",
      "steps per second: 0.56258\n",
      "step: 38750\n",
      "loss: 12.224188804626465\n",
      "steps per second: 0.54296\n",
      "step: 38751\n",
      "loss: 13.634695053100586\n",
      "steps per second: 0.54160\n",
      "step: 38752\n",
      "loss: 12.822443008422852\n",
      "steps per second: 0.62273\n",
      "step: 38753\n",
      "loss: 12.678437232971191\n",
      "steps per second: 0.56192\n",
      "step: 38754\n",
      "loss: 12.662982940673828\n",
      "steps per second: 0.62159\n",
      "step: 38755\n",
      "loss: 12.964107513427734\n",
      "steps per second: 0.56122\n",
      "step: 38756\n",
      "loss: 12.93429946899414\n",
      "steps per second: 0.55918\n",
      "step: 38757\n",
      "loss: 12.626510620117188\n",
      "steps per second: 0.56907\n",
      "step: 38758\n",
      "loss: 12.323022842407227\n",
      "steps per second: 0.56742\n",
      "step: 38759\n",
      "loss: 13.08441162109375\n",
      "steps per second: 0.55464\n",
      "step: 38760\n",
      "loss: 12.44868278503418\n",
      "steps per second: 0.56401\n",
      "step: 38761\n",
      "loss: 12.320249557495117\n",
      "steps per second: 0.58803\n",
      "step: 38762\n",
      "loss: 13.059469223022461\n",
      "steps per second: 0.51084\n",
      "step: 38763\n",
      "loss: 13.347980499267578\n",
      "steps per second: 0.54961\n",
      "step: 38764\n",
      "loss: 12.392748832702637\n",
      "steps per second: 0.54250\n",
      "step: 38765\n",
      "loss: 12.931207656860352\n",
      "steps per second: 0.55110\n",
      "step: 38766\n",
      "loss: 12.493882179260254\n",
      "steps per second: 0.56699\n",
      "step: 38767\n",
      "loss: 12.744791984558105\n",
      "steps per second: 0.58259\n",
      "step: 38768\n",
      "loss: 13.0841646194458\n",
      "steps per second: 0.59500\n",
      "step: 38769\n",
      "loss: 12.993719100952148\n",
      "steps per second: 0.57967\n",
      "step: 38770\n",
      "loss: 13.0239896774292\n",
      "steps per second: 0.53010\n",
      "step: 38771\n",
      "loss: 13.153124809265137\n",
      "steps per second: 0.57998\n",
      "step: 38772\n",
      "loss: 13.077363967895508\n",
      "steps per second: 0.57339\n",
      "step: 38773\n",
      "loss: 12.829861640930176\n",
      "steps per second: 0.52786\n",
      "step: 38774\n",
      "loss: 12.58531665802002\n",
      "steps per second: 0.54882\n",
      "step: 38775\n",
      "loss: 13.062862396240234\n",
      "steps per second: 0.55503\n",
      "step: 38776\n",
      "loss: 12.621492385864258\n",
      "steps per second: 0.57171\n",
      "step: 38777\n",
      "loss: 13.462401390075684\n",
      "steps per second: 0.52109\n",
      "step: 38778\n",
      "loss: 13.152860641479492\n",
      "steps per second: 0.53084\n",
      "step: 38779\n",
      "loss: 12.933479309082031\n",
      "steps per second: 0.56034\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.842039942741394, layer: 10\n",
      "saving at step 38779\n",
      "----------\n",
      "\n",
      "\n",
      "step: 38780\n",
      "loss: 13.230432510375977\n",
      "steps per second: 0.24934\n",
      "step: 38781\n",
      "loss: 12.713788986206055\n",
      "steps per second: 0.52325\n",
      "step: 38782\n",
      "loss: 12.741974830627441\n",
      "steps per second: 0.53827\n",
      "step: 38783\n",
      "loss: 13.199067115783691\n",
      "steps per second: 0.58145\n",
      "step: 38784\n",
      "loss: 13.259072303771973\n",
      "steps per second: 0.56755\n",
      "step: 38785\n",
      "loss: 12.98109245300293\n",
      "steps per second: 0.56423\n",
      "step: 38786\n",
      "loss: 12.730151176452637\n",
      "steps per second: 0.57977\n",
      "step: 38787\n",
      "loss: 12.944091796875\n",
      "steps per second: 0.56698\n",
      "step: 38788\n",
      "loss: 12.517770767211914\n",
      "steps per second: 0.55848\n",
      "step: 38789\n",
      "loss: 12.543944358825684\n",
      "steps per second: 0.55458\n",
      "step: 38790\n",
      "loss: 12.686192512512207\n",
      "steps per second: 0.56011\n",
      "step: 38791\n",
      "loss: 12.9307222366333\n",
      "steps per second: 0.56696\n",
      "step: 38792\n",
      "loss: 12.728123664855957\n",
      "steps per second: 0.58603\n",
      "step: 38793\n",
      "loss: 12.82497501373291\n",
      "steps per second: 0.57171\n",
      "step: 38794\n",
      "loss: 13.444480895996094\n",
      "steps per second: 0.56259\n",
      "step: 38795\n",
      "loss: 12.343412399291992\n",
      "steps per second: 0.56346\n",
      "step: 38796\n",
      "loss: 12.621537208557129\n",
      "steps per second: 0.55923\n",
      "step: 38797\n",
      "loss: 12.758943557739258\n",
      "steps per second: 0.58730\n",
      "step: 38798\n",
      "loss: 13.051728248596191\n",
      "steps per second: 0.61629\n",
      "step: 38799\n",
      "loss: 12.215980529785156\n",
      "steps per second: 0.54968\n",
      "step: 38800\n",
      "loss: 12.525186538696289\n",
      "steps per second: 0.62359\n",
      "step: 38801\n",
      "loss: 12.664738655090332\n",
      "steps per second: 0.48161\n",
      "step: 38802\n",
      "loss: 12.67863941192627\n",
      "steps per second: 0.52403\n",
      "step: 38803\n",
      "loss: 13.21609878540039\n",
      "steps per second: 0.62201\n",
      "step: 38804\n",
      "loss: 12.305817604064941\n",
      "steps per second: 0.62165\n",
      "step: 38805\n",
      "loss: 12.864679336547852\n",
      "steps per second: 0.56454\n",
      "step: 38806\n",
      "loss: 12.856866836547852\n",
      "steps per second: 0.55685\n",
      "step: 38807\n",
      "loss: 13.095380783081055\n",
      "steps per second: 0.51796\n",
      "step: 38808\n",
      "loss: 13.384835243225098\n",
      "steps per second: 0.53758\n",
      "step: 38809\n",
      "loss: 12.251609802246094\n",
      "steps per second: 0.52128\n",
      "step: 38810\n",
      "loss: 12.413043975830078\n",
      "steps per second: 0.52300\n",
      "step: 38811\n",
      "loss: 12.571821212768555\n",
      "steps per second: 0.57403\n",
      "step: 38812\n",
      "loss: 13.203518867492676\n",
      "steps per second: 0.57772\n",
      "step: 38813\n",
      "loss: 13.150311470031738\n",
      "steps per second: 0.55984\n",
      "step: 38814\n",
      "loss: 13.121020317077637\n",
      "steps per second: 0.56848\n",
      "step: 38815\n",
      "loss: 13.07102108001709\n",
      "steps per second: 0.56024\n",
      "step: 38816\n",
      "loss: 12.708332061767578\n",
      "steps per second: 0.58069\n",
      "step: 38817\n",
      "loss: 12.8650484085083\n",
      "steps per second: 0.56354\n",
      "step: 38818\n",
      "loss: 13.512374877929688\n",
      "steps per second: 0.51879\n",
      "step: 38819\n",
      "loss: 12.775581359863281\n",
      "steps per second: 0.55443\n",
      "step: 38820\n",
      "loss: 13.266447067260742\n",
      "steps per second: 0.58242\n",
      "step: 38821\n",
      "loss: 12.401998519897461\n",
      "steps per second: 0.59057\n",
      "step: 38822\n",
      "loss: 13.434459686279297\n",
      "steps per second: 0.53763\n",
      "step: 38823\n",
      "loss: 12.687165260314941\n",
      "steps per second: 0.52566\n",
      "step: 38824\n",
      "loss: 13.048800468444824\n",
      "steps per second: 0.57407\n",
      "step: 38825\n",
      "loss: 13.130956649780273\n",
      "steps per second: 0.55842\n",
      "step: 38826\n",
      "loss: 12.808043479919434\n",
      "steps per second: 0.54888\n",
      "step: 38827\n",
      "loss: 12.4794340133667\n",
      "steps per second: 0.51681\n",
      "step: 38828\n",
      "loss: 13.124592781066895\n",
      "steps per second: 0.48689\n",
      "step: 38829\n",
      "loss: 12.609230041503906\n",
      "steps per second: 0.56814\n",
      "step: 38830\n",
      "loss: 13.156501770019531\n",
      "steps per second: 0.58004\n",
      "step: 38831\n",
      "loss: 12.463719367980957\n",
      "steps per second: 0.57474\n",
      "step: 38832\n",
      "loss: 12.696520805358887\n",
      "steps per second: 0.57073\n",
      "step: 38833\n",
      "loss: 12.908793449401855\n",
      "steps per second: 0.55108\n",
      "step: 38834\n",
      "loss: 12.782463073730469\n",
      "steps per second: 0.61158\n",
      "step: 38835\n",
      "loss: 12.965766906738281\n",
      "steps per second: 0.52763\n",
      "step: 38836\n",
      "loss: 12.736000061035156\n",
      "steps per second: 0.51233\n",
      "step: 38837\n",
      "loss: 12.095946311950684\n",
      "steps per second: 0.55740\n",
      "step: 38838\n",
      "loss: 12.385744094848633\n",
      "steps per second: 0.56118\n",
      "step: 38839\n",
      "loss: 12.353021621704102\n",
      "steps per second: 0.55648\n",
      "step: 38840\n",
      "loss: 12.424999237060547\n",
      "steps per second: 0.50925\n",
      "step: 38841\n",
      "loss: 12.745718002319336\n",
      "steps per second: 0.58254\n",
      "step: 38842\n",
      "loss: 13.175737380981445\n",
      "steps per second: 0.52160\n",
      "step: 38843\n",
      "loss: 12.49305248260498\n",
      "steps per second: 0.55148\n",
      "step: 38844\n",
      "loss: 12.488558769226074\n",
      "steps per second: 0.56797\n",
      "step: 38845\n",
      "loss: 12.41434097290039\n",
      "steps per second: 0.55935\n",
      "step: 38846\n",
      "loss: 12.169698715209961\n",
      "steps per second: 0.51787\n",
      "step: 38847\n",
      "loss: 12.637847900390625\n",
      "steps per second: 0.56299\n",
      "step: 38848\n",
      "loss: 12.30415153503418\n",
      "steps per second: 0.58632\n",
      "step: 38849\n",
      "loss: 13.001517295837402\n",
      "steps per second: 0.61519\n",
      "step: 38850\n",
      "loss: 12.978342056274414\n",
      "steps per second: 0.52886\n",
      "step: 38851\n",
      "loss: 12.838658332824707\n",
      "steps per second: 0.62169\n",
      "step: 38852\n",
      "loss: 12.742647171020508\n",
      "steps per second: 0.58091\n",
      "step: 38853\n",
      "loss: 13.247568130493164\n",
      "steps per second: 0.55100\n",
      "step: 38854\n",
      "loss: 13.657694816589355\n",
      "steps per second: 0.51716\n",
      "step: 38855\n",
      "loss: 12.710769653320312\n",
      "steps per second: 0.57576\n",
      "step: 38856\n",
      "loss: 12.709382057189941\n",
      "steps per second: 0.53903\n",
      "step: 38857\n",
      "loss: 12.69753360748291\n",
      "steps per second: 0.53111\n",
      "step: 38858\n",
      "loss: 13.015949249267578\n",
      "steps per second: 0.55660\n",
      "step: 38859\n",
      "loss: 12.789844512939453\n",
      "steps per second: 0.57132\n",
      "step: 38860\n",
      "loss: 12.26663589477539\n",
      "steps per second: 0.61666\n",
      "step: 38861\n",
      "loss: 12.700366020202637\n",
      "steps per second: 0.52404\n",
      "step: 38862\n",
      "loss: 12.923718452453613\n",
      "steps per second: 0.50609\n",
      "step: 38863\n",
      "loss: 12.642192840576172\n",
      "steps per second: 0.56858\n",
      "step: 38864\n",
      "loss: 12.769948959350586\n",
      "steps per second: 0.56367\n",
      "step: 38865\n",
      "loss: 13.056255340576172\n",
      "steps per second: 0.52874\n",
      "step: 38866\n",
      "loss: 12.486042976379395\n",
      "steps per second: 0.52998\n",
      "step: 38867\n",
      "loss: 12.82131576538086\n",
      "steps per second: 0.55661\n",
      "step: 38868\n",
      "loss: 12.328964233398438\n",
      "steps per second: 0.52836\n",
      "step: 38869\n",
      "loss: 13.065194129943848\n",
      "steps per second: 0.56299\n",
      "step: 38870\n",
      "loss: 13.332389831542969\n",
      "steps per second: 0.54729\n",
      "step: 38871\n",
      "loss: 12.801783561706543\n",
      "steps per second: 0.55481\n",
      "step: 38872\n",
      "loss: 12.358150482177734\n",
      "steps per second: 0.52497\n",
      "step: 38873\n",
      "loss: 12.94657039642334\n",
      "steps per second: 0.57312\n",
      "step: 38874\n",
      "loss: 12.674816131591797\n",
      "steps per second: 0.57475\n",
      "step: 38875\n",
      "loss: 12.253375053405762\n",
      "steps per second: 0.51426\n",
      "step: 38876\n",
      "loss: 12.362250328063965\n",
      "steps per second: 0.55048\n",
      "step: 38877\n",
      "loss: 12.960975646972656\n",
      "steps per second: 0.57675\n",
      "step: 38878\n",
      "loss: 12.549823760986328\n",
      "steps per second: 0.52439\n",
      "step: 38879\n",
      "loss: 13.086650848388672\n",
      "steps per second: 0.52652\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8690177798271179, layer: 12\n",
      "saving at step 38879\n",
      "----------\n",
      "\n",
      "\n",
      "step: 38880\n",
      "loss: 12.765594482421875\n",
      "steps per second: 0.26604\n",
      "step: 38881\n",
      "loss: 13.337862968444824\n",
      "steps per second: 0.56906\n",
      "step: 38882\n",
      "loss: 13.268020629882812\n",
      "steps per second: 0.55184\n",
      "step: 38883\n",
      "loss: 12.658605575561523\n",
      "steps per second: 0.54984\n",
      "step: 38884\n",
      "loss: 12.623559951782227\n",
      "steps per second: 0.55389\n",
      "step: 38885\n",
      "loss: 12.751903533935547\n",
      "steps per second: 0.56324\n",
      "step: 38886\n",
      "loss: 12.843245506286621\n",
      "steps per second: 0.57384\n",
      "step: 38887\n",
      "loss: 13.176461219787598\n",
      "steps per second: 0.55000\n",
      "step: 38888\n",
      "loss: 11.952862739562988\n",
      "steps per second: 0.52037\n",
      "step: 38889\n",
      "loss: 12.519777297973633\n",
      "steps per second: 0.56452\n",
      "step: 38890\n",
      "loss: 13.123079299926758\n",
      "steps per second: 0.56080\n",
      "step: 38891\n",
      "loss: 12.861623764038086\n",
      "steps per second: 0.56851\n",
      "step: 38892\n",
      "loss: 12.859472274780273\n",
      "steps per second: 0.59208\n",
      "step: 38893\n",
      "loss: 12.734123229980469\n",
      "steps per second: 0.56141\n",
      "step: 38894\n",
      "loss: 12.555527687072754\n",
      "steps per second: 0.54679\n",
      "step: 38895\n",
      "loss: 12.873263359069824\n",
      "steps per second: 0.55955\n",
      "step: 38896\n",
      "loss: 12.644893646240234\n",
      "steps per second: 0.54638\n",
      "step: 38897\n",
      "loss: 12.714417457580566\n",
      "steps per second: 0.52480\n",
      "step: 38898\n",
      "loss: 12.763050079345703\n",
      "steps per second: 0.56371\n",
      "step: 38899\n",
      "loss: 12.443596839904785\n",
      "steps per second: 0.55660\n",
      "step: 38900\n",
      "loss: 12.967225074768066\n",
      "steps per second: 0.57172\n",
      "step: 38901\n",
      "loss: 12.851563453674316\n",
      "steps per second: 0.56901\n",
      "step: 38902\n",
      "loss: 13.008842468261719\n",
      "steps per second: 0.54807\n",
      "step: 38903\n",
      "loss: 12.851757049560547\n",
      "steps per second: 0.52574\n",
      "step: 38904\n",
      "loss: 12.599471092224121\n",
      "steps per second: 0.60471\n",
      "step: 38905\n",
      "loss: 12.827058792114258\n",
      "steps per second: 0.50944\n",
      "step: 38906\n",
      "loss: 12.930645942687988\n",
      "steps per second: 0.54573\n",
      "step: 38907\n",
      "loss: 13.022643089294434\n",
      "steps per second: 0.55261\n",
      "step: 38908\n",
      "loss: 12.96027946472168\n",
      "steps per second: 0.54077\n",
      "step: 38909\n",
      "loss: 12.87480640411377\n",
      "steps per second: 0.58040\n",
      "step: 38910\n",
      "loss: 12.944511413574219\n",
      "steps per second: 0.53586\n",
      "step: 38911\n",
      "loss: 12.984907150268555\n",
      "steps per second: 0.52767\n",
      "step: 38912\n",
      "loss: 12.83622932434082\n",
      "steps per second: 0.49501\n",
      "step: 38913\n",
      "loss: 12.982314109802246\n",
      "steps per second: 0.55363\n",
      "step: 38914\n",
      "loss: 12.683248519897461\n",
      "steps per second: 0.55349\n",
      "step: 38915\n",
      "loss: 12.534353256225586\n",
      "steps per second: 0.53935\n",
      "step: 38916\n",
      "loss: 12.723358154296875\n",
      "steps per second: 0.49803\n",
      "step: 38917\n",
      "loss: 12.958412170410156\n",
      "steps per second: 0.56235\n",
      "step: 38918\n",
      "loss: 12.25285816192627\n",
      "steps per second: 0.52894\n",
      "step: 38919\n",
      "loss: 12.627096176147461\n",
      "steps per second: 0.51467\n",
      "step: 38920\n",
      "loss: 12.822776794433594\n",
      "steps per second: 0.52599\n",
      "step: 38921\n",
      "loss: 12.576169967651367\n",
      "steps per second: 0.56008\n",
      "step: 38922\n",
      "loss: 12.072932243347168\n",
      "steps per second: 0.60981\n",
      "step: 38923\n",
      "loss: 13.208366394042969\n",
      "steps per second: 0.52257\n",
      "step: 38924\n",
      "loss: 12.816727638244629\n",
      "steps per second: 0.51949\n",
      "step: 38925\n",
      "loss: 13.013115882873535\n",
      "steps per second: 0.58107\n",
      "step: 38926\n",
      "loss: 13.183182716369629\n",
      "steps per second: 0.52371\n",
      "step: 38927\n",
      "loss: 12.716485023498535\n",
      "steps per second: 0.53855\n",
      "step: 38928\n",
      "loss: 12.5762357711792\n",
      "steps per second: 0.58668\n",
      "step: 38929\n",
      "loss: 12.634788513183594\n",
      "steps per second: 0.56093\n",
      "step: 38930\n",
      "loss: 12.813590049743652\n",
      "steps per second: 0.58665\n",
      "step: 38931\n",
      "loss: 13.159790992736816\n",
      "steps per second: 0.54075\n",
      "step: 38932\n",
      "loss: 12.561470985412598\n",
      "steps per second: 0.56556\n",
      "step: 38933\n",
      "loss: 12.50336742401123\n",
      "steps per second: 0.52537\n",
      "step: 38934\n",
      "loss: 13.084173202514648\n",
      "steps per second: 0.56882\n",
      "step: 38935\n",
      "loss: 12.453372955322266\n",
      "steps per second: 0.56264\n",
      "step: 38936\n",
      "loss: 12.803276062011719\n",
      "steps per second: 0.55426\n",
      "step: 38937\n",
      "loss: 12.581490516662598\n",
      "steps per second: 0.57465\n",
      "step: 38938\n",
      "loss: 12.913982391357422\n",
      "steps per second: 0.55650\n",
      "step: 38939\n",
      "loss: 12.805129051208496\n",
      "steps per second: 0.53958\n",
      "step: 38940\n",
      "loss: 12.406970977783203\n",
      "steps per second: 0.56296\n",
      "step: 38941\n",
      "loss: 12.504487037658691\n",
      "steps per second: 0.57999\n",
      "step: 38942\n",
      "loss: 12.506026268005371\n",
      "steps per second: 0.55889\n",
      "step: 38943\n",
      "loss: 12.608081817626953\n",
      "steps per second: 0.57065\n",
      "step: 38944\n",
      "loss: 13.17268180847168\n",
      "steps per second: 0.53024\n",
      "step: 38945\n",
      "loss: 13.43055534362793\n",
      "steps per second: 0.56661\n",
      "step: 38946\n",
      "loss: 13.04583740234375\n",
      "steps per second: 0.55032\n",
      "step: 38947\n",
      "loss: 12.421690940856934\n",
      "steps per second: 0.58752\n",
      "step: 38948\n",
      "loss: 13.001442909240723\n",
      "steps per second: 0.57085\n",
      "step: 38949\n",
      "loss: 12.948812484741211\n",
      "steps per second: 0.54155\n",
      "step: 38950\n",
      "loss: 12.84959602355957\n",
      "steps per second: 0.53202\n",
      "step: 38951\n",
      "loss: 12.532235145568848\n",
      "steps per second: 0.54987\n",
      "step: 38952\n",
      "loss: 12.906669616699219\n",
      "steps per second: 0.52430\n",
      "step: 38953\n",
      "loss: 12.529335975646973\n",
      "steps per second: 0.47008\n",
      "step: 38954\n",
      "loss: 12.500957489013672\n",
      "steps per second: 0.57002\n",
      "step: 38955\n",
      "loss: 13.417351722717285\n",
      "steps per second: 0.58743\n",
      "step: 38956\n",
      "loss: 12.842292785644531\n",
      "steps per second: 0.52254\n",
      "step: 38957\n",
      "loss: 12.892114639282227\n",
      "steps per second: 0.57393\n",
      "step: 38958\n",
      "loss: 13.207886695861816\n",
      "steps per second: 0.57695\n",
      "step: 38959\n",
      "loss: 12.70156192779541\n",
      "steps per second: 0.57209\n",
      "step: 38960\n",
      "loss: 13.160297393798828\n",
      "steps per second: 0.55091\n",
      "step: 38961\n",
      "loss: 13.30241584777832\n",
      "steps per second: 0.56460\n",
      "step: 38962\n",
      "loss: 12.693875312805176\n",
      "steps per second: 0.53400\n",
      "step: 38963\n",
      "loss: 12.810728073120117\n",
      "steps per second: 0.51603\n",
      "step: 38964\n",
      "loss: 12.491532325744629\n",
      "steps per second: 0.55109\n",
      "step: 38965\n",
      "loss: 12.981513977050781\n",
      "steps per second: 0.59719\n",
      "step: 38966\n",
      "loss: 12.548094749450684\n",
      "steps per second: 0.49553\n",
      "step: 38967\n",
      "loss: 12.96599006652832\n",
      "steps per second: 0.59252\n",
      "step: 38968\n",
      "loss: 12.70544147491455\n",
      "steps per second: 0.54376\n",
      "step: 38969\n",
      "loss: 12.920178413391113\n",
      "steps per second: 0.58390\n",
      "step: 38970\n",
      "loss: 12.939530372619629\n",
      "steps per second: 0.53900\n",
      "step: 38971\n",
      "loss: 12.854137420654297\n",
      "steps per second: 0.52618\n",
      "step: 38972\n",
      "loss: 13.015996932983398\n",
      "steps per second: 0.54904\n",
      "step: 38973\n",
      "loss: 12.790979385375977\n",
      "steps per second: 0.54632\n",
      "step: 38974\n",
      "loss: 12.538963317871094\n",
      "steps per second: 0.53665\n",
      "step: 38975\n",
      "loss: 12.701793670654297\n",
      "steps per second: 0.55148\n",
      "step: 38976\n",
      "loss: 12.882359504699707\n",
      "steps per second: 0.57559\n",
      "step: 38977\n",
      "loss: 12.44025707244873\n",
      "steps per second: 0.55572\n",
      "step: 38978\n",
      "loss: 12.622504234313965\n",
      "steps per second: 0.55584\n",
      "step: 38979\n",
      "loss: 12.90898609161377\n",
      "steps per second: 0.52218\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8545154929161072, layer: 11\n",
      "saving at step 38979\n",
      "----------\n",
      "\n",
      "\n",
      "step: 38980\n",
      "loss: 12.683292388916016\n",
      "steps per second: 0.27866\n",
      "step: 38981\n",
      "loss: 13.08344554901123\n",
      "steps per second: 0.61113\n",
      "step: 38982\n",
      "loss: 12.150187492370605\n",
      "steps per second: 0.55919\n",
      "step: 38983\n",
      "loss: 12.836772918701172\n",
      "steps per second: 0.55634\n",
      "step: 38984\n",
      "loss: 12.857367515563965\n",
      "steps per second: 0.57420\n",
      "step: 38985\n",
      "loss: 12.746308326721191\n",
      "steps per second: 0.57293\n",
      "step: 38986\n",
      "loss: 13.305026054382324\n",
      "steps per second: 0.51743\n",
      "step: 38987\n",
      "loss: 11.931353569030762\n",
      "steps per second: 0.56050\n",
      "step: 38988\n",
      "loss: 13.00379753112793\n",
      "steps per second: 0.53893\n",
      "step: 38989\n",
      "loss: 12.410323143005371\n",
      "steps per second: 0.55932\n",
      "step: 38990\n",
      "loss: 13.102243423461914\n",
      "steps per second: 0.62203\n",
      "step: 38991\n",
      "loss: 12.59597396850586\n",
      "steps per second: 0.49796\n",
      "step: 38992\n",
      "loss: 12.65942096710205\n",
      "steps per second: 0.55168\n",
      "step: 38993\n",
      "loss: 13.197443008422852\n",
      "steps per second: 0.57433\n",
      "step: 38994\n",
      "loss: 12.38514232635498\n",
      "steps per second: 0.55944\n",
      "step: 38995\n",
      "loss: 13.31318473815918\n",
      "steps per second: 0.54660\n",
      "step: 38996\n",
      "loss: 12.885737419128418\n",
      "steps per second: 0.54021\n",
      "step: 38997\n",
      "loss: 12.4564790725708\n",
      "steps per second: 0.55418\n",
      "step: 38998\n",
      "loss: 12.784893989562988\n",
      "steps per second: 0.55247\n",
      "step: 38999\n",
      "loss: 12.902527809143066\n",
      "steps per second: 0.57789\n",
      "step: 39000\n",
      "loss: 13.056787490844727\n",
      "steps per second: 0.56599\n",
      "step: 39001\n",
      "loss: 12.262056350708008\n",
      "steps per second: 0.55954\n",
      "step: 39002\n",
      "loss: 12.768779754638672\n",
      "steps per second: 0.57571\n",
      "step: 39003\n",
      "loss: 12.799676895141602\n",
      "steps per second: 0.55913\n",
      "step: 39004\n",
      "loss: 12.74296760559082\n",
      "steps per second: 0.58031\n",
      "step: 39005\n",
      "loss: 12.473368644714355\n",
      "steps per second: 0.55296\n",
      "step: 39006\n",
      "loss: 13.491854667663574\n",
      "steps per second: 0.53938\n",
      "step: 39007\n",
      "loss: 12.722508430480957\n",
      "steps per second: 0.52934\n",
      "step: 39008\n",
      "loss: 12.519472122192383\n",
      "steps per second: 0.54743\n",
      "step: 39009\n",
      "loss: 13.039236068725586\n",
      "steps per second: 0.54740\n",
      "step: 39010\n",
      "loss: 13.172417640686035\n",
      "steps per second: 0.56135\n",
      "step: 39011\n",
      "loss: 12.655948638916016\n",
      "steps per second: 0.54802\n",
      "step: 39012\n",
      "loss: 12.714673042297363\n",
      "steps per second: 0.54915\n",
      "step: 39013\n",
      "loss: 13.41461181640625\n",
      "steps per second: 0.54687\n",
      "step: 39014\n",
      "loss: 12.46938705444336\n",
      "steps per second: 0.57957\n",
      "step: 39015\n",
      "loss: 12.431601524353027\n",
      "steps per second: 0.56493\n",
      "step: 39016\n",
      "loss: 13.155248641967773\n",
      "steps per second: 0.61323\n",
      "step: 39017\n",
      "loss: 12.195792198181152\n",
      "steps per second: 0.50937\n",
      "step: 39018\n",
      "loss: 12.146687507629395\n",
      "steps per second: 0.49512\n",
      "step: 39019\n",
      "loss: 12.367287635803223\n",
      "steps per second: 0.55568\n",
      "step: 39020\n",
      "loss: 12.383194923400879\n",
      "steps per second: 0.55222\n",
      "step: 39021\n",
      "loss: 13.283452033996582\n",
      "steps per second: 0.53483\n",
      "step: 39022\n",
      "loss: 12.414795875549316\n",
      "steps per second: 0.58615\n",
      "step: 39023\n",
      "loss: 12.93908977508545\n",
      "steps per second: 0.56113\n",
      "step: 39024\n",
      "loss: 12.771590232849121\n",
      "steps per second: 0.56017\n",
      "step: 39025\n",
      "loss: 12.922832489013672\n",
      "steps per second: 0.55319\n",
      "step: 39026\n",
      "loss: 13.076062202453613\n",
      "steps per second: 0.50737\n",
      "step: 39027\n",
      "loss: 12.641966819763184\n",
      "steps per second: 0.55925\n",
      "step: 39028\n",
      "loss: 12.6091890335083\n",
      "steps per second: 0.54630\n",
      "step: 39029\n",
      "loss: 13.519431114196777\n",
      "steps per second: 0.57371\n",
      "step: 39030\n",
      "loss: 12.117372512817383\n",
      "steps per second: 0.56704\n",
      "step: 39031\n",
      "loss: 12.875712394714355\n",
      "steps per second: 0.57897\n",
      "step: 39032\n",
      "loss: 12.589653015136719\n",
      "steps per second: 0.55964\n",
      "step: 39033\n",
      "loss: 12.26448917388916\n",
      "steps per second: 0.54729\n",
      "step: 39034\n",
      "loss: 12.041169166564941\n",
      "steps per second: 0.57359\n",
      "step: 39035\n",
      "loss: 12.76248550415039\n",
      "steps per second: 0.56026\n",
      "step: 39036\n",
      "loss: 12.879862785339355\n",
      "steps per second: 0.51190\n",
      "step: 39037\n",
      "loss: 13.209977149963379\n",
      "steps per second: 0.60521\n",
      "step: 39038\n",
      "loss: 12.741928100585938\n",
      "steps per second: 0.51261\n",
      "step: 39039\n",
      "loss: 13.233743667602539\n",
      "steps per second: 0.57889\n",
      "step: 39040\n",
      "loss: 13.05342960357666\n",
      "steps per second: 0.54719\n",
      "step: 39041\n",
      "loss: 12.693215370178223\n",
      "steps per second: 0.56115\n",
      "step: 39042\n",
      "loss: 12.465597152709961\n",
      "steps per second: 0.53320\n",
      "step: 39043\n",
      "loss: 13.53979206085205\n",
      "steps per second: 0.57930\n",
      "step: 39044\n",
      "loss: 12.555538177490234\n",
      "steps per second: 0.56295\n",
      "step: 39045\n",
      "loss: 13.105942726135254\n",
      "steps per second: 0.55800\n",
      "step: 39046\n",
      "loss: 13.181370735168457\n",
      "steps per second: 0.56789\n",
      "step: 39047\n",
      "loss: 12.823415756225586\n",
      "steps per second: 0.55348\n",
      "step: 39048\n",
      "loss: 12.831750869750977\n",
      "steps per second: 0.58039\n",
      "step: 39049\n",
      "loss: 12.661714553833008\n",
      "steps per second: 0.57879\n",
      "step: 39050\n",
      "loss: 12.935830116271973\n",
      "steps per second: 0.53282\n",
      "step: 39051\n",
      "loss: 12.225546836853027\n",
      "steps per second: 0.52194\n",
      "step: 39052\n",
      "loss: 12.51710319519043\n",
      "steps per second: 0.55588\n",
      "step: 39053\n",
      "loss: 12.527036666870117\n",
      "steps per second: 0.53101\n",
      "step: 39054\n",
      "loss: 12.646040916442871\n",
      "steps per second: 0.55843\n",
      "step: 39055\n",
      "loss: 12.784438133239746\n",
      "steps per second: 0.52033\n",
      "step: 39056\n",
      "loss: 12.75188159942627\n",
      "steps per second: 0.55140\n",
      "step: 39057\n",
      "loss: 12.24004077911377\n",
      "steps per second: 0.54184\n",
      "step: 39058\n",
      "loss: 13.402802467346191\n",
      "steps per second: 0.55227\n",
      "step: 39059\n",
      "loss: 12.205419540405273\n",
      "steps per second: 0.53983\n",
      "step: 39060\n",
      "loss: 12.939935684204102\n",
      "steps per second: 0.53930\n",
      "step: 39061\n",
      "loss: 12.363412857055664\n",
      "steps per second: 0.61359\n",
      "step: 39062\n",
      "loss: 13.277389526367188\n",
      "steps per second: 0.55379\n",
      "step: 39063\n",
      "loss: 12.546003341674805\n",
      "steps per second: 0.52932\n",
      "step: 39064\n",
      "loss: 12.845283508300781\n",
      "steps per second: 0.57963\n",
      "step: 39065\n",
      "loss: 12.820016860961914\n",
      "steps per second: 0.56089\n",
      "step: 39066\n",
      "loss: 12.31915283203125\n",
      "steps per second: 0.52906\n",
      "step: 39067\n",
      "loss: 12.826834678649902\n",
      "steps per second: 0.53231\n",
      "step: 39068\n",
      "loss: 13.132540702819824\n",
      "steps per second: 0.58084\n",
      "step: 39069\n",
      "loss: 13.320456504821777\n",
      "steps per second: 0.56074\n",
      "step: 39070\n",
      "loss: 13.09119701385498\n",
      "steps per second: 0.56205\n",
      "step: 39071\n",
      "loss: 12.92852783203125\n",
      "steps per second: 0.55403\n",
      "step: 39072\n",
      "loss: 12.693404197692871\n",
      "steps per second: 0.55146\n",
      "step: 39073\n",
      "loss: 13.16268253326416\n",
      "steps per second: 0.53233\n",
      "step: 39074\n",
      "loss: 12.45882797241211\n",
      "steps per second: 0.56001\n",
      "step: 39075\n",
      "loss: 12.641100883483887\n",
      "steps per second: 0.59027\n",
      "step: 39076\n",
      "loss: 12.876130104064941\n",
      "steps per second: 0.54148\n",
      "step: 39077\n",
      "loss: 12.805133819580078\n",
      "steps per second: 0.56588\n",
      "step: 39078\n",
      "loss: 13.231216430664062\n",
      "steps per second: 0.54008\n",
      "step: 39079\n",
      "loss: 12.350396156311035\n",
      "steps per second: 0.56130\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.753406822681427, layer: 11\n",
      "saving at step 39079\n",
      "----------\n",
      "\n",
      "\n",
      "step: 39080\n",
      "loss: 13.083738327026367\n",
      "steps per second: 0.28308\n",
      "step: 39081\n",
      "loss: 12.462148666381836\n",
      "steps per second: 0.58082\n",
      "step: 39082\n",
      "loss: 12.963278770446777\n",
      "steps per second: 0.56560\n",
      "step: 39083\n",
      "loss: 12.788384437561035\n",
      "steps per second: 0.57524\n",
      "step: 39084\n",
      "loss: 13.155630111694336\n",
      "steps per second: 0.52124\n",
      "step: 39085\n",
      "loss: 13.066903114318848\n",
      "steps per second: 0.57587\n",
      "step: 39086\n",
      "loss: 12.581649780273438\n",
      "steps per second: 0.55870\n",
      "step: 39087\n",
      "loss: 12.615301132202148\n",
      "steps per second: 0.56586\n",
      "step: 39088\n",
      "loss: 12.754291534423828\n",
      "steps per second: 0.56692\n",
      "step: 39089\n",
      "loss: 12.693835258483887\n",
      "steps per second: 0.58865\n",
      "step: 39090\n",
      "loss: 12.66536808013916\n",
      "steps per second: 0.56509\n",
      "step: 39091\n",
      "loss: 13.148755073547363\n",
      "steps per second: 0.57801\n",
      "step: 39092\n",
      "loss: 12.552018165588379\n",
      "steps per second: 0.55273\n",
      "step: 39093\n",
      "loss: 12.968862533569336\n",
      "steps per second: 0.57680\n",
      "step: 39094\n",
      "loss: 12.347249984741211\n",
      "steps per second: 0.61795\n",
      "step: 39095\n",
      "loss: 13.016451835632324\n",
      "steps per second: 0.56139\n",
      "step: 39096\n",
      "loss: 12.612122535705566\n",
      "steps per second: 0.57685\n",
      "step: 39097\n",
      "loss: 12.672988891601562\n",
      "steps per second: 0.56010\n",
      "step: 39098\n",
      "loss: 12.34070110321045\n",
      "steps per second: 0.53132\n",
      "step: 39099\n",
      "loss: 12.907039642333984\n",
      "steps per second: 0.52038\n",
      "step: 39100\n",
      "loss: 13.313399314880371\n",
      "steps per second: 0.58256\n",
      "step: 39101\n",
      "loss: 13.259559631347656\n",
      "steps per second: 0.51778\n",
      "step: 39102\n",
      "loss: 12.696298599243164\n",
      "steps per second: 0.52833\n",
      "step: 39103\n",
      "loss: 13.02639102935791\n",
      "steps per second: 0.54601\n",
      "step: 39104\n",
      "loss: 13.145994186401367\n",
      "steps per second: 0.55223\n",
      "step: 39105\n",
      "loss: 12.896421432495117\n",
      "steps per second: 0.56848\n",
      "step: 39106\n",
      "loss: 12.771393775939941\n",
      "steps per second: 0.62230\n",
      "step: 39107\n",
      "loss: 12.440011978149414\n",
      "steps per second: 0.49992\n",
      "step: 39108\n",
      "loss: 12.552486419677734\n",
      "steps per second: 0.54770\n",
      "step: 39109\n",
      "loss: 12.958547592163086\n",
      "steps per second: 0.54575\n",
      "step: 39110\n",
      "loss: 12.941954612731934\n",
      "steps per second: 0.56416\n",
      "step: 39111\n",
      "loss: 12.898826599121094\n",
      "steps per second: 0.54472\n",
      "step: 39112\n",
      "loss: 12.836058616638184\n",
      "steps per second: 0.55121\n",
      "step: 39113\n",
      "loss: 13.263443946838379\n",
      "steps per second: 0.53764\n",
      "step: 39114\n",
      "loss: 12.532569885253906\n",
      "steps per second: 0.54336\n",
      "step: 39115\n",
      "loss: 12.405250549316406\n",
      "steps per second: 0.55347\n",
      "step: 39116\n",
      "loss: 13.136902809143066\n",
      "steps per second: 0.55017\n",
      "step: 39117\n",
      "loss: 12.758313179016113\n",
      "steps per second: 0.53715\n",
      "step: 39118\n",
      "loss: 12.932528495788574\n",
      "steps per second: 0.56935\n",
      "step: 39119\n",
      "loss: 12.785540580749512\n",
      "steps per second: 0.57027\n",
      "step: 39120\n",
      "loss: 12.556719779968262\n",
      "steps per second: 0.55086\n",
      "step: 39121\n",
      "loss: 13.281648635864258\n",
      "steps per second: 0.56471\n",
      "step: 39122\n",
      "loss: 12.926568984985352\n",
      "steps per second: 0.50692\n",
      "step: 39123\n",
      "loss: 12.668533325195312\n",
      "steps per second: 0.58358\n",
      "step: 39124\n",
      "loss: 12.866510391235352\n",
      "steps per second: 0.50466\n",
      "step: 39125\n",
      "loss: 12.952766418457031\n",
      "steps per second: 0.55817\n",
      "step: 39126\n",
      "loss: 12.819964408874512\n",
      "steps per second: 0.55480\n",
      "step: 39127\n",
      "loss: 12.55310344696045\n",
      "steps per second: 0.54816\n",
      "step: 39128\n",
      "loss: 12.604679107666016\n",
      "steps per second: 0.55400\n",
      "step: 39129\n",
      "loss: 12.797591209411621\n",
      "steps per second: 0.62183\n",
      "step: 39130\n",
      "loss: 12.725147247314453\n",
      "steps per second: 0.56627\n",
      "step: 39131\n",
      "loss: 12.350693702697754\n",
      "steps per second: 0.57649\n",
      "step: 39132\n",
      "loss: 13.323737144470215\n",
      "steps per second: 0.56653\n",
      "step: 39133\n",
      "loss: 13.03134822845459\n",
      "steps per second: 0.57776\n",
      "step: 39134\n",
      "loss: 13.350264549255371\n",
      "steps per second: 0.57516\n",
      "step: 39135\n",
      "loss: 12.751708984375\n",
      "steps per second: 0.55928\n",
      "step: 39136\n",
      "loss: 12.548365592956543\n",
      "steps per second: 0.47335\n",
      "step: 39137\n",
      "loss: 13.100373268127441\n",
      "steps per second: 0.54887\n",
      "step: 39138\n",
      "loss: 12.808135032653809\n",
      "steps per second: 0.54937\n",
      "step: 39139\n",
      "loss: 13.198708534240723\n",
      "steps per second: 0.53923\n",
      "step: 39140\n",
      "loss: 12.72146987915039\n",
      "steps per second: 0.53579\n",
      "step: 39141\n",
      "loss: 12.742278099060059\n",
      "steps per second: 0.50478\n",
      "step: 39142\n",
      "loss: 12.999921798706055\n",
      "steps per second: 0.57522\n",
      "step: 39143\n",
      "loss: 13.16458797454834\n",
      "steps per second: 0.54394\n",
      "step: 39144\n",
      "loss: 12.238303184509277\n",
      "steps per second: 0.57294\n",
      "step: 39145\n",
      "loss: 13.020874977111816\n",
      "steps per second: 0.52944\n",
      "step: 39146\n",
      "loss: 12.734983444213867\n",
      "steps per second: 0.49831\n",
      "step: 39147\n",
      "loss: 12.995231628417969\n",
      "steps per second: 0.53727\n",
      "step: 39148\n",
      "loss: 12.31329345703125\n",
      "steps per second: 0.53760\n",
      "step: 39149\n",
      "loss: 13.040139198303223\n",
      "steps per second: 0.56623\n",
      "step: 39150\n",
      "loss: 12.908127784729004\n",
      "steps per second: 0.52659\n",
      "step: 39151\n",
      "loss: 13.012800216674805\n",
      "steps per second: 0.52459\n",
      "step: 39152\n",
      "loss: 12.915225982666016\n",
      "steps per second: 0.50050\n",
      "step: 39153\n",
      "loss: 12.54101848602295\n",
      "steps per second: 0.54236\n",
      "step: 39154\n",
      "loss: 12.802457809448242\n",
      "steps per second: 0.54166\n",
      "step: 39155\n",
      "loss: 12.792186737060547\n",
      "steps per second: 0.57425\n",
      "step: 39156\n",
      "loss: 13.088072776794434\n",
      "steps per second: 0.56412\n",
      "step: 39157\n",
      "loss: 12.468809127807617\n",
      "steps per second: 0.52696\n",
      "step: 39158\n",
      "loss: 12.46811580657959\n",
      "steps per second: 0.51179\n",
      "step: 39159\n",
      "loss: 13.031118392944336\n",
      "steps per second: 0.59133\n",
      "step: 39160\n",
      "loss: 13.308773040771484\n",
      "steps per second: 0.55518\n",
      "step: 39161\n",
      "loss: 12.904767990112305\n",
      "steps per second: 0.54584\n",
      "step: 39162\n",
      "loss: 12.236955642700195\n",
      "steps per second: 0.58124\n",
      "step: 39163\n",
      "loss: 12.413748741149902\n",
      "steps per second: 0.53558\n",
      "step: 39164\n",
      "loss: 12.685274124145508\n",
      "steps per second: 0.57591\n",
      "step: 39165\n",
      "loss: 13.249698638916016\n",
      "steps per second: 0.56107\n",
      "step: 39166\n",
      "loss: 12.924245834350586\n",
      "steps per second: 0.53870\n",
      "step: 39167\n",
      "loss: 12.807357788085938\n",
      "steps per second: 0.55583\n",
      "step: 39168\n",
      "loss: 12.853775024414062\n",
      "steps per second: 0.55592\n",
      "step: 39169\n",
      "loss: 12.68278694152832\n",
      "steps per second: 0.57782\n",
      "step: 39170\n",
      "loss: 13.142529487609863\n",
      "steps per second: 0.55511\n",
      "step: 39171\n",
      "loss: 12.490782737731934\n",
      "steps per second: 0.60931\n",
      "step: 39172\n",
      "loss: 12.645755767822266\n",
      "steps per second: 0.56356\n",
      "step: 39173\n",
      "loss: 13.087849617004395\n",
      "steps per second: 0.54111\n",
      "step: 39174\n",
      "loss: 12.881211280822754\n",
      "steps per second: 0.58576\n",
      "step: 39175\n",
      "loss: 12.583389282226562\n",
      "steps per second: 0.58859\n",
      "step: 39176\n",
      "loss: 12.86406135559082\n",
      "steps per second: 0.56376\n",
      "step: 39177\n",
      "loss: 13.293550491333008\n",
      "steps per second: 0.56952\n",
      "step: 39178\n",
      "loss: 13.030900955200195\n",
      "steps per second: 0.52689\n",
      "step: 39179\n",
      "loss: 12.774752616882324\n",
      "steps per second: 0.54651\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8106702566146851, layer: 10\n",
      "saving at step 39179\n",
      "----------\n",
      "\n",
      "\n",
      "step: 39180\n",
      "loss: 12.629634857177734\n",
      "steps per second: 0.27192\n",
      "step: 39181\n",
      "loss: 12.781404495239258\n",
      "steps per second: 0.57045\n",
      "step: 39182\n",
      "loss: 12.51495361328125\n",
      "steps per second: 0.52339\n",
      "step: 39183\n",
      "loss: 12.607884407043457\n",
      "steps per second: 0.53791\n",
      "step: 39184\n",
      "loss: 13.136883735656738\n",
      "steps per second: 0.49570\n",
      "step: 39185\n",
      "loss: 12.627776145935059\n",
      "steps per second: 0.56085\n",
      "step: 39186\n",
      "loss: 12.721592903137207\n",
      "steps per second: 0.50291\n",
      "step: 39187\n",
      "loss: 13.492057800292969\n",
      "steps per second: 0.54819\n",
      "step: 39188\n",
      "loss: 12.83267879486084\n",
      "steps per second: 0.52016\n",
      "step: 39189\n",
      "loss: 12.68876838684082\n",
      "steps per second: 0.61665\n",
      "step: 39190\n",
      "loss: 12.54304027557373\n",
      "steps per second: 0.58939\n",
      "step: 39191\n",
      "loss: 12.562830924987793\n",
      "steps per second: 0.54773\n",
      "step: 39192\n",
      "loss: 12.489578247070312\n",
      "steps per second: 0.52913\n",
      "step: 39193\n",
      "loss: 13.026220321655273\n",
      "steps per second: 0.55313\n",
      "step: 39194\n",
      "loss: 12.409035682678223\n",
      "steps per second: 0.50605\n",
      "step: 39195\n",
      "loss: 13.288897514343262\n",
      "steps per second: 0.53469\n",
      "step: 39196\n",
      "loss: 12.975798606872559\n",
      "steps per second: 0.52812\n",
      "step: 39197\n",
      "loss: 13.111103057861328\n",
      "steps per second: 0.62201\n",
      "step: 39198\n",
      "loss: 13.061532974243164\n",
      "steps per second: 0.54692\n",
      "step: 39199\n",
      "loss: 12.934694290161133\n",
      "steps per second: 0.54290\n",
      "step: 39200\n",
      "loss: 12.499560356140137\n",
      "steps per second: 0.51620\n",
      "step: 39201\n",
      "loss: 13.073747634887695\n",
      "steps per second: 0.56998\n",
      "step: 39202\n",
      "loss: 12.338020324707031\n",
      "steps per second: 0.55565\n",
      "step: 39203\n",
      "loss: 12.541620254516602\n",
      "steps per second: 0.56995\n",
      "step: 39204\n",
      "loss: 13.378093719482422\n",
      "steps per second: 0.53475\n",
      "step: 39205\n",
      "loss: 12.6345796585083\n",
      "steps per second: 0.52189\n",
      "step: 39206\n",
      "loss: 12.851097106933594\n",
      "steps per second: 0.50888\n",
      "step: 39207\n",
      "loss: 13.058362007141113\n",
      "steps per second: 0.54053\n",
      "step: 39208\n",
      "loss: 12.546806335449219\n",
      "steps per second: 0.54683\n",
      "step: 39209\n",
      "loss: 12.40890884399414\n",
      "steps per second: 0.51285\n",
      "step: 39210\n",
      "loss: 12.948972702026367\n",
      "steps per second: 0.57942\n",
      "step: 39211\n",
      "loss: 12.118915557861328\n",
      "steps per second: 0.57855\n",
      "step: 39212\n",
      "loss: 12.901643753051758\n",
      "steps per second: 0.51802\n",
      "step: 39213\n",
      "loss: 13.355565071105957\n",
      "steps per second: 0.53261\n",
      "step: 39214\n",
      "loss: 12.797717094421387\n",
      "steps per second: 0.55379\n",
      "step: 39215\n",
      "loss: 13.30423355102539\n",
      "steps per second: 0.54489\n",
      "step: 39216\n",
      "loss: 12.90159797668457\n",
      "steps per second: 0.55095\n",
      "step: 39217\n",
      "loss: 12.731337547302246\n",
      "steps per second: 0.56651\n",
      "step: 39218\n",
      "loss: 13.243583679199219\n",
      "steps per second: 0.54154\n",
      "step: 39219\n",
      "loss: 12.901816368103027\n",
      "steps per second: 0.55585\n",
      "step: 39220\n",
      "loss: 12.724039077758789\n",
      "steps per second: 0.59245\n",
      "step: 39221\n",
      "loss: 12.73747444152832\n",
      "steps per second: 0.56739\n",
      "step: 39222\n",
      "loss: 13.350359916687012\n",
      "steps per second: 0.56666\n",
      "step: 39223\n",
      "loss: 12.669370651245117\n",
      "steps per second: 0.61692\n",
      "step: 39224\n",
      "loss: 12.706472396850586\n",
      "steps per second: 0.56776\n",
      "step: 39225\n",
      "loss: 13.039223670959473\n",
      "steps per second: 0.55545\n",
      "step: 39226\n",
      "loss: 12.946694374084473\n",
      "steps per second: 0.58320\n",
      "step: 39227\n",
      "loss: 12.647324562072754\n",
      "steps per second: 0.52212\n",
      "step: 39228\n",
      "loss: 12.77968978881836\n",
      "steps per second: 0.57077\n",
      "step: 39229\n",
      "loss: 13.354776382446289\n",
      "steps per second: 0.55927\n",
      "step: 39230\n",
      "loss: 12.853163719177246\n",
      "steps per second: 0.57826\n",
      "step: 39231\n",
      "loss: 13.150516510009766\n",
      "steps per second: 0.58795\n",
      "step: 39232\n",
      "loss: 12.79661750793457\n",
      "steps per second: 0.52942\n",
      "step: 39233\n",
      "loss: 12.460453987121582\n",
      "steps per second: 0.51070\n",
      "step: 39234\n",
      "loss: 12.3740816116333\n",
      "steps per second: 0.58713\n",
      "step: 39235\n",
      "loss: 13.616260528564453\n",
      "steps per second: 0.52876\n",
      "step: 39236\n",
      "loss: 12.933053016662598\n",
      "steps per second: 0.57875\n",
      "step: 39237\n",
      "loss: 13.063053131103516\n",
      "steps per second: 0.57834\n",
      "step: 39238\n",
      "loss: 13.536130905151367\n",
      "steps per second: 0.54035\n",
      "step: 39239\n",
      "loss: 13.081974983215332\n",
      "steps per second: 0.55790\n",
      "step: 39240\n",
      "loss: 13.246528625488281\n",
      "steps per second: 0.55564\n",
      "step: 39241\n",
      "loss: 12.949065208435059\n",
      "steps per second: 0.57819\n",
      "step: 39242\n",
      "loss: 12.788768768310547\n",
      "steps per second: 0.55414\n",
      "step: 39243\n",
      "loss: 12.591621398925781\n",
      "steps per second: 0.52110\n",
      "step: 39244\n",
      "loss: 13.170428276062012\n",
      "steps per second: 0.52620\n",
      "step: 39245\n",
      "loss: 12.75130558013916\n",
      "steps per second: 0.55566\n",
      "step: 39246\n",
      "loss: 12.499114036560059\n",
      "steps per second: 0.57765\n",
      "step: 39247\n",
      "loss: 12.786498069763184\n",
      "steps per second: 0.56790\n",
      "step: 39248\n",
      "loss: 12.44028377532959\n",
      "steps per second: 0.57261\n",
      "step: 39249\n",
      "loss: 12.80611801147461\n",
      "steps per second: 0.53244\n",
      "step: 39250\n",
      "loss: 13.2298002243042\n",
      "steps per second: 0.55728\n",
      "step: 39251\n",
      "loss: 12.907672882080078\n",
      "steps per second: 0.59463\n",
      "step: 39252\n",
      "loss: 12.751623153686523\n",
      "steps per second: 0.58822\n",
      "step: 39253\n",
      "loss: 12.158760070800781\n",
      "steps per second: 0.54745\n",
      "step: 39254\n",
      "loss: 12.383238792419434\n",
      "steps per second: 0.54352\n",
      "step: 39255\n",
      "loss: 12.61711597442627\n",
      "steps per second: 0.54270\n",
      "step: 39256\n",
      "loss: 12.959299087524414\n",
      "steps per second: 0.56898\n",
      "step: 39257\n",
      "loss: 13.167562484741211\n",
      "steps per second: 0.51825\n",
      "step: 39258\n",
      "loss: 12.948967933654785\n",
      "steps per second: 0.55859\n",
      "step: 39259\n",
      "loss: 12.629733085632324\n",
      "steps per second: 0.52816\n",
      "step: 39260\n",
      "loss: 12.498859405517578\n",
      "steps per second: 0.55359\n",
      "step: 39261\n",
      "loss: 12.98559856414795\n",
      "steps per second: 0.55493\n",
      "step: 39262\n",
      "loss: 12.187933921813965\n",
      "steps per second: 0.57736\n",
      "step: 39263\n",
      "loss: 12.187426567077637\n",
      "steps per second: 0.52874\n",
      "step: 39264\n",
      "loss: 12.860803604125977\n",
      "steps per second: 0.55563\n",
      "step: 39265\n",
      "loss: 12.457475662231445\n",
      "steps per second: 0.53862\n",
      "step: 39266\n",
      "loss: 12.517592430114746\n",
      "steps per second: 0.55647\n",
      "step: 39267\n",
      "loss: 13.34604263305664\n",
      "steps per second: 0.56781\n",
      "step: 39268\n",
      "loss: 12.692739486694336\n",
      "steps per second: 0.52840\n",
      "step: 39269\n",
      "loss: 12.904236793518066\n",
      "steps per second: 0.58456\n",
      "step: 39270\n",
      "loss: 12.682841300964355\n",
      "steps per second: 0.54216\n",
      "step: 39271\n",
      "loss: 12.370490074157715\n",
      "steps per second: 0.55083\n",
      "step: 39272\n",
      "loss: 12.746350288391113\n",
      "steps per second: 0.54413\n",
      "step: 39273\n",
      "loss: 12.9456205368042\n",
      "steps per second: 0.62088\n",
      "step: 39274\n",
      "loss: 13.148293495178223\n",
      "steps per second: 0.52772\n",
      "step: 39275\n",
      "loss: 12.682281494140625\n",
      "steps per second: 0.56823\n",
      "step: 39276\n",
      "loss: 12.441003799438477\n",
      "steps per second: 0.52656\n",
      "step: 39277\n",
      "loss: 12.680949211120605\n",
      "steps per second: 0.52875\n",
      "step: 39278\n",
      "loss: 13.092742919921875\n",
      "steps per second: 0.58932\n",
      "step: 39279\n",
      "loss: 12.489832878112793\n",
      "steps per second: 0.58551\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8308655023574829, layer: 10\n",
      "saving at step 39279\n",
      "----------\n",
      "\n",
      "\n",
      "step: 39280\n",
      "loss: 12.365301132202148\n",
      "steps per second: 0.27562\n",
      "step: 39281\n",
      "loss: 13.171319961547852\n",
      "steps per second: 0.54933\n",
      "step: 39282\n",
      "loss: 12.252959251403809\n",
      "steps per second: 0.52759\n",
      "step: 39283\n",
      "loss: 12.509393692016602\n",
      "steps per second: 0.54462\n",
      "step: 39284\n",
      "loss: 12.516914367675781\n",
      "steps per second: 0.51481\n",
      "step: 39285\n",
      "loss: 12.853474617004395\n",
      "steps per second: 0.62145\n",
      "step: 39286\n",
      "loss: 12.261390686035156\n",
      "steps per second: 0.52695\n",
      "step: 39287\n",
      "loss: 12.677190780639648\n",
      "steps per second: 0.53787\n",
      "step: 39288\n",
      "loss: 13.458359718322754\n",
      "steps per second: 0.55160\n",
      "step: 39289\n",
      "loss: 12.802278518676758\n",
      "steps per second: 0.55432\n",
      "step: 39290\n",
      "loss: 13.00632381439209\n",
      "steps per second: 0.50545\n",
      "step: 39291\n",
      "loss: 12.62946605682373\n",
      "steps per second: 0.56073\n",
      "step: 39292\n",
      "loss: 13.038837432861328\n",
      "steps per second: 0.54687\n",
      "step: 39293\n",
      "loss: 12.868188858032227\n",
      "steps per second: 0.54878\n",
      "step: 39294\n",
      "loss: 12.600912094116211\n",
      "steps per second: 0.54555\n",
      "step: 39295\n",
      "loss: 12.600972175598145\n",
      "steps per second: 0.50970\n",
      "step: 39296\n",
      "loss: 12.735320091247559\n",
      "steps per second: 0.59554\n",
      "step: 39297\n",
      "loss: 12.948898315429688\n",
      "steps per second: 0.57931\n",
      "step: 39298\n",
      "loss: 12.996953964233398\n",
      "steps per second: 0.55057\n",
      "step: 39299\n",
      "loss: 13.163819313049316\n",
      "steps per second: 0.60934\n",
      "step: 39300\n",
      "loss: 12.469093322753906\n",
      "steps per second: 0.53186\n",
      "step: 39301\n",
      "loss: 12.956963539123535\n",
      "steps per second: 0.53075\n",
      "step: 39302\n",
      "loss: 12.631357192993164\n",
      "steps per second: 0.58825\n",
      "step: 39303\n",
      "loss: 12.790048599243164\n",
      "steps per second: 0.49379\n",
      "step: 39304\n",
      "loss: 12.638627052307129\n",
      "steps per second: 0.57921\n",
      "step: 39305\n",
      "loss: 12.377924919128418\n",
      "steps per second: 0.58000\n",
      "step: 39306\n",
      "loss: 12.683560371398926\n",
      "steps per second: 0.56725\n",
      "step: 39307\n",
      "loss: 12.70136833190918\n",
      "steps per second: 0.52002\n",
      "step: 39308\n",
      "loss: 12.688497543334961\n",
      "steps per second: 0.56709\n",
      "step: 39309\n",
      "loss: 12.695151329040527\n",
      "steps per second: 0.58439\n",
      "step: 39310\n",
      "loss: 13.6508207321167\n",
      "steps per second: 0.54750\n",
      "step: 39311\n",
      "loss: 12.820380210876465\n",
      "steps per second: 0.54860\n",
      "step: 39312\n",
      "loss: 13.338274002075195\n",
      "steps per second: 0.55347\n",
      "step: 39313\n",
      "loss: 13.103555679321289\n",
      "steps per second: 0.59745\n",
      "step: 39314\n",
      "loss: 12.754799842834473\n",
      "steps per second: 0.58472\n",
      "step: 39315\n",
      "loss: 13.041462898254395\n",
      "steps per second: 0.57577\n",
      "step: 39316\n",
      "loss: 12.926156044006348\n",
      "steps per second: 0.53972\n",
      "step: 39317\n",
      "loss: 13.04773998260498\n",
      "steps per second: 0.58920\n",
      "step: 39318\n",
      "loss: 13.312873840332031\n",
      "steps per second: 0.62078\n",
      "step: 39319\n",
      "loss: 11.891526222229004\n",
      "steps per second: 0.57909\n",
      "step: 39320\n",
      "loss: 12.447020530700684\n",
      "steps per second: 0.52832\n",
      "step: 39321\n",
      "loss: 13.37408447265625\n",
      "steps per second: 0.58059\n",
      "step: 39322\n",
      "loss: 12.89432144165039\n",
      "steps per second: 0.58058\n",
      "step: 39323\n",
      "loss: 12.748909950256348\n",
      "steps per second: 0.53590\n",
      "step: 39324\n",
      "loss: 12.755851745605469\n",
      "steps per second: 0.53816\n",
      "step: 39325\n",
      "loss: 12.821381568908691\n",
      "steps per second: 0.55487\n",
      "step: 39326\n",
      "loss: 12.567575454711914\n",
      "steps per second: 0.54502\n",
      "step: 39327\n",
      "loss: 12.846687316894531\n",
      "steps per second: 0.56533\n",
      "step: 39328\n",
      "loss: 12.652198791503906\n",
      "steps per second: 0.59066\n",
      "step: 39329\n",
      "loss: 12.61147403717041\n",
      "steps per second: 0.58149\n",
      "step: 39330\n",
      "loss: 12.80784797668457\n",
      "steps per second: 0.57263\n",
      "step: 39331\n",
      "loss: 12.245780944824219\n",
      "steps per second: 0.52630\n",
      "step: 39332\n",
      "loss: 13.034706115722656\n",
      "steps per second: 0.56475\n",
      "step: 39333\n",
      "loss: 12.968274116516113\n",
      "steps per second: 0.53000\n",
      "step: 39334\n",
      "loss: 13.288991928100586\n",
      "steps per second: 0.58943\n",
      "step: 39335\n",
      "loss: 12.887425422668457\n",
      "steps per second: 0.61828\n",
      "step: 39336\n",
      "loss: 12.549620628356934\n",
      "steps per second: 0.57154\n",
      "step: 39337\n",
      "loss: 12.816184997558594\n",
      "steps per second: 0.52353\n",
      "step: 39338\n",
      "loss: 12.695294380187988\n",
      "steps per second: 0.57919\n",
      "step: 39339\n",
      "loss: 12.582563400268555\n",
      "steps per second: 0.54776\n",
      "step: 39340\n",
      "loss: 12.309996604919434\n",
      "steps per second: 0.56078\n",
      "step: 39341\n",
      "loss: 12.664708137512207\n",
      "steps per second: 0.54336\n",
      "step: 39342\n",
      "loss: 12.444745063781738\n",
      "steps per second: 0.55307\n",
      "step: 39343\n",
      "loss: 12.6070556640625\n",
      "steps per second: 0.51124\n",
      "step: 39344\n",
      "loss: 13.058466911315918\n",
      "steps per second: 0.52920\n",
      "step: 39345\n",
      "loss: 12.704931259155273\n",
      "steps per second: 0.52192\n",
      "step: 39346\n",
      "loss: 13.176056861877441\n",
      "steps per second: 0.58592\n",
      "step: 39347\n",
      "loss: 12.4419584274292\n",
      "steps per second: 0.50705\n",
      "step: 39348\n",
      "loss: 12.729182243347168\n",
      "steps per second: 0.54676\n",
      "step: 39349\n",
      "loss: 12.8178129196167\n",
      "steps per second: 0.58042\n",
      "step: 39350\n",
      "loss: 12.443791389465332\n",
      "steps per second: 0.57534\n",
      "step: 39351\n",
      "loss: 12.894128799438477\n",
      "steps per second: 0.54947\n",
      "step: 39352\n",
      "loss: 12.591424942016602\n",
      "steps per second: 0.53749\n",
      "step: 39353\n",
      "loss: 12.30465030670166\n",
      "steps per second: 0.57696\n",
      "step: 39354\n",
      "loss: 13.139226913452148\n",
      "steps per second: 0.58369\n",
      "step: 39355\n",
      "loss: 12.52067756652832\n",
      "steps per second: 0.50267\n",
      "step: 39356\n",
      "loss: 12.786822319030762\n",
      "steps per second: 0.52544\n",
      "step: 39357\n",
      "loss: 13.140115737915039\n",
      "steps per second: 0.52658\n",
      "step: 39358\n",
      "loss: 12.733888626098633\n",
      "steps per second: 0.52318\n",
      "step: 39359\n",
      "loss: 13.249565124511719\n",
      "steps per second: 0.56817\n",
      "step: 39360\n",
      "loss: 12.645180702209473\n",
      "steps per second: 0.53955\n",
      "step: 39361\n",
      "loss: 12.298015594482422\n",
      "steps per second: 0.57860\n",
      "step: 39362\n",
      "loss: 12.610346794128418\n",
      "steps per second: 0.55592\n",
      "step: 39363\n",
      "loss: 12.407635688781738\n",
      "steps per second: 0.49601\n",
      "step: 39364\n",
      "loss: 12.78287124633789\n",
      "steps per second: 0.53546\n",
      "step: 39365\n",
      "loss: 12.710664749145508\n",
      "steps per second: 0.55696\n",
      "step: 39366\n",
      "loss: 12.694161415100098\n",
      "steps per second: 0.54967\n",
      "step: 39367\n",
      "loss: 13.264443397521973\n",
      "steps per second: 0.57897\n",
      "step: 39368\n",
      "loss: 12.862643241882324\n",
      "steps per second: 0.57794\n",
      "step: 39369\n",
      "loss: 12.814079284667969\n",
      "steps per second: 0.55639\n",
      "step: 39370\n",
      "loss: 12.390035629272461\n",
      "steps per second: 0.53897\n",
      "step: 39371\n",
      "loss: 12.802728652954102\n",
      "steps per second: 0.54589\n",
      "step: 39372\n",
      "loss: 12.535767555236816\n",
      "steps per second: 0.49633\n",
      "step: 39373\n",
      "loss: 12.560464859008789\n",
      "steps per second: 0.58990\n",
      "step: 39374\n",
      "loss: 13.033968925476074\n",
      "steps per second: 0.52248\n",
      "step: 39375\n",
      "loss: 13.089071273803711\n",
      "steps per second: 0.56242\n",
      "step: 39376\n",
      "loss: 13.143549919128418\n",
      "steps per second: 0.57688\n",
      "step: 39377\n",
      "loss: 12.135934829711914\n",
      "steps per second: 0.54183\n",
      "step: 39378\n",
      "loss: 12.614002227783203\n",
      "steps per second: 0.52662\n",
      "step: 39379\n",
      "loss: 13.188446998596191\n",
      "steps per second: 0.52906\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.84713214635849, layer: 11\n",
      "saving at step 39379\n",
      "----------\n",
      "\n",
      "\n",
      "step: 39380\n",
      "loss: 12.958186149597168\n",
      "steps per second: 0.27306\n",
      "step: 39381\n",
      "loss: 12.643768310546875\n",
      "steps per second: 0.56815\n",
      "step: 39382\n",
      "loss: 13.169214248657227\n",
      "steps per second: 0.54124\n",
      "step: 39383\n",
      "loss: 12.574925422668457\n",
      "steps per second: 0.54235\n",
      "step: 39384\n",
      "loss: 13.25516414642334\n",
      "steps per second: 0.54946\n",
      "step: 39385\n",
      "loss: 12.417645454406738\n",
      "steps per second: 0.53860\n",
      "step: 39386\n",
      "loss: 12.668950080871582\n",
      "steps per second: 0.55489\n",
      "step: 39387\n",
      "loss: 12.749896049499512\n",
      "steps per second: 0.57581\n",
      "step: 39388\n",
      "loss: 12.585942268371582\n",
      "steps per second: 0.56743\n",
      "step: 39389\n",
      "loss: 12.286126136779785\n",
      "steps per second: 0.53736\n",
      "step: 39390\n",
      "loss: 12.656225204467773\n",
      "steps per second: 0.55499\n",
      "step: 39391\n",
      "loss: 12.813261985778809\n",
      "steps per second: 0.60555\n",
      "step: 39392\n",
      "loss: 12.89773941040039\n",
      "steps per second: 0.54656\n",
      "step: 39393\n",
      "loss: 12.653188705444336\n",
      "steps per second: 0.55479\n",
      "step: 39394\n",
      "loss: 12.779029846191406\n",
      "steps per second: 0.52908\n",
      "step: 39395\n",
      "loss: 12.858296394348145\n",
      "steps per second: 0.53985\n",
      "step: 39396\n",
      "loss: 12.963845252990723\n",
      "steps per second: 0.52041\n",
      "step: 39397\n",
      "loss: 12.465807914733887\n",
      "steps per second: 0.61773\n",
      "step: 39398\n",
      "loss: 13.132209777832031\n",
      "steps per second: 0.53547\n",
      "step: 39399\n",
      "loss: 12.37307071685791\n",
      "steps per second: 0.56635\n",
      "step: 39400\n",
      "loss: 13.340261459350586\n",
      "steps per second: 0.57847\n",
      "step: 39401\n",
      "loss: 13.003167152404785\n",
      "steps per second: 0.57609\n",
      "step: 39402\n",
      "loss: 12.824113845825195\n",
      "steps per second: 0.52653\n",
      "step: 39403\n",
      "loss: 12.589739799499512\n",
      "steps per second: 0.57459\n",
      "step: 39404\n",
      "loss: 12.000513076782227\n",
      "steps per second: 0.57821\n",
      "step: 39405\n",
      "loss: 12.970772743225098\n",
      "steps per second: 0.58069\n",
      "step: 39406\n",
      "loss: 13.10580825805664\n",
      "steps per second: 0.52762\n",
      "step: 39407\n",
      "loss: 12.36302661895752\n",
      "steps per second: 0.54515\n",
      "step: 39408\n",
      "loss: 13.061084747314453\n",
      "steps per second: 0.58615\n",
      "step: 39409\n",
      "loss: 12.530983924865723\n",
      "steps per second: 0.53393\n",
      "step: 39410\n",
      "loss: 12.50802993774414\n",
      "steps per second: 0.56793\n",
      "step: 39411\n",
      "loss: 13.039506912231445\n",
      "steps per second: 0.56585\n",
      "step: 39412\n",
      "loss: 12.203618049621582\n",
      "steps per second: 0.54334\n",
      "step: 39413\n",
      "loss: 12.528593063354492\n",
      "steps per second: 0.55313\n",
      "step: 39414\n",
      "loss: 12.825846672058105\n",
      "steps per second: 0.52907\n",
      "step: 39415\n",
      "loss: 12.76744270324707\n",
      "steps per second: 0.47636\n",
      "step: 39416\n",
      "loss: 12.407782554626465\n",
      "steps per second: 0.52987\n",
      "step: 39417\n",
      "loss: 12.476296424865723\n",
      "steps per second: 0.57830\n",
      "step: 39418\n",
      "loss: 12.996912002563477\n",
      "steps per second: 0.53816\n",
      "step: 39419\n",
      "loss: 13.11697769165039\n",
      "steps per second: 0.55357\n",
      "step: 39420\n",
      "loss: 13.037129402160645\n",
      "steps per second: 0.50655\n",
      "step: 39421\n",
      "loss: 13.106127738952637\n",
      "steps per second: 0.48664\n",
      "step: 39422\n",
      "loss: 12.395337104797363\n",
      "steps per second: 0.52065\n",
      "step: 39423\n",
      "loss: 12.637406349182129\n",
      "steps per second: 0.55797\n",
      "step: 39424\n",
      "loss: 12.947980880737305\n",
      "steps per second: 0.56203\n",
      "step: 39425\n",
      "loss: 12.872431755065918\n",
      "steps per second: 0.54951\n",
      "step: 39426\n",
      "loss: 13.212591171264648\n",
      "steps per second: 0.55241\n",
      "step: 39427\n",
      "loss: 12.959953308105469\n",
      "steps per second: 0.58434\n",
      "step: 39428\n",
      "loss: 12.93800163269043\n",
      "steps per second: 0.55975\n",
      "step: 39429\n",
      "loss: 12.792806625366211\n",
      "steps per second: 0.57908\n",
      "step: 39430\n",
      "loss: 13.095949172973633\n",
      "steps per second: 0.52984\n",
      "step: 39431\n",
      "loss: 13.374892234802246\n",
      "steps per second: 0.53096\n",
      "step: 39432\n",
      "loss: 13.492888450622559\n",
      "steps per second: 0.54585\n",
      "step: 39433\n",
      "loss: 13.001038551330566\n",
      "steps per second: 0.53469\n",
      "step: 39434\n",
      "loss: 12.730781555175781\n",
      "steps per second: 0.54649\n",
      "step: 39435\n",
      "loss: 12.680706977844238\n",
      "steps per second: 0.56576\n",
      "step: 39436\n",
      "loss: 13.203644752502441\n",
      "steps per second: 0.57883\n",
      "step: 39437\n",
      "loss: 12.53723430633545\n",
      "steps per second: 0.54091\n",
      "step: 39438\n",
      "loss: 12.923932075500488\n",
      "steps per second: 0.55659\n",
      "step: 39439\n",
      "loss: 12.514503479003906\n",
      "steps per second: 0.53870\n",
      "step: 39440\n",
      "loss: 12.653390884399414\n",
      "steps per second: 0.58972\n",
      "step: 39441\n",
      "loss: 13.08849048614502\n",
      "steps per second: 0.48288\n",
      "step: 39442\n",
      "loss: 12.477264404296875\n",
      "steps per second: 0.56326\n",
      "step: 39443\n",
      "loss: 12.660715103149414\n",
      "steps per second: 0.57938\n",
      "step: 39444\n",
      "loss: 12.793554306030273\n",
      "steps per second: 0.53026\n",
      "step: 39445\n",
      "loss: 12.889276504516602\n",
      "steps per second: 0.59293\n",
      "step: 39446\n",
      "loss: 12.501194953918457\n",
      "steps per second: 0.56394\n",
      "step: 39447\n",
      "loss: 13.109639167785645\n",
      "steps per second: 0.56786\n",
      "step: 39448\n",
      "loss: 12.933112144470215\n",
      "steps per second: 0.53230\n",
      "step: 39449\n",
      "loss: 12.528861045837402\n",
      "steps per second: 0.50507\n",
      "step: 39450\n",
      "loss: 12.866411209106445\n",
      "steps per second: 0.55530\n",
      "step: 39451\n",
      "loss: 12.215258598327637\n",
      "steps per second: 0.54440\n",
      "step: 39452\n",
      "loss: 12.236721992492676\n",
      "steps per second: 0.52793\n",
      "step: 39453\n",
      "loss: 12.753382682800293\n",
      "steps per second: 0.54653\n",
      "step: 39454\n",
      "loss: 12.577159881591797\n",
      "steps per second: 0.58846\n",
      "step: 39455\n",
      "loss: 12.701674461364746\n",
      "steps per second: 0.58930\n",
      "step: 39456\n",
      "loss: 12.946514129638672\n",
      "steps per second: 0.52005\n",
      "step: 39457\n",
      "loss: 12.847392082214355\n",
      "steps per second: 0.52815\n",
      "step: 39458\n",
      "loss: 13.054305076599121\n",
      "steps per second: 0.52576\n",
      "step: 39459\n",
      "loss: 12.675450325012207\n",
      "steps per second: 0.57058\n",
      "step: 39460\n",
      "loss: 12.885625839233398\n",
      "steps per second: 0.49032\n",
      "step: 39461\n",
      "loss: 12.896239280700684\n",
      "steps per second: 0.56320\n",
      "step: 39462\n",
      "loss: 12.479988098144531\n",
      "steps per second: 0.50934\n",
      "step: 39463\n",
      "loss: 12.890059471130371\n",
      "steps per second: 0.49853\n",
      "step: 39464\n",
      "loss: 13.01373291015625\n",
      "steps per second: 0.55771\n",
      "step: 39465\n",
      "loss: 12.335472106933594\n",
      "steps per second: 0.56309\n",
      "step: 39466\n",
      "loss: 12.665769577026367\n",
      "steps per second: 0.53543\n",
      "step: 39467\n",
      "loss: 13.443158149719238\n",
      "steps per second: 0.53850\n",
      "step: 39468\n",
      "loss: 13.231339454650879\n",
      "steps per second: 0.55493\n",
      "step: 39469\n",
      "loss: 12.811984062194824\n",
      "steps per second: 0.62094\n",
      "step: 39470\n",
      "loss: 12.633988380432129\n",
      "steps per second: 0.52896\n",
      "step: 39471\n",
      "loss: 12.543245315551758\n",
      "steps per second: 0.51939\n",
      "step: 39472\n",
      "loss: 12.688151359558105\n",
      "steps per second: 0.55356\n",
      "step: 39473\n",
      "loss: 12.71681022644043\n",
      "steps per second: 0.56961\n",
      "step: 39474\n",
      "loss: 12.76425552368164\n",
      "steps per second: 0.57305\n",
      "step: 39475\n",
      "loss: 12.620551109313965\n",
      "steps per second: 0.54170\n",
      "step: 39476\n",
      "loss: 12.584460258483887\n",
      "steps per second: 0.53468\n",
      "step: 39477\n",
      "loss: 12.700188636779785\n",
      "steps per second: 0.52072\n",
      "step: 39478\n",
      "loss: 12.763346672058105\n",
      "steps per second: 0.52254\n",
      "step: 39479\n",
      "loss: 13.166308403015137\n",
      "steps per second: 0.56377\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8891676068305969, layer: 12\n",
      "saving at step 39479\n",
      "----------\n",
      "\n",
      "\n",
      "step: 39480\n",
      "loss: 12.452427864074707\n",
      "steps per second: 0.28903\n",
      "step: 39481\n",
      "loss: 12.935588836669922\n",
      "steps per second: 0.58875\n",
      "step: 39482\n",
      "loss: 12.52676773071289\n",
      "steps per second: 0.51365\n",
      "step: 39483\n",
      "loss: 12.79499340057373\n",
      "steps per second: 0.54902\n",
      "step: 39484\n",
      "loss: 12.89879035949707\n",
      "steps per second: 0.61153\n",
      "step: 39485\n",
      "loss: 12.369322776794434\n",
      "steps per second: 0.53871\n",
      "step: 39486\n",
      "loss: 12.781283378601074\n",
      "steps per second: 0.56364\n",
      "step: 39487\n",
      "loss: 13.086174011230469\n",
      "steps per second: 0.52802\n",
      "step: 39488\n",
      "loss: 13.390107154846191\n",
      "steps per second: 0.56595\n",
      "step: 39489\n",
      "loss: 13.023000717163086\n",
      "steps per second: 0.51767\n",
      "step: 39490\n",
      "loss: 13.289008140563965\n",
      "steps per second: 0.52519\n",
      "step: 39491\n",
      "loss: 12.810113906860352\n",
      "steps per second: 0.55715\n",
      "step: 39492\n",
      "loss: 12.746121406555176\n",
      "steps per second: 0.55414\n",
      "step: 39493\n",
      "loss: 12.30321216583252\n",
      "steps per second: 0.53348\n",
      "step: 39494\n",
      "loss: 13.015486717224121\n",
      "steps per second: 0.52190\n",
      "step: 39495\n",
      "loss: 12.16088581085205\n",
      "steps per second: 0.50745\n",
      "step: 39496\n",
      "loss: 12.722907066345215\n",
      "steps per second: 0.52936\n",
      "step: 39497\n",
      "loss: 12.57180404663086\n",
      "steps per second: 0.55281\n",
      "step: 39498\n",
      "loss: 12.810004234313965\n",
      "steps per second: 0.54075\n",
      "step: 39499\n",
      "loss: 12.873608589172363\n",
      "steps per second: 0.51899\n",
      "step: 39500\n",
      "loss: 13.042055130004883\n",
      "steps per second: 0.55134\n",
      "step: 39501\n",
      "loss: 12.950547218322754\n",
      "steps per second: 0.54859\n",
      "step: 39502\n",
      "loss: 12.81313419342041\n",
      "steps per second: 0.57397\n",
      "step: 39503\n",
      "loss: 12.998355865478516\n",
      "steps per second: 0.58122\n",
      "step: 39504\n",
      "loss: 12.39802360534668\n",
      "steps per second: 0.56769\n",
      "step: 39505\n",
      "loss: 12.626237869262695\n",
      "steps per second: 0.61626\n",
      "step: 39506\n",
      "loss: 12.77177619934082\n",
      "steps per second: 0.50576\n",
      "step: 39507\n",
      "loss: 12.658559799194336\n",
      "steps per second: 0.57481\n",
      "step: 39508\n",
      "loss: 12.027146339416504\n",
      "steps per second: 0.57240\n",
      "step: 39509\n",
      "loss: 13.152524948120117\n",
      "steps per second: 0.57555\n",
      "step: 39510\n",
      "loss: 13.128446578979492\n",
      "steps per second: 0.54523\n",
      "step: 39511\n",
      "loss: 12.362247467041016\n",
      "steps per second: 0.54745\n",
      "step: 39512\n",
      "loss: 12.375629425048828\n",
      "steps per second: 0.52484\n",
      "step: 39513\n",
      "loss: 12.576735496520996\n",
      "steps per second: 0.55047\n",
      "step: 39514\n",
      "loss: 12.928293228149414\n",
      "steps per second: 0.57151\n",
      "step: 39515\n",
      "loss: 12.822014808654785\n",
      "steps per second: 0.54808\n",
      "step: 39516\n",
      "loss: 12.62613582611084\n",
      "steps per second: 0.55850\n",
      "step: 39517\n",
      "loss: 12.755950927734375\n",
      "steps per second: 0.58592\n",
      "step: 39518\n",
      "loss: 12.822968482971191\n",
      "steps per second: 0.54880\n",
      "step: 39519\n",
      "loss: 13.207466125488281\n",
      "steps per second: 0.55029\n",
      "step: 39520\n",
      "loss: 12.773451805114746\n",
      "steps per second: 0.53674\n",
      "step: 39521\n",
      "loss: 12.96989631652832\n",
      "steps per second: 0.61081\n",
      "step: 39522\n",
      "loss: 12.328150749206543\n",
      "steps per second: 0.61068\n",
      "step: 39523\n",
      "loss: 12.451757431030273\n",
      "steps per second: 0.52347\n",
      "step: 39524\n",
      "loss: 12.65871524810791\n",
      "steps per second: 0.56217\n",
      "step: 39525\n",
      "loss: 12.962238311767578\n",
      "steps per second: 0.61033\n",
      "step: 39526\n",
      "loss: 12.719223976135254\n",
      "steps per second: 0.57280\n",
      "step: 39527\n",
      "loss: 12.904059410095215\n",
      "steps per second: 0.54254\n",
      "step: 39528\n",
      "loss: 12.813414573669434\n",
      "steps per second: 0.54223\n",
      "step: 39529\n",
      "loss: 12.653943061828613\n",
      "steps per second: 0.52052\n",
      "step: 39530\n",
      "loss: 12.659615516662598\n",
      "steps per second: 0.50586\n",
      "step: 39531\n",
      "loss: 13.060547828674316\n",
      "steps per second: 0.53511\n",
      "step: 39532\n",
      "loss: 12.745512008666992\n",
      "steps per second: 0.56068\n",
      "step: 39533\n",
      "loss: 12.66441535949707\n",
      "steps per second: 0.52174\n",
      "step: 39534\n",
      "loss: 12.887165069580078\n",
      "steps per second: 0.55857\n",
      "step: 39535\n",
      "loss: 13.195136070251465\n",
      "steps per second: 0.57188\n",
      "step: 39536\n",
      "loss: 13.42546272277832\n",
      "steps per second: 0.57186\n",
      "step: 39537\n",
      "loss: 13.428024291992188\n",
      "steps per second: 0.53107\n",
      "step: 39538\n",
      "loss: 12.92798900604248\n",
      "steps per second: 0.52261\n",
      "step: 39539\n",
      "loss: 12.4364652633667\n",
      "steps per second: 0.48229\n",
      "step: 39540\n",
      "loss: 13.013124465942383\n",
      "steps per second: 0.58309\n",
      "step: 39541\n",
      "loss: 12.208242416381836\n",
      "steps per second: 0.57271\n",
      "step: 39542\n",
      "loss: 12.308603286743164\n",
      "steps per second: 0.56000\n",
      "step: 39543\n",
      "loss: 12.804670333862305\n",
      "steps per second: 0.57085\n",
      "step: 39544\n",
      "loss: 13.330168724060059\n",
      "steps per second: 0.57267\n",
      "step: 39545\n",
      "loss: 12.636311531066895\n",
      "steps per second: 0.54896\n",
      "step: 39546\n",
      "loss: 13.139189720153809\n",
      "steps per second: 0.53656\n",
      "step: 39547\n",
      "loss: 12.939069747924805\n",
      "steps per second: 0.57393\n",
      "step: 39548\n",
      "loss: 12.778823852539062\n",
      "steps per second: 0.52459\n",
      "step: 39549\n",
      "loss: 12.531966209411621\n",
      "steps per second: 0.57182\n",
      "step: 39550\n",
      "loss: 12.743550300598145\n",
      "steps per second: 0.56115\n",
      "step: 39551\n",
      "loss: 12.763944625854492\n",
      "steps per second: 0.56339\n",
      "step: 39552\n",
      "loss: 13.174600601196289\n",
      "steps per second: 0.53309\n",
      "step: 39553\n",
      "loss: 12.23930549621582\n",
      "steps per second: 0.56051\n",
      "step: 39554\n",
      "loss: 13.099559783935547\n",
      "steps per second: 0.51809\n",
      "step: 39555\n",
      "loss: 12.631558418273926\n",
      "steps per second: 0.56134\n",
      "step: 39556\n",
      "loss: 13.007536888122559\n",
      "steps per second: 0.56001\n",
      "step: 39557\n",
      "loss: 12.911615371704102\n",
      "steps per second: 0.56109\n",
      "step: 39558\n",
      "loss: 13.18743896484375\n",
      "steps per second: 0.55987\n",
      "step: 39559\n",
      "loss: 12.752167701721191\n",
      "steps per second: 0.53050\n",
      "step: 39560\n",
      "loss: 12.965963363647461\n",
      "steps per second: 0.54966\n",
      "step: 39561\n",
      "loss: 12.677111625671387\n",
      "steps per second: 0.61521\n",
      "step: 39562\n",
      "loss: 12.913708686828613\n",
      "steps per second: 0.58000\n",
      "step: 39563\n",
      "loss: 12.75813102722168\n",
      "steps per second: 0.52303\n",
      "step: 39564\n",
      "loss: 12.826644897460938\n",
      "steps per second: 0.55006\n",
      "step: 39565\n",
      "loss: 13.17064094543457\n",
      "steps per second: 0.53057\n",
      "step: 39566\n",
      "loss: 12.27027702331543\n",
      "steps per second: 0.46115\n",
      "step: 39567\n",
      "loss: 12.84012222290039\n",
      "steps per second: 0.56704\n",
      "step: 39568\n",
      "loss: 13.575905799865723\n",
      "steps per second: 0.51009\n",
      "step: 39569\n",
      "loss: 13.05706787109375\n",
      "steps per second: 0.53522\n",
      "step: 39570\n",
      "loss: 12.752429962158203\n",
      "steps per second: 0.59367\n",
      "step: 39571\n",
      "loss: 12.706818580627441\n",
      "steps per second: 0.51770\n",
      "step: 39572\n",
      "loss: 12.577816009521484\n",
      "steps per second: 0.54590\n",
      "step: 39573\n",
      "loss: 13.01761531829834\n",
      "steps per second: 0.56966\n",
      "step: 39574\n",
      "loss: 13.159087181091309\n",
      "steps per second: 0.54756\n",
      "step: 39575\n",
      "loss: 13.183021545410156\n",
      "steps per second: 0.55129\n",
      "step: 39576\n",
      "loss: 12.799561500549316\n",
      "steps per second: 0.52488\n",
      "step: 39577\n",
      "loss: 13.3443021774292\n",
      "steps per second: 0.56122\n",
      "step: 39578\n",
      "loss: 12.588508605957031\n",
      "steps per second: 0.53584\n",
      "step: 39579\n",
      "loss: 13.549627304077148\n",
      "steps per second: 0.52634\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8789201378822327, layer: 11\n",
      "saving at step 39579\n",
      "----------\n",
      "\n",
      "\n",
      "step: 39580\n",
      "loss: 12.784735679626465\n",
      "steps per second: 0.27673\n",
      "step: 39581\n",
      "loss: 12.646248817443848\n",
      "steps per second: 0.56648\n",
      "step: 39582\n",
      "loss: 13.217071533203125\n",
      "steps per second: 0.50384\n",
      "step: 39583\n",
      "loss: 12.66028118133545\n",
      "steps per second: 0.53126\n",
      "step: 39584\n",
      "loss: 12.322446823120117\n",
      "steps per second: 0.53843\n",
      "step: 39585\n",
      "loss: 13.299503326416016\n",
      "steps per second: 0.52282\n",
      "step: 39586\n",
      "loss: 12.358025550842285\n",
      "steps per second: 0.56225\n",
      "step: 39587\n",
      "loss: 12.947589874267578\n",
      "steps per second: 0.57136\n",
      "step: 39588\n",
      "loss: 12.761263847351074\n",
      "steps per second: 0.51778\n",
      "step: 39589\n",
      "loss: 13.140727043151855\n",
      "steps per second: 0.47670\n",
      "step: 39590\n",
      "loss: 12.961996078491211\n",
      "steps per second: 0.54436\n",
      "step: 39591\n",
      "loss: 12.342540740966797\n",
      "steps per second: 0.52036\n",
      "step: 39592\n",
      "loss: 12.478682518005371\n",
      "steps per second: 0.54250\n",
      "step: 39593\n",
      "loss: 12.934657096862793\n",
      "steps per second: 0.50306\n",
      "step: 39594\n",
      "loss: 12.830899238586426\n",
      "steps per second: 0.53515\n",
      "step: 39595\n",
      "loss: 12.153114318847656\n",
      "steps per second: 0.51324\n",
      "step: 39596\n",
      "loss: 12.955721855163574\n",
      "steps per second: 0.60316\n",
      "step: 39597\n",
      "loss: 12.565000534057617\n",
      "steps per second: 0.56285\n",
      "step: 39598\n",
      "loss: 12.636894226074219\n",
      "steps per second: 0.53593\n",
      "step: 39599\n",
      "loss: 12.764010429382324\n",
      "steps per second: 0.56066\n",
      "step: 39600\n",
      "loss: 13.277786254882812\n",
      "steps per second: 0.55449\n",
      "step: 39601\n",
      "loss: 12.271639823913574\n",
      "steps per second: 0.52212\n",
      "step: 39602\n",
      "loss: 12.783278465270996\n",
      "steps per second: 0.54015\n",
      "step: 39603\n",
      "loss: 13.0430269241333\n",
      "steps per second: 0.54102\n",
      "step: 39604\n",
      "loss: 12.557428359985352\n",
      "steps per second: 0.57383\n",
      "step: 39605\n",
      "loss: 13.203323364257812\n",
      "steps per second: 0.55678\n",
      "step: 39606\n",
      "loss: 12.385090827941895\n",
      "steps per second: 0.57141\n",
      "step: 39607\n",
      "loss: 12.65003776550293\n",
      "steps per second: 0.53947\n",
      "step: 39608\n",
      "loss: 12.761079788208008\n",
      "steps per second: 0.54384\n",
      "step: 39609\n",
      "loss: 12.338875770568848\n",
      "steps per second: 0.53877\n",
      "step: 39610\n",
      "loss: 12.998922348022461\n",
      "steps per second: 0.54262\n",
      "step: 39611\n",
      "loss: 12.974477767944336\n",
      "steps per second: 0.52492\n",
      "step: 39612\n",
      "loss: 12.86555004119873\n",
      "steps per second: 0.52867\n",
      "step: 39613\n",
      "loss: 12.626568794250488\n",
      "steps per second: 0.57000\n",
      "step: 39614\n",
      "loss: 12.917258262634277\n",
      "steps per second: 0.52399\n",
      "step: 39615\n",
      "loss: 12.592360496520996\n",
      "steps per second: 0.52493\n",
      "step: 39616\n",
      "loss: 11.987491607666016\n",
      "steps per second: 0.53538\n",
      "step: 39617\n",
      "loss: 12.761300086975098\n",
      "steps per second: 0.55712\n",
      "step: 39618\n",
      "loss: 12.358763694763184\n",
      "steps per second: 0.53451\n",
      "step: 39619\n",
      "loss: 13.015939712524414\n",
      "steps per second: 0.54000\n",
      "step: 39620\n",
      "loss: 12.876641273498535\n",
      "steps per second: 0.61424\n",
      "step: 39621\n",
      "loss: 13.068462371826172\n",
      "steps per second: 0.52320\n",
      "step: 39622\n",
      "loss: 12.89366626739502\n",
      "steps per second: 0.56044\n",
      "step: 39623\n",
      "loss: 12.99956226348877\n",
      "steps per second: 0.56117\n",
      "step: 39624\n",
      "loss: 12.888876914978027\n",
      "steps per second: 0.56986\n",
      "step: 39625\n",
      "loss: 12.383627891540527\n",
      "steps per second: 0.55893\n",
      "step: 39626\n",
      "loss: 12.722328186035156\n",
      "steps per second: 0.55644\n",
      "step: 39627\n",
      "loss: 12.372599601745605\n",
      "steps per second: 0.54536\n",
      "step: 39628\n",
      "loss: 12.534302711486816\n",
      "steps per second: 0.57213\n",
      "step: 39629\n",
      "loss: 13.007997512817383\n",
      "steps per second: 0.56770\n",
      "step: 39630\n",
      "loss: 12.852807998657227\n",
      "steps per second: 0.52597\n",
      "step: 39631\n",
      "loss: 12.325268745422363\n",
      "steps per second: 0.61284\n",
      "step: 39632\n",
      "loss: 12.895682334899902\n",
      "steps per second: 0.57418\n",
      "step: 39633\n",
      "loss: 12.879545211791992\n",
      "steps per second: 0.53458\n",
      "step: 39634\n",
      "loss: 12.752874374389648\n",
      "steps per second: 0.56836\n",
      "step: 39635\n",
      "loss: 12.779972076416016\n",
      "steps per second: 0.55013\n",
      "step: 39636\n",
      "loss: 12.6585693359375\n",
      "steps per second: 0.55947\n",
      "step: 39637\n",
      "loss: 12.737858772277832\n",
      "steps per second: 0.56127\n",
      "step: 39638\n",
      "loss: 12.41582202911377\n",
      "steps per second: 0.54668\n",
      "step: 39639\n",
      "loss: 13.097060203552246\n",
      "steps per second: 0.54963\n",
      "step: 39640\n",
      "loss: 13.159635543823242\n",
      "steps per second: 0.54738\n",
      "step: 39641\n",
      "loss: 13.238287925720215\n",
      "steps per second: 0.53246\n",
      "step: 39642\n",
      "loss: 13.676197052001953\n",
      "steps per second: 0.55810\n",
      "step: 39643\n",
      "loss: 12.790141105651855\n",
      "steps per second: 0.47176\n",
      "step: 39644\n",
      "loss: 13.274801254272461\n",
      "steps per second: 0.51668\n",
      "step: 39645\n",
      "loss: 12.709814071655273\n",
      "steps per second: 0.56953\n",
      "step: 39646\n",
      "loss: 13.162657737731934\n",
      "steps per second: 0.57111\n",
      "step: 39647\n",
      "loss: 12.650809288024902\n",
      "steps per second: 0.57338\n",
      "step: 39648\n",
      "loss: 12.724138259887695\n",
      "steps per second: 0.60968\n",
      "step: 39649\n",
      "loss: 12.476156234741211\n",
      "steps per second: 0.61365\n",
      "step: 39650\n",
      "loss: 13.103496551513672\n",
      "steps per second: 0.50622\n",
      "step: 39651\n",
      "loss: 12.192713737487793\n",
      "steps per second: 0.51934\n",
      "step: 39652\n",
      "loss: 12.794422149658203\n",
      "steps per second: 0.54177\n",
      "step: 39653\n",
      "loss: 13.407021522521973\n",
      "steps per second: 0.58196\n",
      "step: 39654\n",
      "loss: 12.982002258300781\n",
      "steps per second: 0.52967\n",
      "step: 39655\n",
      "loss: 12.268485069274902\n",
      "steps per second: 0.61028\n",
      "step: 39656\n",
      "loss: 13.105134963989258\n",
      "steps per second: 0.54106\n",
      "step: 39657\n",
      "loss: 12.573447227478027\n",
      "steps per second: 0.54609\n",
      "step: 39658\n",
      "loss: 12.902464866638184\n",
      "steps per second: 0.52202\n",
      "step: 39659\n",
      "loss: 13.023980140686035\n",
      "steps per second: 0.53388\n",
      "step: 39660\n",
      "loss: 13.161385536193848\n",
      "steps per second: 0.57691\n",
      "step: 39661\n",
      "loss: 13.011337280273438\n",
      "steps per second: 0.52898\n",
      "step: 39662\n",
      "loss: 12.905054092407227\n",
      "steps per second: 0.55113\n",
      "step: 39663\n",
      "loss: 12.939358711242676\n",
      "steps per second: 0.57445\n",
      "step: 39664\n",
      "loss: 12.542028427124023\n",
      "steps per second: 0.57966\n",
      "step: 39665\n",
      "loss: 12.284393310546875\n",
      "steps per second: 0.57211\n",
      "step: 39666\n",
      "loss: 12.987449645996094\n",
      "steps per second: 0.56028\n",
      "step: 39667\n",
      "loss: 12.652515411376953\n",
      "steps per second: 0.53268\n",
      "step: 39668\n",
      "loss: 12.842028617858887\n",
      "steps per second: 0.54548\n",
      "step: 39669\n",
      "loss: 13.512076377868652\n",
      "steps per second: 0.57329\n",
      "step: 39670\n",
      "loss: 13.224366188049316\n",
      "steps per second: 0.54169\n",
      "step: 39671\n",
      "loss: 12.415464401245117\n",
      "steps per second: 0.57305\n",
      "step: 39672\n",
      "loss: 13.162769317626953\n",
      "steps per second: 0.50599\n",
      "step: 39673\n",
      "loss: 12.60677719116211\n",
      "steps per second: 0.52051\n",
      "step: 39674\n",
      "loss: 12.449670791625977\n",
      "steps per second: 0.57889\n",
      "step: 39675\n",
      "loss: 12.580214500427246\n",
      "steps per second: 0.58242\n",
      "step: 39676\n",
      "loss: 12.546921730041504\n",
      "steps per second: 0.55821\n",
      "step: 39677\n",
      "loss: 12.700544357299805\n",
      "steps per second: 0.54274\n",
      "step: 39678\n",
      "loss: 12.169975280761719\n",
      "steps per second: 0.52565\n",
      "step: 39679\n",
      "loss: 13.226461410522461\n",
      "steps per second: 0.58177\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8611095547676086, layer: 11\n",
      "saving at step 39679\n",
      "----------\n",
      "\n",
      "\n",
      "step: 39680\n",
      "loss: 12.228856086730957\n",
      "steps per second: 0.28698\n",
      "step: 39681\n",
      "loss: 12.619498252868652\n",
      "steps per second: 0.54953\n",
      "step: 39682\n",
      "loss: 12.615547180175781\n",
      "steps per second: 0.52604\n",
      "step: 39683\n",
      "loss: 13.047896385192871\n",
      "steps per second: 0.56063\n",
      "step: 39684\n",
      "loss: 12.6630277633667\n",
      "steps per second: 0.60807\n",
      "step: 39685\n",
      "loss: 12.7379150390625\n",
      "steps per second: 0.54257\n",
      "step: 39686\n",
      "loss: 13.156319618225098\n",
      "steps per second: 0.53435\n",
      "step: 39687\n",
      "loss: 13.11404037475586\n",
      "steps per second: 0.55214\n",
      "step: 39688\n",
      "loss: 12.582657814025879\n",
      "steps per second: 0.51416\n",
      "step: 39689\n",
      "loss: 12.491351127624512\n",
      "steps per second: 0.55512\n",
      "step: 39690\n",
      "loss: 12.645258903503418\n",
      "steps per second: 0.56107\n",
      "step: 39691\n",
      "loss: 12.510173797607422\n",
      "steps per second: 0.52219\n",
      "step: 39692\n",
      "loss: 12.944058418273926\n",
      "steps per second: 0.55944\n",
      "step: 39693\n",
      "loss: 13.044952392578125\n",
      "steps per second: 0.57377\n",
      "step: 39694\n",
      "loss: 12.947431564331055\n",
      "steps per second: 0.55004\n",
      "step: 39695\n",
      "loss: 12.315396308898926\n",
      "steps per second: 0.52153\n",
      "step: 39696\n",
      "loss: 13.411442756652832\n",
      "steps per second: 0.52594\n",
      "step: 39697\n",
      "loss: 12.481639862060547\n",
      "steps per second: 0.51178\n",
      "step: 39698\n",
      "loss: 12.460457801818848\n",
      "steps per second: 0.57409\n",
      "step: 39699\n",
      "loss: 13.089515686035156\n",
      "steps per second: 0.57846\n",
      "step: 39700\n",
      "loss: 13.014183044433594\n",
      "steps per second: 0.55783\n",
      "step: 39701\n",
      "loss: 12.412725448608398\n",
      "steps per second: 0.57117\n",
      "step: 39702\n",
      "loss: 13.240970611572266\n",
      "steps per second: 0.57462\n",
      "step: 39703\n",
      "loss: 12.811187744140625\n",
      "steps per second: 0.52590\n",
      "step: 39704\n",
      "loss: 12.899748802185059\n",
      "steps per second: 0.54677\n",
      "step: 39705\n",
      "loss: 12.581416130065918\n",
      "steps per second: 0.53856\n",
      "step: 39706\n",
      "loss: 12.93526554107666\n",
      "steps per second: 0.53859\n",
      "step: 39707\n",
      "loss: 12.6856689453125\n",
      "steps per second: 0.53514\n",
      "step: 39708\n",
      "loss: 12.804075241088867\n",
      "steps per second: 0.53699\n",
      "step: 39709\n",
      "loss: 12.940506935119629\n",
      "steps per second: 0.56237\n",
      "step: 39710\n",
      "loss: 13.023846626281738\n",
      "steps per second: 0.58839\n",
      "step: 39711\n",
      "loss: 12.802779197692871\n",
      "steps per second: 0.55618\n",
      "step: 39712\n",
      "loss: 12.239384651184082\n",
      "steps per second: 0.50556\n",
      "step: 39713\n",
      "loss: 12.707077980041504\n",
      "steps per second: 0.55002\n",
      "step: 39714\n",
      "loss: 12.57674789428711\n",
      "steps per second: 0.61526\n",
      "step: 39715\n",
      "loss: 13.02480697631836\n",
      "steps per second: 0.61374\n",
      "step: 39716\n",
      "loss: 12.587729454040527\n",
      "steps per second: 0.55788\n",
      "step: 39717\n",
      "loss: 12.048783302307129\n",
      "steps per second: 0.57045\n",
      "step: 39718\n",
      "loss: 12.85239028930664\n",
      "steps per second: 0.55903\n",
      "step: 39719\n",
      "loss: 12.079201698303223\n",
      "steps per second: 0.53613\n",
      "step: 39720\n",
      "loss: 12.083174705505371\n",
      "steps per second: 0.56313\n",
      "step: 39721\n",
      "loss: 12.868483543395996\n",
      "steps per second: 0.56832\n",
      "step: 39722\n",
      "loss: 13.005369186401367\n",
      "steps per second: 0.47867\n",
      "step: 39723\n",
      "loss: 13.418230056762695\n",
      "steps per second: 0.54879\n",
      "step: 39724\n",
      "loss: 12.893989562988281\n",
      "steps per second: 0.56196\n",
      "step: 39725\n",
      "loss: 12.755983352661133\n",
      "steps per second: 0.51189\n",
      "step: 39726\n",
      "loss: 12.944405555725098\n",
      "steps per second: 0.50496\n",
      "step: 39727\n",
      "loss: 12.707655906677246\n",
      "steps per second: 0.61319\n",
      "step: 39728\n",
      "loss: 12.791834831237793\n",
      "steps per second: 0.57238\n",
      "step: 39729\n",
      "loss: 12.524927139282227\n",
      "steps per second: 0.57164\n",
      "step: 39730\n",
      "loss: 13.139443397521973\n",
      "steps per second: 0.55958\n",
      "step: 39731\n",
      "loss: 12.609664916992188\n",
      "steps per second: 0.53646\n",
      "step: 39732\n",
      "loss: 12.647415161132812\n",
      "steps per second: 0.54317\n",
      "step: 39733\n",
      "loss: 12.718162536621094\n",
      "steps per second: 0.54795\n",
      "step: 39734\n",
      "loss: 12.937982559204102\n",
      "steps per second: 0.53460\n",
      "step: 39735\n",
      "loss: 12.993727684020996\n",
      "steps per second: 0.56930\n",
      "step: 39736\n",
      "loss: 12.814350128173828\n",
      "steps per second: 0.57993\n",
      "step: 39737\n",
      "loss: 12.637741088867188\n",
      "steps per second: 0.57114\n",
      "step: 39738\n",
      "loss: 12.330259323120117\n",
      "steps per second: 0.57890\n",
      "step: 39739\n",
      "loss: 12.858339309692383\n",
      "steps per second: 0.56483\n",
      "step: 39740\n",
      "loss: 12.822920799255371\n",
      "steps per second: 0.54107\n",
      "step: 39741\n",
      "loss: 12.703508377075195\n",
      "steps per second: 0.53297\n",
      "step: 39742\n",
      "loss: 12.834860801696777\n",
      "steps per second: 0.52989\n",
      "step: 39743\n",
      "loss: 12.526493072509766\n",
      "steps per second: 0.55894\n",
      "step: 39744\n",
      "loss: 12.686861038208008\n",
      "steps per second: 0.57348\n",
      "step: 39745\n",
      "loss: 11.950560569763184\n",
      "steps per second: 0.59892\n",
      "step: 39746\n",
      "loss: 12.372684478759766\n",
      "steps per second: 0.52328\n",
      "step: 39747\n",
      "loss: 12.837183952331543\n",
      "steps per second: 0.54007\n",
      "step: 39748\n",
      "loss: 13.203930854797363\n",
      "steps per second: 0.57312\n",
      "step: 39749\n",
      "loss: 12.684591293334961\n",
      "steps per second: 0.49960\n",
      "step: 39750\n",
      "loss: 12.840716361999512\n",
      "steps per second: 0.49959\n",
      "step: 39751\n",
      "loss: 12.82764720916748\n",
      "steps per second: 0.53336\n",
      "step: 39752\n",
      "loss: 12.508403778076172\n",
      "steps per second: 0.54676\n",
      "step: 39753\n",
      "loss: 12.686698913574219\n",
      "steps per second: 0.57378\n",
      "step: 39754\n",
      "loss: 12.152669906616211\n",
      "steps per second: 0.53491\n",
      "step: 39755\n",
      "loss: 12.996684074401855\n",
      "steps per second: 0.57981\n",
      "step: 39756\n",
      "loss: 12.957636833190918\n",
      "steps per second: 0.52264\n",
      "step: 39757\n",
      "loss: 13.295171737670898\n",
      "steps per second: 0.56140\n",
      "step: 39758\n",
      "loss: 12.753704071044922\n",
      "steps per second: 0.54992\n",
      "step: 39759\n",
      "loss: 12.471985816955566\n",
      "steps per second: 0.52090\n",
      "step: 39760\n",
      "loss: 12.732170104980469\n",
      "steps per second: 0.57221\n",
      "step: 39761\n",
      "loss: 13.618280410766602\n",
      "steps per second: 0.54195\n",
      "step: 39762\n",
      "loss: 13.465932846069336\n",
      "steps per second: 0.55746\n",
      "step: 39763\n",
      "loss: 12.99212646484375\n",
      "steps per second: 0.54722\n",
      "step: 39764\n",
      "loss: 13.252488136291504\n",
      "steps per second: 0.55002\n",
      "step: 39765\n",
      "loss: 12.928950309753418\n",
      "steps per second: 0.55782\n",
      "step: 39766\n",
      "loss: 12.985885620117188\n",
      "steps per second: 0.53230\n",
      "step: 39767\n",
      "loss: 13.385564804077148\n",
      "steps per second: 0.55988\n",
      "step: 39768\n",
      "loss: 13.021769523620605\n",
      "steps per second: 0.55100\n",
      "step: 39769\n",
      "loss: 12.735535621643066\n",
      "steps per second: 0.57304\n",
      "step: 39770\n",
      "loss: 12.904895782470703\n",
      "steps per second: 0.55868\n",
      "step: 39771\n",
      "loss: 12.849990844726562\n",
      "steps per second: 0.57373\n",
      "step: 39772\n",
      "loss: 12.176313400268555\n",
      "steps per second: 0.54657\n",
      "step: 39773\n",
      "loss: 12.435017585754395\n",
      "steps per second: 0.53482\n",
      "step: 39774\n",
      "loss: 12.868748664855957\n",
      "steps per second: 0.54088\n",
      "step: 39775\n",
      "loss: 12.716798782348633\n",
      "steps per second: 0.57022\n",
      "step: 39776\n",
      "loss: 12.632230758666992\n",
      "steps per second: 0.55725\n",
      "step: 39777\n",
      "loss: 12.88913345336914\n",
      "steps per second: 0.58024\n",
      "step: 39778\n",
      "loss: 12.518210411071777\n",
      "steps per second: 0.54794\n",
      "step: 39779\n",
      "loss: 12.570558547973633\n",
      "steps per second: 0.51006\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8139516711235046, layer: 10\n",
      "saving at step 39779\n",
      "----------\n",
      "\n",
      "\n",
      "step: 39780\n",
      "loss: 12.564027786254883\n",
      "steps per second: 0.25893\n",
      "step: 39781\n",
      "loss: 13.036351203918457\n",
      "steps per second: 0.51306\n",
      "step: 39782\n",
      "loss: 13.031668663024902\n",
      "steps per second: 0.55880\n",
      "step: 39783\n",
      "loss: 12.74748706817627\n",
      "steps per second: 0.57229\n",
      "step: 39784\n",
      "loss: 12.802279472351074\n",
      "steps per second: 0.54528\n",
      "step: 39785\n",
      "loss: 12.995044708251953\n",
      "steps per second: 0.55434\n",
      "step: 39786\n",
      "loss: 12.358441352844238\n",
      "steps per second: 0.57193\n",
      "step: 39787\n",
      "loss: 13.458277702331543\n",
      "steps per second: 0.48068\n",
      "step: 39788\n",
      "loss: 13.333211898803711\n",
      "steps per second: 0.50840\n",
      "step: 39789\n",
      "loss: 12.711973190307617\n",
      "steps per second: 0.50122\n",
      "step: 39790\n",
      "loss: 12.999066352844238\n",
      "steps per second: 0.55596\n",
      "step: 39791\n",
      "loss: 12.612520217895508\n",
      "steps per second: 0.48901\n",
      "step: 39792\n",
      "loss: 12.839245796203613\n",
      "steps per second: 0.54267\n",
      "step: 39793\n",
      "loss: 13.424510955810547\n",
      "steps per second: 0.53579\n",
      "step: 39794\n",
      "loss: 12.515055656433105\n",
      "steps per second: 0.51316\n",
      "step: 39795\n",
      "loss: 13.073284149169922\n",
      "steps per second: 0.55018\n",
      "step: 39796\n",
      "loss: 12.47516918182373\n",
      "steps per second: 0.52696\n",
      "step: 39797\n",
      "loss: 12.990457534790039\n",
      "steps per second: 0.48910\n",
      "step: 39798\n",
      "loss: 13.03046703338623\n",
      "steps per second: 0.56387\n",
      "step: 39799\n",
      "loss: 12.473950386047363\n",
      "steps per second: 0.60233\n",
      "step: 39800\n",
      "loss: 12.9574556350708\n",
      "steps per second: 0.53721\n",
      "step: 39801\n",
      "loss: 12.381868362426758\n",
      "steps per second: 0.54151\n",
      "step: 39802\n",
      "loss: 12.037338256835938\n",
      "steps per second: 0.54014\n",
      "step: 39803\n",
      "loss: 13.243897438049316\n",
      "steps per second: 0.51159\n",
      "step: 39804\n",
      "loss: 13.039643287658691\n",
      "steps per second: 0.57041\n",
      "step: 39805\n",
      "loss: 13.029333114624023\n",
      "steps per second: 0.60076\n",
      "step: 39806\n",
      "loss: 12.420366287231445\n",
      "steps per second: 0.52821\n",
      "step: 39807\n",
      "loss: 13.208271026611328\n",
      "steps per second: 0.51504\n",
      "step: 39808\n",
      "loss: 12.803324699401855\n",
      "steps per second: 0.53948\n",
      "step: 39809\n",
      "loss: 12.68592357635498\n",
      "steps per second: 0.52740\n",
      "step: 39810\n",
      "loss: 12.875143051147461\n",
      "steps per second: 0.57211\n",
      "step: 39811\n",
      "loss: 12.737144470214844\n",
      "steps per second: 0.53355\n",
      "step: 39812\n",
      "loss: 12.766095161437988\n",
      "steps per second: 0.55209\n",
      "step: 39813\n",
      "loss: 12.986998558044434\n",
      "steps per second: 0.52462\n",
      "step: 39814\n",
      "loss: 13.158838272094727\n",
      "steps per second: 0.55354\n",
      "step: 39815\n",
      "loss: 13.030244827270508\n",
      "steps per second: 0.52549\n",
      "step: 39816\n",
      "loss: 13.00828742980957\n",
      "steps per second: 0.56457\n",
      "step: 39817\n",
      "loss: 13.075761795043945\n",
      "steps per second: 0.54037\n",
      "step: 39818\n",
      "loss: 12.754809379577637\n",
      "steps per second: 0.57529\n",
      "step: 39819\n",
      "loss: 12.778301239013672\n",
      "steps per second: 0.52522\n",
      "step: 39820\n",
      "loss: 12.654390335083008\n",
      "steps per second: 0.51066\n",
      "step: 39821\n",
      "loss: 12.984637260437012\n",
      "steps per second: 0.51159\n",
      "step: 39822\n",
      "loss: 13.457075119018555\n",
      "steps per second: 0.53220\n",
      "step: 39823\n",
      "loss: 13.012044906616211\n",
      "steps per second: 0.56821\n",
      "step: 39824\n",
      "loss: 12.184425354003906\n",
      "steps per second: 0.52704\n",
      "step: 39825\n",
      "loss: 12.769244194030762\n",
      "steps per second: 0.56284\n",
      "step: 39826\n",
      "loss: 12.837985038757324\n",
      "steps per second: 0.53327\n",
      "step: 39827\n",
      "loss: 12.72997760772705\n",
      "steps per second: 0.56822\n",
      "step: 39828\n",
      "loss: 12.872882843017578\n",
      "steps per second: 0.53941\n",
      "step: 39829\n",
      "loss: 12.53449821472168\n",
      "steps per second: 0.56198\n",
      "step: 39830\n",
      "loss: 12.735917091369629\n",
      "steps per second: 0.53766\n",
      "step: 39831\n",
      "loss: 12.662943840026855\n",
      "steps per second: 0.54827\n",
      "step: 39832\n",
      "loss: 13.159127235412598\n",
      "steps per second: 0.51465\n",
      "step: 39833\n",
      "loss: 12.701171875\n",
      "steps per second: 0.52772\n",
      "step: 39834\n",
      "loss: 12.652595520019531\n",
      "steps per second: 0.54565\n",
      "step: 39835\n",
      "loss: 12.242530822753906\n",
      "steps per second: 0.56534\n",
      "step: 39836\n",
      "loss: 13.451944351196289\n",
      "steps per second: 0.53719\n",
      "step: 39837\n",
      "loss: 12.981945037841797\n",
      "steps per second: 0.52794\n",
      "step: 39838\n",
      "loss: 12.943306922912598\n",
      "steps per second: 0.54197\n",
      "step: 39839\n",
      "loss: 13.005351066589355\n",
      "steps per second: 0.54091\n",
      "step: 39840\n",
      "loss: 13.453557014465332\n",
      "steps per second: 0.53875\n",
      "step: 39841\n",
      "loss: 12.227154731750488\n",
      "steps per second: 0.51444\n",
      "step: 39842\n",
      "loss: 12.771549224853516\n",
      "steps per second: 0.51235\n",
      "step: 39843\n",
      "loss: 12.960953712463379\n",
      "steps per second: 0.51692\n",
      "step: 39844\n",
      "loss: 13.077492713928223\n",
      "steps per second: 0.53947\n",
      "step: 39845\n",
      "loss: 12.817755699157715\n",
      "steps per second: 0.54928\n",
      "step: 39846\n",
      "loss: 12.688340187072754\n",
      "steps per second: 0.55298\n",
      "step: 39847\n",
      "loss: 13.018573760986328\n",
      "steps per second: 0.54002\n",
      "step: 39848\n",
      "loss: 13.015345573425293\n",
      "steps per second: 0.52419\n",
      "step: 39849\n",
      "loss: 13.369959831237793\n",
      "steps per second: 0.51276\n",
      "step: 39850\n",
      "loss: 12.911982536315918\n",
      "steps per second: 0.51372\n",
      "step: 39851\n",
      "loss: 12.888923645019531\n",
      "steps per second: 0.54893\n",
      "step: 39852\n",
      "loss: 12.948317527770996\n",
      "steps per second: 0.57069\n",
      "step: 39853\n",
      "loss: 12.960885047912598\n",
      "steps per second: 0.55001\n",
      "step: 39854\n",
      "loss: 13.530813217163086\n",
      "steps per second: 0.49485\n",
      "step: 39855\n",
      "loss: 12.277039527893066\n",
      "steps per second: 0.54061\n",
      "step: 39856\n",
      "loss: 12.594350814819336\n",
      "steps per second: 0.54082\n",
      "step: 39857\n",
      "loss: 12.69978141784668\n",
      "steps per second: 0.53105\n",
      "step: 39858\n",
      "loss: 12.177687644958496\n",
      "steps per second: 0.56219\n",
      "step: 39859\n",
      "loss: 12.85561466217041\n",
      "steps per second: 0.53744\n",
      "step: 39860\n",
      "loss: 12.89806079864502\n",
      "steps per second: 0.51117\n",
      "step: 39861\n",
      "loss: 12.607386589050293\n",
      "steps per second: 0.60334\n",
      "step: 39862\n",
      "loss: 12.893656730651855\n",
      "steps per second: 0.54257\n",
      "step: 39863\n",
      "loss: 12.57526683807373\n",
      "steps per second: 0.53935\n",
      "step: 39864\n",
      "loss: 12.782959938049316\n",
      "steps per second: 0.57395\n",
      "step: 39865\n",
      "loss: 12.808900833129883\n",
      "steps per second: 0.51739\n",
      "step: 39866\n",
      "loss: 12.681324005126953\n",
      "steps per second: 0.51435\n",
      "step: 39867\n",
      "loss: 12.450480461120605\n",
      "steps per second: 0.51442\n",
      "step: 39868\n",
      "loss: 12.93723201751709\n",
      "steps per second: 0.55191\n",
      "step: 39869\n",
      "loss: 13.067230224609375\n",
      "steps per second: 0.53182\n",
      "step: 39870\n",
      "loss: 12.762788772583008\n",
      "steps per second: 0.52928\n",
      "step: 39871\n",
      "loss: 12.873567581176758\n",
      "steps per second: 0.54790\n",
      "step: 39872\n",
      "loss: 12.78438663482666\n",
      "steps per second: 0.51543\n",
      "step: 39873\n",
      "loss: 12.483895301818848\n",
      "steps per second: 0.54444\n",
      "step: 39874\n",
      "loss: 12.421038627624512\n",
      "steps per second: 0.56135\n",
      "step: 39875\n",
      "loss: 12.995985984802246\n",
      "steps per second: 0.54229\n",
      "step: 39876\n",
      "loss: 12.70683765411377\n",
      "steps per second: 0.51562\n",
      "step: 39877\n",
      "loss: 12.795843124389648\n",
      "steps per second: 0.50753\n",
      "step: 39878\n",
      "loss: 12.815072059631348\n",
      "steps per second: 0.51720\n",
      "step: 39879\n",
      "loss: 12.498132705688477\n",
      "steps per second: 0.56175\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8020403981208801, layer: 10\n",
      "saving at step 39879\n",
      "----------\n",
      "\n",
      "\n",
      "step: 39880\n",
      "loss: 13.499978065490723\n",
      "steps per second: 0.27961\n",
      "step: 39881\n",
      "loss: 13.374687194824219\n",
      "steps per second: 0.56172\n",
      "step: 39882\n",
      "loss: 13.376750946044922\n",
      "steps per second: 0.53163\n",
      "step: 39883\n",
      "loss: 13.056427955627441\n",
      "steps per second: 0.49488\n",
      "step: 39884\n",
      "loss: 12.195619583129883\n",
      "steps per second: 0.56918\n",
      "step: 39885\n",
      "loss: 12.589184761047363\n",
      "steps per second: 0.53188\n",
      "step: 39886\n",
      "loss: 12.757987022399902\n",
      "steps per second: 0.52464\n",
      "step: 39887\n",
      "loss: 13.322370529174805\n",
      "steps per second: 0.55847\n",
      "step: 39888\n",
      "loss: 12.899186134338379\n",
      "steps per second: 0.56380\n",
      "step: 39889\n",
      "loss: 12.420917510986328\n",
      "steps per second: 0.51227\n",
      "step: 39890\n",
      "loss: 12.189682960510254\n",
      "steps per second: 0.59963\n",
      "step: 39891\n",
      "loss: 12.291221618652344\n",
      "steps per second: 0.48670\n",
      "step: 39892\n",
      "loss: 12.794053077697754\n",
      "steps per second: 0.51807\n",
      "step: 39893\n",
      "loss: 13.135530471801758\n",
      "steps per second: 0.51377\n",
      "step: 39894\n",
      "loss: 13.377425193786621\n",
      "steps per second: 0.54299\n",
      "step: 39895\n",
      "loss: 12.825186729431152\n",
      "steps per second: 0.54147\n",
      "step: 39896\n",
      "loss: 13.017241477966309\n",
      "steps per second: 0.56868\n",
      "step: 39897\n",
      "loss: 12.494657516479492\n",
      "steps per second: 0.55957\n",
      "step: 39898\n",
      "loss: 12.222421646118164\n",
      "steps per second: 0.56656\n",
      "step: 39899\n",
      "loss: 12.4158935546875\n",
      "steps per second: 0.52610\n",
      "step: 39900\n",
      "loss: 12.710671424865723\n",
      "steps per second: 0.51315\n",
      "step: 39901\n",
      "loss: 12.316153526306152\n",
      "steps per second: 0.54421\n",
      "step: 39902\n",
      "loss: 12.409761428833008\n",
      "steps per second: 0.57445\n",
      "step: 39903\n",
      "loss: 12.847962379455566\n",
      "steps per second: 0.49742\n",
      "step: 39904\n",
      "loss: 12.878181457519531\n",
      "steps per second: 0.54173\n",
      "step: 39905\n",
      "loss: 12.154138565063477\n",
      "steps per second: 0.53305\n",
      "step: 39906\n",
      "loss: 12.928719520568848\n",
      "steps per second: 0.53365\n",
      "step: 39907\n",
      "loss: 12.918275833129883\n",
      "steps per second: 0.52067\n",
      "step: 39908\n",
      "loss: 12.924243927001953\n",
      "steps per second: 0.53990\n",
      "step: 39909\n",
      "loss: 12.404163360595703\n",
      "steps per second: 0.57564\n",
      "step: 39910\n",
      "loss: 12.910017013549805\n",
      "steps per second: 0.53537\n",
      "step: 39911\n",
      "loss: 12.751392364501953\n",
      "steps per second: 0.51808\n",
      "step: 39912\n",
      "loss: 12.726609230041504\n",
      "steps per second: 0.54420\n",
      "step: 39913\n",
      "loss: 12.851966857910156\n",
      "steps per second: 0.56977\n",
      "step: 39914\n",
      "loss: 12.66200065612793\n",
      "steps per second: 0.51353\n",
      "step: 39915\n",
      "loss: 13.048375129699707\n",
      "steps per second: 0.52999\n",
      "step: 39916\n",
      "loss: 12.653940200805664\n",
      "steps per second: 0.57365\n",
      "step: 39917\n",
      "loss: 12.95547866821289\n",
      "steps per second: 0.58210\n",
      "step: 39918\n",
      "loss: 13.4043550491333\n",
      "steps per second: 0.51955\n",
      "step: 39919\n",
      "loss: 12.652216911315918\n",
      "steps per second: 0.55793\n",
      "step: 39920\n",
      "loss: 12.66985034942627\n",
      "steps per second: 0.60824\n",
      "step: 39921\n",
      "loss: 13.171175956726074\n",
      "steps per second: 0.57240\n",
      "step: 39922\n",
      "loss: 12.677630424499512\n",
      "steps per second: 0.55526\n",
      "step: 39923\n",
      "loss: 13.101165771484375\n",
      "steps per second: 0.56946\n",
      "step: 39924\n",
      "loss: 12.578990936279297\n",
      "steps per second: 0.46792\n",
      "step: 39925\n",
      "loss: 12.97014331817627\n",
      "steps per second: 0.50962\n",
      "step: 39926\n",
      "loss: 12.452668190002441\n",
      "steps per second: 0.52957\n",
      "step: 39927\n",
      "loss: 12.82990550994873\n",
      "steps per second: 0.51765\n",
      "step: 39928\n",
      "loss: 12.737654685974121\n",
      "steps per second: 0.51571\n",
      "step: 39929\n",
      "loss: 13.205414772033691\n",
      "steps per second: 0.58458\n",
      "step: 39930\n",
      "loss: 13.133610725402832\n",
      "steps per second: 0.54268\n",
      "step: 39931\n",
      "loss: 13.084028244018555\n",
      "steps per second: 0.50650\n",
      "step: 39932\n",
      "loss: 12.725361824035645\n",
      "steps per second: 0.55644\n",
      "step: 39933\n",
      "loss: 12.86978530883789\n",
      "steps per second: 0.56769\n",
      "step: 39934\n",
      "loss: 12.586419105529785\n",
      "steps per second: 0.56860\n",
      "step: 39935\n",
      "loss: 13.025890350341797\n",
      "steps per second: 0.55195\n",
      "step: 39936\n",
      "loss: 12.862427711486816\n",
      "steps per second: 0.53573\n",
      "step: 39937\n",
      "loss: 12.266071319580078\n",
      "steps per second: 0.51860\n",
      "step: 39938\n",
      "loss: 12.932296752929688\n",
      "steps per second: 0.47780\n",
      "step: 39939\n",
      "loss: 12.8451509475708\n",
      "steps per second: 0.53026\n",
      "step: 39940\n",
      "loss: 12.681270599365234\n",
      "steps per second: 0.52370\n",
      "step: 39941\n",
      "loss: 12.720621109008789\n",
      "steps per second: 0.48095\n",
      "step: 39942\n",
      "loss: 12.898294448852539\n",
      "steps per second: 0.52070\n",
      "step: 39943\n",
      "loss: 12.801443099975586\n",
      "steps per second: 0.60612\n",
      "step: 39944\n",
      "loss: 12.826112747192383\n",
      "steps per second: 0.53419\n",
      "step: 39945\n",
      "loss: 12.804215431213379\n",
      "steps per second: 0.56502\n",
      "step: 39946\n",
      "loss: 12.619551658630371\n",
      "steps per second: 0.51774\n",
      "step: 39947\n",
      "loss: 12.992874145507812\n",
      "steps per second: 0.51432\n",
      "step: 39948\n",
      "loss: 12.380891799926758\n",
      "steps per second: 0.52694\n",
      "step: 39949\n",
      "loss: 13.04306411743164\n",
      "steps per second: 0.53704\n",
      "step: 39950\n",
      "loss: 12.804250717163086\n",
      "steps per second: 0.55288\n",
      "step: 39951\n",
      "loss: 12.13620662689209\n",
      "steps per second: 0.52176\n",
      "step: 39952\n",
      "loss: 13.030865669250488\n",
      "steps per second: 0.54603\n",
      "step: 39953\n",
      "loss: 12.487505912780762\n",
      "steps per second: 0.59248\n",
      "step: 39954\n",
      "loss: 12.51251220703125\n",
      "steps per second: 0.55699\n",
      "step: 39955\n",
      "loss: 12.181550979614258\n",
      "steps per second: 0.53419\n",
      "step: 39956\n",
      "loss: 13.155364036560059\n",
      "steps per second: 0.55365\n",
      "step: 39957\n",
      "loss: 12.475292205810547\n",
      "steps per second: 0.54135\n",
      "step: 39958\n",
      "loss: 13.023585319519043\n",
      "steps per second: 0.51675\n",
      "step: 39959\n",
      "loss: 13.220199584960938\n",
      "steps per second: 0.56956\n",
      "step: 39960\n",
      "loss: 13.342789649963379\n",
      "steps per second: 0.53457\n",
      "step: 39961\n",
      "loss: 12.523704528808594\n",
      "steps per second: 0.53919\n",
      "step: 39962\n",
      "loss: 12.650945663452148\n",
      "steps per second: 0.54745\n",
      "step: 39963\n",
      "loss: 12.76978874206543\n",
      "steps per second: 0.54522\n",
      "step: 39964\n",
      "loss: 12.690993309020996\n",
      "steps per second: 0.54051\n",
      "step: 39965\n",
      "loss: 12.509201049804688\n",
      "steps per second: 0.56467\n",
      "step: 39966\n",
      "loss: 13.055228233337402\n",
      "steps per second: 0.55999\n",
      "step: 39967\n",
      "loss: 13.099812507629395\n",
      "steps per second: 0.57200\n",
      "step: 39968\n",
      "loss: 12.50787353515625\n",
      "steps per second: 0.56154\n",
      "step: 39969\n",
      "loss: 12.981432914733887\n",
      "steps per second: 0.54398\n",
      "step: 39970\n",
      "loss: 13.0669584274292\n",
      "steps per second: 0.54892\n",
      "step: 39971\n",
      "loss: 13.011022567749023\n",
      "steps per second: 0.51278\n",
      "step: 39972\n",
      "loss: 12.869695663452148\n",
      "steps per second: 0.56284\n",
      "step: 39973\n",
      "loss: 12.25186824798584\n",
      "steps per second: 0.53283\n",
      "step: 39974\n",
      "loss: 13.106310844421387\n",
      "steps per second: 0.58426\n",
      "step: 39975\n",
      "loss: 13.203129768371582\n",
      "steps per second: 0.56394\n",
      "step: 39976\n",
      "loss: 12.603407859802246\n",
      "steps per second: 0.51697\n",
      "step: 39977\n",
      "loss: 11.919222831726074\n",
      "steps per second: 0.57438\n",
      "step: 39978\n",
      "loss: 12.843147277832031\n",
      "steps per second: 0.55617\n",
      "step: 39979\n",
      "loss: 12.883179664611816\n",
      "steps per second: 0.57706\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8235551714897156, layer: 11\n",
      "saving at step 39979\n",
      "----------\n",
      "\n",
      "\n",
      "step: 39980\n",
      "loss: 12.45949649810791\n",
      "steps per second: 0.29523\n",
      "step: 39981\n",
      "loss: 12.653912544250488\n",
      "steps per second: 0.54689\n",
      "step: 39982\n",
      "loss: 13.597431182861328\n",
      "steps per second: 0.54136\n",
      "step: 39983\n",
      "loss: 12.711153984069824\n",
      "steps per second: 0.53372\n",
      "step: 39984\n",
      "loss: 13.009782791137695\n",
      "steps per second: 0.54016\n",
      "step: 39985\n",
      "loss: 12.925997734069824\n",
      "steps per second: 0.52608\n",
      "step: 39986\n",
      "loss: 13.154126167297363\n",
      "steps per second: 0.54887\n",
      "step: 39987\n",
      "loss: 12.814448356628418\n",
      "steps per second: 0.56981\n",
      "step: 39988\n",
      "loss: 12.837760925292969\n",
      "steps per second: 0.56512\n",
      "step: 39989\n",
      "loss: 12.124649047851562\n",
      "steps per second: 0.52842\n",
      "step: 39990\n",
      "loss: 13.151565551757812\n",
      "steps per second: 0.55821\n",
      "step: 39991\n",
      "loss: 12.867406845092773\n",
      "steps per second: 0.57638\n",
      "step: 39992\n",
      "loss: 13.067602157592773\n",
      "steps per second: 0.58397\n",
      "step: 39993\n",
      "loss: 12.855231285095215\n",
      "steps per second: 0.57934\n",
      "step: 39994\n",
      "loss: 13.01758861541748\n",
      "steps per second: 0.55204\n",
      "step: 39995\n",
      "loss: 13.164642333984375\n",
      "steps per second: 0.55229\n",
      "step: 39996\n",
      "loss: 13.07628345489502\n",
      "steps per second: 0.55050\n",
      "step: 39997\n",
      "loss: 12.414173126220703\n",
      "steps per second: 0.50301\n",
      "step: 39998\n",
      "loss: 12.538578987121582\n",
      "steps per second: 0.57066\n",
      "step: 39999\n",
      "loss: 13.196395874023438\n",
      "steps per second: 0.53928\n",
      "step: 40000\n",
      "loss: 12.629560470581055\n",
      "steps per second: 0.53571\n",
      "step: 40001\n",
      "loss: 12.479850769042969\n",
      "steps per second: 0.57260\n",
      "step: 40002\n",
      "loss: 12.61010456085205\n",
      "steps per second: 0.52293\n",
      "step: 40003\n",
      "loss: 12.58505916595459\n",
      "steps per second: 0.57071\n",
      "step: 40004\n",
      "loss: 13.344747543334961\n",
      "steps per second: 0.57338\n",
      "step: 40005\n",
      "loss: 12.857430458068848\n",
      "steps per second: 0.54571\n",
      "step: 40006\n",
      "loss: 12.369963645935059\n",
      "steps per second: 0.56143\n",
      "step: 40007\n",
      "loss: 12.294816970825195\n",
      "steps per second: 0.56279\n",
      "step: 40008\n",
      "loss: 12.689262390136719\n",
      "steps per second: 0.55880\n",
      "step: 40009\n",
      "loss: 13.00423812866211\n",
      "steps per second: 0.57112\n",
      "step: 40010\n",
      "loss: 13.323973655700684\n",
      "steps per second: 0.55599\n",
      "step: 40011\n",
      "loss: 12.790872573852539\n",
      "steps per second: 0.54998\n",
      "step: 40012\n",
      "loss: 12.402799606323242\n",
      "steps per second: 0.55759\n",
      "step: 40013\n",
      "loss: 12.694904327392578\n",
      "steps per second: 0.51428\n",
      "step: 40014\n",
      "loss: 12.963802337646484\n",
      "steps per second: 0.53265\n",
      "step: 40015\n",
      "loss: 13.692727088928223\n",
      "steps per second: 0.53973\n",
      "step: 40016\n",
      "loss: 12.649925231933594\n",
      "steps per second: 0.55147\n",
      "step: 40017\n",
      "loss: 12.70374870300293\n",
      "steps per second: 0.57593\n",
      "step: 40018\n",
      "loss: 12.756948471069336\n",
      "steps per second: 0.55474\n",
      "step: 40019\n",
      "loss: 12.463061332702637\n",
      "steps per second: 0.54519\n",
      "step: 40020\n",
      "loss: 12.803387641906738\n",
      "steps per second: 0.61490\n",
      "step: 40021\n",
      "loss: 12.66721248626709\n",
      "steps per second: 0.51300\n",
      "step: 40022\n",
      "loss: 13.460063934326172\n",
      "steps per second: 0.54824\n",
      "step: 40023\n",
      "loss: 12.886422157287598\n",
      "steps per second: 0.56627\n",
      "step: 40024\n",
      "loss: 12.762972831726074\n",
      "steps per second: 0.53723\n",
      "step: 40025\n",
      "loss: 13.173812866210938\n",
      "steps per second: 0.51447\n",
      "step: 40026\n",
      "loss: 12.678271293640137\n",
      "steps per second: 0.54809\n",
      "step: 40027\n",
      "loss: 13.136617660522461\n",
      "steps per second: 0.57000\n",
      "step: 40028\n",
      "loss: 12.960687637329102\n",
      "steps per second: 0.55702\n",
      "step: 40029\n",
      "loss: 12.930622100830078\n",
      "steps per second: 0.56803\n",
      "step: 40030\n",
      "loss: 12.588850975036621\n",
      "steps per second: 0.49430\n",
      "step: 40031\n",
      "loss: 12.878938674926758\n",
      "steps per second: 0.57872\n",
      "step: 40032\n",
      "loss: 12.779242515563965\n",
      "steps per second: 0.51220\n",
      "step: 40033\n",
      "loss: 13.135513305664062\n",
      "steps per second: 0.52406\n",
      "step: 40034\n",
      "loss: 12.630002975463867\n",
      "steps per second: 0.52201\n",
      "step: 40035\n",
      "loss: 12.838364601135254\n",
      "steps per second: 0.53845\n",
      "step: 40036\n",
      "loss: 12.529196739196777\n",
      "steps per second: 0.56797\n",
      "step: 40037\n",
      "loss: 12.785416603088379\n",
      "steps per second: 0.53699\n",
      "step: 40038\n",
      "loss: 12.887613296508789\n",
      "steps per second: 0.58196\n",
      "step: 40039\n",
      "loss: 12.455757141113281\n",
      "steps per second: 0.52223\n",
      "step: 40040\n",
      "loss: 12.302170753479004\n",
      "steps per second: 0.51928\n",
      "step: 40041\n",
      "loss: 12.883245468139648\n",
      "steps per second: 0.53986\n",
      "step: 40042\n",
      "loss: 12.638089179992676\n",
      "steps per second: 0.55936\n",
      "step: 40043\n",
      "loss: 12.695138931274414\n",
      "steps per second: 0.61019\n",
      "step: 40044\n",
      "loss: 12.933645248413086\n",
      "steps per second: 0.54243\n",
      "step: 40045\n",
      "loss: 12.788797378540039\n",
      "steps per second: 0.50194\n",
      "step: 40046\n",
      "loss: 12.7453031539917\n",
      "steps per second: 0.57711\n",
      "step: 40047\n",
      "loss: 12.983160018920898\n",
      "steps per second: 0.61769\n",
      "step: 40048\n",
      "loss: 12.551029205322266\n",
      "steps per second: 0.54275\n",
      "step: 40049\n",
      "loss: 13.159219741821289\n",
      "steps per second: 0.55062\n",
      "step: 40050\n",
      "loss: 12.829113006591797\n",
      "steps per second: 0.55292\n",
      "step: 40051\n",
      "loss: 12.194409370422363\n",
      "steps per second: 0.57866\n",
      "step: 40052\n",
      "loss: 12.58684253692627\n",
      "steps per second: 0.57591\n",
      "step: 40053\n",
      "loss: 13.420473098754883\n",
      "steps per second: 0.55814\n",
      "step: 40054\n",
      "loss: 12.692791938781738\n",
      "steps per second: 0.52185\n",
      "step: 40055\n",
      "loss: 12.78631591796875\n",
      "steps per second: 0.49707\n",
      "step: 40056\n",
      "loss: 12.680155754089355\n",
      "steps per second: 0.56936\n",
      "step: 40057\n",
      "loss: 13.233613014221191\n",
      "steps per second: 0.53211\n",
      "step: 40058\n",
      "loss: 12.75285816192627\n",
      "steps per second: 0.53819\n",
      "step: 40059\n",
      "loss: 12.868072509765625\n",
      "steps per second: 0.58870\n",
      "step: 40060\n",
      "loss: 12.67036247253418\n",
      "steps per second: 0.51524\n",
      "step: 40061\n",
      "loss: 13.01917839050293\n",
      "steps per second: 0.59918\n",
      "step: 40062\n",
      "loss: 12.829706192016602\n",
      "steps per second: 0.51630\n",
      "step: 40063\n",
      "loss: 12.365898132324219\n",
      "steps per second: 0.50324\n",
      "step: 40064\n",
      "loss: 12.820953369140625\n",
      "steps per second: 0.52918\n",
      "step: 40065\n",
      "loss: 12.676493644714355\n",
      "steps per second: 0.52325\n",
      "step: 40066\n",
      "loss: 12.696136474609375\n",
      "steps per second: 0.55443\n",
      "step: 40067\n",
      "loss: 12.830315589904785\n",
      "steps per second: 0.53684\n",
      "step: 40068\n",
      "loss: 12.743316650390625\n",
      "steps per second: 0.55404\n",
      "step: 40069\n",
      "loss: 12.404967308044434\n",
      "steps per second: 0.55626\n",
      "step: 40070\n",
      "loss: 12.83549690246582\n",
      "steps per second: 0.46361\n",
      "step: 40071\n",
      "loss: 12.423541069030762\n",
      "steps per second: 0.52280\n",
      "step: 40072\n",
      "loss: 12.443246841430664\n",
      "steps per second: 0.49284\n",
      "step: 40073\n",
      "loss: 12.864106178283691\n",
      "steps per second: 0.51709\n",
      "step: 40074\n",
      "loss: 12.92435359954834\n",
      "steps per second: 0.54602\n",
      "step: 40075\n",
      "loss: 12.808613777160645\n",
      "steps per second: 0.54777\n",
      "step: 40076\n",
      "loss: 13.130568504333496\n",
      "steps per second: 0.52014\n",
      "step: 40077\n",
      "loss: 13.15958309173584\n",
      "steps per second: 0.51030\n",
      "step: 40078\n",
      "loss: 12.963214874267578\n",
      "steps per second: 0.53568\n",
      "step: 40079\n",
      "loss: 12.840596199035645\n",
      "steps per second: 0.52822\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8444371223449707, layer: 11\n",
      "saving at step 40079\n",
      "----------\n",
      "\n",
      "\n",
      "step: 40080\n",
      "loss: 12.912440299987793\n",
      "steps per second: 0.26284\n",
      "step: 40081\n",
      "loss: 13.267021179199219\n",
      "steps per second: 0.54898\n",
      "step: 40082\n",
      "loss: 12.41523265838623\n",
      "steps per second: 0.53645\n",
      "step: 40083\n",
      "loss: 12.505098342895508\n",
      "steps per second: 0.56405\n",
      "step: 40084\n",
      "loss: 12.762616157531738\n",
      "steps per second: 0.53663\n",
      "step: 40085\n",
      "loss: 12.758600234985352\n",
      "steps per second: 0.53451\n",
      "step: 40086\n",
      "loss: 12.86756420135498\n",
      "steps per second: 0.55368\n",
      "step: 40087\n",
      "loss: 12.45464038848877\n",
      "steps per second: 0.51528\n",
      "step: 40088\n",
      "loss: 12.462510108947754\n",
      "steps per second: 0.51999\n",
      "step: 40089\n",
      "loss: 12.526350975036621\n",
      "steps per second: 0.54992\n",
      "step: 40090\n",
      "loss: 13.080835342407227\n",
      "steps per second: 0.56120\n",
      "step: 40091\n",
      "loss: 12.627677917480469\n",
      "steps per second: 0.53085\n",
      "step: 40092\n",
      "loss: 12.295980453491211\n",
      "steps per second: 0.57979\n",
      "step: 40093\n",
      "loss: 12.758918762207031\n",
      "steps per second: 0.57896\n",
      "step: 40094\n",
      "loss: 12.953222274780273\n",
      "steps per second: 0.56727\n",
      "step: 40095\n",
      "loss: 13.086276054382324\n",
      "steps per second: 0.54293\n",
      "step: 40096\n",
      "loss: 12.258028984069824\n",
      "steps per second: 0.56795\n",
      "step: 40097\n",
      "loss: 12.693511009216309\n",
      "steps per second: 0.57916\n",
      "step: 40098\n",
      "loss: 12.92202091217041\n",
      "steps per second: 0.53578\n",
      "step: 40099\n",
      "loss: 12.471177101135254\n",
      "steps per second: 0.54513\n",
      "step: 40100\n",
      "loss: 12.901557922363281\n",
      "steps per second: 0.56319\n",
      "step: 40101\n",
      "loss: 13.157652854919434\n",
      "steps per second: 0.56046\n",
      "step: 40102\n",
      "loss: 12.403218269348145\n",
      "steps per second: 0.57308\n",
      "step: 40103\n",
      "loss: 12.686038970947266\n",
      "steps per second: 0.51683\n",
      "step: 40104\n",
      "loss: 12.805787086486816\n",
      "steps per second: 0.54926\n",
      "step: 40105\n",
      "loss: 12.804024696350098\n",
      "steps per second: 0.55949\n",
      "step: 40106\n",
      "loss: 12.797504425048828\n",
      "steps per second: 0.50457\n",
      "step: 40107\n",
      "loss: 13.054100036621094\n",
      "steps per second: 0.55083\n",
      "step: 40108\n",
      "loss: 13.083581924438477\n",
      "steps per second: 0.51770\n",
      "step: 40109\n",
      "loss: 12.890499114990234\n",
      "steps per second: 0.56067\n",
      "step: 40110\n",
      "loss: 13.054079055786133\n",
      "steps per second: 0.53862\n",
      "step: 40111\n",
      "loss: 12.73167896270752\n",
      "steps per second: 0.47234\n",
      "step: 40112\n",
      "loss: 12.414299964904785\n",
      "steps per second: 0.42023\n",
      "step: 40113\n",
      "loss: 12.641087532043457\n",
      "steps per second: 0.54419\n",
      "step: 40114\n",
      "loss: 13.204367637634277\n",
      "steps per second: 0.51737\n",
      "step: 40115\n",
      "loss: 13.211894989013672\n",
      "steps per second: 0.57056\n",
      "step: 40116\n",
      "loss: 12.267498016357422\n",
      "steps per second: 0.57789\n",
      "step: 40117\n",
      "loss: 12.803994178771973\n",
      "steps per second: 0.56710\n",
      "step: 40118\n",
      "loss: 12.780736923217773\n",
      "steps per second: 0.53205\n",
      "step: 40119\n",
      "loss: 12.27143669128418\n",
      "steps per second: 0.57033\n",
      "step: 40120\n",
      "loss: 13.213932991027832\n",
      "steps per second: 0.56054\n",
      "step: 40121\n",
      "loss: 13.159688949584961\n",
      "steps per second: 0.57569\n",
      "step: 40122\n",
      "loss: 12.936833381652832\n",
      "steps per second: 0.57187\n",
      "step: 40123\n",
      "loss: 13.061707496643066\n",
      "steps per second: 0.55406\n",
      "step: 40124\n",
      "loss: 12.379854202270508\n",
      "steps per second: 0.60873\n",
      "step: 40125\n",
      "loss: 12.702387809753418\n",
      "steps per second: 0.53598\n",
      "step: 40126\n",
      "loss: 12.805829048156738\n",
      "steps per second: 0.53138\n",
      "step: 40127\n",
      "loss: 12.562043190002441\n",
      "steps per second: 0.50553\n",
      "step: 40128\n",
      "loss: 12.403797149658203\n",
      "steps per second: 0.53779\n",
      "step: 40129\n",
      "loss: 12.710039138793945\n",
      "steps per second: 0.40162\n",
      "step: 40130\n",
      "loss: 13.166229248046875\n",
      "steps per second: 0.49610\n",
      "step: 40131\n",
      "loss: 13.138092041015625\n",
      "steps per second: 0.49397\n",
      "step: 40132\n",
      "loss: 12.580280303955078\n",
      "steps per second: 0.52536\n",
      "step: 40133\n",
      "loss: 13.127448081970215\n",
      "steps per second: 0.46524\n",
      "step: 40134\n",
      "loss: 12.553413391113281\n",
      "steps per second: 0.48567\n",
      "step: 40135\n",
      "loss: 12.911785125732422\n",
      "steps per second: 0.52098\n",
      "step: 40136\n",
      "loss: 12.409823417663574\n",
      "steps per second: 0.53674\n",
      "step: 40137\n",
      "loss: 12.54451847076416\n",
      "steps per second: 0.52794\n",
      "step: 40138\n",
      "loss: 12.415074348449707\n",
      "steps per second: 0.54031\n",
      "step: 40139\n",
      "loss: 13.146210670471191\n",
      "steps per second: 0.49286\n",
      "step: 40140\n",
      "loss: 12.701778411865234\n",
      "steps per second: 0.53546\n",
      "step: 40141\n",
      "loss: 13.428305625915527\n",
      "steps per second: 0.52175\n",
      "step: 40142\n",
      "loss: 12.789762496948242\n",
      "steps per second: 0.50594\n",
      "step: 40143\n",
      "loss: 12.72375774383545\n",
      "steps per second: 0.49836\n",
      "step: 40144\n",
      "loss: 12.6170072555542\n",
      "steps per second: 0.52726\n",
      "step: 40145\n",
      "loss: 13.026182174682617\n",
      "steps per second: 0.52076\n",
      "step: 40146\n",
      "loss: 12.353594779968262\n",
      "steps per second: 0.51657\n",
      "step: 40147\n",
      "loss: 12.732027053833008\n",
      "steps per second: 0.50696\n",
      "step: 40148\n",
      "loss: 13.359965324401855\n",
      "steps per second: 0.50612\n",
      "step: 40149\n",
      "loss: 12.796319007873535\n",
      "steps per second: 0.48212\n",
      "step: 40150\n",
      "loss: 12.49756145477295\n",
      "steps per second: 0.52238\n",
      "step: 40151\n",
      "loss: 12.358663558959961\n",
      "steps per second: 0.53870\n",
      "step: 40152\n",
      "loss: 12.763752937316895\n",
      "steps per second: 0.51452\n",
      "step: 40153\n",
      "loss: 12.61330795288086\n",
      "steps per second: 0.53203\n",
      "step: 40154\n",
      "loss: 12.528871536254883\n",
      "steps per second: 0.60793\n",
      "step: 40155\n",
      "loss: 13.232171058654785\n",
      "steps per second: 0.55437\n",
      "step: 40156\n",
      "loss: 13.51646900177002\n",
      "steps per second: 0.54567\n",
      "step: 40157\n",
      "loss: 12.908337593078613\n",
      "steps per second: 0.56193\n",
      "step: 40158\n",
      "loss: 12.693352699279785\n",
      "steps per second: 0.60190\n",
      "step: 40159\n",
      "loss: 12.84671688079834\n",
      "steps per second: 0.52036\n",
      "step: 40160\n",
      "loss: 12.505741119384766\n",
      "steps per second: 0.55492\n",
      "step: 40161\n",
      "loss: 12.808914184570312\n",
      "steps per second: 0.53528\n",
      "step: 40162\n",
      "loss: 13.245064735412598\n",
      "steps per second: 0.53008\n",
      "step: 40163\n",
      "loss: 13.064570426940918\n",
      "steps per second: 0.51862\n",
      "step: 40164\n",
      "loss: 12.970751762390137\n",
      "steps per second: 0.53873\n",
      "step: 40165\n",
      "loss: 13.375823974609375\n",
      "steps per second: 0.59553\n",
      "step: 40166\n",
      "loss: 12.683465003967285\n",
      "steps per second: 0.51984\n",
      "step: 40167\n",
      "loss: 11.924224853515625\n",
      "steps per second: 0.59715\n",
      "step: 40168\n",
      "loss: 12.390288352966309\n",
      "steps per second: 0.55343\n",
      "step: 40169\n",
      "loss: 12.924344062805176\n",
      "steps per second: 0.50870\n",
      "step: 40170\n",
      "loss: 13.00377368927002\n",
      "steps per second: 0.54454\n",
      "step: 40171\n",
      "loss: 13.012069702148438\n",
      "steps per second: 0.52942\n",
      "step: 40172\n",
      "loss: 13.084634780883789\n",
      "steps per second: 0.55447\n",
      "step: 40173\n",
      "loss: 12.764959335327148\n",
      "steps per second: 0.53847\n",
      "step: 40174\n",
      "loss: 12.904047012329102\n",
      "steps per second: 0.52630\n",
      "step: 40175\n",
      "loss: 12.204358100891113\n",
      "steps per second: 0.52111\n",
      "step: 40176\n",
      "loss: 12.900017738342285\n",
      "steps per second: 0.48824\n",
      "step: 40177\n",
      "loss: 12.597070693969727\n",
      "steps per second: 0.55132\n",
      "step: 40178\n",
      "loss: 12.994964599609375\n",
      "steps per second: 0.56137\n",
      "step: 40179\n",
      "loss: 12.85646915435791\n",
      "steps per second: 0.54664\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.857234001159668, layer: 11\n",
      "saving at step 40179\n",
      "----------\n",
      "\n",
      "\n",
      "step: 40180\n",
      "loss: 12.513580322265625\n",
      "steps per second: 0.25584\n",
      "step: 40181\n",
      "loss: 12.218390464782715\n",
      "steps per second: 0.54915\n",
      "step: 40182\n",
      "loss: 12.609884262084961\n",
      "steps per second: 0.52384\n",
      "step: 40183\n",
      "loss: 12.769681930541992\n",
      "steps per second: 0.56673\n",
      "step: 40184\n",
      "loss: 12.617959976196289\n",
      "steps per second: 0.55753\n",
      "step: 40185\n",
      "loss: 12.590994834899902\n",
      "steps per second: 0.53858\n",
      "step: 40186\n",
      "loss: 12.468806266784668\n",
      "steps per second: 0.52584\n",
      "step: 40187\n",
      "loss: 12.680948257446289\n",
      "steps per second: 0.50534\n",
      "step: 40188\n",
      "loss: 12.84998607635498\n",
      "steps per second: 0.55449\n",
      "step: 40189\n",
      "loss: 12.741466522216797\n",
      "steps per second: 0.55796\n",
      "step: 40190\n",
      "loss: 12.977742195129395\n",
      "steps per second: 0.59295\n",
      "step: 40191\n",
      "loss: 12.822443008422852\n",
      "steps per second: 0.48959\n",
      "step: 40192\n",
      "loss: 12.589861869812012\n",
      "steps per second: 0.50008\n",
      "step: 40193\n",
      "loss: 12.786340713500977\n",
      "steps per second: 0.50817\n",
      "step: 40194\n",
      "loss: 12.886344909667969\n",
      "steps per second: 0.51206\n",
      "step: 40195\n",
      "loss: 13.004323959350586\n",
      "steps per second: 0.42920\n",
      "step: 40196\n",
      "loss: 13.144610404968262\n",
      "steps per second: 0.50794\n",
      "step: 40197\n",
      "loss: 13.048890113830566\n",
      "steps per second: 0.49470\n",
      "step: 40198\n",
      "loss: 12.696771621704102\n",
      "steps per second: 0.49458\n",
      "step: 40199\n",
      "loss: 12.835161209106445\n",
      "steps per second: 0.54167\n",
      "step: 40200\n",
      "loss: 12.456171035766602\n",
      "steps per second: 0.49309\n",
      "step: 40201\n",
      "loss: 12.716861724853516\n",
      "steps per second: 0.50904\n",
      "step: 40202\n",
      "loss: 12.94678020477295\n",
      "steps per second: 0.49598\n",
      "step: 40203\n",
      "loss: 12.376021385192871\n",
      "steps per second: 0.53657\n",
      "step: 40204\n",
      "loss: 12.468832015991211\n",
      "steps per second: 0.57283\n",
      "step: 40205\n",
      "loss: 13.023661613464355\n",
      "steps per second: 0.48847\n",
      "step: 40206\n",
      "loss: 12.468037605285645\n",
      "steps per second: 0.54754\n",
      "step: 40207\n",
      "loss: 12.366311073303223\n",
      "steps per second: 0.53691\n",
      "step: 40208\n",
      "loss: 12.600306510925293\n",
      "steps per second: 0.57148\n",
      "step: 40209\n",
      "loss: 12.238752365112305\n",
      "steps per second: 0.54508\n",
      "step: 40210\n",
      "loss: 12.392718315124512\n",
      "steps per second: 0.53721\n",
      "step: 40211\n",
      "loss: 12.581873893737793\n",
      "steps per second: 0.61143\n",
      "step: 40212\n",
      "loss: 12.533989906311035\n",
      "steps per second: 0.58044\n",
      "step: 40213\n",
      "loss: 12.410486221313477\n",
      "steps per second: 0.54250\n",
      "step: 40214\n",
      "loss: 12.639083862304688\n",
      "steps per second: 0.50828\n",
      "step: 40215\n",
      "loss: 12.711902618408203\n",
      "steps per second: 0.53343\n",
      "step: 40216\n",
      "loss: 12.895475387573242\n",
      "steps per second: 0.52050\n",
      "step: 40217\n",
      "loss: 12.875887870788574\n",
      "steps per second: 0.55590\n",
      "step: 40218\n",
      "loss: 12.900238990783691\n",
      "steps per second: 0.52628\n",
      "step: 40219\n",
      "loss: 12.741829872131348\n",
      "steps per second: 0.61360\n",
      "step: 40220\n",
      "loss: 12.800405502319336\n",
      "steps per second: 0.51527\n",
      "step: 40221\n",
      "loss: 13.053736686706543\n",
      "steps per second: 0.55126\n",
      "step: 40222\n",
      "loss: 12.591346740722656\n",
      "steps per second: 0.54071\n",
      "step: 40223\n",
      "loss: 12.983746528625488\n",
      "steps per second: 0.49905\n",
      "step: 40224\n",
      "loss: 12.490904808044434\n",
      "steps per second: 0.53288\n",
      "step: 40225\n",
      "loss: 12.66073226928711\n",
      "steps per second: 0.55109\n",
      "step: 40226\n",
      "loss: 13.062169075012207\n",
      "steps per second: 0.57849\n",
      "step: 40227\n",
      "loss: 12.82223129272461\n",
      "steps per second: 0.57416\n",
      "step: 40228\n",
      "loss: 12.89013957977295\n",
      "steps per second: 0.50256\n",
      "step: 40229\n",
      "loss: 12.995429992675781\n",
      "steps per second: 0.47026\n",
      "step: 40230\n",
      "loss: 13.15908145904541\n",
      "steps per second: 0.54213\n",
      "step: 40231\n",
      "loss: 13.040738105773926\n",
      "steps per second: 0.58158\n",
      "step: 40232\n",
      "loss: 12.644251823425293\n",
      "steps per second: 0.55630\n",
      "step: 40233\n",
      "loss: 13.02087688446045\n",
      "steps per second: 0.54256\n",
      "step: 40234\n",
      "loss: 12.719328880310059\n",
      "steps per second: 0.52930\n",
      "step: 40235\n",
      "loss: 12.697032928466797\n",
      "steps per second: 0.51961\n",
      "step: 40236\n",
      "loss: 12.644980430603027\n",
      "steps per second: 0.50287\n",
      "step: 40237\n",
      "loss: 12.551495552062988\n",
      "steps per second: 0.51019\n",
      "step: 40238\n",
      "loss: 13.341885566711426\n",
      "steps per second: 0.54131\n",
      "step: 40239\n",
      "loss: 12.916540145874023\n",
      "steps per second: 0.57696\n",
      "step: 40240\n",
      "loss: 12.924808502197266\n",
      "steps per second: 0.55102\n",
      "step: 40241\n",
      "loss: 12.791254997253418\n",
      "steps per second: 0.52836\n",
      "step: 40242\n",
      "loss: 12.618595123291016\n",
      "steps per second: 0.53619\n",
      "step: 40243\n",
      "loss: 12.96334171295166\n",
      "steps per second: 0.55810\n",
      "step: 40244\n",
      "loss: 12.977702140808105\n",
      "steps per second: 0.53685\n",
      "step: 40245\n",
      "loss: 12.589469909667969\n",
      "steps per second: 0.52717\n",
      "step: 40246\n",
      "loss: 12.800519943237305\n",
      "steps per second: 0.56430\n",
      "step: 40247\n",
      "loss: 13.309118270874023\n",
      "steps per second: 0.58367\n",
      "step: 40248\n",
      "loss: 12.72459602355957\n",
      "steps per second: 0.55231\n",
      "step: 40249\n",
      "loss: 12.643586158752441\n",
      "steps per second: 0.55487\n",
      "step: 40250\n",
      "loss: 12.968423843383789\n",
      "steps per second: 0.55577\n",
      "step: 40251\n",
      "loss: 12.99481201171875\n",
      "steps per second: 0.55984\n",
      "step: 40252\n",
      "loss: 12.76036262512207\n",
      "steps per second: 0.60449\n",
      "step: 40253\n",
      "loss: 12.643308639526367\n",
      "steps per second: 0.56205\n",
      "step: 40254\n",
      "loss: 12.591869354248047\n",
      "steps per second: 0.53966\n",
      "step: 40255\n",
      "loss: 12.569528579711914\n",
      "steps per second: 0.57246\n",
      "step: 40256\n",
      "loss: 13.12869644165039\n",
      "steps per second: 0.56490\n",
      "step: 40257\n",
      "loss: 12.27535343170166\n",
      "steps per second: 0.54318\n",
      "step: 40258\n",
      "loss: 12.850887298583984\n",
      "steps per second: 0.49038\n",
      "step: 40259\n",
      "loss: 13.50158977508545\n",
      "steps per second: 0.54224\n",
      "step: 40260\n",
      "loss: 13.147509574890137\n",
      "steps per second: 0.55178\n",
      "step: 40261\n",
      "loss: 12.18709945678711\n",
      "steps per second: 0.55307\n",
      "step: 40262\n",
      "loss: 12.535097122192383\n",
      "steps per second: 0.54941\n",
      "step: 40263\n",
      "loss: 12.6843900680542\n",
      "steps per second: 0.52765\n",
      "step: 40264\n",
      "loss: 12.4004487991333\n",
      "steps per second: 0.57434\n",
      "step: 40265\n",
      "loss: 13.076119422912598\n",
      "steps per second: 0.55036\n",
      "step: 40266\n",
      "loss: 12.859304428100586\n",
      "steps per second: 0.52833\n",
      "step: 40267\n",
      "loss: 12.400282859802246\n",
      "steps per second: 0.52980\n",
      "step: 40268\n",
      "loss: 12.318172454833984\n",
      "steps per second: 0.52870\n",
      "step: 40269\n",
      "loss: 12.719449043273926\n",
      "steps per second: 0.51977\n",
      "step: 40270\n",
      "loss: 13.251745223999023\n",
      "steps per second: 0.56999\n",
      "step: 40271\n",
      "loss: 12.649511337280273\n",
      "steps per second: 0.51965\n",
      "step: 40272\n",
      "loss: 13.26526927947998\n",
      "steps per second: 0.55794\n",
      "step: 40273\n",
      "loss: 13.240153312683105\n",
      "steps per second: 0.53898\n",
      "step: 40274\n",
      "loss: 12.842513084411621\n",
      "steps per second: 0.53275\n",
      "step: 40275\n",
      "loss: 12.657734870910645\n",
      "steps per second: 0.54111\n",
      "step: 40276\n",
      "loss: 12.975836753845215\n",
      "steps per second: 0.57335\n",
      "step: 40277\n",
      "loss: 11.859045028686523\n",
      "steps per second: 0.55049\n",
      "step: 40278\n",
      "loss: 12.785758018493652\n",
      "steps per second: 0.47945\n",
      "step: 40279\n",
      "loss: 12.912801742553711\n",
      "steps per second: 0.53293\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8401145339012146, layer: 11\n",
      "saving at step 40279\n",
      "----------\n",
      "\n",
      "\n",
      "step: 40280\n",
      "loss: 12.374293327331543\n",
      "steps per second: 0.27459\n",
      "step: 40281\n",
      "loss: 12.671577453613281\n",
      "steps per second: 0.50281\n",
      "step: 40282\n",
      "loss: 13.04394817352295\n",
      "steps per second: 0.54156\n",
      "step: 40283\n",
      "loss: 13.462268829345703\n",
      "steps per second: 0.52608\n",
      "step: 40284\n",
      "loss: 12.477571487426758\n",
      "steps per second: 0.52942\n",
      "step: 40285\n",
      "loss: 12.660004615783691\n",
      "steps per second: 0.53367\n",
      "step: 40286\n",
      "loss: 12.966344833374023\n",
      "steps per second: 0.53490\n",
      "step: 40287\n",
      "loss: 13.149556159973145\n",
      "steps per second: 0.53056\n",
      "step: 40288\n",
      "loss: 13.07839298248291\n",
      "steps per second: 0.45808\n",
      "step: 40289\n",
      "loss: 13.197851181030273\n",
      "steps per second: 0.49397\n",
      "step: 40290\n",
      "loss: 12.877663612365723\n",
      "steps per second: 0.55595\n",
      "step: 40291\n",
      "loss: 13.119889259338379\n",
      "steps per second: 0.55942\n",
      "step: 40292\n",
      "loss: 12.862944602966309\n",
      "steps per second: 0.54716\n",
      "step: 40293\n",
      "loss: 13.062738418579102\n",
      "steps per second: 0.60462\n",
      "step: 40294\n",
      "loss: 12.277417182922363\n",
      "steps per second: 0.57039\n",
      "step: 40295\n",
      "loss: 12.592704772949219\n",
      "steps per second: 0.51206\n",
      "step: 40296\n",
      "loss: 12.496697425842285\n",
      "steps per second: 0.54439\n",
      "step: 40297\n",
      "loss: 13.026433944702148\n",
      "steps per second: 0.59003\n",
      "step: 40298\n",
      "loss: 13.0164213180542\n",
      "steps per second: 0.44227\n",
      "step: 40299\n",
      "loss: 12.745059967041016\n",
      "steps per second: 0.53833\n",
      "step: 40300\n",
      "loss: 12.861767768859863\n",
      "steps per second: 0.53971\n",
      "step: 40301\n",
      "loss: 12.969419479370117\n",
      "steps per second: 0.52430\n",
      "step: 40302\n",
      "loss: 13.189240455627441\n",
      "steps per second: 0.48514\n",
      "step: 40303\n",
      "loss: 12.852124214172363\n",
      "steps per second: 0.54709\n",
      "step: 40304\n",
      "loss: 13.190680503845215\n",
      "steps per second: 0.52483\n",
      "step: 40305\n",
      "loss: 12.986550331115723\n",
      "steps per second: 0.57546\n",
      "step: 40306\n",
      "loss: 12.944644927978516\n",
      "steps per second: 0.54870\n",
      "step: 40307\n",
      "loss: 12.379602432250977\n",
      "steps per second: 0.55267\n",
      "step: 40308\n",
      "loss: 12.407242774963379\n",
      "steps per second: 0.57016\n",
      "step: 40309\n",
      "loss: 12.479486465454102\n",
      "steps per second: 0.54487\n",
      "step: 40310\n",
      "loss: 13.011842727661133\n",
      "steps per second: 0.55249\n",
      "step: 40311\n",
      "loss: 12.949311256408691\n",
      "steps per second: 0.55311\n",
      "step: 40312\n",
      "loss: 13.097549438476562\n",
      "steps per second: 0.56334\n",
      "step: 40313\n",
      "loss: 12.965261459350586\n",
      "steps per second: 0.53070\n",
      "step: 40314\n",
      "loss: 12.447232246398926\n",
      "steps per second: 0.58365\n",
      "step: 40315\n",
      "loss: 12.65073013305664\n",
      "steps per second: 0.55617\n",
      "step: 40316\n",
      "loss: 12.748551368713379\n",
      "steps per second: 0.42181\n",
      "step: 40317\n",
      "loss: 12.999052047729492\n",
      "steps per second: 0.53128\n",
      "step: 40318\n",
      "loss: 12.736845970153809\n",
      "steps per second: 0.56140\n",
      "step: 40319\n",
      "loss: 13.128656387329102\n",
      "steps per second: 0.55354\n",
      "step: 40320\n",
      "loss: 12.770949363708496\n",
      "steps per second: 0.55950\n",
      "step: 40321\n",
      "loss: 12.80024242401123\n",
      "steps per second: 0.52933\n",
      "step: 40322\n",
      "loss: 12.652751922607422\n",
      "steps per second: 0.52406\n",
      "step: 40323\n",
      "loss: 12.560368537902832\n",
      "steps per second: 0.56603\n",
      "step: 40324\n",
      "loss: 12.253426551818848\n",
      "steps per second: 0.52922\n",
      "step: 40325\n",
      "loss: 13.21948528289795\n",
      "steps per second: 0.57729\n",
      "step: 40326\n",
      "loss: 12.633720397949219\n",
      "steps per second: 0.54972\n",
      "step: 40327\n",
      "loss: 13.018562316894531\n",
      "steps per second: 0.53759\n",
      "step: 40328\n",
      "loss: 13.179361343383789\n",
      "steps per second: 0.54919\n",
      "step: 40329\n",
      "loss: 12.174151420593262\n",
      "steps per second: 0.49102\n",
      "step: 40330\n",
      "loss: 13.035574913024902\n",
      "steps per second: 0.53728\n",
      "step: 40331\n",
      "loss: 12.532125473022461\n",
      "steps per second: 0.51346\n",
      "step: 40332\n",
      "loss: 12.356943130493164\n",
      "steps per second: 0.56882\n",
      "step: 40333\n",
      "loss: 13.111726760864258\n",
      "steps per second: 0.56349\n",
      "step: 40334\n",
      "loss: 12.437804222106934\n",
      "steps per second: 0.58138\n",
      "step: 40335\n",
      "loss: 13.355403900146484\n",
      "steps per second: 0.56899\n",
      "step: 40336\n",
      "loss: 12.889689445495605\n",
      "steps per second: 0.50148\n",
      "step: 40337\n",
      "loss: 12.49620532989502\n",
      "steps per second: 0.53034\n",
      "step: 40338\n",
      "loss: 12.819392204284668\n",
      "steps per second: 0.53104\n",
      "step: 40339\n",
      "loss: 12.512343406677246\n",
      "steps per second: 0.53937\n",
      "step: 40340\n",
      "loss: 13.417555809020996\n",
      "steps per second: 0.55437\n",
      "step: 40341\n",
      "loss: 12.541783332824707\n",
      "steps per second: 0.55442\n",
      "step: 40342\n",
      "loss: 12.65284252166748\n",
      "steps per second: 0.54675\n",
      "step: 40343\n",
      "loss: 12.999253273010254\n",
      "steps per second: 0.55230\n",
      "step: 40344\n",
      "loss: 12.700387001037598\n",
      "steps per second: 0.56360\n",
      "step: 40345\n",
      "loss: 13.086182594299316\n",
      "steps per second: 0.56398\n",
      "step: 40346\n",
      "loss: 12.877909660339355\n",
      "steps per second: 0.50480\n",
      "step: 40347\n",
      "loss: 12.959870338439941\n",
      "steps per second: 0.55628\n",
      "step: 40348\n",
      "loss: 12.951009750366211\n",
      "steps per second: 0.53609\n",
      "step: 40349\n",
      "loss: 12.954627990722656\n",
      "steps per second: 0.55524\n",
      "step: 40350\n",
      "loss: 12.912583351135254\n",
      "steps per second: 0.59591\n",
      "step: 40351\n",
      "loss: 12.950213432312012\n",
      "steps per second: 0.54628\n",
      "step: 40352\n",
      "loss: 12.666706085205078\n",
      "steps per second: 0.53589\n",
      "step: 40353\n",
      "loss: 13.088409423828125\n",
      "steps per second: 0.56878\n",
      "step: 40354\n",
      "loss: 12.467865943908691\n",
      "steps per second: 0.53968\n",
      "step: 40355\n",
      "loss: 13.287161827087402\n",
      "steps per second: 0.57738\n",
      "step: 40356\n",
      "loss: 12.794090270996094\n",
      "steps per second: 0.54436\n",
      "step: 40357\n",
      "loss: 13.017036437988281\n",
      "steps per second: 0.55495\n",
      "step: 40358\n",
      "loss: 13.404400825500488\n",
      "steps per second: 0.52370\n",
      "step: 40359\n",
      "loss: 13.060088157653809\n",
      "steps per second: 0.55905\n",
      "step: 40360\n",
      "loss: 12.665562629699707\n",
      "steps per second: 0.58365\n",
      "step: 40361\n",
      "loss: 12.674891471862793\n",
      "steps per second: 0.57252\n",
      "step: 40362\n",
      "loss: 13.415764808654785\n",
      "steps per second: 0.55434\n",
      "step: 40363\n",
      "loss: 12.409151077270508\n",
      "steps per second: 0.53214\n",
      "step: 40364\n",
      "loss: 12.701557159423828\n",
      "steps per second: 0.54414\n",
      "step: 40365\n",
      "loss: 12.762289047241211\n",
      "steps per second: 0.55329\n",
      "step: 40366\n",
      "loss: 12.732097625732422\n",
      "steps per second: 0.52341\n",
      "step: 40367\n",
      "loss: 12.95068073272705\n",
      "steps per second: 0.53616\n",
      "step: 40368\n",
      "loss: 12.76642894744873\n",
      "steps per second: 0.50064\n",
      "step: 40369\n",
      "loss: 12.33826732635498\n",
      "steps per second: 0.49890\n",
      "step: 40370\n",
      "loss: 12.515734672546387\n",
      "steps per second: 0.54908\n",
      "step: 40371\n",
      "loss: 12.89477252960205\n",
      "steps per second: 0.53189\n",
      "step: 40372\n",
      "loss: 12.726038932800293\n",
      "steps per second: 0.54251\n",
      "step: 40373\n",
      "loss: 12.833996772766113\n",
      "steps per second: 0.60858\n",
      "step: 40374\n",
      "loss: 13.12122631072998\n",
      "steps per second: 0.50333\n",
      "step: 40375\n",
      "loss: 12.508089065551758\n",
      "steps per second: 0.54460\n",
      "step: 40376\n",
      "loss: 12.335725784301758\n",
      "steps per second: 0.55510\n",
      "step: 40377\n",
      "loss: 13.019636154174805\n",
      "steps per second: 0.54322\n",
      "step: 40378\n",
      "loss: 13.015777587890625\n",
      "steps per second: 0.56395\n",
      "step: 40379\n",
      "loss: 12.551899909973145\n",
      "steps per second: 0.52907\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8073544502258301, layer: 11\n",
      "saving at step 40379\n",
      "----------\n",
      "\n",
      "\n",
      "step: 40380\n",
      "loss: 12.338266372680664\n",
      "steps per second: 0.26042\n",
      "step: 40381\n",
      "loss: 12.851787567138672\n",
      "steps per second: 0.55060\n",
      "step: 40382\n",
      "loss: 12.56760311126709\n",
      "steps per second: 0.53722\n",
      "step: 40383\n",
      "loss: 12.045254707336426\n",
      "steps per second: 0.53362\n",
      "step: 40384\n",
      "loss: 12.99166202545166\n",
      "steps per second: 0.53705\n",
      "step: 40385\n",
      "loss: 13.31583023071289\n",
      "steps per second: 0.55710\n",
      "step: 40386\n",
      "loss: 13.432090759277344\n",
      "steps per second: 0.57881\n",
      "step: 40387\n",
      "loss: 12.610516548156738\n",
      "steps per second: 0.57585\n",
      "step: 40388\n",
      "loss: 12.488897323608398\n",
      "steps per second: 0.58527\n",
      "step: 40389\n",
      "loss: 12.01077938079834\n",
      "steps per second: 0.55847\n",
      "step: 40390\n",
      "loss: 12.384991645812988\n",
      "steps per second: 0.57736\n",
      "step: 40391\n",
      "loss: 13.317864418029785\n",
      "steps per second: 0.55862\n",
      "step: 40392\n",
      "loss: 12.648565292358398\n",
      "steps per second: 0.54065\n",
      "step: 40393\n",
      "loss: 12.40366268157959\n",
      "steps per second: 0.53150\n",
      "step: 40394\n",
      "loss: 12.675299644470215\n",
      "steps per second: 0.56303\n",
      "step: 40395\n",
      "loss: 13.029640197753906\n",
      "steps per second: 0.51277\n",
      "step: 40396\n",
      "loss: 12.695472717285156\n",
      "steps per second: 0.53729\n",
      "step: 40397\n",
      "loss: 12.601722717285156\n",
      "steps per second: 0.54823\n",
      "step: 40398\n",
      "loss: 12.570219993591309\n",
      "steps per second: 0.58076\n",
      "step: 40399\n",
      "loss: 13.399138450622559\n",
      "steps per second: 0.54790\n",
      "step: 40400\n",
      "loss: 12.231369018554688\n",
      "steps per second: 0.57082\n",
      "step: 40401\n",
      "loss: 12.647017478942871\n",
      "steps per second: 0.54879\n",
      "step: 40402\n",
      "loss: 13.468820571899414\n",
      "steps per second: 0.54701\n",
      "step: 40403\n",
      "loss: 13.094399452209473\n",
      "steps per second: 0.51638\n",
      "step: 40404\n",
      "loss: 12.85779857635498\n",
      "steps per second: 0.56427\n",
      "step: 40405\n",
      "loss: 12.852571487426758\n",
      "steps per second: 0.53550\n",
      "step: 40406\n",
      "loss: 13.300280570983887\n",
      "steps per second: 0.56445\n",
      "step: 40407\n",
      "loss: 12.864487648010254\n",
      "steps per second: 0.55400\n",
      "step: 40408\n",
      "loss: 12.497947692871094\n",
      "steps per second: 0.50902\n",
      "step: 40409\n",
      "loss: 12.876978874206543\n",
      "steps per second: 0.61643\n",
      "step: 40410\n",
      "loss: 13.279315948486328\n",
      "steps per second: 0.55937\n",
      "step: 40411\n",
      "loss: 12.912392616271973\n",
      "steps per second: 0.53794\n",
      "step: 40412\n",
      "loss: 12.68278980255127\n",
      "steps per second: 0.57651\n",
      "step: 40413\n",
      "loss: 12.392687797546387\n",
      "steps per second: 0.55819\n",
      "step: 40414\n",
      "loss: 13.278629302978516\n",
      "steps per second: 0.58904\n",
      "step: 40415\n",
      "loss: 12.886874198913574\n",
      "steps per second: 0.57769\n",
      "step: 40416\n",
      "loss: 13.182235717773438\n",
      "steps per second: 0.55671\n",
      "step: 40417\n",
      "loss: 13.188254356384277\n",
      "steps per second: 0.57763\n",
      "step: 40418\n",
      "loss: 12.551651000976562\n",
      "steps per second: 0.57103\n",
      "step: 40419\n",
      "loss: 12.91443920135498\n",
      "steps per second: 0.53829\n",
      "step: 40420\n",
      "loss: 12.76990032196045\n",
      "steps per second: 0.57307\n",
      "step: 40421\n",
      "loss: 12.780970573425293\n",
      "steps per second: 0.60898\n",
      "step: 40422\n",
      "loss: 13.039406776428223\n",
      "steps per second: 0.53894\n",
      "step: 40423\n",
      "loss: 12.243408203125\n",
      "steps per second: 0.55264\n",
      "step: 40424\n",
      "loss: 12.727227210998535\n",
      "steps per second: 0.56297\n",
      "step: 40425\n",
      "loss: 12.889557838439941\n",
      "steps per second: 0.54335\n",
      "step: 40426\n",
      "loss: 13.034971237182617\n",
      "steps per second: 0.57211\n",
      "step: 40427\n",
      "loss: 12.509137153625488\n",
      "steps per second: 0.55663\n",
      "step: 40428\n",
      "loss: 13.277830123901367\n",
      "steps per second: 0.55522\n",
      "step: 40429\n",
      "loss: 12.063689231872559\n",
      "steps per second: 0.56845\n",
      "step: 40430\n",
      "loss: 12.567418098449707\n",
      "steps per second: 0.58843\n",
      "step: 40431\n",
      "loss: 13.302366256713867\n",
      "steps per second: 0.53334\n",
      "step: 40432\n",
      "loss: 12.746495246887207\n",
      "steps per second: 0.55427\n",
      "step: 40433\n",
      "loss: 13.065064430236816\n",
      "steps per second: 0.55336\n",
      "step: 40434\n",
      "loss: 12.983882904052734\n",
      "steps per second: 0.55525\n",
      "step: 40435\n",
      "loss: 12.795381546020508\n",
      "steps per second: 0.52898\n",
      "step: 40436\n",
      "loss: 12.794999122619629\n",
      "steps per second: 0.56267\n",
      "step: 40437\n",
      "loss: 13.057845115661621\n",
      "steps per second: 0.57840\n",
      "step: 40438\n",
      "loss: 12.746133804321289\n",
      "steps per second: 0.52670\n",
      "step: 40439\n",
      "loss: 12.822653770446777\n",
      "steps per second: 0.55949\n",
      "step: 40440\n",
      "loss: 12.683502197265625\n",
      "steps per second: 0.55687\n",
      "step: 40441\n",
      "loss: 12.763338088989258\n",
      "steps per second: 0.57919\n",
      "step: 40442\n",
      "loss: 13.370529174804688\n",
      "steps per second: 0.58118\n",
      "step: 40443\n",
      "loss: 12.946815490722656\n",
      "steps per second: 0.57580\n",
      "step: 40444\n",
      "loss: 12.813507080078125\n",
      "steps per second: 0.57345\n",
      "step: 40445\n",
      "loss: 12.316658020019531\n",
      "steps per second: 0.53547\n",
      "step: 40446\n",
      "loss: 12.487354278564453\n",
      "steps per second: 0.54697\n",
      "step: 40447\n",
      "loss: 13.234025001525879\n",
      "steps per second: 0.56409\n",
      "step: 40448\n",
      "loss: 12.71277904510498\n",
      "steps per second: 0.56844\n",
      "step: 40449\n",
      "loss: 12.700371742248535\n",
      "steps per second: 0.52210\n",
      "step: 40450\n",
      "loss: 12.39558219909668\n",
      "steps per second: 0.57220\n",
      "step: 40451\n",
      "loss: 12.579819679260254\n",
      "steps per second: 0.53125\n",
      "step: 40452\n",
      "loss: 12.250025749206543\n",
      "steps per second: 0.56985\n",
      "step: 40453\n",
      "loss: 12.392733573913574\n",
      "steps per second: 0.51456\n",
      "step: 40454\n",
      "loss: 12.908596992492676\n",
      "steps per second: 0.58179\n",
      "step: 40455\n",
      "loss: 12.494129180908203\n",
      "steps per second: 0.57324\n",
      "step: 40456\n",
      "loss: 13.213398933410645\n",
      "steps per second: 0.57534\n",
      "step: 40457\n",
      "loss: 13.033211708068848\n",
      "steps per second: 0.61074\n",
      "step: 40458\n",
      "loss: 12.643239974975586\n",
      "steps per second: 0.53569\n",
      "step: 40459\n",
      "loss: 12.515318870544434\n",
      "steps per second: 0.54643\n",
      "step: 40460\n",
      "loss: 12.930663108825684\n",
      "steps per second: 0.53271\n",
      "step: 40461\n",
      "loss: 12.696081161499023\n",
      "steps per second: 0.54750\n",
      "step: 40462\n",
      "loss: 11.914687156677246\n",
      "steps per second: 0.55968\n",
      "step: 40463\n",
      "loss: 12.297852516174316\n",
      "steps per second: 0.57919\n",
      "step: 40464\n",
      "loss: 12.265474319458008\n",
      "steps per second: 0.58815\n",
      "step: 40465\n",
      "loss: 12.908215522766113\n",
      "steps per second: 0.51162\n",
      "step: 40466\n",
      "loss: 12.645232200622559\n",
      "steps per second: 0.57120\n",
      "step: 40467\n",
      "loss: 12.50271987915039\n",
      "steps per second: 0.56057\n",
      "step: 40468\n",
      "loss: 12.748542785644531\n",
      "steps per second: 0.56478\n",
      "step: 40469\n",
      "loss: 13.564729690551758\n",
      "steps per second: 0.60612\n",
      "step: 40470\n",
      "loss: 12.843122482299805\n",
      "steps per second: 0.57570\n",
      "step: 40471\n",
      "loss: 13.159891128540039\n",
      "steps per second: 0.56978\n",
      "step: 40472\n",
      "loss: 12.301965713500977\n",
      "steps per second: 0.54180\n",
      "step: 40473\n",
      "loss: 12.876411437988281\n",
      "steps per second: 0.57881\n",
      "step: 40474\n",
      "loss: 13.310778617858887\n",
      "steps per second: 0.57210\n",
      "step: 40475\n",
      "loss: 12.650219917297363\n",
      "steps per second: 0.56041\n",
      "step: 40476\n",
      "loss: 13.11458683013916\n",
      "steps per second: 0.58021\n",
      "step: 40477\n",
      "loss: 12.610515594482422\n",
      "steps per second: 0.52314\n",
      "step: 40478\n",
      "loss: 12.64091682434082\n",
      "steps per second: 0.51797\n",
      "step: 40479\n",
      "loss: 12.891560554504395\n",
      "steps per second: 0.55723\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8678011894226074, layer: 10\n",
      "saving at step 40479\n",
      "----------\n",
      "\n",
      "\n",
      "step: 40480\n",
      "loss: 12.69492244720459\n",
      "steps per second: 0.28085\n",
      "step: 40481\n",
      "loss: 13.057625770568848\n",
      "steps per second: 0.51767\n",
      "step: 40482\n",
      "loss: 12.850303649902344\n",
      "steps per second: 0.57118\n",
      "step: 40483\n",
      "loss: 12.746384620666504\n",
      "steps per second: 0.54471\n",
      "step: 40484\n",
      "loss: 13.118024826049805\n",
      "steps per second: 0.61162\n",
      "step: 40485\n",
      "loss: 13.053784370422363\n",
      "steps per second: 0.51012\n",
      "step: 40486\n",
      "loss: 13.04851245880127\n",
      "steps per second: 0.53277\n",
      "step: 40487\n",
      "loss: 12.915274620056152\n",
      "steps per second: 0.57152\n",
      "step: 40488\n",
      "loss: 13.168925285339355\n",
      "steps per second: 0.50953\n",
      "step: 40489\n",
      "loss: 12.716819763183594\n",
      "steps per second: 0.51713\n",
      "step: 40490\n",
      "loss: 12.997648239135742\n",
      "steps per second: 0.57620\n",
      "step: 40491\n",
      "loss: 12.916504859924316\n",
      "steps per second: 0.52761\n",
      "step: 40492\n",
      "loss: 12.414219856262207\n",
      "steps per second: 0.58591\n",
      "step: 40493\n",
      "loss: 12.791545867919922\n",
      "steps per second: 0.58801\n",
      "step: 40494\n",
      "loss: 12.673007011413574\n",
      "steps per second: 0.54866\n",
      "step: 40495\n",
      "loss: 12.621628761291504\n",
      "steps per second: 0.52461\n",
      "step: 40496\n",
      "loss: 12.443033218383789\n",
      "steps per second: 0.56772\n",
      "step: 40497\n",
      "loss: 12.722105026245117\n",
      "steps per second: 0.53541\n",
      "step: 40498\n",
      "loss: 12.888693809509277\n",
      "steps per second: 0.57109\n",
      "step: 40499\n",
      "loss: 12.884093284606934\n",
      "steps per second: 0.49889\n",
      "step: 40500\n",
      "loss: 12.428776741027832\n",
      "steps per second: 0.55444\n",
      "step: 40501\n",
      "loss: 13.466658592224121\n",
      "steps per second: 0.55566\n",
      "step: 40502\n",
      "loss: 12.532486915588379\n",
      "steps per second: 0.57138\n",
      "step: 40503\n",
      "loss: 12.925679206848145\n",
      "steps per second: 0.53311\n",
      "step: 40504\n",
      "loss: 12.36268424987793\n",
      "steps per second: 0.58115\n",
      "step: 40505\n",
      "loss: 12.576478958129883\n",
      "steps per second: 0.56134\n",
      "step: 40506\n",
      "loss: 12.982443809509277\n",
      "steps per second: 0.55620\n",
      "step: 40507\n",
      "loss: 12.757527351379395\n",
      "steps per second: 0.54280\n",
      "step: 40508\n",
      "loss: 12.825089454650879\n",
      "steps per second: 0.53122\n",
      "step: 40509\n",
      "loss: 12.96379566192627\n",
      "steps per second: 0.55052\n",
      "step: 40510\n",
      "loss: 12.468932151794434\n",
      "steps per second: 0.54750\n",
      "step: 40511\n",
      "loss: 13.287091255187988\n",
      "steps per second: 0.49945\n",
      "step: 40512\n",
      "loss: 13.074760437011719\n",
      "steps per second: 0.53631\n",
      "step: 40513\n",
      "loss: 12.789374351501465\n",
      "steps per second: 0.55020\n",
      "step: 40514\n",
      "loss: 13.131927490234375\n",
      "steps per second: 0.57456\n",
      "step: 40515\n",
      "loss: 12.901575088500977\n",
      "steps per second: 0.57632\n",
      "step: 40516\n",
      "loss: 12.714275360107422\n",
      "steps per second: 0.49931\n",
      "step: 40517\n",
      "loss: 12.783745765686035\n",
      "steps per second: 0.54874\n",
      "step: 40518\n",
      "loss: 12.892529487609863\n",
      "steps per second: 0.54569\n",
      "step: 40519\n",
      "loss: 12.724032402038574\n",
      "steps per second: 0.57242\n",
      "step: 40520\n",
      "loss: 12.774033546447754\n",
      "steps per second: 0.51434\n",
      "step: 40521\n",
      "loss: 12.8060884475708\n",
      "steps per second: 0.58199\n",
      "step: 40522\n",
      "loss: 12.305806159973145\n",
      "steps per second: 0.53828\n",
      "step: 40523\n",
      "loss: 12.91787052154541\n",
      "steps per second: 0.55341\n",
      "step: 40524\n",
      "loss: 12.88759994506836\n",
      "steps per second: 0.58086\n",
      "step: 40525\n",
      "loss: 13.034881591796875\n",
      "steps per second: 0.54896\n",
      "step: 40526\n",
      "loss: 12.893143653869629\n",
      "steps per second: 0.56048\n",
      "step: 40527\n",
      "loss: 12.437188148498535\n",
      "steps per second: 0.58487\n",
      "step: 40528\n",
      "loss: 13.046721458435059\n",
      "steps per second: 0.53568\n",
      "step: 40529\n",
      "loss: 12.637094497680664\n",
      "steps per second: 0.57557\n",
      "step: 40530\n",
      "loss: 13.17026138305664\n",
      "steps per second: 0.50797\n",
      "step: 40531\n",
      "loss: 12.411752700805664\n",
      "steps per second: 0.55335\n",
      "step: 40532\n",
      "loss: 13.087738990783691\n",
      "steps per second: 0.55524\n",
      "step: 40533\n",
      "loss: 12.816351890563965\n",
      "steps per second: 0.54900\n",
      "step: 40534\n",
      "loss: 13.086264610290527\n",
      "steps per second: 0.58019\n",
      "step: 40535\n",
      "loss: 12.728189468383789\n",
      "steps per second: 0.50057\n",
      "step: 40536\n",
      "loss: 13.145469665527344\n",
      "steps per second: 0.57290\n",
      "step: 40537\n",
      "loss: 12.864239692687988\n",
      "steps per second: 0.54916\n",
      "step: 40538\n",
      "loss: 12.71644115447998\n",
      "steps per second: 0.57625\n",
      "step: 40539\n",
      "loss: 12.849753379821777\n",
      "steps per second: 0.54261\n",
      "step: 40540\n",
      "loss: 12.765244483947754\n",
      "steps per second: 0.55646\n",
      "step: 40541\n",
      "loss: 13.036286354064941\n",
      "steps per second: 0.52224\n",
      "step: 40542\n",
      "loss: 12.616169929504395\n",
      "steps per second: 0.59584\n",
      "step: 40543\n",
      "loss: 12.815993309020996\n",
      "steps per second: 0.55113\n",
      "step: 40544\n",
      "loss: 12.34101390838623\n",
      "steps per second: 0.52872\n",
      "step: 40545\n",
      "loss: 12.96840763092041\n",
      "steps per second: 0.56101\n",
      "step: 40546\n",
      "loss: 12.661287307739258\n",
      "steps per second: 0.60445\n",
      "step: 40547\n",
      "loss: 13.065767288208008\n",
      "steps per second: 0.57039\n",
      "step: 40548\n",
      "loss: 12.972933769226074\n",
      "steps per second: 0.52793\n",
      "step: 40549\n",
      "loss: 12.574462890625\n",
      "steps per second: 0.55134\n",
      "step: 40550\n",
      "loss: 12.599200248718262\n",
      "steps per second: 0.55917\n",
      "step: 40551\n",
      "loss: 13.430438995361328\n",
      "steps per second: 0.57461\n",
      "step: 40552\n",
      "loss: 12.426966667175293\n",
      "steps per second: 0.61018\n",
      "step: 40553\n",
      "loss: 13.295622825622559\n",
      "steps per second: 0.54487\n",
      "step: 40554\n",
      "loss: 13.370855331420898\n",
      "steps per second: 0.52515\n",
      "step: 40555\n",
      "loss: 12.648825645446777\n",
      "steps per second: 0.62146\n",
      "step: 40556\n",
      "loss: 12.875412940979004\n",
      "steps per second: 0.54221\n",
      "step: 40557\n",
      "loss: 12.628026008605957\n",
      "steps per second: 0.56680\n",
      "step: 40558\n",
      "loss: 12.620160102844238\n",
      "steps per second: 0.55923\n",
      "step: 40559\n",
      "loss: 12.335061073303223\n",
      "steps per second: 0.55819\n",
      "step: 40560\n",
      "loss: 13.386032104492188\n",
      "steps per second: 0.55477\n",
      "step: 40561\n",
      "loss: 12.646407127380371\n",
      "steps per second: 0.58464\n",
      "step: 40562\n",
      "loss: 12.381649017333984\n",
      "steps per second: 0.55299\n",
      "step: 40563\n",
      "loss: 12.160689353942871\n",
      "steps per second: 0.53543\n",
      "step: 40564\n",
      "loss: 12.661975860595703\n",
      "steps per second: 0.49665\n",
      "step: 40565\n",
      "loss: 12.659660339355469\n",
      "steps per second: 0.50207\n",
      "step: 40566\n",
      "loss: 12.747305870056152\n",
      "steps per second: 0.51615\n",
      "step: 40567\n",
      "loss: 13.166807174682617\n",
      "steps per second: 0.54466\n",
      "step: 40568\n",
      "loss: 12.567803382873535\n",
      "steps per second: 0.50606\n",
      "step: 40569\n",
      "loss: 13.128881454467773\n",
      "steps per second: 0.55869\n",
      "step: 40570\n",
      "loss: 13.552189826965332\n",
      "steps per second: 0.52994\n",
      "step: 40571\n",
      "loss: 13.441644668579102\n",
      "steps per second: 0.52292\n",
      "step: 40572\n",
      "loss: 12.730464935302734\n",
      "steps per second: 0.56142\n",
      "step: 40573\n",
      "loss: 12.77269172668457\n",
      "steps per second: 0.61549\n",
      "step: 40574\n",
      "loss: 12.441729545593262\n",
      "steps per second: 0.51568\n",
      "step: 40575\n",
      "loss: 12.672059059143066\n",
      "steps per second: 0.57448\n",
      "step: 40576\n",
      "loss: 12.497663497924805\n",
      "steps per second: 0.55057\n",
      "step: 40577\n",
      "loss: 12.953987121582031\n",
      "steps per second: 0.57351\n",
      "step: 40578\n",
      "loss: 12.850735664367676\n",
      "steps per second: 0.51292\n",
      "step: 40579\n",
      "loss: 12.79401683807373\n",
      "steps per second: 0.49320\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7975470423698425, layer: 11\n",
      "saving at step 40579\n",
      "----------\n",
      "\n",
      "\n",
      "step: 40580\n",
      "loss: 12.45540714263916\n",
      "steps per second: 0.25933\n",
      "step: 40581\n",
      "loss: 12.640735626220703\n",
      "steps per second: 0.58944\n",
      "step: 40582\n",
      "loss: 13.416723251342773\n",
      "steps per second: 0.54666\n",
      "step: 40583\n",
      "loss: 12.606103897094727\n",
      "steps per second: 0.58244\n",
      "step: 40584\n",
      "loss: 12.846577644348145\n",
      "steps per second: 0.46597\n",
      "step: 40585\n",
      "loss: 12.320459365844727\n",
      "steps per second: 0.53347\n",
      "step: 40586\n",
      "loss: 12.250771522521973\n",
      "steps per second: 0.56300\n",
      "step: 40587\n",
      "loss: 12.597766876220703\n",
      "steps per second: 0.55580\n",
      "step: 40588\n",
      "loss: 12.156182289123535\n",
      "steps per second: 0.54904\n",
      "step: 40589\n",
      "loss: 12.397791862487793\n",
      "steps per second: 0.54801\n",
      "step: 40590\n",
      "loss: 12.752079010009766\n",
      "steps per second: 0.55220\n",
      "step: 40591\n",
      "loss: 13.124045372009277\n",
      "steps per second: 0.49389\n",
      "step: 40592\n",
      "loss: 13.086626052856445\n",
      "steps per second: 0.55613\n",
      "step: 40593\n",
      "loss: 12.706121444702148\n",
      "steps per second: 0.57427\n",
      "step: 40594\n",
      "loss: 12.820908546447754\n",
      "steps per second: 0.52272\n",
      "step: 40595\n",
      "loss: 12.619186401367188\n",
      "steps per second: 0.59872\n",
      "step: 40596\n",
      "loss: 13.199414253234863\n",
      "steps per second: 0.54641\n",
      "step: 40597\n",
      "loss: 12.850672721862793\n",
      "steps per second: 0.52951\n",
      "step: 40598\n",
      "loss: 12.541732788085938\n",
      "steps per second: 0.58908\n",
      "step: 40599\n",
      "loss: 12.786009788513184\n",
      "steps per second: 0.56630\n",
      "step: 40600\n",
      "loss: 13.4067964553833\n",
      "steps per second: 0.56349\n",
      "step: 40601\n",
      "loss: 13.427858352661133\n",
      "steps per second: 0.58733\n",
      "step: 40602\n",
      "loss: 13.052248001098633\n",
      "steps per second: 0.56098\n",
      "step: 40603\n",
      "loss: 12.921553611755371\n",
      "steps per second: 0.50163\n",
      "step: 40604\n",
      "loss: 12.741951942443848\n",
      "steps per second: 0.55574\n",
      "step: 40605\n",
      "loss: 12.468254089355469\n",
      "steps per second: 0.55797\n",
      "step: 40606\n",
      "loss: 12.951025009155273\n",
      "steps per second: 0.55494\n",
      "step: 40607\n",
      "loss: 13.347618103027344\n",
      "steps per second: 0.54688\n",
      "step: 40608\n",
      "loss: 12.597810745239258\n",
      "steps per second: 0.55715\n",
      "step: 40609\n",
      "loss: 12.781517028808594\n",
      "steps per second: 0.58822\n",
      "step: 40610\n",
      "loss: 12.822492599487305\n",
      "steps per second: 0.55929\n",
      "step: 40611\n",
      "loss: 12.811579704284668\n",
      "steps per second: 0.53076\n",
      "step: 40612\n",
      "loss: 12.5054931640625\n",
      "steps per second: 0.59317\n",
      "step: 40613\n",
      "loss: 12.778797149658203\n",
      "steps per second: 0.56280\n",
      "step: 40614\n",
      "loss: 12.820066452026367\n",
      "steps per second: 0.55742\n",
      "step: 40615\n",
      "loss: 13.362008094787598\n",
      "steps per second: 0.52054\n",
      "step: 40616\n",
      "loss: 12.464265823364258\n",
      "steps per second: 0.51494\n",
      "step: 40617\n",
      "loss: 12.98744010925293\n",
      "steps per second: 0.55414\n",
      "step: 40618\n",
      "loss: 12.541351318359375\n",
      "steps per second: 0.52559\n",
      "step: 40619\n",
      "loss: 12.151971817016602\n",
      "steps per second: 0.53845\n",
      "step: 40620\n",
      "loss: 12.78586483001709\n",
      "steps per second: 0.55331\n",
      "step: 40621\n",
      "loss: 12.633596420288086\n",
      "steps per second: 0.53810\n",
      "step: 40622\n",
      "loss: 12.436759948730469\n",
      "steps per second: 0.53887\n",
      "step: 40623\n",
      "loss: 12.301032066345215\n",
      "steps per second: 0.52695\n",
      "step: 40624\n",
      "loss: 12.535786628723145\n",
      "steps per second: 0.56507\n",
      "step: 40625\n",
      "loss: 12.79840087890625\n",
      "steps per second: 0.58683\n",
      "step: 40626\n",
      "loss: 12.709184646606445\n",
      "steps per second: 0.55971\n",
      "step: 40627\n",
      "loss: 13.169515609741211\n",
      "steps per second: 0.54139\n",
      "step: 40628\n",
      "loss: 12.986790657043457\n",
      "steps per second: 0.55377\n",
      "step: 40629\n",
      "loss: 13.32612419128418\n",
      "steps per second: 0.58121\n",
      "step: 40630\n",
      "loss: 12.264500617980957\n",
      "steps per second: 0.56411\n",
      "step: 40631\n",
      "loss: 12.341512680053711\n",
      "steps per second: 0.55353\n",
      "step: 40632\n",
      "loss: 12.70656967163086\n",
      "steps per second: 0.56850\n",
      "step: 40633\n",
      "loss: 13.097228050231934\n",
      "steps per second: 0.55947\n",
      "step: 40634\n",
      "loss: 12.73121166229248\n",
      "steps per second: 0.51664\n",
      "step: 40635\n",
      "loss: 12.798571586608887\n",
      "steps per second: 0.53892\n",
      "step: 40636\n",
      "loss: 12.709260940551758\n",
      "steps per second: 0.61798\n",
      "step: 40637\n",
      "loss: 12.585124969482422\n",
      "steps per second: 0.54834\n",
      "step: 40638\n",
      "loss: 12.803976058959961\n",
      "steps per second: 0.55865\n",
      "step: 40639\n",
      "loss: 12.557531356811523\n",
      "steps per second: 0.53885\n",
      "step: 40640\n",
      "loss: 13.002569198608398\n",
      "steps per second: 0.55394\n",
      "step: 40641\n",
      "loss: 12.63305377960205\n",
      "steps per second: 0.54874\n",
      "step: 40642\n",
      "loss: 12.947443008422852\n",
      "steps per second: 0.54272\n",
      "step: 40643\n",
      "loss: 12.69611644744873\n",
      "steps per second: 0.56243\n",
      "step: 40644\n",
      "loss: 12.582612037658691\n",
      "steps per second: 0.56169\n",
      "step: 40645\n",
      "loss: 12.455201148986816\n",
      "steps per second: 0.56194\n",
      "step: 40646\n",
      "loss: 12.756606101989746\n",
      "steps per second: 0.54905\n",
      "step: 40647\n",
      "loss: 12.239359855651855\n",
      "steps per second: 0.55119\n",
      "step: 40648\n",
      "loss: 13.409560203552246\n",
      "steps per second: 0.53889\n",
      "step: 40649\n",
      "loss: 13.069293975830078\n",
      "steps per second: 0.55498\n",
      "step: 40650\n",
      "loss: 12.054041862487793\n",
      "steps per second: 0.58405\n",
      "step: 40651\n",
      "loss: 12.410402297973633\n",
      "steps per second: 0.55518\n",
      "step: 40652\n",
      "loss: 13.881734848022461\n",
      "steps per second: 0.58128\n",
      "step: 40653\n",
      "loss: 13.164462089538574\n",
      "steps per second: 0.58113\n",
      "step: 40654\n",
      "loss: 12.199542045593262\n",
      "steps per second: 0.48724\n",
      "step: 40655\n",
      "loss: 12.668684005737305\n",
      "steps per second: 0.54702\n",
      "step: 40656\n",
      "loss: 13.437312126159668\n",
      "steps per second: 0.58340\n",
      "step: 40657\n",
      "loss: 13.624608993530273\n",
      "steps per second: 0.49904\n",
      "step: 40658\n",
      "loss: 13.152959823608398\n",
      "steps per second: 0.61968\n",
      "step: 40659\n",
      "loss: 12.452885627746582\n",
      "steps per second: 0.56308\n",
      "step: 40660\n",
      "loss: 13.092297554016113\n",
      "steps per second: 0.58728\n",
      "step: 40661\n",
      "loss: 12.256817817687988\n",
      "steps per second: 0.57826\n",
      "step: 40662\n",
      "loss: 13.305219650268555\n",
      "steps per second: 0.53919\n",
      "step: 40663\n",
      "loss: 12.5369234085083\n",
      "steps per second: 0.56237\n",
      "step: 40664\n",
      "loss: 12.819221496582031\n",
      "steps per second: 0.56402\n",
      "step: 40665\n",
      "loss: 13.152048110961914\n",
      "steps per second: 0.54776\n",
      "step: 40666\n",
      "loss: 12.878339767456055\n",
      "steps per second: 0.56110\n",
      "step: 40667\n",
      "loss: 13.193534851074219\n",
      "steps per second: 0.54364\n",
      "step: 40668\n",
      "loss: 13.444714546203613\n",
      "steps per second: 0.54008\n",
      "step: 40669\n",
      "loss: 12.84812068939209\n",
      "steps per second: 0.48018\n",
      "step: 40670\n",
      "loss: 12.61597728729248\n",
      "steps per second: 0.53051\n",
      "step: 40671\n",
      "loss: 12.746341705322266\n",
      "steps per second: 0.51594\n",
      "step: 40672\n",
      "loss: 12.909350395202637\n",
      "steps per second: 0.51174\n",
      "step: 40673\n",
      "loss: 12.856396675109863\n",
      "steps per second: 0.52676\n",
      "step: 40674\n",
      "loss: 12.138472557067871\n",
      "steps per second: 0.56067\n",
      "step: 40675\n",
      "loss: 12.847527503967285\n",
      "steps per second: 0.54852\n",
      "step: 40676\n",
      "loss: 12.995829582214355\n",
      "steps per second: 0.54876\n",
      "step: 40677\n",
      "loss: 12.960433959960938\n",
      "steps per second: 0.51883\n",
      "step: 40678\n",
      "loss: 12.601916313171387\n",
      "steps per second: 0.57327\n",
      "step: 40679\n",
      "loss: 13.029703140258789\n",
      "steps per second: 0.58169\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8610813021659851, layer: 11\n",
      "saving at step 40679\n",
      "----------\n",
      "\n",
      "\n",
      "step: 40680\n",
      "loss: 12.492526054382324\n",
      "steps per second: 0.29809\n",
      "step: 40681\n",
      "loss: 12.978714942932129\n",
      "steps per second: 0.55808\n",
      "step: 40682\n",
      "loss: 12.335756301879883\n",
      "steps per second: 0.55213\n",
      "step: 40683\n",
      "loss: 12.90615177154541\n",
      "steps per second: 0.57670\n",
      "step: 40684\n",
      "loss: 13.047148704528809\n",
      "steps per second: 0.59291\n",
      "step: 40685\n",
      "loss: 13.11861515045166\n",
      "steps per second: 0.55587\n",
      "step: 40686\n",
      "loss: 13.001893043518066\n",
      "steps per second: 0.55197\n",
      "step: 40687\n",
      "loss: 13.286717414855957\n",
      "steps per second: 0.56462\n",
      "step: 40688\n",
      "loss: 12.729395866394043\n",
      "steps per second: 0.61903\n",
      "step: 40689\n",
      "loss: 12.394912719726562\n",
      "steps per second: 0.55199\n",
      "step: 40690\n",
      "loss: 13.397052764892578\n",
      "steps per second: 0.55960\n",
      "step: 40691\n",
      "loss: 13.321235656738281\n",
      "steps per second: 0.55137\n",
      "step: 40692\n",
      "loss: 12.818695068359375\n",
      "steps per second: 0.55875\n",
      "step: 40693\n",
      "loss: 13.345030784606934\n",
      "steps per second: 0.52586\n",
      "step: 40694\n",
      "loss: 12.515691757202148\n",
      "steps per second: 0.55783\n",
      "step: 40695\n",
      "loss: 13.176149368286133\n",
      "steps per second: 0.59454\n",
      "step: 40696\n",
      "loss: 12.941695213317871\n",
      "steps per second: 0.56094\n",
      "step: 40697\n",
      "loss: 13.249991416931152\n",
      "steps per second: 0.56541\n",
      "step: 40698\n",
      "loss: 12.888630867004395\n",
      "steps per second: 0.52719\n",
      "step: 40699\n",
      "loss: 12.671477317810059\n",
      "steps per second: 0.56695\n",
      "step: 40700\n",
      "loss: 13.19399356842041\n",
      "steps per second: 0.56136\n",
      "step: 40701\n",
      "loss: 13.102630615234375\n",
      "steps per second: 0.55664\n",
      "step: 40702\n",
      "loss: 12.737458229064941\n",
      "steps per second: 0.56297\n",
      "step: 40703\n",
      "loss: 12.715047836303711\n",
      "steps per second: 0.55985\n",
      "step: 40704\n",
      "loss: 12.68594741821289\n",
      "steps per second: 0.53914\n",
      "step: 40705\n",
      "loss: 12.33517074584961\n",
      "steps per second: 0.56058\n",
      "step: 40706\n",
      "loss: 12.689130783081055\n",
      "steps per second: 0.53089\n",
      "step: 40707\n",
      "loss: 12.70001220703125\n",
      "steps per second: 0.57073\n",
      "step: 40708\n",
      "loss: 12.456862449645996\n",
      "steps per second: 0.55461\n",
      "step: 40709\n",
      "loss: 13.220370292663574\n",
      "steps per second: 0.56513\n",
      "step: 40710\n",
      "loss: 12.647467613220215\n",
      "steps per second: 0.60929\n",
      "step: 40711\n",
      "loss: 12.394758224487305\n",
      "steps per second: 0.54212\n",
      "step: 40712\n",
      "loss: 12.99258804321289\n",
      "steps per second: 0.54001\n",
      "step: 40713\n",
      "loss: 12.869680404663086\n",
      "steps per second: 0.56093\n",
      "step: 40714\n",
      "loss: 12.241793632507324\n",
      "steps per second: 0.51643\n",
      "step: 40715\n",
      "loss: 12.721970558166504\n",
      "steps per second: 0.57315\n",
      "step: 40716\n",
      "loss: 12.683144569396973\n",
      "steps per second: 0.52993\n",
      "step: 40717\n",
      "loss: 12.973511695861816\n",
      "steps per second: 0.54719\n",
      "step: 40718\n",
      "loss: 13.061836242675781\n",
      "steps per second: 0.52667\n",
      "step: 40719\n",
      "loss: 12.521564483642578\n",
      "steps per second: 0.55631\n",
      "step: 40720\n",
      "loss: 12.994028091430664\n",
      "steps per second: 0.52437\n",
      "step: 40721\n",
      "loss: 12.876633644104004\n",
      "steps per second: 0.56706\n",
      "step: 40722\n",
      "loss: 13.059222221374512\n",
      "steps per second: 0.59433\n",
      "step: 40723\n",
      "loss: 12.832260131835938\n",
      "steps per second: 0.53937\n",
      "step: 40724\n",
      "loss: 12.443726539611816\n",
      "steps per second: 0.52599\n",
      "step: 40725\n",
      "loss: 12.570769309997559\n",
      "steps per second: 0.54242\n",
      "step: 40726\n",
      "loss: 12.85622787475586\n",
      "steps per second: 0.52376\n",
      "step: 40727\n",
      "loss: 12.716621398925781\n",
      "steps per second: 0.51346\n",
      "step: 40728\n",
      "loss: 12.46918773651123\n",
      "steps per second: 0.54183\n",
      "step: 40729\n",
      "loss: 12.390031814575195\n",
      "steps per second: 0.57171\n",
      "step: 40730\n",
      "loss: 13.25731086730957\n",
      "steps per second: 0.53884\n",
      "step: 40731\n",
      "loss: 12.317607879638672\n",
      "steps per second: 0.53883\n",
      "step: 40732\n",
      "loss: 12.741185188293457\n",
      "steps per second: 0.57031\n",
      "step: 40733\n",
      "loss: 12.922623634338379\n",
      "steps per second: 0.56630\n",
      "step: 40734\n",
      "loss: 13.082005500793457\n",
      "steps per second: 0.61040\n",
      "step: 40735\n",
      "loss: 12.52968692779541\n",
      "steps per second: 0.54805\n",
      "step: 40736\n",
      "loss: 13.490745544433594\n",
      "steps per second: 0.54235\n",
      "step: 40737\n",
      "loss: 12.600201606750488\n",
      "steps per second: 0.57764\n",
      "step: 40738\n",
      "loss: 12.202181816101074\n",
      "steps per second: 0.60878\n",
      "step: 40739\n",
      "loss: 12.595836639404297\n",
      "steps per second: 0.55221\n",
      "step: 40740\n",
      "loss: 11.8887300491333\n",
      "steps per second: 0.56753\n",
      "step: 40741\n",
      "loss: 12.428956985473633\n",
      "steps per second: 0.56356\n",
      "step: 40742\n",
      "loss: 13.210797309875488\n",
      "steps per second: 0.50795\n",
      "step: 40743\n",
      "loss: 13.105352401733398\n",
      "steps per second: 0.51775\n",
      "step: 40744\n",
      "loss: 12.474102020263672\n",
      "steps per second: 0.52443\n",
      "step: 40745\n",
      "loss: 13.276671409606934\n",
      "steps per second: 0.54648\n",
      "step: 40746\n",
      "loss: 12.507795333862305\n",
      "steps per second: 0.55163\n",
      "step: 40747\n",
      "loss: 12.377763748168945\n",
      "steps per second: 0.55500\n",
      "step: 40748\n",
      "loss: 12.528167724609375\n",
      "steps per second: 0.56409\n",
      "step: 40749\n",
      "loss: 12.75273323059082\n",
      "steps per second: 0.54004\n",
      "step: 40750\n",
      "loss: 12.851600646972656\n",
      "steps per second: 0.55449\n",
      "step: 40751\n",
      "loss: 12.829333305358887\n",
      "steps per second: 0.55865\n",
      "step: 40752\n",
      "loss: 12.62269401550293\n",
      "steps per second: 0.51687\n",
      "step: 40753\n",
      "loss: 13.148530960083008\n",
      "steps per second: 0.57458\n",
      "step: 40754\n",
      "loss: 12.461395263671875\n",
      "steps per second: 0.55628\n",
      "step: 40755\n",
      "loss: 13.060856819152832\n",
      "steps per second: 0.55516\n",
      "step: 40756\n",
      "loss: 13.091447830200195\n",
      "steps per second: 0.56162\n",
      "step: 40757\n",
      "loss: 12.349549293518066\n",
      "steps per second: 0.55416\n",
      "step: 40758\n",
      "loss: 12.457436561584473\n",
      "steps per second: 0.57639\n",
      "step: 40759\n",
      "loss: 13.034010887145996\n",
      "steps per second: 0.53701\n",
      "step: 40760\n",
      "loss: 12.910475730895996\n",
      "steps per second: 0.54877\n",
      "step: 40761\n",
      "loss: 12.58205509185791\n",
      "steps per second: 0.55422\n",
      "step: 40762\n",
      "loss: 13.036953926086426\n",
      "steps per second: 0.52843\n",
      "step: 40763\n",
      "loss: 13.002158164978027\n",
      "steps per second: 0.55194\n",
      "step: 40764\n",
      "loss: 12.449918746948242\n",
      "steps per second: 0.58567\n",
      "step: 40765\n",
      "loss: 12.684392929077148\n",
      "steps per second: 0.53838\n",
      "step: 40766\n",
      "loss: 13.137161254882812\n",
      "steps per second: 0.54876\n",
      "step: 40767\n",
      "loss: 13.155435562133789\n",
      "steps per second: 0.56743\n",
      "step: 40768\n",
      "loss: 12.961533546447754\n",
      "steps per second: 0.55715\n",
      "step: 40769\n",
      "loss: 12.44579029083252\n",
      "steps per second: 0.51804\n",
      "step: 40770\n",
      "loss: 12.473494529724121\n",
      "steps per second: 0.53334\n",
      "step: 40771\n",
      "loss: 12.6804780960083\n",
      "steps per second: 0.53186\n",
      "step: 40772\n",
      "loss: 12.633316993713379\n",
      "steps per second: 0.52050\n",
      "step: 40773\n",
      "loss: 12.785699844360352\n",
      "steps per second: 0.56221\n",
      "step: 40774\n",
      "loss: 13.188518524169922\n",
      "steps per second: 0.55652\n",
      "step: 40775\n",
      "loss: 12.786160469055176\n",
      "steps per second: 0.51725\n",
      "step: 40776\n",
      "loss: 13.046045303344727\n",
      "steps per second: 0.53847\n",
      "step: 40777\n",
      "loss: 12.377305030822754\n",
      "steps per second: 0.55711\n",
      "step: 40778\n",
      "loss: 12.769309997558594\n",
      "steps per second: 0.56143\n",
      "step: 40779\n",
      "loss: 11.947031021118164\n",
      "steps per second: 0.53159\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.7656784653663635, layer: 10\n",
      "saving at step 40779\n",
      "----------\n",
      "\n",
      "\n",
      "step: 40780\n",
      "loss: 12.613703727722168\n",
      "steps per second: 0.26550\n",
      "step: 40781\n",
      "loss: 13.043512344360352\n",
      "steps per second: 0.54506\n",
      "step: 40782\n",
      "loss: 13.179301261901855\n",
      "steps per second: 0.52996\n",
      "step: 40783\n",
      "loss: 13.257307052612305\n",
      "steps per second: 0.62073\n",
      "step: 40784\n",
      "loss: 12.60228157043457\n",
      "steps per second: 0.56001\n",
      "step: 40785\n",
      "loss: 13.247336387634277\n",
      "steps per second: 0.54392\n",
      "step: 40786\n",
      "loss: 12.805012702941895\n",
      "steps per second: 0.56622\n",
      "step: 40787\n",
      "loss: 12.822525024414062\n",
      "steps per second: 0.57686\n",
      "step: 40788\n",
      "loss: 12.719324111938477\n",
      "steps per second: 0.56225\n",
      "step: 40789\n",
      "loss: 13.256433486938477\n",
      "steps per second: 0.52856\n",
      "step: 40790\n",
      "loss: 12.679366111755371\n",
      "steps per second: 0.55365\n",
      "step: 40791\n",
      "loss: 13.167189598083496\n",
      "steps per second: 0.58475\n",
      "step: 40792\n",
      "loss: 12.387469291687012\n",
      "steps per second: 0.58795\n",
      "step: 40793\n",
      "loss: 13.137778282165527\n",
      "steps per second: 0.56695\n",
      "step: 40794\n",
      "loss: 13.206613540649414\n",
      "steps per second: 0.55977\n",
      "step: 40795\n",
      "loss: 12.647473335266113\n",
      "steps per second: 0.55367\n",
      "step: 40796\n",
      "loss: 12.874432563781738\n",
      "steps per second: 0.52997\n",
      "step: 40797\n",
      "loss: 13.162856101989746\n",
      "steps per second: 0.55262\n",
      "step: 40798\n",
      "loss: 12.49973201751709\n",
      "steps per second: 0.57350\n",
      "step: 40799\n",
      "loss: 12.898274421691895\n",
      "steps per second: 0.56029\n",
      "step: 40800\n",
      "loss: 12.745583534240723\n",
      "steps per second: 0.56163\n",
      "step: 40801\n",
      "loss: 12.605191230773926\n",
      "steps per second: 0.58093\n",
      "step: 40802\n",
      "loss: 13.548630714416504\n",
      "steps per second: 0.53953\n",
      "step: 40803\n",
      "loss: 12.875297546386719\n",
      "steps per second: 0.55147\n",
      "step: 40804\n",
      "loss: 12.718671798706055\n",
      "steps per second: 0.55150\n",
      "step: 40805\n",
      "loss: 12.539539337158203\n",
      "steps per second: 0.55580\n",
      "step: 40806\n",
      "loss: 13.091341972351074\n",
      "steps per second: 0.56661\n",
      "step: 40807\n",
      "loss: 12.644085884094238\n",
      "steps per second: 0.56529\n",
      "step: 40808\n",
      "loss: 12.792404174804688\n",
      "steps per second: 0.54325\n",
      "step: 40809\n",
      "loss: 13.025856971740723\n",
      "steps per second: 0.58568\n",
      "step: 40810\n",
      "loss: 12.818673133850098\n",
      "steps per second: 0.52607\n",
      "step: 40811\n",
      "loss: 13.27914810180664\n",
      "steps per second: 0.54604\n",
      "step: 40812\n",
      "loss: 12.280806541442871\n",
      "steps per second: 0.52032\n",
      "step: 40813\n",
      "loss: 12.871689796447754\n",
      "steps per second: 0.54351\n",
      "step: 40814\n",
      "loss: 12.420239448547363\n",
      "steps per second: 0.55009\n",
      "step: 40815\n",
      "loss: 12.574075698852539\n",
      "steps per second: 0.54015\n",
      "step: 40816\n",
      "loss: 12.934538841247559\n",
      "steps per second: 0.50876\n",
      "step: 40817\n",
      "loss: 12.940176963806152\n",
      "steps per second: 0.53547\n",
      "step: 40818\n",
      "loss: 12.85920238494873\n",
      "steps per second: 0.55184\n",
      "step: 40819\n",
      "loss: 12.822837829589844\n",
      "steps per second: 0.51943\n",
      "step: 40820\n",
      "loss: 12.833547592163086\n",
      "steps per second: 0.54915\n",
      "step: 40821\n",
      "loss: 12.793469429016113\n",
      "steps per second: 0.56230\n",
      "step: 40822\n",
      "loss: 12.823100090026855\n",
      "steps per second: 0.58386\n",
      "step: 40823\n",
      "loss: 13.248466491699219\n",
      "steps per second: 0.53449\n",
      "step: 40824\n",
      "loss: 12.416072845458984\n",
      "steps per second: 0.53673\n",
      "step: 40825\n",
      "loss: 12.05445671081543\n",
      "steps per second: 0.58440\n",
      "step: 40826\n",
      "loss: 12.577862739562988\n",
      "steps per second: 0.55513\n",
      "step: 40827\n",
      "loss: 13.099727630615234\n",
      "steps per second: 0.61544\n",
      "step: 40828\n",
      "loss: 12.775760650634766\n",
      "steps per second: 0.56143\n",
      "step: 40829\n",
      "loss: 12.762341499328613\n",
      "steps per second: 0.57163\n",
      "step: 40830\n",
      "loss: 12.65484619140625\n",
      "steps per second: 0.58529\n",
      "step: 40831\n",
      "loss: 12.947183609008789\n",
      "steps per second: 0.51503\n",
      "step: 40832\n",
      "loss: 12.406489372253418\n",
      "steps per second: 0.57661\n",
      "step: 40833\n",
      "loss: 13.100062370300293\n",
      "steps per second: 0.58781\n",
      "step: 40834\n",
      "loss: 12.42603874206543\n",
      "steps per second: 0.53841\n",
      "step: 40835\n",
      "loss: 13.05671501159668\n",
      "steps per second: 0.56966\n",
      "step: 40836\n",
      "loss: 12.777379989624023\n",
      "steps per second: 0.52551\n",
      "step: 40837\n",
      "loss: 12.888651847839355\n",
      "steps per second: 0.61816\n",
      "step: 40838\n",
      "loss: 12.209253311157227\n",
      "steps per second: 0.57654\n",
      "step: 40839\n",
      "loss: 12.690526008605957\n",
      "steps per second: 0.57946\n",
      "step: 40840\n",
      "loss: 12.969403266906738\n",
      "steps per second: 0.60731\n",
      "step: 40841\n",
      "loss: 12.949440956115723\n",
      "steps per second: 0.52451\n",
      "step: 40842\n",
      "loss: 12.668397903442383\n",
      "steps per second: 0.57469\n",
      "step: 40843\n",
      "loss: 12.564435958862305\n",
      "steps per second: 0.55087\n",
      "step: 40844\n",
      "loss: 12.360102653503418\n",
      "steps per second: 0.50853\n",
      "step: 40845\n",
      "loss: 12.935356140136719\n",
      "steps per second: 0.49959\n",
      "step: 40846\n",
      "loss: 12.510367393493652\n",
      "steps per second: 0.58446\n",
      "step: 40847\n",
      "loss: 13.24625015258789\n",
      "steps per second: 0.52378\n",
      "step: 40848\n",
      "loss: 13.225719451904297\n",
      "steps per second: 0.53317\n",
      "step: 40849\n",
      "loss: 13.022937774658203\n",
      "steps per second: 0.57705\n",
      "step: 40850\n",
      "loss: 12.809454917907715\n",
      "steps per second: 0.55759\n",
      "step: 40851\n",
      "loss: 12.579811096191406\n",
      "steps per second: 0.52209\n",
      "step: 40852\n",
      "loss: 12.335588455200195\n",
      "steps per second: 0.58244\n",
      "step: 40853\n",
      "loss: 12.705824851989746\n",
      "steps per second: 0.61455\n",
      "step: 40854\n",
      "loss: 13.114435195922852\n",
      "steps per second: 0.56252\n",
      "step: 40855\n",
      "loss: 13.195554733276367\n",
      "steps per second: 0.52874\n",
      "step: 40856\n",
      "loss: 12.56762981414795\n",
      "steps per second: 0.55009\n",
      "step: 40857\n",
      "loss: 12.716814994812012\n",
      "steps per second: 0.53411\n",
      "step: 40858\n",
      "loss: 13.075321197509766\n",
      "steps per second: 0.49066\n",
      "step: 40859\n",
      "loss: 12.223774909973145\n",
      "steps per second: 0.57145\n",
      "step: 40860\n",
      "loss: 12.718281745910645\n",
      "steps per second: 0.53660\n",
      "step: 40861\n",
      "loss: 12.370809555053711\n",
      "steps per second: 0.53619\n",
      "step: 40862\n",
      "loss: 12.75998592376709\n",
      "steps per second: 0.54415\n",
      "step: 40863\n",
      "loss: 12.840217590332031\n",
      "steps per second: 0.56545\n",
      "step: 40864\n",
      "loss: 13.163070678710938\n",
      "steps per second: 0.58010\n",
      "step: 40865\n",
      "loss: 12.682778358459473\n",
      "steps per second: 0.55797\n",
      "step: 40866\n",
      "loss: 12.706799507141113\n",
      "steps per second: 0.52115\n",
      "step: 40867\n",
      "loss: 12.62307071685791\n",
      "steps per second: 0.50423\n",
      "step: 40868\n",
      "loss: 13.127045631408691\n",
      "steps per second: 0.58400\n",
      "step: 40869\n",
      "loss: 13.31335735321045\n",
      "steps per second: 0.56147\n",
      "step: 40870\n",
      "loss: 12.960641860961914\n",
      "steps per second: 0.54344\n",
      "step: 40871\n",
      "loss: 12.832804679870605\n",
      "steps per second: 0.55942\n",
      "step: 40872\n",
      "loss: 13.253878593444824\n",
      "steps per second: 0.61915\n",
      "step: 40873\n",
      "loss: 12.675493240356445\n",
      "steps per second: 0.52553\n",
      "step: 40874\n",
      "loss: 12.566344261169434\n",
      "steps per second: 0.56609\n",
      "step: 40875\n",
      "loss: 13.012157440185547\n",
      "steps per second: 0.56426\n",
      "step: 40876\n",
      "loss: 12.93283462524414\n",
      "steps per second: 0.55079\n",
      "step: 40877\n",
      "loss: 13.017212867736816\n",
      "steps per second: 0.53639\n",
      "step: 40878\n",
      "loss: 12.831469535827637\n",
      "steps per second: 0.52096\n",
      "step: 40879\n",
      "loss: 12.979984283447266\n",
      "steps per second: 0.56241\n",
      "\n",
      "\n",
      "----- saving and eval -----\n",
      "best layer loss: 0.8762302994728088, layer: 11\n",
      "saving at step 40879\n",
      "----------\n",
      "\n",
      "\n",
      "step: 40880\n",
      "loss: 12.79306411743164\n",
      "steps per second: 0.28100\n",
      "step: 40881\n",
      "loss: 13.415552139282227\n",
      "steps per second: 0.62044\n",
      "step: 40882\n",
      "loss: 12.535204887390137\n",
      "steps per second: 0.56476\n",
      "step: 40883\n",
      "loss: 12.411191940307617\n",
      "steps per second: 0.55361\n",
      "step: 40884\n",
      "loss: 13.178720474243164\n",
      "steps per second: 0.52134\n",
      "step: 40885\n",
      "loss: 12.66441822052002\n",
      "steps per second: 0.54673\n",
      "step: 40886\n",
      "loss: 12.689393043518066\n",
      "steps per second: 0.61095\n",
      "step: 40887\n",
      "loss: 13.23559284210205\n",
      "steps per second: 0.61506\n",
      "step: 40888\n",
      "loss: 12.954094886779785\n",
      "steps per second: 0.56742\n",
      "step: 40889\n",
      "loss: 12.404662132263184\n",
      "steps per second: 0.53865\n",
      "step: 40890\n",
      "loss: 12.870096206665039\n",
      "steps per second: 0.58621\n",
      "step: 40891\n",
      "loss: 11.953580856323242\n",
      "steps per second: 0.56262\n",
      "step: 40892\n",
      "loss: 13.084779739379883\n",
      "steps per second: 0.54959\n",
      "step: 40893\n",
      "loss: 13.234532356262207\n",
      "steps per second: 0.56510\n",
      "step: 40894\n",
      "loss: 12.835801124572754\n",
      "steps per second: 0.55412\n",
      "step: 40895\n",
      "loss: 12.300041198730469\n",
      "steps per second: 0.52654\n",
      "step: 40896\n",
      "loss: 12.822487831115723\n",
      "steps per second: 0.55959\n",
      "step: 40897\n",
      "loss: 12.288124084472656\n",
      "steps per second: 0.52628\n",
      "step: 40898\n",
      "loss: 12.840094566345215\n",
      "steps per second: 0.56077\n",
      "step: 40899\n",
      "loss: 12.669971466064453\n",
      "steps per second: 0.55161\n",
      "step: 40900\n",
      "loss: 12.865340232849121\n",
      "steps per second: 0.55867\n",
      "step: 40901\n",
      "loss: 12.9254150390625\n",
      "steps per second: 0.57965\n",
      "step: 40902\n",
      "loss: 12.308051109313965\n",
      "steps per second: 0.59847\n",
      "step: 40903\n",
      "loss: 12.244573593139648\n",
      "steps per second: 0.57459\n",
      "step: 40904\n",
      "loss: 13.241660118103027\n",
      "steps per second: 0.52205\n",
      "step: 40905\n",
      "loss: 12.783727645874023\n",
      "steps per second: 0.57735\n",
      "step: 40906\n",
      "loss: 12.672249794006348\n",
      "steps per second: 0.58100\n",
      "step: 40907\n",
      "loss: 12.609188079833984\n",
      "steps per second: 0.58566\n",
      "step: 40908\n",
      "loss: 12.481935501098633\n",
      "steps per second: 0.56151\n",
      "step: 40909\n",
      "loss: 12.866033554077148\n",
      "steps per second: 0.54372\n",
      "step: 40910\n",
      "loss: 12.273300170898438\n",
      "steps per second: 0.53114\n",
      "step: 40911\n",
      "loss: 12.801026344299316\n",
      "steps per second: 0.56456\n",
      "step: 40912\n",
      "loss: 12.33964729309082\n",
      "steps per second: 0.55835\n",
      "step: 40913\n",
      "loss: 13.578335762023926\n",
      "steps per second: 0.57580\n",
      "step: 40914\n",
      "loss: 12.774014472961426\n",
      "steps per second: 0.55376\n",
      "step: 40915\n",
      "loss: 13.28818130493164\n",
      "steps per second: 0.55915\n",
      "step: 40916\n",
      "loss: 13.224366188049316\n",
      "steps per second: 0.57261\n",
      "step: 40917\n",
      "loss: 12.451445579528809\n",
      "steps per second: 0.54251\n",
      "step: 40918\n",
      "loss: 12.435890197753906\n",
      "steps per second: 0.51937\n",
      "step: 40919\n",
      "loss: 13.702261924743652\n",
      "steps per second: 0.55503\n",
      "step: 40920\n",
      "loss: 12.477015495300293\n",
      "steps per second: 0.55538\n",
      "step: 40921\n",
      "loss: 12.938922882080078\n",
      "steps per second: 0.50192\n",
      "step: 40922\n",
      "loss: 12.474275588989258\n",
      "steps per second: 0.60792\n",
      "step: 40923\n",
      "loss: 12.421760559082031\n",
      "steps per second: 0.52844\n",
      "step: 40924\n",
      "loss: 13.317574501037598\n",
      "steps per second: 0.57568\n",
      "step: 40925\n",
      "loss: 12.660123825073242\n",
      "steps per second: 0.51802\n",
      "step: 40926\n",
      "loss: 13.41501522064209\n",
      "steps per second: 0.55802\n",
      "step: 40927\n",
      "loss: 13.026721954345703\n",
      "steps per second: 0.51511\n",
      "step: 40928\n",
      "loss: 12.153726577758789\n",
      "steps per second: 0.54622\n",
      "step: 40929\n",
      "loss: 13.226181030273438\n",
      "steps per second: 0.57207\n",
      "step: 40930\n",
      "loss: 13.044063568115234\n",
      "steps per second: 0.54385\n",
      "step: 40931\n",
      "loss: 12.52390193939209\n",
      "steps per second: 0.57992\n",
      "step: 40932\n",
      "loss: 12.546460151672363\n",
      "steps per second: 0.53579\n",
      "step: 40933\n",
      "loss: 12.860620498657227\n",
      "steps per second: 0.57052\n",
      "step: 40934\n",
      "loss: 13.40481185913086\n",
      "steps per second: 0.52801\n",
      "step: 40935\n",
      "loss: 12.429187774658203\n",
      "steps per second: 0.52505\n",
      "step: 40936\n",
      "loss: 12.380901336669922\n",
      "steps per second: 0.55531\n",
      "step: 40937\n",
      "loss: 12.805938720703125\n",
      "steps per second: 0.52608\n",
      "step: 40938\n",
      "loss: 13.369538307189941\n",
      "steps per second: 0.54761\n",
      "step: 40939\n",
      "loss: 12.664271354675293\n",
      "steps per second: 0.60876\n",
      "step: 40940\n",
      "loss: 12.705344200134277\n",
      "steps per second: 0.55883\n",
      "step: 40941\n",
      "loss: 13.120899200439453\n",
      "steps per second: 0.55247\n",
      "step: 40942\n",
      "loss: 12.608681678771973\n",
      "steps per second: 0.55096\n",
      "step: 40943\n",
      "loss: 12.543874740600586\n",
      "steps per second: 0.57846\n",
      "step: 40944\n",
      "loss: 12.735747337341309\n",
      "steps per second: 0.57756\n",
      "step: 40945\n",
      "loss: 12.796138763427734\n",
      "steps per second: 0.57057\n",
      "step: 40946\n",
      "loss: 13.026737213134766\n",
      "steps per second: 0.49904\n",
      "step: 40947\n",
      "loss: 12.825194358825684\n",
      "steps per second: 0.56350\n",
      "step: 40948\n",
      "loss: 12.723305702209473\n",
      "steps per second: 0.54860\n",
      "step: 40949\n",
      "loss: 13.711203575134277\n",
      "steps per second: 0.50990\n",
      "step: 40950\n",
      "loss: 12.436210632324219\n",
      "steps per second: 0.57767\n",
      "step: 40951\n",
      "loss: 12.897281646728516\n",
      "steps per second: 0.54125\n",
      "step: 40952\n",
      "loss: 12.671358108520508\n",
      "steps per second: 0.55038\n",
      "step: 40953\n",
      "loss: 12.150328636169434\n",
      "steps per second: 0.55886\n",
      "step: 40954\n",
      "loss: 12.6605224609375\n",
      "steps per second: 0.52071\n",
      "step: 40955\n",
      "loss: 12.82988166809082\n",
      "steps per second: 0.59326\n",
      "step: 40956\n",
      "loss: 12.897476196289062\n",
      "steps per second: 0.52671\n",
      "step: 40957\n",
      "loss: 12.36561107635498\n",
      "steps per second: 0.55680\n",
      "step: 40958\n",
      "loss: 12.262402534484863\n",
      "steps per second: 0.52075\n",
      "step: 40959\n",
      "loss: 12.77016830444336\n",
      "steps per second: 0.54087\n",
      "step: 40960\n",
      "loss: 12.513678550720215\n",
      "steps per second: 0.60252\n",
      "step: 40961\n",
      "loss: 12.526671409606934\n",
      "steps per second: 0.57039\n",
      "step: 40962\n",
      "loss: 13.30603313446045\n",
      "steps per second: 0.55735\n",
      "step: 40963\n",
      "loss: 12.321866035461426\n",
      "steps per second: 0.55374\n",
      "step: 40964\n",
      "loss: 12.872178077697754\n",
      "steps per second: 0.57664\n",
      "step: 40965\n",
      "loss: 12.993071556091309\n",
      "steps per second: 0.56461\n",
      "step: 40966\n",
      "loss: 13.092373847961426\n",
      "steps per second: 0.53078\n",
      "step: 40967\n",
      "loss: 12.841938018798828\n",
      "steps per second: 0.53640\n",
      "step: 40968\n",
      "loss: 13.133976936340332\n",
      "steps per second: 0.59500\n",
      "step: 40969\n",
      "loss: 13.006918907165527\n",
      "steps per second: 0.52035\n",
      "step: 40970\n",
      "loss: 12.800687789916992\n",
      "steps per second: 0.53255\n",
      "step: 40971\n",
      "loss: 12.215052604675293\n",
      "steps per second: 0.56002\n",
      "step: 40972\n",
      "loss: 12.393275260925293\n",
      "steps per second: 0.56249\n",
      "step: 40973\n",
      "loss: 12.875728607177734\n",
      "steps per second: 0.54525\n"
     ]
    }
   ],
   "source": [
    "loop_time = time.time()\n",
    "\n",
    "for n, batch in enumerate(train_loader):\n",
    "    # do the actual training\n",
    "\n",
    "    probe_state = probe_train_step(probe_state, batch)\n",
    "    \n",
    "    probe_losses.append(probe_state['loss'])\n",
    "    \n",
    "    print(f'step: {probe_state[\"step\"]}')\n",
    "    print('loss: {}'.format(probe_state['loss']))\n",
    "    old_time = loop_time\n",
    "    loop_time = time.time()\n",
    "    print('steps per second: {:.5f}'.format(1/(loop_time - old_time)))\n",
    "\n",
    "    \n",
    "\n",
    "    if n % 100 == 0:\n",
    "        print('\\n\\n----- saving and eval -----')\n",
    "        #optimal_loss = optimal_loss_fn(batch)\n",
    "        #print(f'optimal_loss: {optimal_loss}')\n",
    "        layer_losses = probe_loss_fn(probe_state['params'],batch,reduce_layers=False)\n",
    "        layer, best_layer_loss = layer_losses.argmin(), layer_losses.min()\n",
    "        print(f'best layer loss: {best_layer_loss}, layer: {layer}')\n",
    "        probe_save_step = probe_state['step']\n",
    "        print(f'saving at step {probe_save_step}')\n",
    "        probe_save_dict = {'probe_state': probe_state,\n",
    "                     'probe_loss': np.array(probe_losses)\n",
    "                     }\n",
    "        probe_save_args = orbax_utils.save_args_from_target(probe_save_dict)\n",
    "        probe_checkpoint_manager.save(probe_save_step, probe_save_dict, save_kwargs={'save_args': probe_save_args})\n",
    "        print('----------\\n\\n')\n",
    "    \n",
    "\n",
    "# note that completely oblivious loss is 1.3862944 for 4 options (since dataset is uniformly random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b633361d-0849-4d1d-91e8-4dddc4319eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving at step 30478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBE SAVING\n",
    "probe_save_step = probe_state['step']\n",
    "print(f'saving at step {probe_save_step}')\n",
    "probe_save_dict = {'probe_state': probe_state,\n",
    "             'probe_loss': np.array(probe_losses)\n",
    "             }\n",
    "probe_save_args = orbax_utils.save_args_from_target(probe_save_dict)\n",
    "probe_checkpoint_manager.save(probe_save_step, probe_save_dict, save_kwargs={'save_args': probe_save_args})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076dd653-2de5-48c8-ada4-f32c94a42801",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Baseline setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e237352-ffaf-4c56-b12a-385ba7ce9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = jnp.array(state['params']['params']['decoder']['Embed_0']['embedding'])\n",
    "\n",
    "def get_baseline_features(data):\n",
    "    # Baseline features:\n",
    "    # for simplicity, we just concatenate the embeddings of the past two tokens.\n",
    "    data = jnp.array(data)\n",
    "    features = embeddings[data]\n",
    "\n",
    "    two_step_features = jnp.concatenate([features[:,:-1,:],features[:,1:,:]],axis=-1)\n",
    "    seq_len = two_step_features.shape[1]\n",
    "    out = rearrange(two_step_features, 'bs seq feat -> (bs seq) feat')\n",
    "\n",
    "    return out, seq_len\n",
    "    \n",
    "\n",
    "n_baseline_probe_layers = 1\n",
    "n_hidden = emb_dim\n",
    "\n",
    "class BaselineProbe(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for layer in range(n_baseline_probe_layers):\n",
    "            x = nn.Dense(n_hidden)(x)\n",
    "            x = nn.relu(x)\n",
    "        out = nn.Dense(4)(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd9547c-5755-4e54-8550-04b79cca780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Start a new experiment\n",
    "\n",
    "losses = []\n",
    "eval_losses = []\n",
    "\n",
    "#batch = next(iter(train_loader))\n",
    "\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "rng, key = random.split(key)\n",
    "\n",
    "baseline_probe = BaselineProbe()\n",
    "\n",
    "x, seq_len = get_baseline_features(batch['data'])\n",
    "\n",
    "tx = optax.adamw(1e-4)\n",
    "\n",
    "params = baseline_probe.init(rng, x)\n",
    "\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "baseline_state = {'params': params, 'opt_state': opt_state, 'loss': 0., 'step': 0}\n",
    "\n",
    "@jax.jit\n",
    "def baseline_loss_fn(params, batch):\n",
    "  inputs, seq_len = get_baseline_features(batch['data'])\n",
    "      \n",
    "  assert seq_len == batch['data'].shape[1] - 1\n",
    "\n",
    "  targets = batch['data'][jnp.arange(0,batch_size),batch['end_index']-1]\n",
    "\n",
    "  new_targets = targets.copy()\n",
    "\n",
    "  for k, v in target_dict.items():\n",
    "      new_targets = jnp.where(targets==k,v,new_targets)\n",
    "\n",
    "  #print(new_targets)\n",
    "  # expanding targets along seq len (result is shape \n",
    "  repeated_targets = jnp.repeat(new_targets, seq_len, axis = 0)\n",
    "  assert repeated_targets.shape == (seq_len * batch_size,)\n",
    "\n",
    "  probe_pred = baseline_probe.apply(params, inputs)\n",
    "\n",
    "  assert repeated_targets.shape == probe_pred.shape[:-1]\n",
    "\n",
    "  assert len(repeated_targets.shape)==1\n",
    "\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "      logits = probe_pred,\n",
    "      labels = repeated_targets\n",
    "  )\n",
    "\n",
    "  # loss is of shape (batch_size * seq_len, num_layer). First, undo the reshaping.\n",
    "\n",
    "  loss = rearrange(loss, '(bs seq) -> bs seq', bs=batch_size, seq=seq_len)\n",
    "  \n",
    "  idx = jnp.arange(seq_len)[None, :]\n",
    "\n",
    "  mask = jnp.where((idx < batch['end_index'][:, None]-1) & (idx > batch['start_index'][:, None]-1), 1., 0.)\n",
    "\n",
    "  # need to broadcast mask over the last layer dimension\n",
    "  assert mask.shape == (batch_size,seq_len)\n",
    "  loss = loss * mask\n",
    "  loss = loss.sum()/mask.sum()\n",
    "\n",
    "  return loss\n",
    "\n",
    "@jax.jit\n",
    "def baseline_train_step(baseline_state,batch):\n",
    "  params = baseline_state['params']\n",
    "  opt_state = baseline_state['opt_state']\n",
    "  loss, grads = jax.value_and_grad(baseline_loss_fn)(params,batch)\n",
    "  updates, opt_state = tx.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  step = baseline_state['step'] + 1\n",
    "\n",
    "  return {'params': params, 'opt_state': opt_state, 'loss': loss, 'step': step}\n",
    "\n",
    "baseline_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d33d6f41-004f-494a-854c-f1779ec9a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "baseline_options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=1000)\n",
    "baseline_checkpoint_manager = orbax.checkpoint.CheckpointManager(os.path.join(base_path,'baseline_probe_1hidden_loops'), orbax_checkpointer, baseline_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a34f0da-fe97-4119-883b-5aef43d39c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading step 91002\n",
      "1.0657879\n"
     ]
    }
   ],
   "source": [
    "# BASELINE PROBE LOADING\n",
    "dummy_dict = {\n",
    "            'baseline_state': baseline_state,\n",
    "            'baseline_loss': np.zeros(1)}\n",
    "\n",
    "step = baseline_checkpoint_manager.latest_step()\n",
    "print(f'loading step {step}')\n",
    "load_dict = baseline_checkpoint_manager.restore(step, items=dummy_dict)\n",
    "baseline_state = load_dict['baseline_state']\n",
    "baseline_losses = list(load_dict['baseline_loss'])\n",
    "print(np.mean(baseline_losses[-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210da22-9113-4741-bc50-6af3e06d3d89",
   "metadata": {},
   "source": [
    "# Probe analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a363ce4f-7a95-486b-8567-1a4394420383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30478\n",
      "12.799657\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(probe_losses))\n",
    "print(np.mean(probe_losses[-100:]))\n",
    "\n",
    "#plt.plot(old_probe_losses)\n",
    "plt.plot(probe_losses)\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.title('Probe training (losses summed over all layers)')\n",
    "plt.xlabel('Training steps')\n",
    "plt.ylabel('Average cross-entropy loss')\n",
    "plt.savefig('probe_training.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "784de233-e83d-4d67-a2dd-2a3cace63036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def optimal_loss_fn(batch):\n",
    "\n",
    "    path_len = batch['true_probs'].shape[1]\n",
    "    true_probs = batch['true_probs']\n",
    "    assert true_probs.shape == (batch_size, path_len, 4)\n",
    "    \n",
    "    targets = batch['data'][jnp.arange(0,batch['data'].shape[0]),batch['end_index']-1]\n",
    "    new_targets = targets.copy()\n",
    "    \n",
    "    for k, v in target_dict.items():\n",
    "      new_targets = jnp.where(targets==k,v,new_targets)\n",
    "\n",
    "    #print(new_targets)\n",
    "    # expanding targets along seq len (result is shape \n",
    "    repeated_targets = jnp.repeat(new_targets, path_len, axis = 0)\n",
    "    assert repeated_targets.shape == (path_len * batch_size,)\n",
    "\n",
    "    true_probs_reshaped = rearrange(true_probs, 'bs seq i -> (bs seq) i')\n",
    "    assert true_probs_reshaped.shape == (path_len * batch_size, 4)\n",
    "\n",
    "    log_probs = jnp.log(true_probs_reshaped)\n",
    "\n",
    "    #print(log_probs[:10])\n",
    "    #print(repeated_targets[:10])\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "          logits = log_probs,\n",
    "          labels = repeated_targets\n",
    "      )\n",
    "    #print(loss[:10])\n",
    "    \n",
    "    # loss is of shape (batch_size * seq_len). First, undo the reshaping.\n",
    "    \n",
    "    loss = rearrange(loss, '(bs seq) -> bs seq ', bs=batch_size, seq=path_len)\n",
    "    \n",
    "    idx = jnp.arange(path_len)[None, :]\n",
    "    \n",
    "    mask = jnp.where((idx <= batch['true_probs_end_index'][:, None]), 1., 0.)\n",
    "    \n",
    "    assert mask.shape == (batch_size,path_len)\n",
    "    loss = np.ma.array(loss, mask=1-mask)\n",
    "    #print(loss[:10])\n",
    "\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3ff741af-bcde-48f7-9852-f551ef829c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get an estimate of optimal loss, and probe losses for all layers, and baseline \n",
    "\n",
    "dataset = CustomMazeDataset(include_maze=False,no_loops=True)\n",
    "train_loader_iter = iter(NumpyLoader(dataset, batch_size=batch_size, num_workers=n_worker))\n",
    "\n",
    "probe_loss = {i: [] for i in range(13)}\n",
    "optimal_loss = []\n",
    "baseline_loss = []\n",
    "\n",
    "for n in range(10):\n",
    "    batch = next(train_loader_iter)\n",
    "    loss = optimal_loss_fn(batch)\n",
    "    optimal_loss.append(loss)\n",
    "    loss = probe_loss_fn(probe_state['params'],batch, reduce_layers=False)\n",
    "    for i in range(13):\n",
    "        probe_loss[i].append(loss[i])\n",
    "    loss = baseline_loss_fn(baseline_state['params'], batch)\n",
    "    baseline_loss.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "946d3721-a0a1-42c0-a956-96cf0de5ef63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate mean values for the horizontal lines\n",
    "optimal_loss_value = np.mean(optimal_loss)\n",
    "baseline_loss_value = np.mean(baseline_loss)\n",
    "\n",
    "# Sample data for the bar plot\n",
    "categories = [f'{i}' for i in range(13)]\n",
    "values = [np.mean(probe_loss[i]) for i in range(13)] \n",
    "\n",
    "# Adding the horizontal lines with labels\n",
    "plt.axhline(optimal_loss_value, color='green', linestyle='--', label='Optimal Loss')\n",
    "plt.axhline(baseline_loss_value, color='red', linestyle='--', label='Baseline Loss')\n",
    "\n",
    "# Creating a bar plot\n",
    "plt.bar(categories, values)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.ylabel('Average Cross-Entropy Loss')\n",
    "plt.title('Losses for Probes on Different Layers')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot and save the figure\n",
    "\n",
    "plt.savefig('probes_different_layers_baseline.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f768d-0980-437d-b23f-64b5b690a29f",
   "metadata": {},
   "source": [
    "# Other analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03bb1918-3363-4fee-92fd-b656b208dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probe_probs(probe_params, act):\n",
    "      \n",
    "  acts, seq_len = prepare_acts(act)\n",
    "\n",
    "  probe_preds = probe.apply(probe_params, acts)\n",
    "\n",
    "  probe_probs = nn.softmax(probe_preds, axis=-1)\n",
    "\n",
    "  #undo the batching again. Have a new dimension with the 4 probe probabilities now\n",
    "\n",
    "  return rearrange(probe_probs, '(bs seq) layer pred -> bs seq layer pred', bs=batch_size, seq=seq_len)\n",
    "\n",
    "reverse_target_dict = {v:k for k,v in target_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c654486d-6e9b-4e32-93eb-ef0e053bec9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 11, 1: 27, 2: 29, 3: 35}\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomMazeDataset(include_maze=True)\n",
    "train_loader = NumpyLoader(dataset, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(reverse_target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3999edd8-a2b3-4551-aa3c-fee94bb1ddad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ex 0\n",
      "[ 6 14 12 11 13 16 21 29  7]\n",
      "[14 12 11 13 16 21 29]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'maze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][i,batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_index\u001b[39m\u001b[38;5;124m'\u001b[39m][i]:batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_index\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(jnp\u001b[38;5;241m.\u001b[39margmax(preds[i,batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_index\u001b[39m\u001b[38;5;124m'\u001b[39m][i]:batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_index\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m plot \u001b[38;5;241m=\u001b[39m MazePlot(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaze\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[i])\n\u001b[1;32m     11\u001b[0m path \u001b[38;5;241m=\u001b[39m ints_to_coords(jnp\u001b[38;5;241m.\u001b[39margmax(preds[i,batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_index\u001b[39m\u001b[38;5;124m'\u001b[39m][i]:batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_index\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#plot.add_predicted_path(path)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'maze'"
     ]
    }
   ],
   "source": [
    "# first, print the mazes again\n",
    "\n",
    "preds, act = model.apply(state['params'],batch['data'])\n",
    "\n",
    "for i in range(20):\n",
    "  print(f'\\nex {i}')\n",
    "\n",
    "  print(batch['data'][i,batch['start_index'][i]:batch['end_index'][i]+1])\n",
    "  print(jnp.argmax(preds[i,batch['start_index'][i]:batch['end_index'][i]-1],axis=-1))\n",
    "  plot = MazePlot(batch['maze'][i])\n",
    "  path = ints_to_coords(jnp.argmax(preds[i,batch['start_index'][i]:batch['end_index'][i]-1],axis=-1))\n",
    "  #plot.add_predicted_path(path)\n",
    "  plot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51f71621-c48a-44ee-b1f1-b90a6885a407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -0.126997   -11.275179   -11.832291   -11.381224   -11.654282\n",
      " -11.633515   -11.169894    -2.4338238  -11.505222   -11.5296135\n",
      " -11.401617    -2.2874143   -0.95169276   0.43281287  -0.02550286\n",
      "  -3.0204818   -2.3760512   -1.2796539   -1.1155916   -0.43775648\n",
      "  -0.3464061   -0.73989344  -1.9397299   -0.5459655   -0.2479166\n",
      "  -1.4032001    4.1710773   -2.5551846    3.9759185   -2.0420656\n",
      "   2.077173     0.20904179   1.1315657    2.729878    -0.3045999\n",
      "   5.756655  ]\n",
      "\n",
      "time step 0\n",
      "current input <PATH_START>\n",
      "maze transformer prediction\n",
      "[('(0,0)', Array(0.99996865, dtype=float32)), ('(0,2)', Array(2.592906e-05, dtype=float32)), ('(2,1)', Array(8.403131e-07, dtype=float32)), ('(2,2)', Array(8.1038553e-07, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(0,0)', Array(0.39317104, dtype=float32)), ('(4,0)', Array(0.3553958, dtype=float32)), ('(4,4)', Array(0.19600609, dtype=float32)), ('(0,4)', Array(0.05542712, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 1\n",
      "current input (0,0)\n",
      "maze transformer prediction\n",
      "[('(1,0)', Array(0.7360284, dtype=float32)), ('<PATH_END>', Array(0.26380002, dtype=float32)), ('(0,1)', Array(3.681828e-05, dtype=float32)), ('(3,0)', Array(3.5164194e-05, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(0,0)', Array(0.537996, dtype=float32)), ('(4,0)', Array(0.194069, dtype=float32)), ('(4,4)', Array(0.18817636, dtype=float32)), ('(0,4)', Array(0.07975869, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 2\n",
      "current input (1,0)\n",
      "maze transformer prediction\n",
      "[('(2,0)', Array(0.9999964, dtype=float32)), ('(0,0)', Array(1.1516705e-06, dtype=float32)), ('(3,0)', Array(6.919264e-07, dtype=float32)), ('(3,1)', Array(5.895748e-07, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(4,4)', Array(0.42753467, dtype=float32)), ('(4,0)', Array(0.283443, dtype=float32)), ('(0,4)', Array(0.19725518, dtype=float32)), ('(0,0)', Array(0.09176715, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 3\n",
      "current input (2,0)\n",
      "maze transformer prediction\n",
      "[('(2,1)', Array(0.7018899, dtype=float32)), ('(3,0)', Array(0.2978387, dtype=float32)), ('(3,2)', Array(6.991467e-05, dtype=float32)), ('(1,0)', Array(6.836013e-05, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(4,4)', Array(0.45162153, dtype=float32)), ('(4,0)', Array(0.25747898, dtype=float32)), ('(0,4)', Array(0.19277455, dtype=float32)), ('(0,0)', Array(0.09812498, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 4\n",
      "current input (2,1)\n",
      "maze transformer prediction\n",
      "[('(2,2)', Array(0.99999726, dtype=float32)), ('(3,1)', Array(1.6023384e-06, dtype=float32)), ('(1,1)', Array(3.1783705e-07, dtype=float32)), ('(0,2)', Array(2.502335e-07, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(4,4)', Array(0.56263435, dtype=float32)), ('(0,4)', Array(0.3105631, dtype=float32)), ('(0,0)', Array(0.06988795, dtype=float32)), ('(4,0)', Array(0.05691464, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 5\n",
      "current input (2,2)\n",
      "maze transformer prediction\n",
      "[('(3,2)', Array(0.99999964, dtype=float32)), ('(4,3)', Array(1.6385482e-07, dtype=float32)), ('(4,1)', Array(7.786433e-08, dtype=float32)), ('(2,3)', Array(6.4050425e-08, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(0,4)', Array(0.4689859, dtype=float32)), ('(4,4)', Array(0.46138957, dtype=float32)), ('(0,0)', Array(0.0587361, dtype=float32)), ('(4,0)', Array(0.01088838, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 6\n",
      "current input (3,2)\n",
      "maze transformer prediction\n",
      "[('(4,2)', Array(0.99657226, dtype=float32)), ('(3,3)', Array(0.00341651, dtype=float32)), ('(4,4)', Array(2.3909713e-06, dtype=float32)), ('(2,2)', Array(2.0290438e-06, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(4,4)', Array(0.51747763, dtype=float32)), ('(0,4)', Array(0.43378082, dtype=float32)), ('(0,0)', Array(0.02998713, dtype=float32)), ('(4,0)', Array(0.01875444, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 7\n",
      "current input (4,2)\n",
      "maze transformer prediction\n",
      "[('(4,3)', Array(0.9999206, dtype=float32)), ('(4,1)', Array(5.4548476e-05, dtype=float32)), ('(3,4)', Array(1.2706853e-05, dtype=float32)), ('(3,2)', Array(4.7264193e-06, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(4,4)', Array(0.6030062, dtype=float32)), ('(0,4)', Array(0.3273414, dtype=float32)), ('(4,0)', Array(0.04476931, dtype=float32)), ('(0,0)', Array(0.02488314, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 8\n",
      "current input (4,3)\n",
      "maze transformer prediction\n",
      "[('(4,4)', Array(0.9999958, dtype=float32)), ('(2,4)', Array(1.8144315e-06, dtype=float32)), ('(2,2)', Array(4.527729e-07, dtype=float32)), ('(1,3)', Array(2.6482417e-07, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(4,4)', Array(0.64328605, dtype=float32)), ('(0,4)', Array(0.28601223, dtype=float32)), ('(4,0)', Array(0.04514204, dtype=float32)), ('(0,0)', Array(0.02555969, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 9\n",
      "current input (4,4)\n",
      "maze transformer prediction\n",
      "[('(3,4)', Array(0.54547346, dtype=float32)), ('<PATH_END>', Array(0.4543545, dtype=float32)), ('(1,4)', Array(4.1773706e-05, dtype=float32)), ('(4,3)', Array(3.6357505e-05, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(4,4)', Array(0.7958913, dtype=float32)), ('(0,4)', Array(0.1784046, dtype=float32)), ('(0,0)', Array(0.01929891, dtype=float32)), ('(4,0)', Array(0.00640515, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 10\n",
      "current input (3,4)\n",
      "maze transformer prediction\n",
      "[('(2,4)', Array(0.9999969, dtype=float32)), ('(1,1)', Array(6.925401e-07, dtype=float32)), ('(1,3)', Array(5.3558597e-07, dtype=float32)), ('(4,4)', Array(2.9746747e-07, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(0,4)', Array(0.86436254, dtype=float32)), ('(4,4)', Array(0.07385226, dtype=float32)), ('(4,0)', Array(0.03286934, dtype=float32)), ('(0,0)', Array(0.0289159, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 11\n",
      "current input (2,4)\n",
      "maze transformer prediction\n",
      "[('(1,4)', Array(0.9999981, dtype=float32)), ('(0,3)', Array(5.826281e-07, dtype=float32)), ('(2,3)', Array(3.1609287e-07, dtype=float32)), ('(3,4)', Array(2.2705106e-07, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(0,4)', Array(0.9486978, dtype=float32)), ('(4,4)', Array(0.0252235, dtype=float32)), ('(0,0)', Array(0.01955584, dtype=float32)), ('(4,0)', Array(0.00652285, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 12\n",
      "current input (1,4)\n",
      "maze transformer prediction\n",
      "[('(0,4)', Array(0.99999666, dtype=float32)), ('(1,3)', Array(1.3596463e-06, dtype=float32)), ('(1,4)', Array(4.6998068e-07, dtype=float32)), ('(2,4)', Array(3.1868063e-07, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(0,4)', Array(0.82848746, dtype=float32)), ('(0,0)', Array(0.10279714, dtype=float32)), ('(4,4)', Array(0.03965397, dtype=float32)), ('(4,0)', Array(0.02906145, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 13\n",
      "current input (0,4)\n",
      "maze transformer prediction\n",
      "[('<PATH_END>', Array(0.99999297, dtype=float32)), ('(3,4)', Array(3.3224171e-06, dtype=float32)), ('(1,4)', Array(1.1780502e-06, dtype=float32)), ('(0,4)', Array(7.689721e-07, dtype=float32))]\n",
      "probe probs layer 10:\n",
      "[('(0,4)', Array(0.97593576, dtype=float32)), ('(0,0)', Array(0.0124311, dtype=float32)), ('(4,4)', Array(0.0066409, dtype=float32)), ('(4,0)', Array(0.00499222, dtype=float32))]\n",
      "target:\n",
      "(0,4)\n"
     ]
    }
   ],
   "source": [
    "maze_num = 3\n",
    "timestep = 3\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "x_mod = copy.deepcopy(batch)\n",
    "\n",
    "#x_mod['data'][maze_num][x_mod['start_index'][maze_num]+2] = vocab_map('(1,1)')\n",
    "#x_mod['data'][maze_num][x_mod['start_index'][maze_num]+7] = vocab_map('(3,4)')\n",
    "#x_mod['data'][maze_num][x_mod['start_index'][maze_num]+8] = vocab_map('(4,4)')\n",
    "#x_mod['data'][maze_num][x_mod['start_index'][maze_num]+9] = vocab_map('(4,3)')\n",
    "#x_mod['end_index'][maze_num]=x['end_index'][maze_num]+1\n",
    "#intervention = get_intervention(probe_state,alpha=200, min_layer=5, goal_i=target_dict[vocab_map('(4,4)')])\n",
    "#intervention = get_intervention(probe_state,x_mod, alpha=100, min_layer=3, goal_i=target_dict[vocab_map('(0,0)')],timestep=timestep)\n",
    "#intervention = lambda y,i:y\n",
    "\n",
    "preds, act = model.apply(state['params'],x_mod['data'])#, intervention=intervention)\n",
    "print(preds[maze_num][timestep])\n",
    "\n",
    "\n",
    "probe_probs = get_probe_probs(probe_state['params'],act)\n",
    "\n",
    "\n",
    "for t in range(x_mod['start_index'][maze_num],x_mod['end_index'][maze_num]):\n",
    "  print('\\ntime step {}'.format(t-x_mod['start_index'][maze_num]))\n",
    "  print('current input {}'.format(reverse_map[x_mod['data'][maze_num][t]]))\n",
    "  print('maze transformer prediction')\n",
    "  print(sorted([(reverse_map[i],nn.softmax(preds[maze_num][t])[i]) for i in range(len(reverse_map))],key=lambda x: -x[1])[:4])\n",
    "  \n",
    "  for layer in [10]:\n",
    "      print(f'probe probs layer {layer}:')\n",
    "      step_probs = sorted([(reverse_map[pos],probe_probs[maze_num][t][layer][i]) for pos, i in target_dict.items()],key=lambda x: -x[1])[:4]\n",
    "      print(step_probs)\n",
    "  print('target:')\n",
    "  print(reverse_map[x_mod['data'][maze_num][x_mod['end_index'][maze_num]-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e0767-484d-4808-a208-719b86e33a71",
   "metadata": {},
   "source": [
    "# Logit lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670645f-ac50-4dae-8609-a9c7615abb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to extract the unembedding from the model state\n",
    "# Then apply it to all the captured activations, for each layer\n",
    "# Print the output distribution for each layer, as well as the loss...\n",
    "\n",
    "class LogitLens(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.LayerNorm(name='encoderdecoder_norm')(x)\n",
    "        x = nn.Dense(vocab_size, name='logitdense')(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0ac2c-826d-469b-a47a-5dbdbed896a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logitlens = LogitLens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd7988d6-809f-470d-83b0-9eca295ec357",
   "metadata": {},
   "outputs": [
    {
     "ename": "CallCompactUnboundModuleError",
     "evalue": "Can't call compact methods on unbound modules (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.CallCompactUnboundModuleError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCallCompactUnboundModuleError\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/maze-goal/lib/python3.10/site-packages/flax/linen/module.py:584\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    583\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 584\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/maze-goal/lib/python3.10/site-packages/flax/linen/module.py:1083\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_compact_method:\n\u001b[1;32m   1082\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mCallCompactUnboundModuleError()\n\u001b[1;32m   1084\u001b[0m   is_recurrent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39min_compact_method\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39min_compact_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mCallCompactUnboundModuleError\u001b[0m: Can't call compact methods on unbound modules (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.CallCompactUnboundModuleError)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a29ef1ce-b708-4e0b-8d63-af47541f7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_lens(state, batch):\n",
    "    preds, act = model.apply(state['params'],batch['data'])\n",
    "    acts = rearrange(act,'layer b seq dmodel -> b seq layer dmodel')\n",
    "    nn.LayerNorm()\n",
    "    \n",
    "    b = state['params']['params']['decoder']['logitdense']['bias']\n",
    "    W = state['params']['params']['decoder']['logitdense']['kernel']\n",
    "    logits = b[None,None,None,:] + einsum(W, act['stream'], 'dmodel o, layer b seq dmodel -> b seq layer o')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bec150c3-73e5-4cf6-93e8-e887c4e49098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.65857315e+00 -6.74975348e+00 -7.65854454e+00 -6.89025116e+00\n",
      "  -8.08977890e+00 -7.01545763e+00 -8.52140427e+00  2.95713973e+00\n",
      "  -6.98287249e+00 -7.31125641e+00 -6.78649950e+00 -3.30939561e-01\n",
      "  -7.30514765e-01 -2.83564782e+00  2.62907457e+00  3.08047986e+00\n",
      "   6.52385280e-02 -3.18877029e+00 -7.70843685e-01 -4.18714857e+00\n",
      "   2.80602837e+00  7.63528824e-01  3.14187765e-01 -3.07798266e+00\n",
      "   1.03060150e+00 -1.78376377e-01  3.35681129e+00  1.27394724e+00\n",
      "   3.91255164e+00  1.01682276e-01  2.56090426e+00 -7.80808568e-01\n",
      "  -9.36475635e-01 -8.85703862e-01 -1.44930542e+00  4.71074390e+00]\n",
      " [ 3.22508659e+01 -1.09103088e+01 -1.45863819e+01 -7.16484308e+00\n",
      "  -1.31753864e+01 -6.51650047e+00 -8.71810436e+00  1.31070232e+00\n",
      "  -8.71619987e+00 -7.52824497e+00 -7.20918512e+00 -2.87929273e+00\n",
      "  -6.59068680e+00 -5.36014509e+00 -1.43197298e+00  2.47716475e+00\n",
      "  -1.64355755e+00 -6.61014700e+00 -8.20018482e+00  5.40718973e-01\n",
      "   1.44265807e+00  1.00457668e+00 -3.29704523e-01 -3.50654507e+00\n",
      "   5.81479692e+00 -1.64918900e+00  5.93607235e+00 -9.81637895e-01\n",
      "   7.78881550e+00 -2.46479130e+00  6.35267496e+00 -4.65612173e+00\n",
      "  -9.59672260e+00  7.22259700e-01  1.27237928e+00  2.80274725e+00]\n",
      " [ 2.32027939e+02 -1.98139172e+01 -3.57799301e+01 -2.88273735e+01\n",
      "  -3.51436081e+01 -2.29334259e+01 -2.04480286e+01  3.86417294e+00\n",
      "  -2.39084892e+01 -2.90929413e+01 -2.32626705e+01 -1.03776503e+01\n",
      "  -1.17681484e+01 -8.97262383e+00 -2.38974667e+00 -3.36574783e+01\n",
      "  -1.26381016e+01 -7.74201155e+00 -5.17843914e+00 -1.27360258e+01\n",
      "   3.67323780e+00 -8.97040129e-01  2.29593635e+00 -8.06331825e+00\n",
      "   9.21215630e+00 -1.82077885e+01  2.43793602e+01 -1.82021980e+01\n",
      "   2.33041458e+01 -9.75375271e+00  3.02832184e+01  2.31195660e+01\n",
      "  -1.65442238e+01  2.03418388e+01  4.78798676e+00  1.07925949e+01]\n",
      " [ 3.40046173e+02 -2.80224075e+01 -4.85800552e+01 -3.58210030e+01\n",
      "  -4.82861023e+01 -3.66863403e+01 -2.23375835e+01  7.79526711e+00\n",
      "  -3.14498367e+01 -3.85495377e+01 -3.19486752e+01 -1.39444046e+01\n",
      "  -1.45946074e+01 -1.69476545e+00 -9.95216656e+00 -4.33240204e+01\n",
      "  -1.09459648e+01 -1.40208807e+01 -1.62898707e+00 -1.65940590e+01\n",
      "   7.15616655e+00  1.66645069e+01 -5.08251858e+00 -2.14065571e+01\n",
      "   8.68064022e+00 -3.15195713e+01  3.24510651e+01 -2.33965282e+01\n",
      "   2.56175823e+01 -1.57433634e+01  3.71507072e+01  2.73540077e+01\n",
      "  -2.44600697e+01  3.08003845e+01 -5.52811670e+00  7.09675455e+00]\n",
      " [ 4.18378418e+02 -3.82957649e+01 -6.58449326e+01 -5.02055664e+01\n",
      "  -6.46914902e+01 -5.31180344e+01 -3.14289036e+01  9.08419514e+00\n",
      "  -4.64140625e+01 -5.30498199e+01 -4.12475662e+01 -2.04676437e+01\n",
      "  -1.80891132e+01 -2.67725587e+00 -1.76994362e+01 -4.63377380e+01\n",
      "  -1.77744675e+01 -1.87565517e+01  1.86013544e+00 -1.31192913e+01\n",
      "   4.30420256e+00  1.82650108e+01 -1.01226206e+01 -1.90146866e+01\n",
      "   7.99512053e+00 -4.51416359e+01  3.68107147e+01 -3.06005211e+01\n",
      "   3.01987839e+01 -1.91930351e+01  4.18001328e+01  2.47320614e+01\n",
      "  -3.25916634e+01  3.21780243e+01 -1.66395342e+00  2.25928783e+00]\n",
      " [ 4.49717194e+02 -3.37779655e+01 -6.24165802e+01 -4.59314728e+01\n",
      "  -6.13889999e+01 -4.82191849e+01 -2.73273335e+01  1.08607416e+01\n",
      "  -4.01669388e+01 -4.87123184e+01 -3.58331146e+01 -2.01718540e+01\n",
      "  -1.77918530e+01 -3.85131145e+00 -1.33781109e+01 -4.58913307e+01\n",
      "  -1.97629681e+01 -1.98517513e+01  1.16139457e-01 -1.17177553e+01\n",
      "   5.33111906e+00  1.65975952e+01 -9.73119259e+00 -1.93683300e+01\n",
      "   6.45680046e+00 -4.96712532e+01  3.58666878e+01 -3.22272644e+01\n",
      "   3.10057011e+01 -1.74701653e+01  4.23790588e+01  1.99168816e+01\n",
      "  -3.46445694e+01  3.15159531e+01 -5.51544249e-01  1.14832306e+00]\n",
      " [ 4.86017822e+02 -4.06540108e+01 -6.91209793e+01 -4.93499374e+01\n",
      "  -6.82111359e+01 -5.31949348e+01 -3.15890980e+01  1.14315348e+01\n",
      "  -4.65089340e+01 -5.51735001e+01 -3.88423462e+01 -2.18034019e+01\n",
      "  -1.66099472e+01 -3.98603654e+00 -1.22294903e+01 -4.88287277e+01\n",
      "  -2.01140099e+01 -2.27963276e+01  9.78272915e-01 -8.31320000e+00\n",
      "   8.88088131e+00  1.60557060e+01 -5.82075214e+00 -1.95750828e+01\n",
      "   8.21380997e+00 -5.40456581e+01  3.44650459e+01 -3.37805862e+01\n",
      "   3.11400928e+01 -1.59901619e+01  4.12623062e+01  1.91739502e+01\n",
      "  -3.39488411e+01  3.30845108e+01  1.45967269e+00 -2.71032572e-01]\n",
      " [ 4.92057434e+02 -4.64582405e+01 -7.75229187e+01 -5.51336746e+01\n",
      "  -7.38760147e+01 -5.89105186e+01 -3.59810028e+01  1.13040533e+01\n",
      "  -5.20816460e+01 -6.01585236e+01 -4.59738693e+01 -1.90153637e+01\n",
      "  -1.40983572e+01 -4.01327419e+00 -1.07505493e+01 -4.48131790e+01\n",
      "  -1.66788044e+01 -2.01352654e+01 -5.52756071e-01 -9.72474384e+00\n",
      "   8.00524807e+00  1.22717552e+01 -6.44910192e+00 -1.70714703e+01\n",
      "   6.42071009e+00 -5.22460480e+01  3.15128059e+01 -3.69832420e+01\n",
      "   2.99428902e+01 -1.63252888e+01  4.36572227e+01  1.77310028e+01\n",
      "  -3.12984180e+01  3.29491272e+01  1.96656692e+00  2.11898947e+00]\n",
      " [ 5.20117615e+02 -4.40622787e+01 -7.57354507e+01 -4.98078156e+01\n",
      "  -6.95522766e+01 -5.45864716e+01 -3.00342102e+01  1.20999403e+01\n",
      "  -4.89446259e+01 -5.62178802e+01 -4.14299316e+01 -2.48550014e+01\n",
      "  -1.71441135e+01 -9.69488049e+00 -1.14756870e+01 -4.91099930e+01\n",
      "  -1.98899269e+01 -2.38834343e+01 -6.39354467e+00 -4.56998301e+00\n",
      "   3.80165458e+00  9.97478962e+00 -5.92697477e+00 -1.09466972e+01\n",
      "   1.30608311e+01 -4.99687271e+01  3.61813087e+01 -3.86517792e+01\n",
      "   3.30591316e+01 -1.75439396e+01  4.81584129e+01  1.86974678e+01\n",
      "  -3.05942421e+01  3.15188522e+01  5.71761560e+00  4.72194052e+00]\n",
      " [ 5.22413757e+02 -5.54680405e+01 -8.58505173e+01 -5.68030319e+01\n",
      "  -7.99363327e+01 -6.49420853e+01 -3.92828217e+01  1.03220949e+01\n",
      "  -5.87248421e+01 -6.50351486e+01 -5.19627838e+01 -2.45893593e+01\n",
      "  -1.07468319e+01 -3.41759562e+00 -7.71641207e+00 -4.03847084e+01\n",
      "  -1.58208199e+01 -1.87032185e+01  7.68847764e-03 -5.64105892e+00\n",
      "   6.27451038e+00  7.00186777e+00  4.30363953e-01 -1.94427872e+01\n",
      "   9.56973457e+00 -4.67644997e+01  2.97870007e+01 -4.59518013e+01\n",
      "   2.64379559e+01 -1.93233852e+01  4.32751923e+01  1.65782471e+01\n",
      "  -3.14031296e+01  2.67619438e+01  4.02453375e+00 -5.34501839e+00]\n",
      " [ 5.29670654e+02 -7.72587967e+01 -1.07870438e+02 -7.70230484e+01\n",
      "  -1.00733322e+02 -8.78548126e+01 -6.06306992e+01  1.29741182e+01\n",
      "  -7.93575821e+01 -8.45597687e+01 -7.39904175e+01 -2.36481266e+01\n",
      "  -9.80850124e+00  3.80994987e+00 -5.02976131e+00 -5.37006760e+01\n",
      "  -1.39662151e+01 -1.32607098e+01  7.12622786e+00 -3.02602530e+00\n",
      "   1.31945407e+00  6.55846500e+00  7.23755407e+00 -1.54978304e+01\n",
      "   6.54095316e+00 -4.52260399e+01  3.08950214e+01 -5.83597412e+01\n",
      "   2.02200966e+01 -2.53835850e+01  3.96629715e+01  1.71822262e+01\n",
      "  -2.85804424e+01  3.05569382e+01  7.74819517e+00 -1.13970890e+01]\n",
      " [ 5.17496826e+02 -1.12908691e+02 -1.42599182e+02 -1.09622032e+02\n",
      "  -1.32490631e+02 -1.21773102e+02 -9.78531494e+01  7.13365698e+00\n",
      "  -1.14143242e+02 -1.14493607e+02 -1.07156670e+02 -2.97494183e+01\n",
      "  -9.09716415e+00  1.57327509e+01  1.16360652e+00 -4.91720619e+01\n",
      "  -2.29316216e+01 -1.21519222e+01 -4.65836859e+00 -5.61740112e+00\n",
      "   1.38626993e+00 -9.13550377e-01  3.99600363e+00 -1.34379120e+01\n",
      "   3.84389853e+00 -3.96642036e+01  3.60105286e+01 -5.92558708e+01\n",
      "   2.79339161e+01 -3.46088486e+01  4.30447235e+01  1.52034116e+00\n",
      "  -2.69315910e+01  3.14182148e+01 -1.70281696e+00  6.56327629e+00]\n",
      " [ 3.72144135e+02 -2.06275131e+02 -2.33345947e+02 -2.06163269e+02\n",
      "  -2.24594284e+02 -2.17356628e+02 -1.94808243e+02 -9.87874374e+01\n",
      "  -2.11573517e+02 -2.13312759e+02 -2.04081757e+02 -9.62901382e+01\n",
      "  -5.01673546e+01  2.05462952e+01 -4.95962977e-01 -1.13232597e+02\n",
      "  -9.73946533e+01 -4.89266853e+01 -3.96566582e+01 -9.07439518e+00\n",
      "  -9.23643017e+00 -2.37905655e+01 -6.09894333e+01 -1.70191936e+01\n",
      "   4.44562054e+00 -6.35465927e+01  1.53006042e+02 -1.16008568e+02\n",
      "   1.45455124e+02 -8.19201202e+01  8.73996964e+01  6.20881414e+00\n",
      "   2.83235874e+01  1.01763199e+02 -6.91058540e+00  1.99231918e+02]]\n",
      "\n",
      "time step 0\n",
      "current input <PATH_START>\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(3,4)', Array(0.34943023, dtype=float32)), ('(4,2)', Array(0.24957521, dtype=float32)), ('(4,0)', Array(0.06773873, dtype=float32)), ('(4,4)', Array(0.05836046, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,4)', Array(2.404768e-10, dtype=float32)), ('(3,1)', Array(8.1424714e-11, dtype=float32)), ('(4,0)', Array(8.052722e-11, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,4)', Array(6.5360285e-17, dtype=float32)), ('(4,3)', Array(6.454484e-18, dtype=float32)), ('(3,2)', Array(5.281558e-18, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,4)', Array(1.0855137e-24, dtype=float32)), ('(4,2)', Array(3.301499e-25, dtype=float32)), ('(4,1)', Array(1.0586698e-25, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,1)', Array(7.908479e-31, dtype=float32)), ('(4,2)', Array(5.2048374e-32, dtype=float32)), ('(3,4)', Array(5.9719044e-33, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32)), ('<TARGET_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32)), ('<TARGET_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32)), ('<TARGET_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32)), ('<TARGET_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32)), ('<TARGET_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(0,0)', Array(1.9728575e-27, dtype=float32)), ('(1,1)', Array(4.9563956e-35, dtype=float32)), ('(3,4)', Array(1.6134368e-38, dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(0,0)', Array(1., dtype=float32)), ('(0,2)', Array(9.425438e-22, dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(0,0)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 1\n",
      "current input (0,0)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(3,4)', Array(0.5459985, dtype=float32)), ('(1,4)', Array(0.10719894, dtype=float32)), ('<PATH_END>', Array(0.09596452, dtype=float32)), ('(3,2)', Array(0.0921266, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(0.99974364, dtype=float32)), ('(3,4)', Array(0.00010524, dtype=float32)), ('(1,4)', Array(8.4707586e-05, dtype=float32)), ('(3,2)', Array(4.3019434e-05, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(2.747662e-10, dtype=float32)), ('(1,4)', Array(1.9236418e-12, dtype=float32)), (';', Array(1.1337403e-12, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(0.9999999, dtype=float32)), ('(1,0)', Array(1.2536354e-07, dtype=float32)), ('<PATH_END>', Array(4.4807704e-09, dtype=float32)), ('(2,3)', Array(1.2982032e-10, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PADDING>', Array(1.0145124e-15, dtype=float32)), ('<ADJLIST_END>', Array(1.1808657e-16, dtype=float32)), ('<-->', Array(5.1955936e-17, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PADDING>', Array(1.9645906e-18, dtype=float32)), ('<ADJLIST_END>', Array(3.4105994e-20, dtype=float32)), ('<-->', Array(1.2776264e-20, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,0)', Array(2.5165777e-25, dtype=float32)), ('<PADDING>', Array(3.665382e-27, dtype=float32)), ('<PATH_END>', Array(2.9072743e-28, dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,0)', Array(4.1973186e-27, dtype=float32)), ('<PATH_END>', Array(3.4130434e-29, dtype=float32)), ('<PADDING>', Array(2.1288002e-29, dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,0)', Array(2.2272995e-28, dtype=float32)), ('<PATH_END>', Array(1.1657125e-36, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,0)', Array(3.985738e-12, dtype=float32)), ('<PATH_END>', Array(2.360482e-26, dtype=float32)), ('<PATH_START>', Array(2.9929622e-38, dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('(1,0)', Array(1., dtype=float32)), ('<PATH_END>', Array(2.2694923e-32, dtype=float32)), ('<ADJLIST_START>', Array(2.673813e-37, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(1,0)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(1,0)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 2\n",
      "current input (1,0)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(4,0)', Array(0.4848967, dtype=float32)), ('(1,4)', Array(0.26609895, dtype=float32)), ('(0,0)', Array(0.07865646, dtype=float32)), ('<PATH_END>', Array(0.04899121, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(0.99794704, dtype=float32)), ('(1,4)', Array(0.00151598, dtype=float32)), ('(4,0)', Array(0.00016732, dtype=float32)), ('(3,1)', Array(0.00015215, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(0,0)', Array(5.237853e-09, dtype=float32)), ('(1,4)', Array(2.061075e-09, dtype=float32)), ('(2,0)', Array(4.4613e-10, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(0.9986985, dtype=float32)), ('(2,0)', Array(0.00119038, dtype=float32)), ('(0,0)', Array(0.00011091, dtype=float32)), ('(1,4)', Array(1.05504135e-07, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,0)', Array(5.8927696e-10, dtype=float32)), ('(1,4)', Array(3.3436663e-18, dtype=float32)), ('(3,2)', Array(1.9526119e-20, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,0)', Array(7.645785e-12, dtype=float32)), ('(3,0)', Array(1.7054678e-24, dtype=float32)), ('(0,0)', Array(1.8601651e-25, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,0)', Array(9.703576e-24, dtype=float32)), ('(2,1)', Array(4.3328947e-38, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,0)', Array(1.4222827e-25, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,0)', Array(5.3210116e-26, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('<ADJLIST_START>', Array(0.9998357, dtype=float32)), ('(2,0)', Array(0.00016424, dtype=float32)), ('(3,0)', Array(2.8763127e-33, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('(2,0)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(3.649814e-18, dtype=float32)), ('(3,0)', Array(1.7196426e-33, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(2,0)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(2,0)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 3\n",
      "current input (2,0)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(4,0)', Array(0.38941613, dtype=float32)), ('(0,4)', Array(0.15970773, dtype=float32)), ('(0,0)', Array(0.13928567, dtype=float32)), ('(4,1)', Array(0.04700954, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,0)', Array(1.9665394e-11, dtype=float32)), ('(2,3)', Array(1.3072471e-11, dtype=float32)), ('(3,1)', Array(1.0999247e-11, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,0)', Array(9.695678e-20, dtype=float32)), ('(1,0)', Array(6.006093e-22, dtype=float32)), ('(3,2)', Array(5.919996e-22, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,0)', Array(1.801705e-23, dtype=float32)), ('(1,0)', Array(9.441983e-24, dtype=float32)), (';', Array(2.2171537e-27, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_START>', Array(1.9228923e-32, dtype=float32)), (';', Array(9.040995e-33, dtype=float32)), ('<-->', Array(4.8002443e-33, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,0)', Array(2.1775818e-27, dtype=float32)), ('<-->', Array(4.1191124e-38, dtype=float32)), ('(1,0)', Array(2.1079338e-38, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,0)', Array(7.9178245e-19, dtype=float32)), ('(1,0)', Array(3.7234863e-35, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,0)', Array(2.1169553e-19, dtype=float32)), ('(1,0)', Array(1.6630061e-34, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,0)', Array(7.949469e-37, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,0)', Array(5.1959487e-25, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,0)', Array(2.2081457e-21, dtype=float32)), ('(2,1)', Array(1.5386105e-26, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(2,1)', Array(1., dtype=float32)), ('(3,0)', Array(2.0793501e-23, dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(2,1)', Array(1., dtype=float32)), ('(3,0)', Array(3.281645e-34, dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 4\n",
      "current input (2,1)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('<PATH_END>', Array(0.41563997, dtype=float32)), ('(3,0)', Array(0.26610503, dtype=float32)), ('(0,3)', Array(0.08386487, dtype=float32)), ('(4,1)', Array(0.03851634, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(0.99999785, dtype=float32)), ('(4,3)', Array(6.6240443e-07, dtype=float32)), ('(2,3)', Array(3.6124825e-07, dtype=float32)), ('(3,2)', Array(2.9659867e-07, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,1)', Array(8.550665e-12, dtype=float32)), ('(3,0)', Array(1.1257998e-12, dtype=float32)), ('(3,2)', Array(6.2305044e-13, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,1)', Array(1.0405533e-12, dtype=float32)), ('(2,0)', Array(2.2992445e-15, dtype=float32)), ('(3,3)', Array(3.2916801e-16, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,0)', Array(3.0683377e-22, dtype=float32)), ('(3,1)', Array(5.1430497e-29, dtype=float32)), ('(3,0)', Array(4.1297514e-29, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,0)', Array(3.2858723e-23, dtype=float32)), ('(3,0)', Array(4.8790637e-30, dtype=float32)), ('(2,2)', Array(1.9009559e-30, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,0)', Array(4.3425097e-30, dtype=float32)), ('(2,2)', Array(2.1961582e-36, dtype=float32)), ('(3,0)', Array(1.0090667e-37, dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,0)', Array(2.3061134e-30, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,2)', Array(2.3279913e-36, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,2)', Array(2.4456315e-28, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,2)', Array(2.886268e-13, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(2,2)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(2,2)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 5\n",
      "current input (2,2)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(2,1)', Array(0.8146962, dtype=float32)), ('(3,1)', Array(0.03884511, dtype=float32)), ('(3,0)', Array(0.02459517, dtype=float32)), ('(4,4)', Array(0.01828969, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(0.98805827, dtype=float32)), ('(3,1)', Array(0.0085125, dtype=float32)), ('(2,1)', Array(0.00140452, dtype=float32)), ('(1,4)', Array(0.00116006, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(0.99722517, dtype=float32)), ('(2,1)', Array(0.00267412, dtype=float32)), ('(3,2)', Array(9.530791e-05, dtype=float32)), ('(2,3)', Array(2.8199104e-06, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,1)', Array(5.233262e-08, dtype=float32)), ('(3,2)', Array(2.5323577e-09, dtype=float32)), ('(1,2)', Array(3.8960106e-11, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,1)', Array(1.5549317e-10, dtype=float32)), ('(3,2)', Array(3.1545482e-12, dtype=float32)), ('(2,3)', Array(8.053626e-19, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,2)', Array(1.5284064e-21, dtype=float32)), ('(2,1)', Array(1.4421112e-22, dtype=float32)), ('(2,3)', Array(1.28957525e-33, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,2)', Array(3.383594e-27, dtype=float32)), ('(2,1)', Array(4.1686933e-34, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,2)', Array(3.799518e-28, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,2)', Array(4.556624e-31, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('<ADJLIST_START>', Array(0.9999703, dtype=float32)), ('(3,2)', Array(2.9626288e-05, dtype=float32)), ('(2,1)', Array(1.0123468e-18, dtype=float32)), ('(4,2)', Array(3.0963466e-23, dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('(3,2)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(4.0427848e-09, dtype=float32)), ('(2,1)', Array(4.1962997e-32, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(3,2)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(3,2)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 6\n",
      "current input (3,2)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(3,0)', Array(0.17420027, dtype=float32)), ('<ORIGIN_END>', Array(0.140873, dtype=float32)), ('<TARGET_END>', Array(0.09216966, dtype=float32)), ('(0,4)', Array(0.08410706, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(0.99999225, dtype=float32)), ('(1,4)', Array(5.538375e-06, dtype=float32)), ('(3,1)', Array(1.3454811e-06, dtype=float32)), ('(3,4)', Array(2.0271885e-07, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(3,1)', Array(1.6708494e-08, dtype=float32)), ('(4,2)', Array(6.009338e-10, dtype=float32)), ('(3,3)', Array(4.7624354e-10, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,2)', Array(2.4956761e-20, dtype=float32)), ('(3,1)', Array(2.5688722e-21, dtype=float32)), ('(3,3)', Array(1.5960046e-21, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PADDING>', Array(8.8201114e-26, dtype=float32)), ('<-->', Array(8.5955714e-26, dtype=float32)), ('<TARGET_END>', Array(7.845907e-26, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<-->', Array(9.459603e-26, dtype=float32)), (';', Array(4.7670838e-27, dtype=float32)), ('<PADDING>', Array(9.2269705e-28, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PADDING>', Array(2.116375e-33, dtype=float32)), ('<TARGET_END>', Array(2.3804201e-34, dtype=float32)), ('<-->', Array(6.383443e-35, dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,2)', Array(5.432588e-33, dtype=float32)), ('<TARGET_END>', Array(1.4845865e-33, dtype=float32)), ('<PADDING>', Array(1.0501659e-34, dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,2)', Array(1.01512975e-19, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('(4,2)', Array(0.9773047, dtype=float32)), ('<ADJLIST_START>', Array(0.02269527, dtype=float32)), ('(3,2)', Array(9.43507e-24, dtype=float32)), ('(1,2)', Array(1.0380734e-27, dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,2)', Array(7.950961e-13, dtype=float32)), ('(3,3)', Array(4.8583004e-37, dtype=float32)), ('(4,4)', Array(4.2302174e-37, dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(4,2)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(4,2)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 7\n",
      "current input (4,2)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(4,1)', Array(0.3994633, dtype=float32)), ('(1,2)', Array(0.1941262, dtype=float32)), ('(4,3)', Array(0.10019951, dtype=float32)), ('(0,4)', Array(0.05857243, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('(4,3)', Array(0.8929858, dtype=float32)), ('<ADJLIST_START>', Array(0.09062723, dtype=float32)), ('(3,2)', Array(0.00876732, dtype=float32)), ('(3,3)', Array(0.00473605, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('(4,3)', Array(0.9992084, dtype=float32)), ('<ADJLIST_START>', Array(0.00067968, dtype=float32)), ('(3,2)', Array(8.8785266e-05, dtype=float32)), ('(4,1)', Array(2.312778e-05, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('(4,3)', Array(0.9959675, dtype=float32)), ('<ADJLIST_START>', Array(0.00390456, dtype=float32)), ('(4,1)', Array(0.00012656, dtype=float32)), ('(3,2)', Array(1.399828e-06, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(0.99998677, dtype=float32)), ('(4,3)', Array(1.3217201e-05, dtype=float32)), ('(4,1)', Array(3.5618182e-08, dtype=float32)), ('(3,2)', Array(9.35167e-13, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(0.9999976, dtype=float32)), ('(4,3)', Array(2.3633024e-06, dtype=float32)), ('(4,1)', Array(4.993987e-10, dtype=float32)), ('(3,2)', Array(4.6024e-16, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,3)', Array(1.0491707e-08, dtype=float32)), ('(4,1)', Array(4.0107644e-13, dtype=float32)), ('(3,2)', Array(2.0232034e-21, dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,3)', Array(4.1950982e-10, dtype=float32)), ('(4,1)', Array(5.9533954e-16, dtype=float32)), ('(3,2)', Array(3.0158865e-29, dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(0.9852968, dtype=float32)), ('(4,3)', Array(0.01470317, dtype=float32)), ('(4,1)', Array(2.5356672e-10, dtype=float32)), ('<PATH_END>', Array(6.7914765e-23, dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('(4,3)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(5.875334e-16, dtype=float32)), ('(4,1)', Array(9.499295e-21, dtype=float32)), ('<PATH_END>', Array(2.095136e-26, dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('(4,3)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(2.7532333e-29, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(4,3)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(4,3)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 8\n",
      "current input (4,3)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(0,4)', Array(0.69257164, dtype=float32)), ('(1,0)', Array(0.11008929, dtype=float32)), ('(1,3)', Array(0.07621478, dtype=float32)), ('(4,4)', Array(0.03015759, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(0.9999999, dtype=float32)), ('(3,3)', Array(2.1311466e-08, dtype=float32)), ('(4,4)', Array(1.9984292e-08, dtype=float32)), ('(3,1)', Array(1.9430495e-08, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,4)', Array(3.859131e-13, dtype=float32)), ('(4,2)', Array(1.8481583e-14, dtype=float32)), ('(4,3)', Array(1.2055454e-14, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,4)', Array(2.3410556e-21, dtype=float32)), ('(3,2)', Array(3.7421037e-24, dtype=float32)), ('(4,2)', Array(6.236099e-26, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,4)', Array(1.3167232e-23, dtype=float32)), ('(4,2)', Array(2.2435967e-27, dtype=float32)), ('(3,2)', Array(4.707645e-28, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,4)', Array(4.249329e-17, dtype=float32)), ('(4,2)', Array(2.7711873e-22, dtype=float32)), ('(1,0)', Array(8.870991e-27, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,4)', Array(1.974524e-17, dtype=float32)), ('(4,2)', Array(1.8229619e-25, dtype=float32)), ('(1,0)', Array(8.707536e-29, dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('(4,4)', Array(0.99686015, dtype=float32)), ('<ADJLIST_START>', Array(0.00313988, dtype=float32)), ('(1,0)', Array(6.2299567e-18, dtype=float32)), ('(2,4)', Array(2.4114973e-22, dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('(4,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(8.991649e-16, dtype=float32)), ('(1,0)', Array(1.47921e-26, dtype=float32)), ('(2,4)', Array(7.4864125e-27, dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('(4,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(1.8023704e-32, dtype=float32)), ('(2,4)', Array(2.3193146e-35, dtype=float32)), ('(4,2)', Array(5.749357e-38, dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('(4,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(4,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(4,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 9\n",
      "current input (4,4)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(3,4)', Array(0.16502994, dtype=float32)), ('(3,0)', Array(0.16319913, dtype=float32)), ('(1,4)', Array(0.15407194, dtype=float32)), ('<PATH_END>', Array(0.10712119, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(0.9999783, dtype=float32)), ('(1,4)', Array(7.4190166e-06, dtype=float32)), ('(3,4)', Array(6.1841297e-06, dtype=float32)), ('(2,3)', Array(5.747117e-06, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(0.9999999, dtype=float32)), ('<PATH_END>', Array(1.5670906e-07, dtype=float32)), ('(4,3)', Array(1.0165459e-08, dtype=float32)), ('(3,4)', Array(7.42492e-09, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(1.6012122e-10, dtype=float32)), ('(4,3)', Array(1.4166823e-12, dtype=float32)), (';', Array(2.9064635e-14, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(4.007243e-15, dtype=float32)), ('<PADDING>', Array(7.776875e-16, dtype=float32)), (';', Array(2.7668637e-17, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(3.7534584e-15, dtype=float32)), ('<PADDING>', Array(5.5488516e-18, dtype=float32)), (';', Array(5.167574e-21, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(2.0366535e-29, dtype=float32)), ('<PADDING>', Array(5.68955e-32, dtype=float32)), (';', Array(2.9410416e-34, dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(9.1428905e-26, dtype=float32)), ('<PADDING>', Array(6.2037716e-37, dtype=float32)), ('(3,4)', Array(1.9252729e-37, dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(1.673943e-16, dtype=float32)), ('(3,4)', Array(5.1049447e-25, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(2.0772856e-11, dtype=float32)), ('(3,4)', Array(7.2364116e-16, dtype=float32)), ('<TARGET_END>', Array(4.0159553e-35, dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('<ADJLIST_START>', Array(0.9999993, dtype=float32)), ('(3,4)', Array(7.2715e-07, dtype=float32)), ('<PATH_END>', Array(2.9927776e-08, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(3,4)', Array(0.9999993, dtype=float32)), ('<PATH_END>', Array(6.6537416e-07, dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(3,4)', Array(1., dtype=float32)), ('<PATH_END>', Array(1.1145203e-17, dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 10\n",
      "current input (3,4)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(4,4)', Array(0.4738857, dtype=float32)), ('(2,4)', Array(0.11350856, dtype=float32)), ('(0,4)', Array(0.08823451, dtype=float32)), ('<PATH_END>', Array(0.0702965, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(0.99999964, dtype=float32)), ('(4,4)', Array(3.2841623e-07, dtype=float32)), ('(3,3)', Array(2.590785e-08, dtype=float32)), ('(1,4)', Array(1.02074615e-08, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,4)', Array(9.720987e-17, dtype=float32)), ('(2,4)', Array(5.580803e-17, dtype=float32)), ('(3,3)', Array(4.9999635e-18, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(4,4)', Array(4.3185122e-20, dtype=float32)), ('(2,4)', Array(1.1452001e-20, dtype=float32)), ('(1,4)', Array(2.2597053e-24, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,4)', Array(1.0514941e-24, dtype=float32)), ('(3,3)', Array(3.715714e-33, dtype=float32)), ('(1,4)', Array(3.4484617e-34, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,4)', Array(8.554493e-17, dtype=float32)), ('(4,4)', Array(1.1247089e-28, dtype=float32)), ('(3,3)', Array(7.252997e-30, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,4)', Array(2.1256219e-34, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,4)', Array(4.239476e-36, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32)), ('<TARGET_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,4)', Array(4.9125258e-32, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(2,4)', Array(3.097663e-33, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(2,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(2,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 11\n",
      "current input (2,4)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(3,0)', Array(0.4692728, dtype=float32)), ('(3,4)', Array(0.1796457, dtype=float32)), ('(1,4)', Array(0.13696767, dtype=float32)), ('(4,1)', Array(0.05096765, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(0.9958276, dtype=float32)), ('(1,4)', Array(0.00272821, dtype=float32)), ('(3,4)', Array(0.00140534, dtype=float32)), ('(2,3)', Array(2.5520292e-05, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(0.99999774, dtype=float32)), ('(1,4)', Array(1.6308072e-06, dtype=float32)), ('(3,4)', Array(6.259991e-07, dtype=float32)), ('(2,3)', Array(2.5877128e-12, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,4)', Array(8.187287e-12, dtype=float32)), ('(3,4)', Array(1.3335891e-12, dtype=float32)), ('(3,0)', Array(1.8259305e-21, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,4)', Array(1.3963132e-08, dtype=float32)), ('(3,4)', Array(5.4169036e-10, dtype=float32)), ('(3,0)', Array(1.0072466e-24, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(0.9999995, dtype=float32)), ('(1,4)', Array(5.2115575e-07, dtype=float32)), ('(3,4)', Array(1.4475601e-09, dtype=float32)), ('<PATH_END>', Array(4.0258103e-23, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,4)', Array(3.469114e-17, dtype=float32)), ('(3,4)', Array(2.6928288e-24, dtype=float32)), ('<PATH_END>', Array(2.94951e-36, dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,4)', Array(5.3349814e-16, dtype=float32)), ('(3,4)', Array(1.16596396e-32, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,4)', Array(6.7110536e-21, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,4)', Array(3.156224e-14, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,4)', Array(3.049168e-14, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(1,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(1,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 12\n",
      "current input (1,4)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('<ADJLIST_START>', Array(0.30389845, dtype=float32)), ('(0,4)', Array(0.29555914, dtype=float32)), ('(4,1)', Array(0.11360329, dtype=float32)), ('(4,2)', Array(0.06831943, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<ORIGIN_END>', Array(1.2993107e-10, dtype=float32)), ('<PATH_START>', Array(1.05677744e-10, dtype=float32)), ('<TARGET_END>', Array(5.716119e-11, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(0,4)', Array(1.09002036e-16, dtype=float32)), ('<PATH_START>', Array(2.6635174e-17, dtype=float32)), ('<ORIGIN_END>', Array(2.1806354e-17, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(0,4)', Array(8.519539e-12, dtype=float32)), ('(2,4)', Array(4.4911042e-14, dtype=float32)), ('(1,3)', Array(1.613228e-15, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(0,4)', Array(6.865496e-18, dtype=float32)), ('(2,4)', Array(2.8933928e-18, dtype=float32)), ('(1,3)', Array(3.164385e-22, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(0,4)', Array(7.86756e-15, dtype=float32)), ('(2,4)', Array(1.4485775e-16, dtype=float32)), ('(1,3)', Array(4.4353486e-20, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(0,4)', Array(7.186807e-17, dtype=float32)), ('(1,3)', Array(9.841152e-20, dtype=float32)), ('(2,4)', Array(2.6681098e-20, dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(1,3)', Array(1.3820747e-22, dtype=float32)), ('(0,4)', Array(1.0952086e-25, dtype=float32)), ('(2,4)', Array(1.3293338e-33, dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('(0,4)', Array(7.0727995e-31, dtype=float32)), ('(1,3)', Array(6.6064203e-31, dtype=float32)), ('(2,4)', Array(2.2713389e-38, dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('<ADJLIST_START>', Array(0.94454443, dtype=float32)), ('(0,4)', Array(0.05545554, dtype=float32)), ('(2,4)', Array(1.3553552e-12, dtype=float32)), ('(4,4)', Array(8.366581e-14, dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('(0,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(5.884846e-08, dtype=float32)), ('(2,4)', Array(7.91086e-09, dtype=float32)), ('(3,4)', Array(4.0122713e-17, dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('(0,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('(0,4)', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n",
      "\n",
      "time step 13\n",
      "current input (0,4)\n",
      "maze transformer prediction\n",
      "logit lense layer 0:\n",
      "[('(3,0)', Array(0.5679514, dtype=float32)), ('(2,1)', Array(0.08237944, dtype=float32)), ('(2,2)', Array(0.07724677, dtype=float32)), ('(3,3)', Array(0.04626871, dtype=float32))]\n",
      "logit lense layer 1:\n",
      "[('<ADJLIST_START>', Array(0.92485535, dtype=float32)), ('(1,4)', Array(0.06255596, dtype=float32)), ('(4,4)', Array(0.0041534, dtype=float32)), ('(2,3)', Array(0.00223936, dtype=float32))]\n",
      "logit lense layer 2:\n",
      "[('<ADJLIST_START>', Array(0.99998295, dtype=float32)), ('(1,4)', Array(1.5340776e-05, dtype=float32)), ('<PATH_END>', Array(1.6210098e-06, dtype=float32)), ('(0,3)', Array(8.964026e-09, dtype=float32))]\n",
      "logit lense layer 3:\n",
      "[('<ADJLIST_START>', Array(0.99998796, dtype=float32)), ('(1,4)', Array(8.522957e-06, dtype=float32)), ('<PATH_END>', Array(3.616261e-06, dtype=float32)), ('(0,3)', Array(1.868491e-11, dtype=float32))]\n",
      "logit lense layer 4:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(2.6723201e-10, dtype=float32)), ('(1,4)', Array(1.21961365e-11, dtype=float32)), ('(0,3)', Array(9.36571e-18, dtype=float32))]\n",
      "logit lense layer 5:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(1.623891e-15, dtype=float32)), ('(0,3)', Array(9.1215985e-20, dtype=float32)), ('(1,4)', Array(4.8416263e-24, dtype=float32))]\n",
      "logit lense layer 6:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(1.8546804e-18, dtype=float32)), ('(0,3)', Array(1.6124483e-28, dtype=float32)), ('(1,4)', Array(6.733681e-33, dtype=float32))]\n",
      "logit lense layer 7:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(6.570292e-20, dtype=float32)), ('(1,4)', Array(1.9490397e-35, dtype=float32)), ('(0,3)', Array(2.7228904e-37, dtype=float32))]\n",
      "logit lense layer 8:\n",
      "[('<ADJLIST_START>', Array(1., dtype=float32)), ('<PATH_END>', Array(3.1499766e-17, dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 9:\n",
      "[('<ADJLIST_START>', Array(0.9197586, dtype=float32)), ('<PATH_END>', Array(0.0802414, dtype=float32)), ('(1,4)', Array(8.1703526e-27, dtype=float32)), ('(3,0)', Array(7.000106e-27, dtype=float32))]\n",
      "logit lense layer 10:\n",
      "[('<ADJLIST_START>', Array(0.990724, dtype=float32)), ('<PATH_END>', Array(0.009276, dtype=float32)), ('(1,4)', Array(4.9049847e-30, dtype=float32)), ('(2,4)', Array(4.1351816e-33, dtype=float32))]\n",
      "logit lense layer 11:\n",
      "[('<PATH_END>', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "logit lense layer 12:\n",
      "[('<PATH_END>', Array(1., dtype=float32)), ('<ADJLIST_START>', Array(0., dtype=float32)), ('<ADJLIST_END>', Array(0., dtype=float32)), ('<TARGET_START>', Array(0., dtype=float32))]\n",
      "target:\n",
      "(0,4)\n"
     ]
    }
   ],
   "source": [
    "maze_num = 3\n",
    "timestep = 3\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "x_mod = copy.deepcopy(batch)\n",
    "\n",
    "#x_mod['data'][maze_num][x_mod['start_index'][maze_num]+2] = vocab_map('(1,1)')\n",
    "#x_mod['data'][maze_num][x_mod['start_index'][maze_num]+7] = vocab_map('(3,4)')\n",
    "#x_mod['data'][maze_num][x_mod['start_index'][maze_num]+8] = vocab_map('(4,4)')\n",
    "#x_mod['data'][maze_num][x_mod['start_index'][maze_num]+9] = vocab_map('(4,3)')\n",
    "#x_mod['end_index'][maze_num]=x['end_index'][maze_num]+1\n",
    "#intervention = get_intervention(probe_state,alpha=200, min_layer=5, goal_i=target_dict[vocab_map('(4,4)')])\n",
    "#intervention = get_intervention(probe_state,x_mod, alpha=100, min_layer=3, goal_i=target_dict[vocab_map('(0,0)')],timestep=timestep)\n",
    "#intervention = lambda y,i:y\n",
    "\n",
    "logits = logit_lens(state,batch)\n",
    "print(logits[maze_num][timestep])\n",
    "\n",
    "#probe_probs = get_probe_probs(probe_state['params'],act)\n",
    "\n",
    "\n",
    "for t in range(x_mod['start_index'][maze_num],x_mod['end_index'][maze_num]):\n",
    "    print('\\ntime step {}'.format(t-x_mod['start_index'][maze_num]))\n",
    "    print('current input {}'.format(reverse_map[x_mod['data'][maze_num][t]]))\n",
    "    print('maze transformer prediction')\n",
    "\n",
    "  \n",
    "    for layer in range(13):\n",
    "        print(f'logit lense layer {layer}:')\n",
    "        print(sorted([(reverse_map[i],nn.softmax(logits[maze_num][t][layer])[i]) for i in range(len(reverse_map))],key=lambda x: -x[1])[:4])\n",
    "    print('target:')\n",
    "    print(reverse_map[x_mod['data'][maze_num][x_mod['end_index'][maze_num]-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846bdce-6b61-476d-9389-78d915ce0c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
