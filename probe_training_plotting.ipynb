{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6180d7-9c28-4589-b3b5-2aa691a5eebc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannes/miniforge3/envs/maze-goal/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import jax\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "from einops import einsum, rearrange, reduce\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from flax.training import orbax_utils\n",
    "\n",
    "import orbax.checkpoint\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "from maze_dataset.plotting import MazePlot\n",
    "from maze_dataset.tokenization.token_utils import strings_to_coords\n",
    "\n",
    "from dataset import CustomMazeDataset\n",
    "from dataset import NumpyLoader\n",
    "\n",
    "from model import TransformerLM, TransformerConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc55bc5-a3fd-4443-bc89-cad58ee6070e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# config details\n",
    "checkpoint_path = \"data/2023-10-31_16-24-46\"\n",
    "base_path = \"data\"\n",
    "save = True\n",
    "\n",
    "np_seed = 0\n",
    "jnp_seed = 0\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "n_train_steps = 10000000\n",
    "\n",
    "save_every_n_steps = 1000\n",
    "keep_n_checkpoints = 100\n",
    "\n",
    "n_worker = 8\n",
    "\n",
    "# n_eval = 1024\n",
    "emb_dim: int = 256\n",
    "num_heads: int = 16\n",
    "num_layers: int = 12\n",
    "qkv_dim: int = 256  # 512\n",
    "mlp_dim: int = 1024  # 2048\n",
    "max_len = 256\n",
    "\n",
    "grid_n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77bdb3e3-461f-4947-85d9-287fdaf98ac3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading step 953000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    params = state['params']\n",
    "    opt_state = state['opt_state']\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, batch)\n",
    "    updates, opt_state = tx.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    step = state['step'] + 1\n",
    "\n",
    "    return {'params': params, 'opt_state': opt_state, 'loss': loss, 'step': step}\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    params = state['params']\n",
    "    loss = loss_fn(params, batch)\n",
    "    return loss\n",
    "\n",
    "dataset = CustomMazeDataset(include_maze=False)\n",
    "train_loader = NumpyLoader(dataset, batch_size=batch_size, num_workers=n_worker)\n",
    "\n",
    "losses = []\n",
    "eval_losses = []\n",
    "\n",
    "key = random.PRNGKey(jnp_seed)\n",
    "rng, key = random.split(key)\n",
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    output_vocab_size=dataset.vocab_size,\n",
    "    max_len=max_len,\n",
    "    emb_dim=emb_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    qkv_dim=qkv_dim,\n",
    "    mlp_dim=mlp_dim\n",
    ")\n",
    "\n",
    "model = TransformerLM(config=config)\n",
    "\n",
    "\n",
    "def loss_fn(params, batch):\n",
    "    preds, act = model.apply(params, batch['data'])\n",
    "    preds = preds[:, 0:-1]\n",
    "    targets = batch['data'][:, 1:]\n",
    "    idx = jnp.arange(targets.shape[1])[None, :]\n",
    "    mask = jnp.where((idx < batch['end_index'][:, None]) & (idx >= batch['start_index'][:, None]), 1., 0.)\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=preds,\n",
    "        labels=targets\n",
    "    ) * mask\n",
    "\n",
    "    loss = loss.sum() / mask.sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "tx = optax.adamw(lr)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "params = model.init(rng, batch['data'])\n",
    "\n",
    "apply_fn = jax.jit(model.apply)\n",
    "\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "state = {'params': params, 'opt_state': opt_state, 'loss': 0., 'step': 0}\n",
    "\n",
    "# checkpoint management / loading model\n",
    "\n",
    "if save and not checkpoint_path:\n",
    "    # make new run dir ect\n",
    "\n",
    "    # Get the current date and time\n",
    "    current_datetime = datetime.datetime.now()\n",
    "\n",
    "    # Create a directory name with the date and unique ID\n",
    "    checkpoint_dir_name = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Create the full path for the checkpoint directory\n",
    "    checkpoint_path = os.path.join(base_path, checkpoint_dir_name)\n",
    "\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        # Create the directory\n",
    "        os.makedirs(checkpoint_path)\n",
    "        print(f\"Checkpoint directory created: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Checkpoint directory already exists: {checkpoint_path}\")\n",
    "\n",
    "if checkpoint_path:\n",
    "    orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "    options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=keep_n_checkpoints)\n",
    "    checkpoint_manager = orbax.checkpoint.CheckpointManager(checkpoint_path, orbax_checkpointer, options)\n",
    "\n",
    "    dummy_dict = {\n",
    "        'state': state,\n",
    "        'loss': np.array([0.])}\n",
    "\n",
    "\n",
    "    step = checkpoint_manager.latest_step()\n",
    "\n",
    "    if step:\n",
    "        print(f'loading step {step}')\n",
    "        load_dict = checkpoint_manager.restore(step, items=dummy_dict)\n",
    "        state = load_dict['state']\n",
    "        losses = list(load_dict['loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1347783-d28e-4513-aaec-826305d0d80f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reverse_map = {v: k for k, v in dataset.tokenizer.tokenizer_map.items()}\n",
    "vocab_map = dataset.tokenizer.tokenizer_map.get\n",
    "\n",
    "from dataset import find_from_right\n",
    "\n",
    "def ints_to_coords(arr):\n",
    "    # Map the integers in the list back to their corresponding tokens\n",
    "    tok_list = [reverse_map.get(i) for i in list(np.array(arr))]\n",
    "    coords = strings_to_coords(tok_list[:find_from_right(tok_list,'<PATH_END>')])\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_dict = {\n",
    "      11: 0,\n",
    "      27: 1,\n",
    "      29: 2,\n",
    "      35: 3\n",
    "}\n",
    "\n",
    "[(key,reverse_map[key]) for key in target_dict.keys()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.apply(state[\"params\"], batch[\"data\"][:2], intervention=lambda x, layer: x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(losses))\n",
    "\n",
    "plt.plot(losses)\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "plt.title('Maze transformer training')\n",
    "plt.xlabel('Training steps')\n",
    "plt.ylabel('Average per-token cross-entropy loss')\n",
    "plt.savefig('maze_transformer_training.pdf')\n",
    "\n",
    "print(np.mean(losses[-100:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def optimal_pred_loss_fn(batch):\n",
    "\n",
    "    path_len = batch['true_preds'].shape[1]\n",
    "    true_preds = batch['true_preds']\n",
    "    assert true_preds.shape == (batch_size, path_len, dataset.vocab_size)\n",
    "    \n",
    "    targets = batch['data']\n",
    "\n",
    "    pred_loss = 0\n",
    "    n = 0\n",
    "\n",
    "    last_step = np.zeros((1,dataset.vocab_size))\n",
    "    last_step[0,7] = 1.\n",
    "\n",
    "    for i, (sample, true_pred) in enumerate(zip(batch['data'],true_preds)):\n",
    "        target = sample[batch['start_index'][i]+1:batch['end_index'][i]+1]\n",
    "        pred = jnp.concatenate([true_pred[:1+batch['true_preds_end_index'][i]], last_step], axis=0)\n",
    "        #print(pred)\n",
    "        #print(target)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits = jnp.log(pred),\n",
    "        labels = target\n",
    "        )\n",
    "        pred_loss += loss.sum()\n",
    "        n+=target.shape[0]\n",
    "\n",
    "    return pred_loss/n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# to start with, get an estimate of optimal loss, and of loss of our model, for the no loops case\n",
    "\n",
    "dataset = CustomMazeDataset(include_maze=False,no_loops=True)\n",
    "train_loader_iter = iter(NumpyLoader(dataset, batch_size=batch_size, num_workers=n_worker))\n",
    "\n",
    "model_loss = []\n",
    "optimal_loss = []\n",
    "\n",
    "for n in range(10):\n",
    "    batch = next(train_loader_iter)\n",
    "    loss = optimal_pred_loss_fn(batch)\n",
    "    optimal_loss.append(loss)\n",
    "    loss = loss_fn(state['params'],batch)\n",
    "    model_loss.append(loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "categories = ['Optimal Loss', 'Model Loss']\n",
    "values = [np.mean(optimal_loss), np.mean(model_loss)]  # Example values for optimal loss and model loss\n",
    "errors = [np.std(optimal_loss)/np.sqrt(len(optimal_loss)), np.std(model_loss)/np.sqrt(len(model_loss))]  # Calculating standard errors\n",
    "\n",
    "# Creating a bar plot with error bars\n",
    "plt.bar(categories, values, yerr=errors, capsize=5)\n",
    "\n",
    "# Adding labels\n",
    "plt.ylabel('Average per-token cross-entropy loss')\n",
    "plt.title('Maze transformer vs. Bayes-optimal policy')\n",
    "\n",
    "# Display the plot\n",
    "\n",
    "plt.savefig('optimal_vs_model.pdf')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Chat GPT generated\n",
    "\n",
    "def is_valid_path(maze, path, start_pos):\n",
    "    \"\"\"\n",
    "    Checks if the given path is valid in the maze.\n",
    "    \n",
    "    Args:\n",
    "    maze (list[list[list[bool]]]): The maze as a 3D list where maze[z][y][x] is True if the cell at (x, y) is connected\n",
    "                                    to the cell at (x + 1, y) (if z == 0) or (x, y + 1) (if z == 1).\n",
    "    path (list[tuple[int, int]]): The path through the maze as a list of (x, y) coordinates.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the path is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    rows, cols = len(maze[0]), len(maze[0][0])\n",
    "\n",
    "    #print(maze)\n",
    "    #print(path)\n",
    "    #print(start_pos)\n",
    "\n",
    "    if not path[-1] in [(0,0),(0,4),(4,0),(4,4)]:\n",
    "        return False\n",
    "    if not path[0] == start_pos:\n",
    "        return False\n",
    "\n",
    "    for i in range(len(path) - 1):\n",
    "        x1, y1 = path[i]\n",
    "        x2, y2 = path[i + 1]\n",
    "        #print(f'looking at {x1,y1,x2,y2}')\n",
    "\n",
    "        # Check if the path goes out of maze bounds\n",
    "        if not (0 <= x1 < cols and 0 <= y1 < rows and 0 <= x2 < cols and 0 <= y2 < rows):\n",
    "            return False\n",
    "\n",
    "        # Check if the path is a valid move\n",
    "        if x1 + 1 == x2 and y1 == y2:  # Move down\n",
    "            if not maze[0][x1][y1]:\n",
    "                return False\n",
    "        elif x1 == x2 and y1 + 1 == y2:  # Move right\n",
    "            if not maze[1][x1][y1]:\n",
    "                return False\n",
    "        elif x1 == x2 + 1 and y1 == y2:  # Move up\n",
    "            if not maze[0][x2][y2]:\n",
    "                return False\n",
    "        elif x1 == x2 and y1 == y2 +1:  # Move left\n",
    "            if not maze[1][x2][y2]:\n",
    "                return False\n",
    "        else:  # Invalid move\n",
    "            return False\n",
    "\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Chat GPT generated\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs_shortest_path_length(maze, start, end):\n",
    "    \"\"\"\n",
    "    Finds the length of the shortest path in the maze using BFS.\n",
    "\n",
    "    Args:\n",
    "    maze (list[list[list[bool]]]): The maze as a 3D list.\n",
    "    start, end (tuple[int, int]): The starting and ending coordinates in the maze.\n",
    "\n",
    "    Returns:\n",
    "    int: The length of the shortest path. -1 if no path exists.\n",
    "    \"\"\"\n",
    "    rows, cols = len(maze[0]), len(maze[0][0])\n",
    "    queue = deque([(start, 0)])  # (position, distance)\n",
    "    visited = set([start])\n",
    "\n",
    "    while queue:\n",
    "        (x, y), dist = queue.popleft()\n",
    "\n",
    "        # Check if we've reached the end\n",
    "        if (x, y) == end:\n",
    "            return dist\n",
    "\n",
    "        # Check possible moves\n",
    "        # down\n",
    "        if x + 1 < cols and maze[0][x][y] and (x + 1, y) not in visited:\n",
    "            visited.add((x + 1, y))\n",
    "            queue.append(((x + 1, y), dist + 1))\n",
    "\n",
    "        # right\n",
    "        if y + 1 < rows and maze[1][x][y] and (x, y + 1) not in visited:\n",
    "            visited.add((x, y + 1))\n",
    "            queue.append(((x, y + 1), dist + 1))\n",
    "\n",
    "        # up\n",
    "        if x - 1 >= 0 and maze[0][x-1][y] and (x - 1, y) not in visited:\n",
    "            visited.add((x-1, y))\n",
    "            queue.append(((x-1, y), dist + 1))\n",
    "\n",
    "        # left\n",
    "        if y - 1 >= 0 and maze[1][x][y-1] and (x, y - 1) not in visited:\n",
    "            visited.add((x, y - 1))\n",
    "            queue.append(((x, y - 1), dist + 1))\n",
    "\n",
    "    return -1  # No path found\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = CustomMazeDataset(include_maze=True, no_loops=False)\n",
    "train_loader_iter = iter(NumpyLoader(dataset, batch_size=batch_size, num_workers=0))\n",
    "batch = next(train_loader_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# debug implementation\n",
    "\n",
    "plot = MazePlot(batch['maze'][0])\n",
    "plot.plot()\n",
    "plt.show()\n",
    "connection_list = batch['maze'][0].connection_list\n",
    "# check some paths\n",
    "print(connection_list)\n",
    "\n",
    "paths = [\n",
    "    [(3,0),(4,0)],\n",
    "    [(4,0)],\n",
    "    [(3,0),(3,1),(4,1),(4,0)],\n",
    "    [(3,0),(2,0),(1,0),(0,0)],\n",
    "    [(3,0),(2,0),(1,0),(2,0),(3,0),(4,0)],\n",
    "    [(0,0),(0,1),(1,1),(1,0),(0,0)]\n",
    "]\n",
    "\n",
    "for path in paths:\n",
    "\n",
    "    start_pos = (3,0)\n",
    "    print(path)\n",
    "    print(is_valid_path(connection_list, path, start_pos))\n",
    "    print(bfs_shortest_path_length(connection_list,path[0],path[-1])==len(path)-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_test_batches = 10\n",
    "\n",
    "# couldn't be bothered to implement batched inference, so this takes ages\n",
    "\n",
    "corner_stats = [0]*4\n",
    "path_valid = []\n",
    "path_shortest = []\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "for no in range(n_test_batches):\n",
    "    print(f' batch no {no}')\n",
    "    batch = next(train_loader_iter)\n",
    "    for maze_num in range(batch_size):\n",
    "        print(f'maze no {maze_num}')\n",
    "    \n",
    "        start_pos = reverse_map[batch['data'][maze_num,batch['start_index'][maze_num]+1]]\n",
    "        start_pos = (int(start_pos[1]), int(start_pos[3]))\n",
    "    \n",
    "        model_gen = []\n",
    "    \n",
    "        prompt = list(batch['data'][maze_num][:batch['start_index'][maze_num]+1])\n",
    "    \n",
    "        t=0\n",
    "    \n",
    "        for t in range(25):\n",
    "\n",
    "            # pad to avoid unnecessary jit compiles\n",
    "            input = prompt + model_gen\n",
    "\n",
    "            final_pos = len(input) - 1\n",
    "\n",
    "            input = input + [0]*(256-final_pos-1)\n",
    "            assert len(input)==256\n",
    "            \n",
    "            input = jnp.array(input)[None,:]\n",
    "            \n",
    "            pred, act = jax.jit(model.apply)(state['params'],input)\n",
    "            \n",
    "            rng, key = random.split(key)\n",
    "            sample = random.categorical(rng, pred[0][final_pos])\n",
    "    \n",
    "            model_gen.append(int(sample))\n",
    "    \n",
    "            if model_gen[-1]==vocab_map('<PATH_END>'):\n",
    "                break\n",
    "    \n",
    "        curr_maze = batch['maze'][maze_num]\n",
    "        path = [(int(s[1]),int(s[3])) for s in [reverse_map[k] for k in model_gen[:-1]]]\n",
    "        connection_list = curr_maze.connection_list\n",
    "    \n",
    "        is_valid = is_valid_path(connection_list, path, start_pos)\n",
    "        path_valid.append(is_valid)\n",
    "        if is_valid:\n",
    "    \n",
    "            shortest_len = bfs_shortest_path_length(connection_list, path[0], path[-1])\n",
    "            assert shortest_len <= len(path)-1 or not is_valid \n",
    "            is_shortest = shortest_len == len(path)-1\n",
    "\n",
    "            \n",
    "            corner_stats[target_dict[model_gen[-2]]]+=1\n",
    "        \n",
    "            path_shortest.append(is_shortest)\n",
    "\n",
    "            if not is_shortest:\n",
    "                print('not shortest!!')\n",
    "                # display maze\n",
    "                print(connection_list)\n",
    "                print(start_pos)\n",
    "                print(path)\n",
    "\n",
    "                plot = MazePlot(curr_maze)\n",
    "                path = ints_to_coords(model_gen)\n",
    "                plot.add_predicted_path(path)\n",
    "                plot.plot()\n",
    "            \n",
    "                print(maze_num)\n",
    "                plt.show()\n",
    "        else:\n",
    "\n",
    "            print('not valid!!')\n",
    "            # display maze\n",
    "            plot = MazePlot(curr_maze)\n",
    "            path = ints_to_coords(model_gen)\n",
    "            plot.add_predicted_path(path)\n",
    "            plot.plot()\n",
    "        \n",
    "            print(maze_num)\n",
    "            plt.show()\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('\\ncorner statistics:')\n",
    "print(corner_stats)\n",
    "print(f'total: {np.sum(corner_stats)}')\n",
    "print([(reverse_map[corner], p) for corner, p in zip(target_dict.keys(), np.array(corner_stats)/np.sum(corner_stats))])\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Given values\n",
    "p = [0.25, 0.25, 0.25, 0.25]  # Probabilities for each outcome\n",
    "n = np.sum(corner_stats)  # Total number of trials\n",
    "\n",
    "# Expected frequencies\n",
    "expected = [n * pi for pi in p]\n",
    "\n",
    "# Perform the chi-square test\n",
    "chi2_stat, p_value = chisquare(corner_stats, f_exp=expected)\n",
    "\n",
    "print(f'chi-square p value: {p_value}')\n",
    "\n",
    "valid_percent = np.mean(path_valid)\n",
    "print(f'\\nvalid path percentage: {valid_percent}')\n",
    "print(1280-np.sum(path_valid))\n",
    "\n",
    "\n",
    "shortest_percent = np.mean(path_shortest)\n",
    "print(f'\\nshortest path percentage: {shortest_percent}')\n",
    "print(1280-np.sum(path_shortest))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Probe setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    # input should be batch_size x n_layers (one probe per layer) x emb_dim\n",
    "    #y = nn.Dense(features=128)(x)\n",
    "    #y = nn.relu(y)\n",
    "    #y = nn.Dense(features=64)(x)\n",
    "    #y = nn.relu(y)\n",
    "    # treat the layers as 1 D spatial dimension, the model dim as channel dim\n",
    "    bs = x.shape[0]\n",
    "    #print(x.shape)\n",
    "    #W_in = self.param('W', nn.initializers.xavier_uniform(), (num_layers+1, emb_dim, emb_dim))\n",
    "    W_out = self.param('W', nn.initializers.xavier_uniform(), (num_layers+1, emb_dim, 4))\n",
    "    b = self.param('b', nn.initializers.zeros, (num_layers+1, 4))\n",
    "    assert x.shape == (bs, num_layers+1, emb_dim)\n",
    "    x = nn.LayerNorm(reduction_axes=2, feature_axes=(1,2))(x)\n",
    "    #y = einsum(W_in, x, 'l m h, bs l m -> bs l h')\n",
    "    #y = nn.relu(y)\n",
    "    y = einsum(W_out, x, 'l m o, bs l m -> bs l o')\n",
    "    return y + b\n",
    "\n",
    "#def concat_acts(act, first_layer, last_layer):\n",
    "#  assert len(act['stream'])>=last_layer\n",
    "#  stream = act['stream'][first_layer:last_layer+1]\n",
    "#  #concat along model dim (i.e. keep batch and sequence positions separate)\n",
    "# acts = jnp.concatenate(stream,axis=-1)\n",
    "#  #print(acts.shape)\n",
    "#  #acts = acts[:,:,:]\n",
    "#  #print(acts.shape)\n",
    "#  return acts\n",
    "\n",
    "def prepare_acts(act):\n",
    "    assert act['stream'][0].shape[0] == batch_size, act['stream'][0].shape[2] == emb_dim\n",
    "    seq_len = act['stream'][0].shape[1]\n",
    "    # note: input has dimensions n_layer x batch_size x sequence_len x emb_dim\n",
    "    \n",
    "    acts  = rearrange(act['stream'], 'layer bs seq dim -> (bs seq) layer dim')\n",
    "\n",
    "    \n",
    "    # return a tensor in which all seq positions are in one big batch, then layer, then model dim\n",
    "    assert len(acts.shape)==3\n",
    "    assert acts.shape == (batch_size * seq_len, num_layers+1, emb_dim)\n",
    "\n",
    "    return acts, seq_len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Linear probe training\n",
    "\n",
    "# Start a new experiment\n",
    "\n",
    "\n",
    "losses = []\n",
    "eval_losses = []\n",
    "\n",
    "#batch = next(iter(train_loader))\n",
    "\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "rng, key = random.split(key)\n",
    "\n",
    "\n",
    "#print(f'default loss: {jnp.log()}')\n",
    "\n",
    "pred, act = model.apply(state['params'], batch['data'])\n",
    "\n",
    "acts, seq_len = prepare_acts(act) # this is just to initialize the probe, so take first sequence position arbitrarily\n",
    "\n",
    "x = acts\n",
    "\n",
    "probe = LinearProbe()\n",
    "\n",
    "tx = optax.adamw(3e-4)\n",
    "\n",
    "params = probe.init(rng, x)\n",
    "\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "probe_state = {'params': params, 'opt_state': opt_state, 'loss': 0., 'step': 0}\n",
    "\n",
    "target_dict = {\n",
    "      11: 0,\n",
    "      27: 1,\n",
    "      29: 2,\n",
    "      35: 3\n",
    "  }\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(2,))\n",
    "def probe_loss_fn(params, batch, reduce_layers = 1):\n",
    "  pred, act = model.apply(state['params'], batch['data'])\n",
    "      \n",
    "  acts, seq_len = prepare_acts(act)\n",
    "  assert seq_len == batch['data'].shape[1]\n",
    "\n",
    "  targets = batch['data'][jnp.arange(0,pred.shape[0]),batch['end_index']-1]\n",
    "\n",
    "  #print(targets)\n",
    "\n",
    "  new_targets = targets.copy()\n",
    "\n",
    "  for k, v in target_dict.items():\n",
    "      new_targets = jnp.where(targets==k,v,new_targets)\n",
    "\n",
    "  #print(new_targets)\n",
    "  # expanding targets along seq len (result is shape \n",
    "  repeated_targets = jnp.repeat(new_targets, seq_len, axis = 0)\n",
    "  assert repeated_targets.shape == (seq_len * batch_size,)\n",
    "  repeated_targets = jnp.repeat(repeated_targets[:,None], num_layers+1, axis=1)\n",
    "  assert repeated_targets.shape == (seq_len * batch_size,num_layers+1)\n",
    "  \n",
    "\n",
    "  probe_pred = probe.apply(params, acts)\n",
    "\n",
    "  # probe preds are of shape (batch_size * seq_len, num_layer, 4)\n",
    "\n",
    "  assert repeated_targets.shape == probe_pred.shape[:-1]\n",
    "\n",
    "  assert len(repeated_targets.shape)==2\n",
    "\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "      logits = probe_pred,\n",
    "      labels = repeated_targets\n",
    "  )\n",
    "\n",
    "  # loss is of shape (batch_size * seq_len, num_layer). First, undo the reshaping.\n",
    "\n",
    "  loss = rearrange(loss, '(bs seq) layer -> bs seq layer', bs=batch_size, seq=seq_len, layer=num_layers+1)\n",
    "  \n",
    "  idx = jnp.arange(seq_len)[None, :]\n",
    "\n",
    "  mask = jnp.where((idx < batch['end_index'][:, None]) & (idx > batch['start_index'][:, None]), 1., 0.)\n",
    "\n",
    "  # need to broadcast mask over the last layer dimension\n",
    "  assert mask.shape == (batch_size,seq_len)\n",
    "  loss = loss * mask[:,:,None]\n",
    "\n",
    "  if reduce_layers:\n",
    "      # just add up losses over all the layers. that's fine.\n",
    "      loss = loss.sum() / mask.sum()\n",
    "  else:\n",
    "      loss = loss.sum(axis=(0,1))/mask.sum()\n",
    "\n",
    "  return loss\n",
    "\n",
    "@jax.jit\n",
    "def probe_train_step(probe_state,batch):\n",
    "  params = probe_state['params']\n",
    "  opt_state = probe_state['opt_state']\n",
    "  loss, grads = jax.value_and_grad(probe_loss_fn)(params,batch)\n",
    "  updates, opt_state = tx.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  step = probe_state['step'] + 1\n",
    "\n",
    "  return {'params': params, 'opt_state': opt_state, 'loss': loss, 'step': step}\n",
    "\n",
    "@jax.jit\n",
    "def probe_eval_step(state,batch):\n",
    "  params = state['params']\n",
    "  loss = probe_loss_fn(params,batch)\n",
    "  return loss\n",
    "\n",
    "probe_losses = []\n",
    "probe_eval_losses = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def optimal_loss_fn(batch):\n",
    "\n",
    "    path_len = batch['true_probs'].shape[1]\n",
    "    true_probs = batch['true_probs']\n",
    "    assert true_probs.shape == (batch_size, path_len, 4)\n",
    "    \n",
    "    targets = batch['data'][jnp.arange(0,batch['data'].shape[0]),batch['end_index']-1]\n",
    "    new_targets = targets.copy()\n",
    "    \n",
    "    for k, v in target_dict.items():\n",
    "      new_targets = jnp.where(targets==k,v,new_targets)\n",
    "\n",
    "    #print(new_targets)\n",
    "    # expanding targets along seq len (result is shape \n",
    "    repeated_targets = jnp.repeat(new_targets, path_len, axis = 0)\n",
    "    assert repeated_targets.shape == (path_len * batch_size,)\n",
    "\n",
    "    true_probs_reshaped = rearrange(true_probs, 'bs seq i -> (bs seq) i')\n",
    "    assert true_probs_reshaped.shape == (path_len * batch_size, 4)\n",
    "\n",
    "    log_probs = jnp.log(true_probs_reshaped)\n",
    "\n",
    "    #print(log_probs[:10])\n",
    "    #print(repeated_targets[:10])\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "          logits = log_probs,\n",
    "          labels = repeated_targets\n",
    "      )\n",
    "    #print(loss[:10])\n",
    "    \n",
    "    # loss is of shape (batch_size * seq_len). First, undo the reshaping.\n",
    "    \n",
    "    loss = rearrange(loss, '(bs seq) -> bs seq ', bs=batch_size, seq=path_len)\n",
    "    \n",
    "    idx = jnp.arange(path_len)[None, :]\n",
    "    \n",
    "    mask = jnp.where((idx <= batch['true_probs_end_index'][:, None]), 1., 0.)\n",
    "    \n",
    "    assert mask.shape == (batch_size,path_len)\n",
    "    loss = np.ma.array(loss, mask=1-mask)\n",
    "    #print(loss[:10])\n",
    "\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "probe_options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=keep_n_checkpoints)\n",
    "probe_checkpoint_manager = orbax.checkpoint.CheckpointManager(os.path.join(base_path,'probe_layernorm_bias'), orbax_checkpointer, probe_options)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PROBE LOADING\n",
    "\n",
    "\n",
    "dummy_dict = {\n",
    "            'probe_state': probe_state,\n",
    "            'probe_loss': np.zeros(1)}\n",
    "\n",
    "step = probe_checkpoint_manager.latest_step()\n",
    "print(f'loading step {step}')\n",
    "load_dict = probe_checkpoint_manager.restore(step, items=dummy_dict)\n",
    "probe_state = load_dict['probe_state']\n",
    "probe_losses = list(load_dict['probe_loss'])\n",
    "print(np.mean(probe_losses[-100:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Probe training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = CustomMazeDataset(include_maze=False, no_loops=False)\n",
    "train_loader = NumpyLoader(dataset, batch_size=batch_size, num_workers=n_worker)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loop_time = time.time()\n",
    "\n",
    "for n, batch in enumerate(train_loader):\n",
    "    # do the actual training\n",
    "\n",
    "    probe_state = probe_train_step(probe_state, batch)\n",
    "    \n",
    "    probe_losses.append(probe_state['loss'])\n",
    "    \n",
    "    print(f'step: {probe_state[\"step\"]}')\n",
    "    print('loss: {}'.format(probe_state['loss']))\n",
    "    old_time = loop_time\n",
    "    loop_time = time.time()\n",
    "    print('steps per second: {:.5f}'.format(1/(loop_time - old_time)))\n",
    "\n",
    "    \n",
    "\n",
    "    if n % 100 == 0:\n",
    "        print('\\n\\n----- saving and eval -----')\n",
    "        #optimal_loss = optimal_loss_fn(batch)\n",
    "        #print(f'optimal_loss: {optimal_loss}')\n",
    "        layer_losses = probe_loss_fn(probe_state['params'],batch,reduce_layers=False)\n",
    "        layer, best_layer_loss = layer_losses.argmin(), layer_losses.min()\n",
    "        print(f'best layer loss: {best_layer_loss}, layer: {layer}')\n",
    "        probe_save_step = probe_state['step']\n",
    "        print(f'saving at step {probe_save_step}')\n",
    "        probe_save_dict = {'probe_state': probe_state,\n",
    "                     'probe_loss': np.array(probe_losses)\n",
    "                     }\n",
    "        probe_save_args = orbax_utils.save_args_from_target(probe_save_dict)\n",
    "        probe_checkpoint_manager.save(probe_save_step, probe_save_dict, save_kwargs={'save_args': probe_save_args})\n",
    "        print('----------\\n\\n')\n",
    "    \n",
    "\n",
    "# note that completely oblivious loss is 1.3862944 for 4 options (since dataset is uniformly random)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PROBE SAVING\n",
    "probe_save_step = probe_state['step']\n",
    "print(f'saving at step {probe_save_step}')\n",
    "probe_save_dict = {'probe_state': probe_state,\n",
    "             'probe_loss': np.array(probe_losses)\n",
    "             }\n",
    "probe_save_args = orbax_utils.save_args_from_target(probe_save_dict)\n",
    "probe_checkpoint_manager.save(probe_save_step, probe_save_dict, save_kwargs={'save_args': probe_save_args})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings = jnp.array(state['params']['params']['decoder']['Embed_0']['embedding'])\n",
    "\n",
    "def get_baseline_features(data):\n",
    "    # Baseline features:\n",
    "    # for simplicity, we just concatenate the embeddings of the past two tokens.\n",
    "    data = jnp.array(data)\n",
    "    features = embeddings[data]\n",
    "\n",
    "    two_step_features = jnp.concatenate([features[:,:-1,:],features[:,1:,:]],axis=-1)\n",
    "    seq_len = two_step_features.shape[1]\n",
    "    out = rearrange(two_step_features, 'bs seq feat -> (bs seq) feat')\n",
    "\n",
    "    return out, seq_len\n",
    "    \n",
    "\n",
    "n_baseline_probe_layers = 1\n",
    "n_hidden = emb_dim\n",
    "\n",
    "class BaselineProbe(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for layer in range(n_baseline_probe_layers):\n",
    "            x = nn.Dense(n_hidden)(x)\n",
    "            x = nn.relu(x)\n",
    "        out = nn.Dense(4)(x)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Start a new experiment\n",
    "\n",
    "losses = []\n",
    "eval_losses = []\n",
    "\n",
    "#batch = next(iter(train_loader))\n",
    "\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "rng, key = random.split(key)\n",
    "\n",
    "baseline_probe = BaselineProbe()\n",
    "\n",
    "x, seq_len = get_baseline_features(batch['data'])\n",
    "\n",
    "tx = optax.adamw(1e-4)\n",
    "\n",
    "params = baseline_probe.init(rng, x)\n",
    "\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "baseline_state = {'params': params, 'opt_state': opt_state, 'loss': 0., 'step': 0}\n",
    "\n",
    "@jax.jit\n",
    "def baseline_loss_fn(params, batch):\n",
    "  inputs, seq_len = get_baseline_features(batch['data'])\n",
    "      \n",
    "  assert seq_len == batch['data'].shape[1] - 1\n",
    "\n",
    "  targets = batch['data'][jnp.arange(0,batch_size),batch['end_index']-1]\n",
    "\n",
    "  new_targets = targets.copy()\n",
    "\n",
    "  for k, v in target_dict.items():\n",
    "      new_targets = jnp.where(targets==k,v,new_targets)\n",
    "\n",
    "  #print(new_targets)\n",
    "  # expanding targets along seq len (result is shape \n",
    "  repeated_targets = jnp.repeat(new_targets, seq_len, axis = 0)\n",
    "  assert repeated_targets.shape == (seq_len * batch_size,)\n",
    "\n",
    "  probe_pred = baseline_probe.apply(params, inputs)\n",
    "\n",
    "  assert repeated_targets.shape == probe_pred.shape[:-1]\n",
    "\n",
    "  assert len(repeated_targets.shape)==1\n",
    "\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "      logits = probe_pred,\n",
    "      labels = repeated_targets\n",
    "  )\n",
    "\n",
    "  # loss is of shape (batch_size * seq_len, num_layer). First, undo the reshaping.\n",
    "\n",
    "  loss = rearrange(loss, '(bs seq) -> bs seq', bs=batch_size, seq=seq_len)\n",
    "  \n",
    "  idx = jnp.arange(seq_len)[None, :]\n",
    "\n",
    "  mask = jnp.where((idx < batch['end_index'][:, None]-1) & (idx > batch['start_index'][:, None]-1), 1., 0.)\n",
    "\n",
    "  # need to broadcast mask over the last layer dimension\n",
    "  assert mask.shape == (batch_size,seq_len)\n",
    "  loss = loss * mask\n",
    "  loss = loss.sum()/mask.sum()\n",
    "\n",
    "  return loss\n",
    "\n",
    "@jax.jit\n",
    "def baseline_train_step(baseline_state,batch):\n",
    "  params = baseline_state['params']\n",
    "  opt_state = baseline_state['opt_state']\n",
    "  loss, grads = jax.value_and_grad(baseline_loss_fn)(params,batch)\n",
    "  updates, opt_state = tx.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  step = baseline_state['step'] + 1\n",
    "\n",
    "  return {'params': params, 'opt_state': opt_state, 'loss': loss, 'step': step}\n",
    "\n",
    "baseline_losses = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "baseline_options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=1000)\n",
    "baseline_checkpoint_manager = orbax.checkpoint.CheckpointManager(os.path.join(base_path,'baseline_probe_1hidden_loops'), orbax_checkpointer, baseline_options)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# BASELINE PROBE LOADING\n",
    "dummy_dict = {\n",
    "            'baseline_state': baseline_state,\n",
    "            'baseline_loss': np.zeros(1)}\n",
    "\n",
    "step = baseline_checkpoint_manager.latest_step()\n",
    "print(f'loading step {step}')\n",
    "load_dict = baseline_checkpoint_manager.restore(step, items=dummy_dict)\n",
    "baseline_state = load_dict['baseline_state']\n",
    "baseline_losses = list(load_dict['baseline_loss'])\n",
    "print(np.mean(baseline_losses[-100:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Probe analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(probe_losses))\n",
    "print(np.mean(probe_losses[-100:]))\n",
    "\n",
    "#plt.plot(old_probe_losses)\n",
    "plt.plot(probe_losses)\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.title('Probe training (losses summed over all layers)')\n",
    "plt.xlabel('Training steps')\n",
    "plt.ylabel('Average cross-entropy loss')\n",
    "plt.savefig('probe_training.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def optimal_loss_fn(batch):\n",
    "\n",
    "    path_len = batch['true_probs'].shape[1]\n",
    "    true_probs = batch['true_probs']\n",
    "    assert true_probs.shape == (batch_size, path_len, 4)\n",
    "    \n",
    "    targets = batch['data'][jnp.arange(0,batch['data'].shape[0]),batch['end_index']-1]\n",
    "    new_targets = targets.copy()\n",
    "    \n",
    "    for k, v in target_dict.items():\n",
    "      new_targets = jnp.where(targets==k,v,new_targets)\n",
    "\n",
    "    #print(new_targets)\n",
    "    # expanding targets along seq len (result is shape \n",
    "    repeated_targets = jnp.repeat(new_targets, path_len, axis = 0)\n",
    "    assert repeated_targets.shape == (path_len * batch_size,)\n",
    "\n",
    "    true_probs_reshaped = rearrange(true_probs, 'bs seq i -> (bs seq) i')\n",
    "    assert true_probs_reshaped.shape == (path_len * batch_size, 4)\n",
    "\n",
    "    log_probs = jnp.log(true_probs_reshaped)\n",
    "\n",
    "    #print(log_probs[:10])\n",
    "    #print(repeated_targets[:10])\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "          logits = log_probs,\n",
    "          labels = repeated_targets\n",
    "      )\n",
    "    #print(loss[:10])\n",
    "    \n",
    "    # loss is of shape (batch_size * seq_len). First, undo the reshaping.\n",
    "    \n",
    "    loss = rearrange(loss, '(bs seq) -> bs seq ', bs=batch_size, seq=path_len)\n",
    "    \n",
    "    idx = jnp.arange(path_len)[None, :]\n",
    "    \n",
    "    mask = jnp.where((idx <= batch['true_probs_end_index'][:, None]), 1., 0.)\n",
    "    \n",
    "    assert mask.shape == (batch_size,path_len)\n",
    "    loss = np.ma.array(loss, mask=1-mask)\n",
    "    #print(loss[:10])\n",
    "\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now get an estimate of optimal loss, and probe losses for all layers, and baseline \n",
    "\n",
    "dataset = CustomMazeDataset(include_maze=False,no_loops=True)\n",
    "train_loader_iter = iter(NumpyLoader(dataset, batch_size=batch_size, num_workers=n_worker))\n",
    "\n",
    "probe_loss = {i: [] for i in range(13)}\n",
    "optimal_loss = []\n",
    "baseline_loss = []\n",
    "\n",
    "for n in range(10):\n",
    "    batch = next(train_loader_iter)\n",
    "    loss = optimal_loss_fn(batch)\n",
    "    optimal_loss.append(loss)\n",
    "    loss = probe_loss_fn(probe_state['params'],batch, reduce_layers=False)\n",
    "    for i in range(13):\n",
    "        probe_loss[i].append(loss[i])\n",
    "    loss = baseline_loss_fn(baseline_state['params'], batch)\n",
    "    baseline_loss.append(loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate mean values for the horizontal lines\n",
    "optimal_loss_value = np.mean(optimal_loss)\n",
    "baseline_loss_value = np.mean(baseline_loss)\n",
    "\n",
    "# Sample data for the bar plot\n",
    "categories = [f'{i}' for i in range(13)]\n",
    "values = [np.mean(probe_loss[i]) for i in range(13)] \n",
    "\n",
    "# Adding the horizontal lines with labels\n",
    "plt.axhline(optimal_loss_value, color='green', linestyle='--', label='Optimal Loss')\n",
    "plt.axhline(baseline_loss_value, color='red', linestyle='--', label='Baseline Loss')\n",
    "\n",
    "# Creating a bar plot\n",
    "plt.bar(categories, values)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.ylabel('Average Cross-Entropy Loss')\n",
    "plt.title('Losses for Probes on Different Layers')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot and save the figure\n",
    "\n",
    "plt.savefig('probes_different_layers_baseline.pdf')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}