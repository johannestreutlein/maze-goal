{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6180d7-9c28-4589-b3b5-2aa691a5eebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 16:49:26.486827: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-31 16:49:26.541937: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-31 16:49:27.305035: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/johannes/mambaforge/envs/maze-goal/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import jax\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "from einops import einsum, rearrange\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from flax.training import orbax_utils\n",
    "\n",
    "import orbax.checkpoint\n",
    "\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "from maze_dataset.plotting import MazePlot\n",
    "from maze_dataset.tokenization.token_utils import strings_to_coords\n",
    "\n",
    "from dataset import CustomMazeDataset\n",
    "from dataset import NumpyLoader\n",
    "\n",
    "from model import TransformerLM, TransformerConfig\n",
    "\n",
    "# config details\n",
    "checkpoint_path = \"data/2023-10-31_16-24-46\"\n",
    "base_path = \"data\"\n",
    "save = True\n",
    "\n",
    "np_seed = 0\n",
    "jnp_seed = 0\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "n_train_steps = 10000000\n",
    "\n",
    "save_every_n_steps = 1000\n",
    "keep_n_checkpoints = 100\n",
    "\n",
    "n_worker = 8\n",
    "\n",
    "# n_eval = 1024\n",
    "emb_dim: int = 256\n",
    "num_heads: int = 16\n",
    "num_layers: int = 12\n",
    "qkv_dim: int = 256  # 512\n",
    "mlp_dim: int = 1024  # 2048\n",
    "max_len = 256\n",
    "\n",
    "grid_n = 5\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    params = state['params']\n",
    "    opt_state = state['opt_state']\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, batch)\n",
    "    updates, opt_state = tx.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    step = state['step'] + 1\n",
    "\n",
    "    return {'params': params, 'opt_state': opt_state, 'loss': loss, 'step': step}\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    params = state['params']\n",
    "    loss = loss_fn(params, batch)\n",
    "    return loss\n",
    "\n",
    "dataset = CustomMazeDataset(include_maze=False)\n",
    "train_loader = NumpyLoader(dataset, batch_size=batch_size, num_workers=n_worker)\n",
    "\n",
    "losses = []\n",
    "eval_losses = []\n",
    "\n",
    "key = random.PRNGKey(jnp_seed)\n",
    "rng, key = random.split(key)\n",
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    output_vocab_size=dataset.vocab_size,\n",
    "    max_len=max_len,\n",
    "    emb_dim=emb_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    qkv_dim=qkv_dim,\n",
    "    mlp_dim=mlp_dim\n",
    ")\n",
    "\n",
    "model = TransformerLM(config=config)\n",
    "\n",
    "def loss_fn(params, batch):\n",
    "    preds, act = model.apply(params, batch['data'])\n",
    "    preds = preds[:, 0:-1]\n",
    "    targets = batch['data'][:, 1:]\n",
    "    idx = jnp.arange(targets.shape[1])[None, :]\n",
    "    mask = jnp.where((idx <= batch['end_index'][:, None]) & (idx >= batch['start_index'][:, None]), 1., 0.)\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=preds,\n",
    "        labels=targets\n",
    "    ) * mask\n",
    "\n",
    "    loss = loss.sum() / mask.sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "tx = optax.adamw(lr)\n",
    "\n",
    "x = next(iter(train_loader))\n",
    "params = model.init(rng, x['data'])\n",
    "\n",
    "apply_fn = jax.jit(model.apply)\n",
    "\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "state = {'params': params, 'opt_state': opt_state, 'loss': 0., 'step': 0}\n",
    "\n",
    "# checkpoint management / loading model\n",
    "\n",
    "if save and not checkpoint_path:\n",
    "    # make new run dir ect\n",
    "\n",
    "    # Get the current date and time\n",
    "    current_datetime = datetime.datetime.now()\n",
    "\n",
    "    # Create a directory name with the date and unique ID\n",
    "    checkpoint_dir_name = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Create the full path for the checkpoint directory\n",
    "    checkpoint_path = os.path.join(base_path, checkpoint_dir_name)\n",
    "\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        # Create the directory\n",
    "        os.makedirs(checkpoint_path)\n",
    "        print(f\"Checkpoint directory created: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Checkpoint directory already exists: {checkpoint_path}\")\n",
    "\n",
    "if checkpoint_path:\n",
    "    orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "    options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=keep_n_checkpoints)\n",
    "    checkpoint_manager = orbax.checkpoint.CheckpointManager(checkpoint_path, orbax_checkpointer, options)\n",
    "\n",
    "    dummy_dict = {\n",
    "        'state': state}\n",
    "\n",
    "\n",
    "    step = checkpoint_manager.latest_step()\n",
    "\n",
    "    if step:\n",
    "        print(f'loading step {step}')\n",
    "        load_dict = checkpoint_manager.restore(step, items=dummy_dict)\n",
    "        state = load_dict['state']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d622e-54be-4d98-9bf9-601eedce6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(state[\"params\"], x[|\"data\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
